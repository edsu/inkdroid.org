---
layout: post
title: Selling the Van
tags:
- llm
- artificial-intelligence
- capitalism
---

I don't know about you, but this presentation [Tech Promised Everything. Did it deliver?](https://www.youtube.com/watch?v=dVG8W-0p6vg&t=32s) by [Scott Hanselman](https://www.hanselman.com/) seemed to pop up all over the place for me recently, in podcasts and social media posts. At some point I initially watched about 1/2 of it, and stopped because although it seemed kind of sweet I couldn't understand what the fuss was about. Then later after seeing it mentioned in another context I finished watching it. What follows will only make sense if you have seen it. If you haven't seen it already I recommend you just close this tab in your browser and move on with your life.

The central conceit of the talk seemed to be that his family sold their (highly valued) van in order to buy a [Commodore 64](https://en.wikipedia.org/wiki/Commodore_64) based on the belief held by Hanselman, his parents, and his fifth grade teacher Mrs Hill, that computing technology was a "gift" that was going to "change everything" for the better. As the title suggests, his presentation is asking whether it turned out that way or not.

After talking about the exponential growth of computing in the following decades he goes on to describe how the use of Large Language Models (LLM) in schools has resulted in course material being written with LLMs, which are then completed by students with LLMs, and graded with LLMs, with neither teacher or student engaging in a collaborative and creative learning process. He suggests that this *is not* realizing the promise of computer technology.

I agree, as I imagine you do too. I guess it was actually kind of brave for Hanselman (a Microsoft employee, who essentially own OpenAI who created ChatGPT) to get on a stage in front of a large audience and compare generative AI technology, or Large Language Models, to "computer [Mad Libs](https://en.wikipedia.org/wiki/Mad_Libs)", and that not all is well with big-tech. If you've been following the news and discourse around AI and LLMs at all then this certainly will not be news to you. However, hearing it from Microsoft VP might seem a bit ... unusual?

Hanselman goes on to propose that a better use of the technology would be for students to use generative AI to chat with Albert Einstein, Maya Angelou, or Nelson Mandela. That the technology should allow us to do things that "were not possible before" while maintaining "connection, convenience and creativity". That we should all learn how generative AI works, so we can be more thoughtful about how we integrate it into our lives.

But is it not already possible for students to understand Albert Einstein, Maya Angelou and Nelson Mandela by reading **what they actually wrote**, and **what they actually said**? We have books, and recordings, and all kinds of information for teachers and students to engage with, which we can source, cite and share, preserve and make accessible. How is chatting with some massively inefficient technology running in a centralized data center, that will invent things that they might have said, better than what we have now? The only thing that LLMs do seem to be better at currently is routing around the copyright laws that prevent these works from being widely available online.

I suspect that the reason why this talk resonated so much in tech circles is that it is an *[apology](https://en.wikipedia.org/wiki/Apologia)* for generative AI, that recognizes the so called "AI bubble" that is threatening to burst, while at the same time reinscribing, and reauthorizing, the use of generative AI in society. It presents as a meaningful critique, but is instead a piece of kitsch commentary that is meant to appeal to a very specific demographic (people like me) who grew up with these computers, and are grappling with their complicit participation in a sociotechnical enterprise that has gone well off the rails.

This is Big Tech consoling us, that it has a moral compass. Or more generously, it is one person in Big Tech, trying to recover (on stage) what it felt like to have a moral compass.

One truly startling moment in the talk was when Hanselman showed his smart phone, smart ring, smart watch, and an insulin sensor on his arm, to point out that it represents more computation than existed on the planet in 1980. I'm not sure if this analogy holds, but in broad strokes it does seem significant right?

But for me this analogy invites some questions. Who has these devices in their pocket, on the wrist, on their finger, and stuck in their arm? How many resources needed to be extracted for this to happen? Who benefited from the extraction, manufacture, and dissemination of this technology? Is all this compute power *actually* needed to do things we couldn't do before? How much centralization of compute (especially by generative AI) in resource draining data centers run by multinational corporations has occurred to allow these personal devices to proliferate? Perhaps that will be the topic of part two of the talk? I'm not holding my breath.

I also am not holding my breath for the "AI Bubble" to burst. I think it will take more than changes in stock market valuations to right this ship that drifted off course well before generative AI. Big Tech is a symptom and not a cure.
