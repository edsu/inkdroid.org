---
layout: post
title: Rules for Robots
tags:
- llm
- bots
- crawling
---

<figure>

<a href="https://commons.wikimedia.org/wiki/File:Smilodon_and_Canis_dirus.jpg">
  <img class="img-fluid" src="/images/tarpit.jpg">
</a>

<figcaption>
  A fight in the La Brea Tar Pits (from <a href="https://commons.wikimedia.org/wiki/File:Smilodon_and_Canis_dirus.jpg">Wikipedia</a>)
</figcaption>

</figure>

With the bot swarms descending to [ransack] websites for every scrap of text they can get in the attempt to improve their ever-larger-language-models we've started to see the emergence tools to trap them in an endless maze of text. The explicit goal of these tools is to make the bots waste time and energy making their models worse.

Some examples of include Cloudflare's [AI Labyrinth] service and the [Nepenthes], [Babble], [Gabble] tools. These tools take different approaches to generating the maze of hyperlinked trash. Sometimes the site can be pre-generated, as is the case with AI Labyrinth, to save on the energy usage. Or it can be generated on demand, like Babble and Gabble, using [resource efficient compiled languages] (Rust and Go respectively). Nepenthes uses Lua, which I believe is more efficient than other interpreted language like Python or Ruby, but maybe not as much as a well written program in a compiled language. Perhaps that's why Babble and Gabble got written? At any rate, no matter how these mazes are generated, the sites are often called [tarpits] because of how the bots get snarled up in them.

One interesting dimension to this is what to put in the tarpit's `robots.txt` or whether to even have one. The [Robots Exclusion Protocol](https://en.wikipedia.org/wiki/Robots.txt) is a simple text file you put at the root of your website. When crawling a website, a well behaved robot will read it to determine what parts of the website the website owner wants to be crawled, and by whom (which bots).

Part of the problem is that ill-behaved robots, like the ones aggressively crawling websites (using botnets [running off of infected phones], etc) have zero interest in looking at a `robots.txt`, let alone parsing it in order to determine what to crawl. They are programmed to take anything and everything they can find.

So if they aren't going to look at it why does it matter what you put in a tarpit's `robots.txt`?

**If you care about website owners being able to decide how they want their content to be crawled and used it's really important that the `robots.txt` tells bots to *go away*.**

Part of the reason here is that there are well-behaved bots, that read the `robots.txt`, to generate search indexes, archives of web content, which are generally useful. In the interests of a healthy web it's a good idea to warn them away from the tarpit.

But the most important reason for this is that, given enough tarpits, it will be in the interests of bot creators to modify their software to read and respect the `robots.txt` so they don't get stuck in a maze of hypertrash. This feels like a subtle bit game theory, which I was [reminded recently] by Ivan Topolsky who pointed out that that it was an approach taken [decades ago] to try to trap bots scraping the web for email addresses.

```text
User-agent: *
Disallow: /
```

NOTE: the slash there is *really* important because if it's not there it says the opposite--that *everyone* is allowed in.

I guess if you are one of those people that have a problem with plagiarism machines more generally, even ones that *do* respect `robots.txt`, you may want to explicitly allow those in. This would be kind of like the inverse of the [ai.robots.txt] list.

Who knows whether it will work or not, but I thought it was interesting that rules can be important to have even when they aren't being followed by everyone.

[ransack]: https://diff.wikimedia.org/2025/04/01/how-crawlers-impact-the-operations-of-the-wikimedia-projects/
[AI Labyrinth]: https://blog.cloudflare.com/ai-labyrinth/
[Nepenthes]: https://zadzmo.org/code/nepenthes/
[Babble]: https://git.jsbarretto.com/zesterer/babble
[Gabble]: https://codeberg.org/reed/gabble
[running off of infected phones]: https://jan.wildeboer.net/2025/02/Blocking-Stealthy-Botnets/
[resource efficient compiled languages]: https://greenlab.di.uminho.pt/wp-content/uploads/2017/10/sleFinal.pdf
[reminded recently]: https://mstdn.science/@dryak/114541881600634649
[tarpits]: https://arstechnica.com/tech-policy/2025/01/ai-haters-build-tarpits-to-trap-and-trick-ai-scrapers-that-ignore-robots-txt/
[decades ago]: https://www.virusbulletin.com/virusbulletin/2007/09/fighting-spam-using-tar-pits/
[me]: https://inkdroid.org/robots.txt
[ai.robots.txt]: https://github.com/ai-robots-txt/ai.robots.txt#readme
