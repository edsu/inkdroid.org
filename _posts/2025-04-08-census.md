---
layout: post
title: Archiving the US Census
tags:
- government
- web-archives 
---

*TL;DR Just because someone says they've archived something from the web doesn't mean it isn't worth checking and possibly archiving again. We need better tools and methods for doing this type of appraisal work.*

---

As the Trump administration's destruction of the federal government has spilled out into the removal of web pages and entire websites we have seen the emergence, and re-emergence, of several efforts to try to collect imperiled government web pages. There has been a lot of valuable coverage of this work in the mainstream media, and I won't pretend to do it justice by trying to summarize it here.

However, one constant theme is the pivotal role that the [End of Term Web Archive] plays. Since 2008 a group of institutions centered on the Internet Archive have archived US government websites at the end of presidential administrations. It's hard to overstate how important this work is, as the federal government has shifted since the late 1990s to web publishing, instead of distributing physical media (tapes, CD-ROMs, paper documents of various kinds) as part of the [Federal Depository Library Program].

Occasionally people will mention the work of the End of Term Web Archive to allay fears that government websites are at risk, and that the work of protecting this data is done and dusted, as it were. "This is already being taken care of." But the reality is that the .gov web is so large that it's quite difficult to say how much of it has been reliably archived.

<figure>
  <img class="img-fluid" src="/images/census-ftp.png">
  <figcaption>Logging in to ftp.census.gov with ncftp</figcaption>
</figure>

As concern about the state of federal information on the web spread at my workplace some became particularly interested in the status of the US Census, specifically the venerable Census FTP site (ftp.census.gov), which is also made available on the web at [https://www2.census.gov](https://www2.census.gov). There are worries about whether the data will continue to be made available, whether the data will continue to be collected in the way that it has, and if the already published data will be revised after the fact for political ends.

As an experiment, a colleague of mine ([Andrew Berger]) got interested in how feasible it was to download the entirety of the Census FTP site over his home network connection using [lftp]. He started it up and it ran for 4 weeks and collected 5.9 Terabytes of data. As he started and restarted the collection he noticed some files disappearing and others appearing. This isn't entirely surprising since parts of the FTP site seem to resemble more a collection of random documents on somebody's desktop than a well organized archive. Be that as it may, other parts of the site are very well organized with documentation and clear path hierarchies.

While discussing this work Andrew expressed an interest in knowing how much of the FTP data might be available in the [Internet Archive's Wayback Machine], which is where the End of Term Web Archive data ultimately lands.

It was straightforward to turn the file system into a list of URLs since the paths map directly to their location on the web. What was as bit more tricky was efficiently looking these URLs up in the Wayback Machine. The Internet Archive do have [an API] for looking up a given URL and seeing what snapshots of it are available. But it can take multiple seconds to look up a URL, and given that there were 4,496,871 of them, a best case scenario is that could take over 52 days of constant API requests to check them all.

The checking is complicated by the fact that the Wayback Machine's API will return snapshot records for pages that it failed to retrieve: HTTP redirects (3XX) and HTTP Errors (4XX). In spot checking a few of the results it was clear that archiving web crawlers sometimes received 403 Forbidden error when crawling. For example take a look at the snapshot for [https://www2.census.gov/geo/tiger/TIGER2024/ADDRFN/tl_2024_49039_addrfn.zip](https://web.archive.org/web/20241216035850/https://www2.census.gov/geo/tiger/TIGER2024/ADDRFN/tl_2024_49039_addrfn.zip#expand).

Perhaps the server flagged the crawler as a malicious bot requesting too many resources in too short a time? It's kind of difficult to say. Nevertheless, in order to ascertain how well these Census files have been archived it's important to ignore these false positives, and only count snapshots that resulted in 200 OK HTTP responses.

<figure>
  <img class="img-fluid" src="/images/census-403.png">
  <figcaption>A view of <a href="https://www2.census.gov/geo/tiger/TIGER2024/ADDRFN/tl_2024_49039_addrfn.zip">this ZIP file</a> in the Wayback Machine.</figcaption>
</figure>

This process of deciding what to accession into an archive is known in archival practice as [appraisal]. It's not uncommon to use statistical sampling when appraising archival collections [@Cook:1991; @Kolish:1994]. However sampling is usually done because there is only space to store a representative sample of an entire set of records. In this case the sampling is helpful for determining whether a given set of records is in need of archiving, based on whether the records have already been archived elsewhere.

According to [this calculator], if I want 95% confidence with 5% margin of error, I can randomly sample 385 URLs out of the 4,496,871 and test only those. That's a lot more manageable to do. A sample like this obviously doesn't provide an exhaustive list of everything at ftp.census.gov that is in need of archiving. But it can give a sense of the coverage in the Wayback Machine, which can help guide decision making around whether to archive this dataset.

So what did I find? If you want to see the details check out this [Jupyter notebook].

Basically the results suggest with 95% confidence, and a 5% margin of error, that only 46% of the Census FTP URLs have a snapshot in the Wayback Machine. I was kind of surprised by this result, so I ran 5 other samples and found they were all in the 5% margin of error.

Of the 4,496,871 files there is actually a subset that is of particular interest to researchers at work:

* `programs-surveys/acs/summary_file/` - [American Community Survey Summary File](https://www.census.gov/programs-surveys/acs/data/summary-file.html)
* `programs-surveys/decennial/` - [Decennial Census of Population and Housing](https://www.census.gov/programs-surveys/decennial-census.html)
* `geo/tiger/TIGER2024/` - [TIGER/Line Shapefiles 2024](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html)
* `geo/tiger/TIGER2023/` - [TIGER Line Shapefiles 2023](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html)
* `geo/tiger/TIGER2022/` - [TIGER Line Shapefiles 2022](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html)

These files only account for 12% of the total files on the Census FTP site. So I thought it was possible that lack of coverage in other directories, that contain many more files, could be skewing the results for these high value datasets.

To account for this I sampled each subdirectory individually, tested and gathered results:

* `programs-surveys/acs/summary_file/`: 22.8% archived
* `programs-surveys/decennial/`: 62.6%  archived
* `geo/tiger/TIGER2024/`: 39.5% archived
* `geo/tiger/TIGER2023/`: 86.0% archived
* `geo/tiger/TIGER2022/`: 99.2% archived

Clearly there's quite a bit of variability here. So what does this mean for deciding whether to archive this data? One way of interpreting the results is that:

1. The End of Term / Internet Archive work has not been able to collect *all* the files made available in the US Census FTP site. This casts some doubt on the coverage of less prominent, harder to crawl, federal government websites.
2. There is value in collecting and accessioning this data into an institutional repository, especially if members of our community place a high value on being able to use it.
3. It always helps to work with domain experts who understand the web content be crawled: what the content is useful for, what is required to use it, how often it is updated, etc. Understanding the mechanics of acquiring web content is necessary but not sufficient for web archiving practice.

Basically, we shouldn't take it for granted that these datasets have been archived and will remain available in their current form. That being said, nobody at my place of work has indicated that we are in fact going to archive this data. This post was just part of some work to help inform that decision making. But as we archive the web it's important to be able to determine how well archived sites are already, and this exploration was just scratching the surface of that need.

I've made the sample datasets generated by the notebook available as CSV files if you want to examine some the hits and misses:

* [Full Sample CSV](https://media.githubusercontent.com/media/edsu/notebooks/refs/heads/main/data/census-sample.csv)
* [ACS Summary File CSV](https://media.githubusercontent.com/media/edsu/notebooks/refs/heads/main/data/census-summary-file.csv)
* [Decennial Survey CSV](https://github.com/edsu/notebooks/raw/refs/heads/main/data/census-decennial.csv)
* [TIGER 2024 CSV](https://github.com/edsu/notebooks/raw/refs/heads/main/data/census-tiger2024.csv)
* [TIGER 2023 CSV](https://github.com/edsu/notebooks/raw/refs/heads/main/data/census-tiger2023.csv)
* [TIGER 2022 CSV](https://github.com/edsu/notebooks/raw/refs/heads/main/data/census-tiger2022.csv)

### References

[Jupyter notebook]: https://github.com/edsu/notebooks/blob/main/CensusFTP.ipynb
[End of Term Web Archive]: https://eotarchive.org/about/
[Federal Depository Library Program]: https://en.wikipedia.org/wiki/Federal_Depository_Library_Program
[lftp]: https://lftp.yar.ru/
[Internet Archive's Wayback Machine]: https://archive.org/web/
[an API]: https://archive.org/developers/wayback-cdx-server.html
[this calculator]: https://www.calculator.net/sample-size-calculator.html?type=1&cl=95&ci=5&pp=50&ps=4496871&x=Calculate
[Andrew Berger]: https://profiles.stanford.edu/andrew-berger
[appraisal]: https://dictionary.archivists.org/entry/appraisal.html
