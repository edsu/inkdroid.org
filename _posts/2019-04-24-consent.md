---
title: Consent
layout: post
tags:
- ethics
- social media
---

In the [second phase] of the [Documenting the Now] project we are continuing to support and develop [tools] that embody ethical practices for social media archiving. This work is being informed by a series of [workshops] with activist communities, as well an effort to bring social media archiving practices [into the college classroom]. The structure of the project reflects an understanding that was developed early on in Phase 1 of the project, that one of the biggest challenges that social media archiving practice faces is how to meaningfully work with issues of *consent*.

Simply because a user has quickly agreed to a social media platform's *Terms of Service* does not mean that they have agreed to have their content collected and preserved for the long term in an archive. Consider the case of the not fully realized Library of Congress Twitter archive, in which users had no opportunity to opt-in or opt-out of having their tweets archived for perpetuity. This archive, and an increased anxiety about the archival nature of social media, gave rise to a variety of services like [Tweet Delete], [Tweet Wipe] and [Tweet Deleter] that allow you to routinely or fully purge your Twitter history. I think it's productive to look at these services as a mirror of [records management] processes in more traditional archives, where appraisal and retention schedules determine what records are placed into an archive, how long particular records are to be held.

I also think it's interesting to speculate for a moment how the Library of Congress Twitter partnership could have been significantly altered if Twitter had added a checkbox to the user settings page to indicate whether to archive a user's posts and media at the Library of Congress. What should the default setting have been? How could it have been explained? You may think that this is no longer an issue for the Library of Congress since they have stopped archiving all public tweets [@LC:2017]:

> As the twelfth year of Twitter draws to a close, the Library has 
> decided to change its collection strategy for receipt of tweets on 
> December 31, 2017. After this time, the Library will continue to 
> acquire tweets but will do so on a very selective basis under the 
> overall guidance provided in the Libraryâ€™s Collections Policy 
> Statements and associated documents. Generally, the tweets collected 
> and archived will be thematic and event-based, including events 
> such as elections, or themes of ongoing national interest, e.g. 
> public policy.

But even when *selectively* archiving content from social media, doesn't consent still play a significant role? If the content is created by public figures, such as elected officials, a strong case can be made that consent isn't so much of an issue. After all, these are individuals that are familiar with being in the limelight, and often have teams of people that help them manage it. But this will not be the case for all people posting with an election hashtag. Furthermore, if only public figures' tweets are to be made part of the historical record, doesn't that privilege particular voices in society? Doesn't that privileging shortchange the democratizing and transformational potential of having a social media archive in the first place?

These issues of consent were brought into stark relief for us, on a much smaller scale, as we collected [28,560,078 tweets] surrounding the protests in Ferguson after the murder of Michael Brown, and the subsequent decision not to indict Darren Wilson, the police officer that killed him. We were [interested] in how these tweets from people on the ground mobilized media awareness of the events in Ferguson, and documented the subsequent state response. We were interested in how these individual and collective voices formed an essential part of the historical record, and as documentation for achieving social justice. Our work was operating out of a concern that if archivists aren't able to collect this material when needed, then these voices and their actions would be lost to history, and forgotten.

However, as we learned, we were not the only actors who were interested in collecting this material. Law enforcement, and the [technology] [companies] that serve them, were actively collecting and analyzing Ferguson tweets too. Some of them contacted us directly asking for access to the tweets we had collected. These corporate and government actors were also interested in the voices of the Ferguson community, and the many people that traveled there to participate in the protests, but for purposes of political and social control--not social justice and history. They wanted to be able to use this data in order to *predict* when uprisings occur, in order to foreclose on them, and to identify, track and incarcerate the activists involved.

This should come as no surprise to archival practitioners and scholars who understand that archives are always instruments and sites for the exercise of power. Whether they are managed by an archivist at a large state institution, or a policy expert working at a social media company, or by a volunteer in a small community archive, or an individual scholar as they do field work onsite, record keeping practices both enact power, and hold power accountable [@Ketelaar:2002].

Archival practices can reinforce and calcify dominant cultural forms, or augment, subvert, destabilize and overcome them by offering *counterstories* that surface submerged records, and present multiple narratives and truths [@Dunbar:2006]. The key question for us then became, *whose* power is our Ferguson archive serving? As Dunbar writes, Critical Race Theory (CRT) provides a lens for examining what medial forms are permitted in an archive, and stresses the importance, and degree to which record creators can participate in the creation of archives:

> CRT techniques of evidential rectifying could be useful to archival
> discourse in terms of broadening notions of what constitutes a record,
> the role of human subjects documented as co-creators of the record,
> and assumptions about archives and archivists as neutral third parties
> in the preservation and use of the record and other forms of
> historical evidence. (p. 114)

This analysis, and archival approaches to social justice more generally [@Punzalan:2016 ; @Flinn:2011], informed our own work, but we were left with the conundrum of how to enact these principles while working with millions of records from hundreds of thousands of people.

What does consent mean at that scale? Is consent, as it is conventionally framed, even the right conceptual tool to center in our application designs? The expedient answer to these questions is to say consent can't be done at scale and either a) ignore social media archiving, or b) say the ethics of consent don't apply to "public" tweets, and to collect it all. And yet few Twitter users are aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent [@Fiesler:2018]. So we chose instead to build on some of the privacy ensuring features in our tools by informing our design work by reviewing the archival and human computer interaction (HCI) research literature around consent and social media data collection, to see what the state of the art was. Below are a few things we've found so far.

### ToS & EULA

The general consensus in the literature, which many of us know from direct experience, is that Terms of Service (ToS), End User License Agreements (EULA), and the *notice-and-consent* model as currently construed, are ineffective for managing users expectations of how their data will be used. We all have the experience of signing up for a new service and being presented with a long document, which we must agree to in order to continue. @McDonald:2008 estimate that it would take approximately 201 hours per year for an individual (reading at 250 words per minute) to read all the Terms of Service documents they encounter in a year. In terms of the number of people online in the United States, and the average income, this expenditure in time would amount to $781 billion dollars per year. Most of us cannot make this kind of time commitment, and so the ToS documents go unread. 

The purpose of these ToS documents is to reduce power asymmetries between corporate and government entities on the one side and individual actors on the other. Legal theories around privacy and consent assume an informed, rational and autonomous data subject. However, in a digital environment these assumptions have led to some rather pernicious side effects where individuals are inundated with increasingly opaque requests for consent. As noted by @Schermer:2014, increased regulation around consent, such as the General Data Protection Regulation (GDPR), can perversely lead to *Consent Transaction Overload* where users are desensitized to the importance of consent. Paternalistic approaches to consent can result in users blindly accepting cookie notifications, and clicking through consent transactions in order to get on with whatever task they are performing [@Bohme:2010]. 

Even if users were able to spend the time required to read ToS documents, it is unlikely that they would understand the full ramifications of their choices. @Acquisit:2005 found that 41% of individuals with high privacy concerns admit that they don't read privacy policies. Individuals generally lack sufficient information to make decisions about their privacy and often trade off their long-term privacy for short term benefits. As Schermer indicates, it is increasingly unlikely that we will be able to comprehend the processual flows of data bound up in the computer systems that EULA's describe:

> as data processing becomes more and more complex, more factors need to 
> be taken into account. The result is that the reality of data processing
> will become even further removed from the simplistic mental models 
> employed by data subjects.

Internet protocols allow data to be easily copied, and instantly transported at great distance, which can yield extremely complex, forking data flows, that can lead to a loss of control of information as it moves between contexts. @Nissenbaum:2011 refers to this as the *Transparency Paradox* where the full picture of these data flows cannot be understood, let alone read, while on the other hand, summarizing practices remove details that could ultimately be quite important.

To further complicate matters, acts of consent happen at a particular point in time, like when signing up to use an online service or platform. Afterwards giving consent your data participates in information systems that then adapt and change as business models are adjusted or pivoted, or utterly transformed, as is the case when businesses go bankrupt or are acquired. @Custers:2016 notes that a click should not confer consent forever, and that perhaps EULA's need expiration dates that require re-consenting, particularly at significant junctures. 

Finally our ethical frameworks, such as the [Belmont Principles], and Institutional Review Boards (IRB) that protect the privacy of research subjects are being strained to the breaking point by online research. Overly simplistic notions of what constitutes public and private on the web can lead to situations such as when researchers at Aarhus University released a dataset of personal profiles collected from the OKCupid dating website. This data was released as a public dataset, was not anonymized, and included information such as geographic location and sexual preference, which put many individuals at risk [@Zimmer:2016]. However many IRB processes will allow such research to take place because the data is "public". Despite this example, @Vitak:2016 have found that in fact there are emerging ethical research practices, which suggest three areas to extend frameworks like the Belmont Principles:

1. Transparency with participants.
2. Ethical deliberations with colleagues.
3. Caution with sharing with results.

Admittedly, there are often disciplinary pressures that limit transparency with participants, as is the case when researchers are concerned with potential [Observer Effect]. Never the less, it is generally good practice to inform participants when their data is being collected, even when it is perceived to be public and on the web. The challenge is how to do it, especially when bulk data collection is at play. While these suggestions speak specifically to the research community they also suggest productive ways that archives can transform their own practice, and operate as trusted places where this research can happen. In what ways can our data collection methodologies and tools, such as the ones we are building in the Documenting the Now project, embody these emerging principles?

Two other important themes that emerge from the HCI literature around privacy are that 1) privacy is *context* dependent, and 2) online spaces themselves provide a context that can shape user's perceptions and decisions. Nissenbaum's idea of [Contextual Integrity] has been hugely influential because of her insight that privacy is a function of appropriate flows of data, that are determined by contextual norms. For example your expectation that your doctor may share your health records with another doctor to assist in treatment, but will not sell it to a marketing company for profit @Nissenbaum:2011. These norms of behavior when they happen online are not generalized or monolithic, but are as varied as the privacy contexts we find ourselves in everyday:

> Not only is life online integrated into social life, and hence not
> productively conceived as a discrete context, it is *radically heterogeneous*,
> comprising multiple social contexts, not just one, and certainly is not just
> a commercial context where protecting privacy amounts to protecting 
> *consumer* privacy and commercial information ... the contexts in which 
> activities are grounded shape expectations that, when unmet, cause
> anxiety, fright, and resistance.

The central question for Nissenbaum then is identifying the relevant contextual norms for a given application domain, and then using that understanding to craft the appropriate flows of data. @Hutton:2015 have found that contextual norms can actually be put to use in calibrating the consent transactions that take place online. They distinguish between *secured consent*, where consent is granted once, and *sustained consent*, where consent is continually requested of an individual, in order to highlight the ineffectiveness of both approaches with respect to the *burden* they place on the individual, and the *validity* of the consent that is granted. These two factors, burden and validity, are linked together: attempts at attaining greater validity require increased burden on the consentor (consent transactions)...but lowering this burden, also lowers the validity, making the consent granted increasingly meaningless. Instead, they suggest a middle ground of *contextual integrity consent* be used, which reduces the number of consent requests that are made based on the how well the consentor's previous answers fit in with established norms. The difficulty here is determining what these norms are, and the degree to which they are being shaped by the online environments themselves, and people operating within them.

Indeed, the design of online environments, their interfaces, actions, affordances, and protocols all work to fashion experience of privacy online. In a recent extensive review of the design literature around privacy @Acquisti:2017 catalog the ways in which behavioral economy and experimental psychology are being used to delimit and influence privacy decisions taken online using a technique called *nudges*.

> Every design choice is a nudge.

Whether they are aware of it or not, the designers and implementors of online systems present and highlight particular options. The extensive use of [A/B testing] allows social media platforms like Google and Facebook to constantly evolve their platforms to maximize desired outcomes using small nudges. However Acquisiti et. al. document a large set of nudge types that can be used to influence user behavior around privacy issues. These include:

**Nudging with Information**: providing 

**Nudging with Presentation:** use of icons (social humans).

**Nudging with Defaults:**

**Nudging with Incentives:**

**Facilitating Reversibility:**

**Timing of Nudges:**



- Hawthorne effect: observer effect - does this matter for Humanities work? (Galinas)
- https://www.darkpatterns.org/

### Solutions 

- consent is transformative (ethics chapter, philosophy)
- better design of information (Schaub, 2015) from Acquisti:2017 p. 44:14
- gap between what individuals want and ToS documents isn't unsurmountable.  most criteria that are important to users can be found in most privacy policies [@Custers:2014].
- design ideas! Schaub:2015 ; every design choice is a nudge, Acquisti:2017 ; A/B testing treating us like lab rats
- p3p semweb Cranor:2002, now machine learning w/ archive of ToS Wilson:2016 and Wilson:2016a

- burden / validity tension
- Contextual Integrity: between secured consent and sustained consent (Hutton, 2015). balancing 
- normative aspect to contextual integrity: what are norms, how are they measured
- Human Data Interaction: legibility, agency, negotiability (Hutton, 2017)

- Making ToS more readable. (McDonald)

### Archival Solutions

- archives are obsessed with context
- time 
- third party privacy & contextual integrity (Bingo, 2011)
- trust (Metzger, 2006)
- participatory archives
- oral history ethics?

### App Design

### References

[second phase]: https://news.docnow.io/documenting-the-now-phase-2-83d76a9ee0a8
[tools]: https://github.com/docnow/
[Documenting the Now]: https://www.docnow.io
[28,560,078 tweets]: https://archive.org/details/ferguson-tweet-ids
[interested]: https://medium.com/on-archivy/documenting-the-now-ferguson-in-the-archives-adcdbe1d5788
[workshops]: https://twitter.com/documentnow/status/1110555760948199425
[into the college classroom]: https://news.virginia.edu/content/black-twitter-101-what-it-where-did-it-originate-where-it-headed
[Tweet Delete]: https://tweetdelete.net/
[Tweet Wipe]: https://twitwipe.com/
[Tweet Deleter]: https://tweetdeleter.com/
[technology]: https://www.baltimoresun.com/news/maryland/investigations/bs-md-geofeedia-police-20160902-story.html
[companies]: https://www.typeinvestigations.org/investigation/2018/02/27/palantir-secretly-use-new-orleans-test-predictive-policing
[records management]: https://en.wikipedia.org/wiki/Records_management
[Contextual Integrity]: https://en.wikipedia.org/wiki/Contextual_Integrity
[Observer Effect]: https://en.wikipedia.org/wiki/Hawthorne_effect
[A/B Testing]: https://en.wikipedia.org/wiki/A/B_testing

