---
layout: post
title: Black Boxes
tag:
- algorithms
- machine-learning
---

One of the primary reasons I've resisted using machine learning models in my professional work is that they always appear to me in the guise of a [black box](https://en.wikipedia.org/wiki/Black_box).

A black box is a metaphor used to talk about some computational process which has known inputs and outputs, but whose internal workings are not known. It's an idea that goes back to the beginnings of cybernetics and modern computing.

Computer professionals are trained to distinguish between the [interpretability and explainability](https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html) of machine learning models. But at the end of the day our ability to understand and adequately communicate how and why these models work are [active areas of research](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence).

Of course, our ability to understand is constrained because the details of some models are guarded as business secrets. But more and more we are seeing documentation efforts increase transparency about how "open source" models are built. We've seen significant efforts to [regulate](https://en.wikipedia.org/wiki/Artificial_Intelligence_Act) how they are used and by who. But it's important to recognize that even these "Glass Boxes" don't explain *why* some of the models work.

And that's the *glass half full* version.

The *glass half empty* version is that *we do understand* how these models work, if you define "we" as *someone*, or some set of people, somewhere. The past isn't evenly distributed too.

For a given model *someone* knows what software was used to build it--because someone installed it right? Someone made choices about what algorithms and data structures were used. Someone knows what data was used to train it because they collected it and fed it in. Someone knows what data was not fed in, and (maybe) how that biases the model. Someone knows how much money was spent training and using the model because the electricity bill got paid.

But these systems are very complex that these *someones* are almost certainly not the same people. They may not work in the same organizations. They may not be willing, or even able, to communicate these things to you, even if you could find them.

This is why Nick Seaver says that algorithmic systems are in fact *culture*, which require ethnographic methods to understand [@Seaver:2017]. 

I got my start as a professional programmer when the World Wide Web was being built in the mid-1990s. I didn't have a computer science degree, but I was fortunate to be able to lean on the modest programming experience I got in high school (BASIC, Pascal and Fortran) to learn a new language [Perl](https://en.wikipedia.org/wiki/Perl) and apply it. I didn't become a .com millionaire, but it helped pay the rent. I felt lucky, and in truth I was. I was already a participant in a network of privilege. 

Perl had lots of features for working with text, which leant itself to be useful in web applications. It had an [open source](https://en.wikipedia.org/wiki/Open_source) culture that encouraged the sharing of "modules" on [CPAN](https://en.wikipedia.org/wiki/CPAN) that extended the language to do things like talk to databases, or work with specific data formats, or thousands of other things. I loved the attention to detail in the [Camel Book](https://en.wikipedia.org/wiki/Programming_Perl) about how the language was built and evolved. I knew that the [real programmers](https://xkcd.com/378/) had written the Perl interpreter in C. I didn't really know C that well, but I knew other people did, and understood that (in theory) I could too. It was all a [simple matter of programming](https://en.wikipedia.org/wiki/Small_matter_of_programming), as they say.

But the reality was, I didn't know the details of some of the open source Perl modules I installed and used. I *trusted* that the authors did. I didn't know how some of the Perl language primitives were coded in C, but I learned from experience using them, while in conversation with other programmers, and knew that someone else somewhere did understand. I participated in the culture of programming in Perl.

The scary thing for me about generative AI is that I don't have any meaningful trust relationships with the people who know parts of the answers to why/how it works. At a fundamental level I don't know if it is even theoretically possible to know. Not only that, I have an active *mistrust* of some of the organizations that are trying to convince professionals that this is The New Way, because they stand to gain so much, while workers and consumers stand to gain so little, and the environment stands to lose so much more.

But, if I'm being generous, perhaps it appears that way because it's not my culture?

It's important to recognize that all of us experience some aspect of computing as a black box. Whether it is using a model from HuggingFace, an ATM, an app on our phone, or ChatGPT. I think the important question to ask is how do we participate in its the creation and use of this technology?

Can we open the black box? Can we open the other black boxes we find inside? Do we know other people who can? What do they say? What is the culture of this black box? Do I want to be a part of it? Why? Why not? What choices do I have? What about others?

I no longer program in Perl, but I still marvel at how [Larry Wall](https://en.wikipedia.org/wiki/Larry_Wall) practiced computing as a culture.

## References
