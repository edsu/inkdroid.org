---
layout: post
status: publish
published: true
title: The Web as a Preservation Medium
author:
  display_name: ed
  login: ed
  email: ehs@pobox.com
  url: http://www.inkdroid.org
author_login: ed
author_email: ehs@pobox.com
author_url: http://www.inkdroid.org
wordpress_id: 6738
wordpress_url: http://inkdroid.org/journal/?p=6738
date: '2013-11-26 17:59:46 +0000'
date_gmt: '2013-11-27 00:59:46 +0000'
tags:
- google
- web-archives
- facebook
- twitter
- internet-archive
comments:
- id: 86676
  author: google.com/accounts/o8&hellip;
  author_email: azaroth42@gmail.com
  author_url: https://www.google.com/accounts/o8/id?id=AItOawmHT2lGj3VrhMg1rx3kD7yImR_BtvfcZc0
  date: '2013-11-27 14:28:08 +0000'
  date_gmt: '2013-11-27 21:28:08 +0000'
  content: |
    A little more information about the Memento work (<a href="http://www.mementoweb.org/" rel="nofollow">http://www.mementoweb.org/</a> as Ed didn't link to it) ...
    The issue referred to is a classic distributed search problem to which there are many possible solutions, from parallel searching to centralized federation and all of the hybrids in between.  There's also very strong caching possibilities, as it's very rare that archived copies go away once published.
    The prototype referred to was intended exclusively for members of the IIPC.  The live Memento Aggregators use various techniques, but they do <i>not</i> use the data provided by IIPC partner institutions.   Mostly these are aggressive caching and pre-fetching based on common requests, and quickest-response on-demand with the other archives being filled in and cached for future requests.
    Hope that clarifies a bit,
    -- Rob Sanderson
- id: 86677
  author: The Dramatic Creation Story of HTML5
  author_email: ''
  author_url: http://www.magiclogix.com/blog/the-dramatic-creation-story-of-html5/
  date: '2013-11-27 15:56:11 +0000'
  date_gmt: '2013-11-27 22:56:11 +0000'
  content: |
    [&#8230;] The Web as a Preservation Medium [&#8230;]
- id: 86679
  author: Your item is my story; my story is your item &#8211; talkingtothecan
  author_email: ''
  author_url: http://talkingtothecan.com/your-item-my-story/
  date: '2013-11-28 13:42:50 +0000'
  date_gmt: '2013-11-28 20:42:50 +0000'
  content: |
    [&#8230;] small data and collections needing to be &#8216;regions of stability&#8217; in his keynote talk, The web as a preservation medium. And more on that from Michael Lascarides in his talk and this pertinent [&#8230;]
- id: 86683
  author: 'Editor&#8217;s Choice: The Web as a Preservation Medium | Digital Humanities
    Now'
  author_email: ''
  author_url: http://digitalhumanitiesnow.org/2013/12/editors-choice-the-web-as-a-preservation-medium/
  date: '2013-12-03 10:01:13 +0000'
  date_gmt: '2013-12-03 17:01:13 +0000'
  content: |
    [&#8230;] By Ed Summers | December 3, 2013   Next year it will be 25 years since Tim Berners-Lee wrote his proposal to build the World Wide Web. I’ve spent almost half of my life working with the technology of the Web. The Web has been good to me. I imagine it has been good to you as well. I highly doubt I would be standing here talking to you today if it wasn’t for the Web. Perhaps the National Digital Forum would not exist, if it was not for the Web. Sometimes I wonder if we need the Web to continue to survive as a species. It’s certainly hard for my kids to imagine a world without the Web. In a way it’s even hard for me to remember it. This is the way of media, slipping into the very fabric of experience. Today I’d like to talk to you about what it means to think about the Web as a preservation medium.  Read Full Post Here. [&#8230;]
- id: 86685
  author: 'Happenings in the Web Archiving World | The Signal: Digital Preservation'
  author_email: ''
  author_url: http://blogs.loc.gov/digitalpreservation/2013/12/happenings-in-the-web-archives-world/
  date: '2013-12-04 11:11:45 +0000'
  date_gmt: '2013-12-04 18:11:45 +0000'
  content: |
    [&#8230;] Library of Congress gave the keynote address at the National Digital Forum in New Zealand titled The Web as  Preservation Medium. Ed is a software developer and offers a great perspective into some technical aspects of [&#8230;]
- id: 86687
  author: 6 Digital Historiography and Strategy Grad Seminars I&#8217;d Love to Teach
    | Trevor Owens
  author_email: ''
  author_url: http://www.trevorowens.org/2013/12/6-digital-historiography-and-strategy-grad-seminars-id-love-to-teach/
  date: '2013-12-06 10:20:30 +0000'
  date_gmt: '2013-12-06 17:20:30 +0000'
  content: |
    [&#8230;] Digital Strategy for Cultural Heritage Organizations:  Digital is increasingly becoming a key part of nearly every function of cultural heritage organizations (Libraries, Archives, Museums etc.). We are increasingly acquiring, preserving and exhibiting born-digital and digitized materials, using social media for outreach and public relations, supporting researchers and fielding reference questions through digital channels, and supporting all of that work with a substantive IT infrastructure.  Looking across each of these areas, this course would focus on exploring ideas for how organizations should be structured, about the role of software development should play, embedding &#8220;digital into the design, decision making, strategy and all the operations&#8221; of cultural heritage orgs and the role that the web should play as a platform and organizing principle for orgs. [&#8230;]
- id: 86689
  author: Archiviare il web | Bicycle Mind
  author_email: ''
  author_url: http://bicyclemind.it/2013/12/08/archiviare-il-web/
  date: '2013-12-08 08:17:51 +0000'
  date_gmt: '2013-12-08 15:17:51 +0000'
  content: |
    [&#8230;] Archiviare il web [&#8230;]
- id: 86698
  author: The Dramatic Creation Story of HTML5 | MagiclogixBlog
  author_email: ''
  author_url: http://www.magiclogixstaging.com/tooblerbeta/blog/?p=4442
  date: '2013-12-16 02:54:27 +0000'
  date_gmt: '2013-12-16 09:54:27 +0000'
  content: |
    [&#8230;] The Web as a Preservation Medium [&#8230;]
- id: 86749
  author: 'Weekly web archiving roundup: January 8, 2014 | Web Archiving Roundtable'
  author_email: ''
  author_url: http://webarchivingrt.wordpress.com/2014/01/08/weekly-web-archives-roundup-january-8-2014/
  date: '2014-01-08 14:10:00 +0000'
  date_gmt: '2014-01-08 21:10:00 +0000'
  content: |
    [&#8230;] Discussion about issues surrounding web-based content: http://inkdroid.org/journal/2013/11/26/the-web-as-a-preservation-medium/ [&#8230;]
- id: 86930
  author: Archiving the Internet | z657 Digital Humanities Spring 2014
  author_email: ''
  author_url: http://biblicon.org/14z657/?p=619
  date: '2014-04-13 17:28:00 +0000'
  date_gmt: '2014-04-14 00:28:00 +0000'
  content: |
    [&#8230;] important part of the web that most people might not be aware of. Ed Summers&#8217;s article &#8220;The Web as a Preservation Medium&#8221; provides a good introduction to the many forms of Internet archiving going on today, and I [&#8230;]
- id: 86981
  author: Thoughts on Archival Practice | 0000005 | enjangada
  author_email: ''
  author_url: http://enjangada.wordpress.com/2014/06/18/thoughts-on-archival-practice-0000005/
  date: '2014-06-18 16:03:06 +0000'
  date_gmt: '2014-06-18 23:03:06 +0000'
  content: |
    [&#8230;] Read Ed Summers’ talk here (Web as a Preservation Medium): http://inkdroid.org/journal/2013/11/26/the-web-as-a-preservation-medium/. [&#8230;]
- id: 86984
  author: Thoughts on Archival Practice | 0000006 | enjangada
  author_email: ''
  author_url: http://enjangada.wordpress.com/2014/07/08/thoughts-on-archival-practice-0000006/
  date: '2014-07-08 05:06:23 +0000'
  date_gmt: '2014-07-08 12:06:23 +0000'
  content: |
    [&#8230;] to their institutional web presence. My colleague, Ed Summers at the Library of Congress, recently spoke to the National Digital Forum in Wellington, New Zealand and stressed that “if you are not [&#8230;]
- id: 87207
  author: What is still on the web after 10 years of archiving? par Andy Jackson (Web
    Archiving Technical Lead, The British Library) | Web90 &#8211; Patrimoine, Mémoires
    et Histoire du Web dans les années 1990
  author_email: ''
  author_url: http://web90.hypotheses.org/557
  date: '2014-11-16 13:07:51 +0000'
  date_gmt: '2014-11-16 20:07:51 +0000'
  content: "[&#8230;] long since gone? If those URLs are still working, is the content
    the same as it was? How has our archival sliver of the web [&#8230;]"
---

<p><em>This is the text of a talk I gave at the (wonderful) <a href="http://www.ndf.org.nz/">National Digital Forum</a> in Wellington, New Zealand on November 27th, 2013. You can also find my slides <a href="http://edsu.github.io/webpresmed/">here</a>, and the video <a href="https://www.youtube.com/watch?v=HpJgX8a9d3I">here</a>. If you do happen to watch the video, you'll probably notice I spent more time thinking about the text than I did practicing my talk.</em></p>
<hr />
<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/ragesoss/3835494997/"><img width="250" src="http://inkdroid.org/images/webpresmed/aaron.jpg" /></a>
</div>
<p>Hi there. Thanks for inviting me to NDF 2013, it is a real treat and honor to be here. I'd like to dedicate this talk to <a href="https://en.wikipedia.org/wiki/Aaron_Swartz">Aaron Swartz</a>. Aaron cared deeply about the Web. In a heartbreaking way I think he may have cared more than he was able to. I'm not going to talk much about Aaron specifically, but his work and spirit underly pretty much everything I'm going to talk about today. If there is one message that I would like you to get from my talk today it's that we need to work together as professionals to care for the Web in the same way Aaron cared for it.</p>
<p>Next year it will be 25 years since Tim Berners-Lee wrote his <a href="http://www.w3.org/History/1989/proposal.html">proposal</a> to build the World Wide Web. I've spent almost half of my life working with the technology of the Web. The Web has been good to me. I imagine it has been good to you as well. I highly doubt I would be standing here talking to you today if it wasn't for the Web. Perhaps the National Digital Forum would not exist, if it was not for the Web. Sometimes I wonder if we need the Web to continue to survive as a species. It's certainly hard for my kids to imagine a world without the Web. In a way it's even hard for me to remember it. This is the way of media, slipping into the very fabric of experience. Today I'd like to talk to you about what it means to think about the Web as a <em>preservation medium</em>.</p>
<div style="float:right; margin-left: 10px;">
  <a href="https://en.wikipedia.org/wiki/File:Marshall_McLuhan_holding_a_mirror.jpg"> <img width="250" src="http://inkdroid.org/images/webpresmed/mcluhan.jpg" /> </a>
</div>
<p>Medium and preservation are some pretty fuzzy, heavy words, and I'm not going to try to pin them down too much. We know from Marshall McLuhan that the medium is the message. I like this definition because it disorients more than it defines. McLuhan reminds us of how we are shaped by our media, just as we shape new forms of media. In her book <a href="http://www.amazon.com/Always-Already-New-History-Culture/dp/0262572478">Always Already New</a>, <a href="http://steinhardt.nyu.edu/faculty_bios/view/Lisa_Gitelman">Lisa Gitelman</a> offers up a definition of media that gives us a bit more to chew on:</p>
<blockquote>
<p>I define media as socially realized structures of communication, where structures include both technological forms and their associated protocols, and where communication is a cultural practice, a ritualized collocation of different people on the same mental map, sharing or engaged with popular ontologies of representation.</p>
</blockquote>
<p>I like Gitelman's definition because it emphasizes how important the social dimension is to our understanding of media. The affordances of media, how media are used by people to do things, and how media does things to us, are just as important as the technical qualities of media. In the spirit of Latour she casts media as a fully fledged actor, not as some innocent bystander or tool to be used by the real and only actors, namely people.</p>
<p>When Matthew Oliver wrote to invite me to speak here he said that in recent years NDF had focused on the museum, and that there was some revival of interest in libraries. The spread of the Web has unified the cultural heritage sector, showing how much libraries, archives and museums have in common, despite their use of subtly different words to describe what they do. I think preservation is a similar unifying concept. We all share an interest in keeping the stuff (paintings, sculptures, books, manuscripts, etc) around for another day, so that someone will be able to see it, use it, cite it, re-interpret it.</p>
<p>Unlike the traditional media we care for, the Web confounds us all equally. We've traditionally thought of preservation and access as different activities, that often were at odds with each other. <a href="https://twitter.com/mkirschenbaum">Matthew Kirschenbaum</a> dispels this notion:</p>
<blockquote>
<p>... the preservation of digital objects is logically inseparable from the act of their creation -- the lag between creation and preservation collapses completely, since a digital object may only ever be said to be preserved if it is accessible, and each individual access creates the object anew. <a href="http://www.digitalhumanities.org/dhq/vol/7/1/000151/000151.html">The .txtual Condition</a></p>
</blockquote>
<p>Or, as my colleague David Brunton has said, in a McLuhan-esque way:</p>
<blockquote>
<p>Digital preservation is access...in the future.</p>
</blockquote>
<p>The underlying implication here is that if you are not providing meaningful access in the present to digital content, then you are not preserving it.</p>
<p>In light of these loose definitions I'm going to spend the rest of the time exploring what the Web means as a preservation medium by telling some stories. I'm hoping that they will help illuminate what preservation means in the context of the Web. By the end I hope to convince you of two things: the Web needs us to care for it, and more importantly, we need the Web to do our jobs effectively. For those of you who don't need convincing about either of these points, I hope to give you a slightly different lens for looking at preservation and the Web. It's a hopeful and humanistic lens, that is informed by thinking about the Web as an archive. But more on that later.</p>
<h2>Everything is Broken</h2>
<p>Even the casual user of the Web has run across the problem of the 404 Not Found. In a recent survey of Web citations found in Thompson Reuter's Web of Science, <a href="http://www.biomedcentral.com/1471-2105/14/S14/S5">Hennessey and Ge</a> found that only 69% of the URLs were still available, and the median lifetime for a URL was 9.3 years. The Internet Archive had archived 62% of these URLs. In a similar study of URLs found in recent papers in the popular arXiv pre-print repository <a href="http://arxiv.org/abs/1105.3459">Sanderson, Phillips and Van de Sompel</a> found that of the 144,087 unique URLs referenced in papers, only 70% were still available and of these, 45% were not archived in the Internet Archive, Web Citation, the Library of Congress or the UK National Archive.</p>
<p><a href="http://arxiv.org/abs/1105.3459"><img style="border: thin solid
#eeeeee; width: 100%;" src="http://inkdroid.org/images/ndf-01.png" /></a></p>
<p>Bear in mind, this isn't the World Wild Web of dot com bubbles, failed business plans, and pivots we're talking about. These URLs were found in a small pocket of the Web for academic research, a body of literature that is built on a foundation of citation, and written by practitioners whose very livelihood is dependent on how they are cited by others.</p>
<p>A few months ago the 404 made mainstream news in the US when Adam Liptak's story <a href="http://www.nytimes.com/2013/09/24/us/politics/in-supreme-court-opinions-clicks-that-lead-nowhere.html">In Supreme Court Opinions, Web Links to Nowhere</a> broke in the New York Times. Liptak's story spotlighted a recent <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2329161">study</a> by Zittrain and Albert which found that 50% of links in United States Supreme Court opinions were broken. As its name suggests, the Supreme Court is the highest federal court in the United States...it is the final interpreter of our Constitution. These opinions in turn document the decisions of the Supreme Court, and have increasingly referenced content on the Web for context, which becomes important later for interpretation. 50% of the URLs found in the opinions suffered from what the authors call <em>reference rot</em>. Reference rot includes situations of link rot (404 Not Found and other HTTP level errors), but it also includes when the URL appears to technically work, but the content that was cited is no longer available. The point was dramatically and humorously illustrated by the New York Times since someone had bought <a href="http://ssnat.com/">one of the lapsed domains</a> and put up a message for Justice Alito:</p>
<p><a href="http://ssnat.com"><img style="border: thin solid #eeeeee;
padding-top: 40px; padding-bottom: 40px; width: 100%;" src="http://inkdroid.org/images/ndf-02.png" /></a></p>
<p>Zittrain and Albert propose a new web archiving project called <a href="http://perma.cc">perma.cc</a>, which relies on libraries to select web pages and websites that need to be archived. As proposed perma.cc would be similar in principle to <a href="http://webcitation.org">WebCite</a>, which is built around submission of URLs by scholars. But WebCite's future is uncertain due to a fund drive to raise money to support its operation. perma.cc also has the potential to offer a governance structure similar to how cultural heritage organizations support the Internet Archive in their crawls of the Web.</p>
<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/joi/3869613988/"> <img width="200" src="http://inkdroid.org/images/webpresmed/brewster.jpg" /> </a>
</div>
<p><a href="http://archive.org">Internet Archive</a> was started by Brewster Kahle in 1996. It now contains 366 billion web pages or captures (not unique URLs). In 2008 Google Engineers reported that their index contained 1 trillion unique URLs. That's 5 years ago now. If we assume it hasn't grown since then, and overlook the fact that there are often multiple captures of a given URL over time, Internet Archive contains about 37% of the Web. This is overly generous since the Web has almost certainly grown in the past 5 years, and we're comparing apples and oranges, web captures to unique URLs.</p>
<p>Of course, it's not really fair (or prudent) to put the weight of preserving the Web on one institution. So thankfully, the Internet Archive isn't alone. The <a href="http://netpreserve.org">International Internet Preservation Consortium</a> is a member organization made up of national libraries, universities, and other organizations that do Web archiving. The National Library of New Zealand is a member, and has its own <a href="http://natlib.govt.nz/collections/a-z/new-zealand-web-archive">Web archive</a>. According to the <a href="https://en.wikipedia.org/wiki/List_of_Web_archiving_initiatives#Archived_data">list of Web archiving initiatives</a> Wikipedia article the archive is comprised of 346 million URLs. Perhaps someone in the audience has a rough idea of how big this is relative to the size of the Kiwi Web. It's a bit of a technical problem even to identify national boundaries on the Web. Since the <a href="http://www.legislation.govt.nz/act/public/2003/0019/latest/DLM191962.html">National Library of New Zealand Act of 2003</a>, the National Library has been authorized to crawl the New Zealand portion of the Web. In this regard, New Zealand is light years ahead of the United States, which still is required by law to ask for permission to collect selected, non-governmental websites.</p>
<p>Protocols and tools for sharing the size and makeup of these IIPC collections are still lacking, but the <a href="http://www.mementoweb.org/">Memento project</a> spurred on some <a href="http://arxiv.org/abs/1309.4008">possible</a> approaches out of necessity. For the Memento prototype to work they needed to collect the URL/timestamp combinations for all archived webpages. This turned out to be difficult both for the archive to share, and to aggregate in one place efficiently--and the moment it was done it was already out of date. David Rosenthal has some <a href="http://blog.dshr.org/2013/03/re-thinking-memento-aggregation.html">interesting ideas</a> for aggregators to collect summary data from web archives, which is used to instead provide <em>hints</em> about where a given URL may be archived. Hopefully we'll see some development in this area, as it's increasingly important that Web archives do collection development more closely, to encourage diversity of approaches, and ensure that one isn't a single point of failure.</p>
<p>Even when you consider the work of the International Internet Preservation Consortium, which adds roughly 75 billion URLs (also not unique) we still are only seeing 44% of the Web being archived. And of course this is a very generous guesstimate, since the 366 billion Internet Archive captures are not unique URLs: e.g. a given URL like the <a href="http://www.bbc.co.uk">BBC homepage</a> has been fetched 13,863 times between December 21, 1996 and November 14, 2013. And there is almost certainly overlap between the various IIPC web archives and the Internet Archive.</p>
<h2>The Archival Sliver</h2>
<p>I am citing these statistics not to say the job of archiving the Web is impossible, or a waste of resources. Much to the contrary. I raise it here to introduce one of the archival lenses I want to encourage you to look at Web preservation through: Verne Harris' notion of the <a href="http://www.nyu.edu/pages/classes/bkg/methods/harris.pdf">archival sliver</a>. Harris is a South African archivist, writer and director of the Archive at the Nelson Mandela Centre of Memory. He participated in the transformation of South Africa’s apartheid public records system, and got to see up close how the contents of archives are shaped by the power structures in which they are embedded. Harris' ideas have a distinctly post-modern flavor, and contrast with positivist theories of the archive that assert that the archive's goal is to reflect reality.</p>
<blockquote>
<p>Even if archivists in a particular country were to preserve every record generated throughout the land, they would still have only a sliver of a window into that country’s experience. But of course in practice, this record universum is substantially reduced through deliberate and inadvertent destruction by records creators and managers, leaving a sliver of a sliver from which archivists select what they will preserve. And they do not preserve much.</p>
</blockquote>
<p>I like Harris' notion of the archival sliver, because he doesn't see it as a cause for despair, but rather as a reason to celebrate the role that this archival sliver has in the process of social memory, and the archivist who tends to it.</p>
<blockquote>
<p>The archival record ... is best understood as a sliver of a sliver of a sliver of a window into process. It is a fragile thing, an enchanted thing, defined not by its connections to “reality,” but by its open-ended layerings of construction and reconstruction. Far from constituting the solid structure around which imagination can play, it is itself the stuff of imagination.</p>
</blockquote>
<h2>The First URL</h2>
<p>So instead of considering the preservation of billions of URLs, lets change tack a bit and take a look at the preservation of one, namely the first URL.</p>
<blockquote>
<p>http://info.cern.ch/hypertext/WWW/TheProject.html</p>
</blockquote>
<p>On April 30th, 1993 CERN made (in hindsight) the momentous decision to freely-release the Web technology software that Tim Berners-Lee, Ari Luotonen and Henrik Nielsen created for making the first website. But 20 years later, that website was no longer available. To celebrate the 20th anniversary of the software release Dan Noyes from CERN led a project to bring the original website back online, at the same address using a completely different software stack. The original content was collected from a variety of places: some from the W3C, some from a 1999 backup of Tim Berners-Lee's NeXT. While the content is how it looked then, the resurrected website isn't running the original Web server software, it's running a modern version of Apache.</p>
<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/adactio/9818910816/"> <img width="200" src="http://inkdroid.org/images/webpresmed/lmb.jpg" /> </a>
</div>
<p>CERN also hosted a group of <a href="http://first-website.web.cern.ch/blog/line-mode-browser-dev-days-participants-announced">11 volunteer developers</a> to spend 2 days coding at CERN (expenses paid) to recreate the experience of using the line mode browser (LMB). The LMB allowed users with an Internet connection to use the Web without having to install any software: they could simply telnet to info.cern.ch and start browsing the emerging Web using their terminal. These developers created a NodeJS JavaScript <a href="http://line-mode.cern.ch/www/hypertext/WWW/TheProject.html">application</a> that simulates the experience of using the early Web. You can even use it to navigate to other pages, for example the current <a href="http://line-mode.cern.ch/www/proxy?url=http://www.w3.org">World Wide Web Consortium page</a>.</p>
<p>In a lot of ways I think this work illustrates James Governor's <a href="http://redmonk.com/jgovernor/2007/04/05/why-applications-are-like-fish-and-data-is-like-wine/">adage</a>:</p>
<blockquote>
<p>Applications are like fish, data is like wine. Only one improves with age.</p>
</blockquote>
<p>As any old school LISP programmer will tell you, sometimes <a href="https://en.wikipedia.org/wiki/Homoiconicity">code is data and data is code</a>. But it is remarkable that this 20 year old HTML still renders just fine in a modern Web browser. This is no accident, but is the result of thoughtful, just-in-time <a href="http://www.ics.uci.edu/~fielding/pubs/dissertation/net_app_arch.htm">design</a> that encouraged the evolvability, extensibility and customizability of the Web. I think we as a community still have lots to learn from the Web's example, and lots more to import into our practices. More about HTML in a bit.</p>
<h2>Permalinks</h2>
<div style="float: right; margin-left: 10px;">
  <a href="http://www.onfocus.com/about"> <img width="200" src="http://inkdroid.org/images/webpresmed/bausch.jpg" /> </a>
</div>
<p>Now obviously this sort of attention can't be paid to all broken URLs on the
Web. But it seems like an interesting example of how an archival sliver of the
Web was cared for, respected and valued. Despite popular opinion, the care for
URLs is not something foreign to the Web. For example lets take a look at the
idea of the <a href="https://en.wikipedia.org/wiki/Permalink">permalink</a> that
was popularized by the blogging community. As you know, a blog is typically a
stream of content. In 2000 <a href="https://twitter.com/pbausch">Paul Bausch</a>
at Blogger <a
href="https://web.archive.org/web/20130308041757/http://www.sixapart.com/blog/2003/09/interview_with.html">came up with</a> a way to assign URLs to individual posts in the stream. This practice is so ubiquitous now it's difficult to see what an innovation it was at the time. As its name implies, the idea of the permalink is that it is stable over time, so that the content can be persistently referenced. Apart from longevity, permalinks have beneficial SEO characteristics: the more that people link to the page over time, the higher its <a href="https://en.wikipedia.org/wiki/PageRank">PageRank</a>, and the more people who will find it in search results.</p>
<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/jimgris/65769319/"> <img width="200" src="http://inkdroid.org/images/webpresmed/timbl.jpg" /> </a>
</div>
<p>A couple years before the blogging community started talking about permalinks, Tim Berners-Lee wrote a short W3C design note entitled <a href="http://www.w3.org/Provider/Style/URI.html">Cool URIs Don't Change</a>. In it he provides some (often humorously snarky) advice for people to think about their URLs, and namespaces with an eye to their future. One of Berners-Lee's great insights was to allow any HTML document to link to any other HTML document, without permission. This decision allowed the Web to grow in a decentralized fashion. It also means that links can break when pages drift apart, and move to new locations, or disappear. But just because a link <em>can</em> break doesn't mean that it <em>must</em> break.</p>
<div style="float: right; margin-left: 10px;">
  <a href="http://www.flickr.com/photos/anjeve/72048767/"> <img width="200" src="http://inkdroid.org/images/webpresmed/lod.jpg" /> </a>
</div>
<p>The idea of Cool URIs saw new life in 2006 when Leo Sauerman and Richard Cyganiak began work on <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a>, which became a seminal document for the Linked Data movement. Their key insight was that identity (URLs) matters on the Web, especially when you are trying to create a distributed database like the Semantic Web.</p>
<p>Call them permalinks or Cool URIs, the idea is the same. Well managed websites will be rewarded by more links to their content, improved SEO, and ultimately more users. But most of all they will be rewarded by a better understanding of what they are putting on the Web. Organizations, particularly cultural heritage organization should take note -- especially their "architects". Libraries, archives and museums need to become regions of stability on the Web, where URLs don't capriciously fail because some exhibit is over, or some content management system is swapped out for another. This doesn't mean content can't change, move or even be deleted. It just means we need to know when we are doing it, and say where something has <a href="http://httpstatusdogs.com/301-moved-permanently">moved</a>, or say that what was once there is now <a href="http://httpstatusdogs.com/410-gone">gone</a>. If we can't do it, the websites that do will become the new libraries and archives of the Web.</p>
<h2>Community</h2>
<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/laughingsquid/3227372389/"> <img width="200" src="http://inkdroid.org/images/webpresmed/textfiles.jpg" /> </a>
</div>
<p>Clearly there is a space between large scale projects to archive the entire Web, and efforts to curate a particular website. Consider the work of <a href="http://www.archiveteam.org/">ArchiveTeam</a>, a volunteer organization formed in 2009 that keeps an eye on when websites are in danger of, actually are, closing their doors and shutting down. Using their <a href="http://www.archiveteam.org/index.php?title=Main_Page">wiki</a>, IRC chatrooms, and software tools they have built up a community of practice around archiving <a href="http://www.archiveteam.org/index.php?title=Category:Rescued_Sites">websites</a>, which have included some 60 sites, such as Geocities and Friendster. They maintain a page called the <a href="http://www.archiveteam.org/index.php?title=Deathwatch">Death Watch</a> where they list sites that are dying (pining for the fjords), or in danger of dying (pre-emptive alarm bells). These activist archivists run something called the <a href="http://www.archiveteam.org/index.php?title=Warrior">Warrior</a> which is a virtual appliance you can install on a workstation, which gets instructions from the <a href="http://tracker.archiveteam.org/">Archive Team tracker</a> about which URLs to download, and coordinates the collection. The tracker then collects statistics, that allow participants to see how much they have contributed relative to others. The collected data is then packed up as WARC files and delivered to the Internet Archive where it is reviewed by an adminstrator, and added to their Web collection.</p>
<p>ArchiveTeam is a labor of love for its creator <a href="http://twitter.com/textfiles">Jason Scott Sadofsky</a> (aka Jason Scott) who is himself an accomplished documenter of computing history, with films such as <a href="http://www.bbsdocumentary.com/">BBS: The Documentary</a> (early bulletin board systems), <a href="http://www.getlamp.com/">Get Lamp</a> (interactive fiction) and <a href="http://ascii.textfiles.com/archives/3984">DEFCON: The Documentary</a>. Apart from mobilizing action, his regular talks have raised awareness about the impermanence on the Web, and have connected with other like minded Web archivists in a way that traditional digital preservation projects have struggled to. I suspect that this self-described "collective of rogue archivists, programmers, writers and loudmouths dedicated to saving our digital heritage" is the shape of things to come for the profession. ArchiveTeam are not the only activists archiving parts of the Web, lets take a look at a few more examples.</p>
<p>Earlier this year Google <a href="http://googlereader.blogspot.com/2013/03/powering-down-google-reader.html">announced</a> that they were pulling Google Reader offline. This caused much grief and anger to be vented from the blogging community...but it spurred one person into action. <a href="http://blog.persistent.info/">Mihai Parparita</a> was an engineer who helped create Google Reader at Google, but he no longer worked there, and wanted to do something to help people retain both their data and the experience of Google Reader. Because he felt that the snapshots of data provided weren't complete, he quickly put together a project <a href="http://readerisdead.com/">ReaderIsDead</a>, which is also <a href="https://github.com/mihaip/readerisdead">available on GitHub</a>. ReaderIsDead is actually a collection of different tools: one for pulling down your personal Reader data from Google while the Google Reader servers were still alive, and a simple web application called ZombieReader that serves that data up, for when the Google Reader servers actually went dead. Mihai put his knowledge of how the Google Reader talked to backend Web service APIs to build ZombieReader.</p>
<p><iframe width="420" height="315" src="//www.youtube.com/embed/Xjbso_9-yGg" frameborder="0" allowfullscreen></iframe></p>
<p>Ordinarily fat client interfaces like Google Reader pose problems for traditional Web archiving tools like Internet Archive's Heretrix. Fat Web clients are applications that largely run in your browser, using JavaScript. These applications typically talk back to Web service APIs to fetch more data (often JSON) based on interaction in the browser. Web archiving crawlers don't typically execute JavaScript that is crawled from the Web, and have a hard (if not impossible) time simulating user behavior, which then triggers the calls back to the Web service. And of course the Web Service is what goes dead as well, so even if the Web archive has a snapshot of the requested data, the JavaScript would need to be changed to fetch it. This means the Web archive is left with a largely useless shell.</p>
<p>But in the case of Zombie Reader, the fat client provided a data abstraction that proved to be an excellent way to preserve both the personal data and the user experience of using Google Reader. Mihai was able to use the same HTML, CSS and JavaScript from GoogleReader, but instead of the application talking back to Google's API he had it talk back to a local Web Service that sat on top of the archived data. Individual users could continue to use their personal archives. ZombieReader became a read-only snapshot of what they were reading on the Web, and their interactions with it. Their sliver of a sliver of a sliver.</p>
<h2 id="impermanence">Impermanence</h2>
<p>Of course .com failures aren't the only reason why content disappears from the Web. People intentionally remove content from the Web all the time for a variety of reasons. Let's consider the strange, yet fascinating cases of Mark Pilgrim and Jonathan Gillette. Both were highly prolific software developers, bloggers, authors and well known spokespeople for open source and the Web commons.</p>
<div style="float: left; margin-right: 10px;">
  <a href="https://en.wikipedia.org/wiki/File:Mark_Pilgrim.jpg"> <img width="200" src="http://inkdroid.org/images/webpresmed/pilgrim.jpg" /> </a>
</div>
<p>Among other things, Mark Pilgrim was very active in the area of feed syndication technology (RSS, Atom). He wrote the feed validator and Universal Feed Parser that makes working with syndicated much easier. He also pushed the boundaries of technical writing by writing <a href="http://diveintohtml5.info/">Dive Into HTML 5</a> and <a href="http://www.diveinto.org/python3/">Dive Into Python 3</a> which were published traditionally as books, but also made available on the Web with a CC-BY license. Pilgrim also worked at Google, where he helped promote and evolve the Web with his involvement in the Web Hypertext Application Technology Working Group <a href="http://www.whatwg.org/">WHATWG</a>.</p>
<div style="float: right; margin-left: 10px;">
  <a href="http://www.flickr.com/photos/pragdave/173650575/"> <img width="200" src="http://inkdroid.org/images/webpresmed/why.jpg" /> </a>
</div>
<p>Jonathan Gillette, also known as Why the Lucky Stiff or _why, was a prolific writer, cartoonist, artist, and computer programmer who helped popularize the Ruby programming language. His online book <a href="http://mislav.uniqpath.com/poignant-guide/">Why's (Poignant) Guide to Ruby</a> introduced people of all ages to the practice of programming with wit and humor that will literally make you laugh out loud as you learn. His projects such like <a href="http://tryruby.org/levels/1/challenges/0">Try Ruby</a> and <a href="http://hackety.com/">Hackety Hack!</a> lowered the barriers to getting a working software development environment set up by moving it to the Web. He also wrote a great deal of software such as hpricot for parsing HTML, and the minimalist Web framework <a href="http://camping.io/">camping</a>.</p>
<p>Apart from all these similarities Mark Pilgrim and Jonathan Gillette share something else in common: on October 4, 2011 and August 19, 2009 respectively they both decided to completely delete their online presence from the Web. They committed <em>info-suicide</em>. Their online books, blogs, social media accounts, and github projects were simply removed. No explanations were made, they just blinked out of existence. They are still alive here in the physical world, but they aren't participating online as they were previously...or at least not using the same personas. I like to think Pilgrim and _why were doing performance art to illustrate the fundamental nature of the Web, its nowness, its fragility, it's impermanence. As Dan Connolly said once:</p>
<blockquote>
<p>The point of the Web arch[itecture] is that it builds the illusion of a shared information space.</p>
</blockquote>
<p>If someone decides to turn off a server or delete a website it's gone for the entire world, the illusion dissolves. Maybe it lives on buried in a Web archive, but it's previous life out on the Web is over. Or is it?</p>
<p>It's interesting to see what happened after the info-suicides. Why's (Poignant) Guide to Ruby was <a href="http://mislav.uniqpath.com/poignant-guide/">rescued</a> by <a href="http://mislav.uniqpath.com/">Mislav Marohnic</a> a software developer living in Croatia. He was able to piece the book back together based on content in the Internet Archive, and put it back online at a new URL, as if nothing had happened. In addition he has continued to curate it: updating code samples to work with the latest version of Ruby, enabling syntax highlighting, converting it to use Markdown, and more.</p>
<p>Similarly Mark Pilgrim's Dive Into HTML 5 and Dive Into Python 3 were assembled from copies and re-deployed to the Web. Prior to his departure Pilgrim used <a href="http://github.com">Github</a> to manage the content for his books. Github is a distributed revision control system, where everyone working with the code has a full copy of it local on their machine. So rather than needing to get content out of the Internet Archive, developers created the <a href="http://github.com/diveintomark">diveintomark</a> organization account on Github, and pushed their clones of the original repositories there.</p>
<p>Much of Why and Pilgrim's code was also developed on GitHub. So even though the master was deleted, many people had clones, and were able to work together to establish a new master. Philip Cromer created the <a href="https://github.com/whymirror/">whymirror</a> on Github, which collected _why's code. Jeremy Ruten created <a href="http://viewsourcecode.org/why/">_why's Estate</a> which is a hypertext archive collects pointers to the various software archives, and writings that have been preserved in Internet Archive and elsewhere.</p>
<p>So, it turns out that the supposedly brittle medium of the Web, where a link can easily break, and a whole website can be capriciously turned off, is a bit more persistent than we think. These events remind me of Matthew Kirschenbaum's book <a href="http://www.amazon.com/Mechanisms-New-Media-Forensic-Imagination/dp/026251740X">Mechanisms</a> which deconstructs notions of electronic media being fleeting or impermanent to show how electronic media (especially that which is stored on hard drives) is actually quite resilient and resistent to change. Mechanisms contains a fascinating study of how <a href="https://en.wikipedia.org/wiki/William_Gibson">William Gibson</a>'s poem <a href="https://en.wikipedia.org/wiki/Agrippa_(a_book_of_the_dead)">Agrippa</a> (which was engineered to encrypt itself and become unreadable after a single reading) saw new life on the Internet, as it was copied around on FTP, USENET, email listservs, and ultimately the Web:</p>
<blockquote>
<p>Agrippa owes its transmission and continuing availability to a complex network of individuals, communities, ideologies, markets, technologies, and motives ... from its example we can see that the preservation of digital media has a profound social dimension that is at least as important as purely technical considerations. <a href="http://agrippa.english.ucsb.edu/kirschenbaum-matthew-g-hacking-agrippa-the-source-of-the-online-text">Hacking Agrippa</a></p>
</blockquote>
<h2>Small Data</h2>
<p>In the forensic spirit of Mechanisms, let's take a closer look at Web technology, specifically HTML. Remember the first URL and how CERN was able to revive it? When you think about it, it's kind of amazing that you can still look at that HTML in your modern browser, right? Do you think you could view your 20 year old word processing documents today? Jeff Rothenberg cynically <a href="http://www.clir.org/pubs/archives/ensuring.pdf">observed</a></p>
<blockquote>
<p>digital information lasts forever—or five years, whichever comes first</p>
</blockquote>
<p>Maybe if we focus on the archival sliver instead of the impossibility of <em>everything</em> we're not doing so bad.</p>
<p>As we saw in the cases of Pilgirm and _why the Internet Archive and other web archiving projects play an important role in snapshotting Web pages. But we are also starting to see social media companies are building tools that allow their users to easily extract or "archive" their content. These tools are using HTML in an interesting new way that is worth taking a closer look at.</p>
<p>How many Facebook users are there here? How many of you have requested your archive? If you navigate to the right place in your settings you can "Download a copy of your Facebook data." When you click on the button you set in motion a process that gathers together your profile, contact information, wall, photos, synced photos, videos, friends, messages, pokes, events, settings, security and (ahem) ads. This takes Facebook a bit of time, it took a day the last time I tried it, and you get an email when it's finished which contains a link to download a zip file. The zip file contains HTML, JPEG, MP4 files which you can open in your browser. You don't need to be connected to the Internet, everything is available locally.</p>
<p>Similarly Twitter allow you to request an archive periodically, which triggers an email when it is ready for you to pick up. Much like the Facebook archive it is delivered as a zip file, which contains an easily browsable HTML package. The Twitter archive is actually more like a dynamic application, since it includes a JavaScript application called Grailbird. Grailbird lets you search your tweets, and examine tweets from different time periods. Just like the Facebook archive everything Grailbird needs is available locally, and the application will work when you are disconnected from the Internet. Although user's avatar thumbnail images are still loaded directly from the Web. But all your tweet data is available as JavaScript and CSV. The application depends on some popular JavaScript libraries like jQuery and Underscore, but those also are bundled right with the archive. It would be nice to see Twitter <a href="https://dev.twitter.com/discussions/14148">release Grailbird as a project on Github</a> as many of their other software projects are. Thinking of Grailbird as a visualization framework for tweets would allow interested parties to add new visualizations (e.g. tweets on a map, network graphs, etc). You could also imagine tools for reaching out into the network of an individual's tweets to fetch tweets that they were replying to, and persisting them back locally to the package.</p>
<p>Some of you may remember that the <a href="http://www.dataliberation.org/">Data Liberation Front</a> (led by <a href="https://twitter.com/therealfitz">Brian Fitzpatrick</a> at Google) and the eventual product offering <a href="https://en.wikipedia.org/wiki/Google_Takeout">Google Takeout</a> were early innovators in this area. Google Takeout allows you to download data from 14 of their products as a zip file. The service isn't without <a href="http://www.pcworld.com/article/255920/liberating_your_data_from_google_and_what_that_really_means.html">criticism</a>, because it doesn't include things like your Gmail archive or your search history. The contents of the archive are also somewhat more difficult to work with, compared to the Facebook and Twitter equivalents. For example, each Google+ update is represented as a single HTML file, and there isn't a notion of a minimal, static application that you can use to browse them. The HTML also references content out on the Web, and isn't as self-contained as Twitter and Facebook's archive. But having snapshots of your Youtube videos, and contents of your Google Drive is extremely handy. As Brad Fitzpatrick <a href="http://queue.acm.org/detail.cfm?id=1868432">wrote</a> in 2010, Google Takeout is kind of a remarkable achievement, or realization for a big publicly traded behometh to make:</p>
<blockquote>
<p>Locking your users in, of course, has the advantage of making it harder for them to leave you for a competitor. Likewise, if your competitors lock their users in, it is harder for those users to move to your product. Nonetheless, it is far preferable to spend your engineering effort on innovation than it is to build bigger walls and stronger doors that prevent users from leaving. Making it easier for users to experiment today greatly increases their trust in you, and they are more likely to return to your product line tomorrow.</p>
</blockquote>
<p>I mention Facebook, Twitter and Google here because I think these archiving services are important for memory institutions like museums, libraries and archives. They allow individuals to download their data from the huge corpus that is available--a sliver of a sliver of a sliver. When a writer or politician donates their papers, what if we accessioned their Facebook or Twitter archive? <a href="https://en.wikipedia.org/wiki/Dave_Winer">Dave Winer</a> for example has started <a href="http://threads2.scripting.com/2012/december/uploadYourTwitterArchive">collecting</a> Twitter archives that have been donated to him, that meet a certain criteria, and making them public. If we have decided to add someone's papers to a collection, why not acquire their social media archives and store them along with their other born digital and traditional content? Yes, Twitter (as a whole) is being archived by the Library of Congress, as so called <em>big data</em>. But why don't we consider these personal archives as small data, where context and <a href="http://www2.archivists.org/glossary/terms/o/original-order">original order</a> are preserved with other relevant material in a highly usable way? As <a href="https://twitter.com/rufuspollock">Rufus Pollock</a> of the <a href="http://okfn.org/">Open Knowledge Foundation</a> <a href="http://blog.okfn.org/2013/04/22/forget-big-data-small-data-is-the-real-revolution/">said</a></p>
<blockquote>
<p>This next decade belongs to distributed models not centralized ones, to collaboration not control, and to small data not big data.</p>
</blockquote>
<div style="float: left; margin-right: 10px;">
  <a href="http://thenounproject.com/noun/box/#icon-No17863"> <img src="http://inkdroid.org/images/webpresmed/package.png" /> </a>
</div>
<p>The other interesting thing about these services is their use of HTML as a packaging format. My coworker <a href="http://twitter.com/acdha">Chris Adams</a> once remarked that the one format he expects to be able to read in 100 years is HTML. Of course we can't predict the future. But I suspect he may be right. We need best practices, or really just patterns for creating HTML packages of archival content. We need to make sure our work sits on top of common tools for the Web. We need to support the Web browser, particularly open source ones. We need to track and participate in Web standardization efforts such as the W3C and the WHATWG. We must keep the usability of the archive in mind: is it easy to open up in your browser and wander around in the archive as with the Twitter and Facebook examples? And most importantly, as Johan van der Knijff of the National Library of the Netherlands discusses in his study of <a href="http://www.openplanetsfoundation.org/blogs/2013-05-23-epub-archival-preservation-update">EPUB</a>, it is important that all resources are local to the package. Loading images, JavaScript, etc from a remote location makes the archive vulnerable, since it becomes dependent on some part of the Web staying alive. Perhaps we also need tools like <a href="http://www.archiveready.com/">ArchiveReady</a> for inspecting local HTML packages (in addition to websites) and reporting on their archivability?</p>
<h2>Conclusion</h2>
<p>So how to wrap up this strange, fragmented, incomplete tour through Web preservation? I feel like I should say something profound, but I was hoping these stories of the Web would do that for me. I can only say for myself that I want to give back to the Web the way it has given to me. With 25 years behind us the Web needs us more than ever to help care for the archival slivers it contains. I think libraries, museums and archives that realize that they are custodians of the Web, and align their mission with the grain of the Web, will be the ones that survive, and prosper. Brian Fitzpatrick, Jason Scott, Brewster Kahle, Mislav Marohnic, Philip Cromer, Jeremy Ruten and Aaron Swartz demonstrated their willingness to work with the Web as a medium in need of preservation, as well as a medium for <em>doing</em> the preservation. We need more of them. We need to provide spaces for them to do their work. They are the new faces of our profession.</p>
