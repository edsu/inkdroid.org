<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your site. -->

<!-- To import this information into a WordPress site follow these steps: -->
<!-- 1. Log in to that site as an administrator. -->
<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
<!-- 3. Install the "WordPress" importer from the list. -->
<!-- 4. Activate & Run Importer. -->
<!-- 5. Upload this file using the form provided on that page. -->
<!-- 6. You will first be asked to map the authors in this export file to users -->
<!--    on the site. For each author, you may choose to map to an -->
<!--    existing user on the site or to create a new user. -->
<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
<!--    contained in this file into your site. -->

<!-- generator="WordPress/4.3" created="2015-08-31 08:19" -->
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>
	<title>inkdroid</title>
	<link>http://inkdroid.org/</link>
	<description>paper or plastic?</description>
	<pubDate>Mon, 31 Aug 2015 08:19:06 +0000</pubDate>
	<language>en-US</language>
	<wp:wxr_version>1.2</wp:wxr_version>
	<wp:base_site_url>http://inkdroid.org/</wp:base_site_url>
	<wp:base_blog_url>http://inkdroid.org/</wp:base_blog_url>

	<wp:author><wp:author_id>1</wp:author_id><wp:author_login>admin</wp:author_login><wp:author_email>ed.summers@gmail.com</wp:author_email><wp:author_display_name><![CDATA[Administrator]]></wp:author_display_name><wp:author_first_name><![CDATA[Ed]]></wp:author_first_name><wp:author_last_name><![CDATA[Summers]]></wp:author_last_name></wp:author>
	<wp:author><wp:author_id>2</wp:author_id><wp:author_login>ed</wp:author_login><wp:author_email>ehs@pobox.com</wp:author_email><wp:author_display_name><![CDATA[ed]]></wp:author_display_name><wp:author_first_name><![CDATA[Ed]]></wp:author_first_name><wp:author_last_name><![CDATA[Summers]]></wp:author_last_name></wp:author>


	<generator>http://wordpress.org/?v=4.3</generator>

	<item>
		<title> </title>
		<link>http://inkdroid.org/?p=26</link>
		<pubDate>Thu, 01 Sep 2005 02:24:27 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=26</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>26</wp:post_id>
		<wp:post_date>2005-08-31 19:24:27</wp:post_date>
		<wp:post_date_gmt>2005-09-01 02:24:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
	</item>
	<item>
		<title>swd amsterdam</title>
		<link>http://inkdroid.org/?p=169</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=169</guid>
		<description></description>
		<content:encoded><![CDATA[<img src="http://www.w3.org/Icons/SW/sw-cube.png" alt="w3c semweb" />]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>169</wp:post_id>
		<wp:post_date>0000-00-00 00:00:00</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
	</item>
	<item>
		<title>sudafed</title>
		<link>http://inkdroid.org/?p=178</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=178</guid>
		<description></description>
		<content:encoded><![CDATA[<img src="/images/sudafed.png" style="float: left;" /><a href="http://en.wikipedia.org/wiki/Pseudoephedrine">sudafed</a> induced <a href="http://xkcd.com/313/">insomnia</a> got me wandering around in <a href="http://marklindner.info/blog/2007/12/30/books-read-in-2007/">Mark Linder's list</a> of books he read in 2007, which led to wandering around in <a href="http://www.worldcat.org">OCLC's WorldCat</a>, where I noticed the Library of Congress as a holding institution for a particular <a href="http://www.worldcat.org/oclc/45621736">book</a>, and that there was a hyperlink to the Library of Congress. I got to wondering--given the use of sessions in LC's Voyager Catalog, how does WorldCat persistently link to a bibliographic record at LC?

<blockquote><a href="http://www.worldcat.org/wcpa/oclc/45621736?page=frame&url=http%3A%2F%2Fwww.loc.gov%2Fz39voy%3Foperation%3DsearchRetrieve%26version%3D1.1%26recordSchema%3Dmarcxml%26startRecord%3D1%26maximumRecords%3D10%26stylesheet%3Dhttp%3A%2F%2Fwww.loc.gov%2Fz3950%2Fowcbrief.xsl%26query%3Dbath.standardIdentifier%3D0792368134&title=Library+of+Congress&linktype=opac&detail=DLC%3ALibrary+of+Congress%3AFederal%2FNational+Gov">
http://www.worldcat.org/wcpa/oclc/45621736?page=frame&url=
http%3A%2F%2Fwww.loc.gov%2Fz39voy%3Foperation%3Dsearch
Retrieve%26version%3D1.1%26recordSchema%3Dmarcxml%26
startRecord%3D1%26maximumRecords%3D10%26stylesheet%3D
http%3A%2F%2Fwww.loc.gov%2Fz3950%2Fowcbrief.xsl%26query
%3Dbath.standardIdentifier%3D0792368134&title=Library+of+Congress
&linktype=opac&detail=DLC%3ALibrary+of+Congress%3AFederal
%2FNational+Gov
</a>
</blockquote>

On the surface the URL appears to be just another uncool URI...but after a few seconds you can see there is another URL hiding inside which targets LC's SRU service:

<blockquote><a href="http://www.loc.gov/z39voy?operation=searchRetrieve&version=1.1&recordSchema=marcxml&startRecord=1&maximumRecords=10&stylesheet=http://www.loc.gov/z3950/owcbrief.xsl&query=bath.standardIdentifier=0792368134">http://www.loc.gov/z39voy?operation=searchRetrieve&version=1.1
&recordSchema=marcxml&startRecord=1&maximumRecords=10
&stylesheet=http%3A%2F%2Fwww.loc.gov%2Fz3950%2Fowcbrief.xsl
&query=bath.standardIdentifier=0792368134</a>
</blockquote>

which in turn contains another URL for a stylesheet to make the SRU marcxml look pretty for WorldCat:

<blockquote>
<a href="http://www.loc.gov/z3950/owcbrief.xsl">http://www.loc.gov/z3950/owcbrief.xsl</a>
</blockquote>

<a href="http://en.wikipedia.org/wiki/Matryoshka_doll"><img src="/images/matroshka.jpg" border="0" width="450" /></a>

So I started out this posting just wanting to note somewhere (other than a delicious <a href="http://del.icio.us/inkdroid/metadata">bookmark</a>) how you can easily get DublinCore for a given URL by splicing a ISBN into a given URL like:

<blockquote>
http://www.loc.gov/z39voy?operation=searchRetrieve&version=1.1
&recordSchema=dc&startRecord=1&maximumRecords=10&
query=bath.standardIdentifier={isbn}
</blockquote>

But we've come to expect this sort of thing with web2.0 data APIs. What's more interesting is how OCLC is mashing up WorldCat with catalogs all over the world. I tried a handful of other links out to other catalogs and they all worked, linking directly to bibliographic records in local catalogs...and these were not using SRU but seemingly arbitrary vendor-specific URLs. I guess somewhere in the guts of worldcat there is some <a href="http://weblog.infoworld.com/udell/stories/2002/12/11/librarylookup.html">library-lookup-like</a> logic that is able to splice ISBNs into URLs for (how many?) local library catalogs? I'd love to know how comprehensive this linking is. 
 
And finally something occurred to me as I realized the sun was coming up and I was realized I was finally tired, and the rest of the house was waking up for a new day. This pattern of URI composition (URIs that contain other URIs) is something that's been coming up in my mind quite a bit, and probably deserves a posting of its own.
http://en.wikipedia.org/wiki/Representational_State_Transfer
http://zoom.z3950.org/bind/web/outline.html]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>178</wp:post_id>
		<wp:post_date>2008-01-02 08:51:54</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
	</item>
	<item>
		<title>openlibcon</title>
		<link>http://inkdroid.org/?p=189</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=189</guid>
		<description></description>
		<content:encoded><![CDATA[- librarycoverthing
- marc records donated
- aaronsw and book reviews (nytimes, etc)
- infobase (w/ versioning!!)
- http://www.mediawiki.org/wiki/Extension:Semantic_Forms
- copyright algorithms!
- wikipedia citation gathering pushing to openlibrary 
- loss of biodiversity and loss of cultural diversity]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>189</wp:post_id>
		<wp:post_date>2008-02-29 14:23:21</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=198</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=198</guid>
		<description></description>
		<content:encoded><![CDATA[We had a really productive meeting of the <a href="http://www.w3.org/2006/07/SWD/">Semantic Web Deployment Group</a> last week. 

<a href="http://www.slideshare.net/gardensofmeaning/simple-knowledge-organization-system-skos-in-the-context-of-semantic-web-deployment-library-of-congress-may-2008">Simple Knowledge Organization System (SKOS) Deployment in the Context of Semantic Web Deployment</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>198</wp:post_id>
		<wp:post_date>2008-05-13 08:43:22</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>oai2lod</title>
		<link>http://inkdroid.org/?p=199</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=199</guid>
		<description></description>
		<content:encoded><![CDATA[So I saw timbl's <a href="http://lists.w3.org/Archives/Public/semantic-web/2008Apr/0183.html">email</a>, and I even saw the pre-print for the paper -- but I failed for whatever reason to dig into them, which is surprising given my interest in both linked-data and oai-pmh. 

http://memory.loc.gov/ammem/oamh/lcoa1_content.html
http://www.mediaspaces.info/tools/oai2lod/

<pre>
http://www.mediaspaces.info:2020/resource/item/oai:lcoa1.loc.gov:loc.gmd/g8014p.ct001366

</pre>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>199</wp:post_id>
		<wp:post_date>2008-05-15 10:15:33</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>QuerySet</title>
		<link>http://inkdroid.org/?p=206</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=206</guid>
		<description></description>
		<content:encoded><![CDATA[I found myself discovering something unexpected about <a href="http://django-project.org">django's</a> use of database connections this weekend. I added some properties to a model I had ... it's a webapp for visualizing newspapers, and newspapers are made of pages:

<pre lang="python">
class Page(models.Model):
    sequence = models.IntegerField()
    number = models.IntegerField()
    tiff = models.TextField()
    jp2 = models.TextField()
    jp2_width = models.IntegerField()
    jp2_length = models.IntegerField()
    ocr = models.TextField()
    pdf = models.TextField()
    ocr_text = models.TextField()
    word_coordinates_json = models.TextField()
    issue = models.ForeignKey('Issue', related_name='pages')
</pre>

The properties I added were <code>ocr_text</code> and <code>word_coordinates_json</code>.  They happen to be fairly large chunks of text. So I had some code I was using to iterate through these pages:

<pre lang="python">
  from chronam.web import Page

  for page in Page.objects.all():
      ...
</pre>

This code ran without incident (I thought) before, but after adding the big ass columns my python process quickly grabbed all of the memory available on my machine and started swapping like crazy. I was always under the impression that all() returned an iterator, which it does. 



But it turns out that 


After a bit of googling I ran across a thread:

<a href="http://groups.google.com/group/django-developers/browse_thread/thread/487bb9dcad37e7b3">thread</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>206</wp:post_id>
		<wp:post_date>2008-07-08 13:36:43</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=771</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=771</guid>
		<description></description>
		<content:encoded><![CDATA[It's pretty awesome to see ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>771</wp:post_id>
		<wp:post_date>2009-03-02 12:00:44</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>VocabularySoup</title>
		<link>http://inkdroid.org/?p=876</link>
		<pubDate>Sun, 15 Mar 2009 10:21:20 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=876</guid>
		<description></description>
		<content:encoded><![CDATA[I had the opportunity to attend the <a href="http://www.w3.org/2007/eGov/IG/wiki/F2F2">W3C eGovernment Meeting</a> last week, and go out for dinner/drinks with some of the folks here in DC. One of the highlights for me was hearing John Sheridan of the National Archives <a href="http://www.w3.org/2009/03/OPSI-LinkedData.pdf">talk</a> about Linked Data, specifically in online publications like the London Gazette.  

I had an interesting conversation with <a href="http://www.advocatehope.org/">Daniel Bennett</a> at

<pre>
<span style="color: blue;">&lt;mods version="3.0" xmlns="http://www.loc.gov/mods/v3"&gt;
 &lt;titleInfo&gt;
  &lt;title&gt;Dive into Python&lt;/title&gt;
 &lt;/titleInfo&gt;
 &lt;location&gt; 
  &lt;url&gt;http://www.diveintopython.org/download/diveintopython-pdf-5.4.zip&lt;/url&gt;
 &lt;/location&gt;
&lt;/mods&gt;</span>
</pre>

<pre>
<span style="color: orange;">&lt;fixity xmlns="info:lc/xmlns/premis-v2"&gt;
 &lt;messageDigestAlgorithm&gt;MD5&lt;/messageDigestAlgorithm&gt;
 &lt;messageDigest&gt;344f040e317a39f342cf277a1630e28b&lt;/messageDigest&gt;
&lt;/fixity&gt;</span>
</pre>


<pre>
<span style="color: blue;">&lt;mods version="3.0" xmlns="http://www.loc.gov/mods/v3"&gt;
 &lt;titleInfo&gt;
  &lt;title&gt;Dive into Python&lt;/title&gt;
 &lt;/titleInfo&gt;
 &lt;location&gt;
  &lt;url&gt;http://www.diveintopython.org/download/diveintopython-pdf-5.4.zip&lt;/url&gt;</span>
 <span style="color: orange;">  &lt;premis:fixity xmlns:premis="info:lc/xmlns/premis-v2"&gt;
    &lt;premis:messageDigestAlgorithm&gt;MD5&lt;/premis:messageDigestAlgorithm&gt;
    &lt;premis:messageDigest&gt;344f040e317a39f342cf277a1630e28b&lt;/premis:messageDigest&gt;
   &lt;/premis:fixity&gt;</span>
 <span style="color: blue;"> &lt;/location&gt;
&lt;/mods&gt;</span>
</pre>

<pre>
&lt;xsd:any processContents="lax" minOccurs="0" maxOccurs="unbounded" /&gt;
</pre>

<pre>
@prefix bibo: &lt;http://purl.org/ontology/bibo/&gt; .
@prefix dct: &lt;http://purl.org/dc/terms/&gt; .
@prefix premis: &lt;info:lc/xmlns/premis-v2/&gt; .

&lt;http://www.diveintopython.org/download/diveintopython-pdf-5.4.zip&gt;
 a bibo:Book;
 dct:title "Dive into Python";
 premis:hasFixity [
  premis:messageDigestAlgorithm "MD5";
  premis:messageDigest "344f040e317a39f342cf277a1630e28b"
 ] .
</pre>

http://imageweb.zoo.ox.ac.uk/pub/2009/citobase/cito-20090311/cito-content/owldoc/
+ bibo]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>876</wp:post_id>
		<wp:post_date>2009-03-15 03:21:20</wp:post_date>
		<wp:post_date_gmt>2009-03-15 10:21:20</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>kthnkxbye facebook</title>
		<link>http://inkdroid.org/?p=1324</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1324</guid>
		<description></description>
		<content:encoded><![CDATA[I just deleted my Facebook account. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1324</wp:post_id>
		<wp:post_date>2009-10-25 04:44:08</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>syndicating linked data</title>
		<link>http://inkdroid.org/?p=1470</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1470</guid>
		<description></description>
		<content:encoded><![CDATA[The value proposition of Linked Data as compared to other types of data available on the web is primarily in its linky-ness, or as Tim Berners-Lee <a href="http://www.w3.org/DesignIssues/LinkedData.html">says</a>, when returning representations of things via HTTP:

<blockquote>
Include links to other URIs. so that they can discover more things.
</blockquote>

The idea is that resources can gather what Stefano Mazzocchi calls <a href="http://www.betaversion.org/~stefano/linotype/news/304/">relational density</a>, context, or additional value by being part of a network of resources. Extracting the value in the relational density of web documents was the essential <a href="http://en.wikipedia.org/wiki/PageRank">innovation</a> that Google made a little over a decade ago. Being able to <em>follow your nose</em> is a fundamental aspect of the web, which sometimes gets called the <a href="http://www.w3.org/2001/tag/doc/selfDescribingDocuments.html">self describing web</a>, and more generally <a href="http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven">Hypertext As The Engine Of Application State (HATEOS)</a>. Or <em>even more generally</em> as one of my favorite philosophers said:

<blockquote>
There is nothing to be known about anything except an initially large, and forever expandable, web of relations to other things. Everything that can serve as a term of relation can be dissolved into another set of relations, and so on for ever. There are, so to speak, relations all the way down, all the way up, and all the way out in every direction: you never reach something which is not just one more nexus of relations.
<cite>
Richard Rorty, <a href="http://en.wikipedia.org/wiki/Philosophy_and_Social_Hope">Philosophy and Social Hope</a>, p 53-54.
</cite>
</blockquote>

But never mind that...The real advantage of a data model like RDF is that at its core it is all about self-description and linky-ness. Unfortunately I think this advantage is also the biggest stumbling block to understanding for someone who is new to Linked Data and RDF. As an example consider this stripped down RDF representation of the SKOS concept for "World Wide Web" at <a href="http://id.loc.gov/authorities/sh95000541">id.loc.gov</a>:

<pre lang="xml" escaped="true">
&lt;rdf:RDF
   xmlns:dcterms="http://purl.org/dc/terms/"
   xmlns:owl="http://www.w3.org/2002/07/owl#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:skos="http://www.w3.org/2004/02/skos/core#"&gt;
 &lt;rdf:Description rdf:about="http://id.loc.gov/authorities/sh95000541#concept"&gt;
    &lt;rdf:type rdf:resource="http://www.w3.org/2004/02/skos/core#Concept" /&gt;
    &lt;skos:prefLabel xml:lang="en"&gt;World Wide Web&lt;/skos&gt;
    &lt;skos:broader rdf:resource="http://id.loc.gov/authorities/sh88002671#concept" /&gt;
    &lt;skos:narrower rdf:resource="http://id.loc.gov/authorities/sh2002000569#concept" /&gt;
    &lt;skos:related rdf:resource="http://id.loc.gov/authorities/sh92002816#concept" /&gt;
    &lt;skos:closeMatch rdf:resource="http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb13319953j" /&gt;
  &lt;/rdf:Description&gt;
&lt;/rdf:RDF&gt;
</pre>


<prefix>
SELECT ?a ?b 
WHERE {?a owl:sameAs ?b .}
</prefix>

To get a result set like <a href="http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&should-sponge=&query=select+%3Fa+%3Fb+%0D%0Awhere+{%3Fa+owl%3AsameAs+%3Fb+.}&format=text%2Fhtml&debug=on&timeout=">this</a>.


<pre lang="xml" escaped="true">
&lt;rdf:RDF
  xmlns="http://purl.org/rss/1.0/"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:owl="http://www.w3.org/2002/07/owl#"
  xmlns:dc="http://purl.org/dc/elements/1.1"&gt;

  &lt;channel&gt;
    &lt;title&gt;Library of Congress Subject Headings&lt;/title&gt;
    &lt;link&gt;http://id.loc.gov/authorities&lt;/link&gt;
    &lt;items&gt;
      &lt;rdf:Seq&gt;
        &lt;rdf:li resource="http://id.loc.gov/authorities/sh95000541#concept" /&gt;
      &lt;/rdf:Seq&gt;
    &lt;/items&gt;
  &lt;/channel&gt;

  &lt;item rdf:about="http://id.loc.gov/authorities/sh95000541#concept"&gt;
    &lt;title&gt;World Wide Web&lt;/title&gt;
    &lt;link&gt;http://id.loc.gov/authorities/sh95000541#concept&lt;/link&gt;
    &lt;dc:date&gt;2009-11-02T16:11:02-04:00&lt;/dc:date&gt;
    &lt;owl:sameAs&gt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb13319953j&lt;/owl:sameAs&gt;
  &lt;/item&gt;

&lt;/rdf:RDF&gt;
</pre>

<pre lang="xml">
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dcterms="http://purl.org/dc/terms/">
  <title>Library of Congress Authorities</title>
  <id>http://id.loc.gov/authorities/feed</id>
  <updated>2009-11-11T20:01:30-04:00</updated>
  <link href="http://id.loc.gov/authorities/feed/" rel="self" />

  <entry>
    <title>World Wide Web</title>
    <id>http://id.loc.gov/authorities/sh85080003</id>
    <updated>2009-11-02T16:11:02-04:00</updated>
    <author><name>Library of Congress</name></author>
    <link rel="alternate" href="http://id.loc.gov/authorities/sh85080003" />
    <link rel="http://www.w3.org/2002/07/owl#sameAs" href="http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb13319953j" />
  </entry>

  <!-- more entries here -->

</feed>
</pre>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1470</wp:post_id>
		<wp:post_date>2009-11-11 22:48:08</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title> </title>
		<link>http://inkdroid.org/?p=1515</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1515</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1515</wp:post_id>
		<wp:post_date>2009-12-01 13:16:04</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=1592</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1592</guid>
		<description></description>
		<content:encoded><![CDATA[The ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1592</wp:post_id>
		<wp:post_date>2010-01-05 07:17:10</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>openpub acquisition links</title>
		<link>http://inkdroid.org/?p=1672</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1672</guid>
		<description></description>
		<content:encoded><![CDATA[acquisition links in:

  http://www.feedbooks.com/books/recent.atom

as compared with google books feed:

  http://code.google.com/apis/books/docs/gdata/developers_guide_protocol.html
  http://books.google.com/books/feeds/volumes?q=web

similar use of opensearch, dublincore
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1672</wp:post_id>
		<wp:post_date>2010-02-10 11:09:01</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=1675</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1675</guid>
		<description></description>
		<content:encoded><![CDATA[I'm planning on being at the JISC's <a href="http://dev8d.org">dev8d</a> workshop next week (Feb 24-27). One of the many interesting things that the dev8d organizers have done in preparation for the event was installing <a href="http://semantic-mediawiki.org/wiki/Semantic_MediaWiki">Semantic Media-Wiki</a> for people to use to self-organize around the various events. As part of the registration process people were asked about their homepages, blogs and twitter usernames, and were given a checkbox to grant dev8d the ability to publish that information. This allowed the ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1675</wp:post_id>
		<wp:post_date>2010-02-16 05:44:54</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>NAL Thesaurus</title>
		<link>http://inkdroid.org/?p=3819</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3819</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3819</wp:post_id>
		<wp:post_date>2013-05-10 01:29:02</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=4313</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4313</guid>
		<description></description>
		<content:encoded><![CDATA[http://scholarlykitchen.sspnet.org/2012/03/07/e-journal-preservation-and-archiving-whether-how-who-which-where-and-when/
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4313</wp:post_id>
		<wp:post_date>2013-05-10 01:28:38</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=4429</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4429</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="https://twitter.com/#!/gavialib">Library Loon</a>'s recent <a href="http://gavialib.com/2012/04/discovery-layers-and-metadata/">post</a> highlights that memory institutions are starting to understand that open licensing of the data sets they release to the public is important. Those that know the Library Loon's alter ego will know that this is no new topic for her. Until very recently I've mentally steered clear of licensing issues because they seem to inevitably involve lawyers, and institutional politics that seem beyond the abilities of a software developer working away in the code mines. Occasionally when I've had to work with 3rd party data I've griped about <em>other people's</em> data policies, but only <a href="http://inkdroid.org/2012/03/27/cc0-and-git-for-data/">recently</a> have I grown up enough (still lots of growing to do) to take a look at the licensing issues around data that I have created. I've rationalized that it's better to concentrate on the technical issues of making data available on the Web using an often implicit public domain license, than to break off my lance on getting an explicit license. But recent developments I think show that 

I'm just going to echo what Library Loon has already said, because it seems very important:

On April 4th Nature Publishing Group <a href="http://www.nature.com/press_releases/linkeddata.html">released</a> metadata for 450,000 articles that they have published since 1869 using the <a href="http://creativecommons.org/publicdomain/zero/1.0/">CC0 license</a>. This allows people to not just build toy experiments, but real production services around the data in a way that they couldn't do with confidence before. 

On 

LOD-LAM ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4429</wp:post_id>
		<wp:post_date>2013-05-10 01:28:37</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=4582</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4582</guid>
		<description></description>
		<content:encoded><![CDATA[These instructions rely on you having <a href="http://s3tools.org/s3cmd">s3cmd</a> installed and configured with your AWS credentials. 

<h2>Create the S3 Bucket</h2>

To get started you'll first need to create an S3 bucket where your input and output files will live.

<pre>
s3cmd mb s3://viaf/
</pre>

<h2>Copy Data Files to S3</h2>

First we'll copy the VIAF RDF/XML file up to S3. Note, to save on storage we don't decompress the gzip file (more on that later).

<pre>
s3cmd put viaf-20120422-clusters-rdf.xml.gz s3://viaf/
</pre>

We also need to copy our little program that reads the RDF/XML and outputs ntriples which was talked about <a href="http://inkdroid.org/2012/05/15/diving-into-viaf/">previously</a>. This is our map function, so it's called map.py (it could be called anything though):

<pre lang="python">
#!/usr/bin/env python

import sys

import rdflib

sys.stderr.write(rdflib.__version__ + "\n");

for line in sys.stdin:
    g = rdflib.Graph()
    g.parse(data=line)
    print g.serialize(format='nt').encode('utf-8'),
</pre>

And finally since the Amazon AMI does not come with rdflib installed we create a simple shell script to install it, by first installing <a href="http://pypi.python.org/pypi/pip">pip</a> (Python's package manager) and then using pip to install rdflib.

<pre>
s3cmd put map.py s3://viaf/map.py
</pre>

<pre lang="bash">
#!/bin/sh

sudo apt-get --assume-yes install python-pip
sudo pip install rdflib
</pre>

Put bootstrap.sh into the bucket, more on how this gets executed later:

<pre>
s3cmd put bootstrap.sh s3://viaf/map.py
</pre>

<h2>Create the Workflow</h2>

This step is where the primary bits of the workflow are defined. The input source is set to the gzipped RDF/XML that we copied to s3. An s3 location is picked for the output file. The mapper is set to the python program we uploaded (map.py). <a href="http://hadoop.apache.org/common/docs/current/streaming.html#Specifying+Map-Only+Jobs">map only job</a>

<a href="http://inkdroid.org/images/viaf-emr-1.png"><img src="http://inkdroid.org/images/viaf-emr-1.png" style="border: thin solid gray"/></a>

<h2>Specify Parameters</h2>

<a href="http://inkdroid.org/images/viaf-emr-2.png"><img src="http://inkdroid.org/images/viaf-emr-2.png" style="border: thin solid gray"/></a>

<h2>Configure EC2 Instances</h2>

<a href="http://inkdroid.org/images/viaf-emr-3.png"><img src="http://inkdroid.org/images/viaf-emr-3.png" style="border: thin solid gray"/></a>

<h2>Advanced Options</h2>

<a href="http://inkdroid.org/images/viaf-emr-4.png"><img src="http://inkdroid.org/images/viaf-emr-4.png" style="border: thin solid gray"/></a>

<h2>Bootstrap Options</h2>

<a href="http://inkdroid.org/images/viaf-emr-5.png"><img src="http://inkdroid.org/images/viaf-emr-5.png" style="border: thin solid gray"/></a>

<h2>Start Your Engines!</h2>

<a href="http://inkdroid.org/images/viaf-emr-6.png"><img src="http://inkdroid.org/images/viaf-emr-6.png" style="border: thin solid gray"/></a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4582</wp:post_id>
		<wp:post_date>2013-05-10 01:28:35</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>to Mark</title>
		<link>http://inkdroid.org/?p=5535</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5535</guid>
		<description></description>
		<content:encoded><![CDATA[Wow, there's a lot to investigate here, in the best possible ways; so thanks so much for writing it down. 

Not to get too meta or synecdochic, but I wonder: is your ability to write down your thoughts about emotion, archives, hypertext, and linked data here, in your blog, while providing contextual links out to other content (the articles and books you mentioned), and sharing it with the world (assuming computer of some ilk with Internet connection) using open standards ... do you consider all that to be emblematic of the work you are describing? Where do you see the gaps, or areas for improvement in your experience of writing this blog post? I know you've take a hiatus from writing in your blog. Were the reasons for that largely emotional: not having anything to say, or, not having anything to say to "the world"; or related to the editing tools -- all the above, something else?

It seems that one thing that Facebook offers people is an audience to share with, or create for. The platform gives people someone to write for, someone to share pictures with -- a context for creation. When I write in my blog, I often struggle to know who I am writing it for, and lately have come to understand that writing just for me is perhaps acceptable -- but somewhat unsatisfying. Since you kickstarted my interest in archives, and reading some of Tim's blog posts, I've found myself thinking more and more about improved description environments (that use Linked Data). One of the amazing things that archivists do is to try to document the context of their collections so other people can find them,

I know it's nerdy, but one of the most amazing experiences I've had in my life was clicking on a hypertext link and mentally "going somewhere" else, talking to another computer somewhere else, and getting a little chunk of digestible information back from it, where the context had shifted. So I really am bonding with you about the importance of hypertext. Tim Berners-Lee did an amazing technical and political thing in creating the Web, and allowing anyone with a text editor to compose a document that linked out to anywhere on the Web, without needing to ask for permission.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5535</wp:post_id>
		<wp:post_date>2013-05-10 01:26:58</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Hello WordPress</title>
		<link>http://inkdroid.org/2005/04/24/hello-wordpress/</link>
		<pubDate>Mon, 25 Apr 2005 01:40:37 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=2</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Hello WordPress, bye bye custom blog code written in Perl. Well the old code is still <a href="http://www.inkdroid.org/perl/b">running</a>, but I've wanted to install WordPress for the past few months and finally got around to it this weekend.  I had a little bit of trouble getting PHP installed, only because I decided to use the older php4 with the latest mysql, and php4 didn't seem to want to configure itself using the latest mysql. Fortunately using php5 was a different story and WordPress was a breeze to install.</p>

<p>My reasons for switching from my homegrown code to WordPress are several.</p>

<ul>
        <li>there was really no way of commenting on stories, only adding them.
        </li><li>the old code didn't really archive or categorize stories the way I wanted to</li>
    <li>links to stories didn't work, and I wanted to join dan's <a href="http://web.archive.org/web/20050930004259/http://curtis.med.yale.edu:80/code4lib/">Planet #code4lib</a>.</li>
        <li>I didn't use the RSS aggregation features I wrote since I started using <a href="http://www.bloglines.com">Bloglines</a>.</li>
         <li>I've been coding more in Python these days and don't feel particularly tied to my Perl code base any longer. WordPress is PHP, which I'm not a huge fan of, but I think this had more to do with the PHP that I was exposed to more than the language itself. Installing WordPress and the various plugins like the <a href="http://web.archive.org/web/20090206085517/http://www.tedpearson.com:80/blog/?page_id=778">audioscrobbler</a> one you see to the right was very pleasant.</li>
        <li>the WordPress community is extremely rich. I spent some time with Kesa looking at different themes, but in the end decided to stay with the default for now. There are tons of neat plugins to look at.</li>
</ul>

<p>So what you can expect here is more of the same. I'm going to try to write more about my work as a programmer, mainly as a journal for myself to keep track of what I'm working on, where I've been, and where I'd like to go. Perhaps you are thinking spare me the details, where are the pictures of Chloe?! If this is the case you should see a link to the photos over on the right.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2</wp:post_id>
		<wp:post_date>2005-04-24 18:40:37</wp:post_date>
		<wp:post_date_gmt>2005-04-25 01:40:37</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>hello-wordpress</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>HTML/HTTP </title>
		<link>http://inkdroid.org/2005/04/29/htmlhttp/</link>
		<pubDate>Fri, 29 Apr 2005 16:31:35 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=3</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.ietf.org/rfc/rfc2397">RFC 2397</a> has been around since August 1998 and I'm just <a href="http://bitworking.org/news/Sparklines_in_data_URIs_in_Python">learning</a> about the data URL scheme today. Perhaps browser support for it is new? Basically data URLs allow you to embed data, like images directly in an HTML page. Data URLs remind me of Fred Lindberg's old idea (circa 2001) of "mailing the web" by freezing web pages as email with MIME attachments.</p>

<p>

</p>

<p>It's fun to be learning new things about HTML/HTTP: technologies that I thought I was familiar with already. Perhaps I've been out of web development for long enough to fall behind.  The other day I learned about <a href="http://www.dyn-web.com/dhtml/iframes/">iframes</a> from my friend and sometime coworker <a href="http://www.multiply.org/notebook/">Jason</a> and was similarly blown away by something new under the sun. iframes are esentially the same things as regular frames but for the browser user they don't see separate panes. Useful for scrolling panels inside of pages and other things I'm sure.</p>

<p>I guess this is all part of the <a href="http://web.archive.org/web/20080907174711/http://www.adaptivepath.com/publications/essays/">web renaissance</a> that is going on now, spurred on by Google's forays and investment in <a href="http://web.archive.org/web/20080821140921/http://www.adaptivepath.com/publications/essays/archives/000385.php">javascript and xml</a>. It's really interesting to see how a big player like Google can redefine what is acceptable technology to rely on in web applications. For years I've avoided doing too much in javascript since it was a headache to get it working across different browsers, at least for this programmer. Now javascript is on my list of things to learn more about.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3</wp:post_id>
		<wp:post_date>2005-04-29 09:31:35</wp:post_date>
		<wp:post_date_gmt>2005-04-29 16:31:35</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>htmlhttp</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>2</wp:comment_id>
			<wp:comment_author><![CDATA[plural]]></wp:comment_author>
			<wp:comment_author_email>jason@multiply.org</wp:comment_author_email>
			<wp:comment_author_url>http://www.multiply.org/notebook/</wp:comment_author_url>
			<wp:comment_author_IP>67.175.194.127</wp:comment_author_IP>
			<wp:comment_date>2005-04-30 19:05:20</wp:comment_date>
			<wp:comment_date_gmt>2005-05-01 02:05:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[oh man.  Don't make it sound like i am a gung ho iframe fan.  :)

They can definitely be awesome.  Or they can make simple links a complicated mess of javascript that is *slightly* different on 40 pages.  Not that i know from experience.  

teehee

other than that, when used careful, they are a powerful tool.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>4</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>name authority fun</title>
		<link>http://inkdroid.org/2005/05/01/4/</link>
		<pubDate>Mon, 02 May 2005 02:39:34 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=4</guid>
		<description></description>
		<content:encoded><![CDATA[<p>As a joke <a href="http://web.archive.org/web/20070527073140/http://curtis.med.yale.edu:80/dchud/log">dchud</a> suggested that panizzi (the friendly neighborhood bot in  <a href="irc:://irc.freenode.net/#code4lib">#code4lib</a>) should have a plugin for querying the Library of Congress Name Authority File that OCLC <a href="http://alcme.oclc.org/eprintsUK/index.html">provides</a>.  The Name Authority File allows librarians the world over to use the same established names when cataloging books, etc. It would serve no purpose in irc, but it could be a good conversation piece...</p>

<p>I had goofed around writing a command line <a href="/code/tools/naf">app</a> about half a year ago so I figured it couldn't be that hard to hack this into  the <a href="http://www.infobot.org">infobot</a> source code. However I guessed wrong...granted I only tried for 30 minutes or so.</p>

<p>Fortunately, python's <a href="http://supybot.com">supybot</a> was a different story. It's more modern, has command line programs for configuring a supybot, has built in support for plugins -- and has <em>documentation</em>. There is even a command line program <em>supybot-newplugin</em> that will ask a few questions and then autogenerate a template plugin module. All you have to do after that is add a method (with a particular signature given in the docs) which will then do the work and respond.</p>

<pre lang="python">

from urllib import urlencode
from urllib2 import urlopen
from elementtree.ElementTree import parse

class Naf(callbacks.Privmsg):   

    def naf(self,irc,msg,args):
        """&lt;name&gt;

        Lookup a personal name in the NAF file at OCLC
        """

        alcme = "http://alcme.oclc.org/eprintsUK/services/NACOMatch"
        name = privmsgs.getArgs(args)
        query = urlencode( { \
            "method"          : "getCompleteSelectedNameAuthority",
            "serviceType"     : "rest",
            "name"            : name,
            "maxList"         : "10",
            "isPersonalName"  : "true" } )

        url = urlopen( alcme + "?" + query)
        tree = parse(url)
        elem = tree.getroot()

        matches = elem.find("wordMatches").getchildren()
        irc.reply( matches[0][0].text )

</pre>

<p>As an added bonus along the way I got to try out <a href="http://effbot.org/zone/element-index.htm">ElementTree</a> which has to be the nicest XML library I've ever used.  It turned out to be a fun experiment, and will hopefully add to the merriment of the room.</p>

<pre>
22:03 < edsu> panizzi naf sigmund freud
22:03 < panizzi> edsu: Freud, Sigmund,--1856-1939
</pre>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4</wp:post_id>
		<wp:post_date>2005-05-01 19:39:34</wp:post_date>
		<wp:post_date_gmt>2005-05-02 02:39:34</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>4</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="irc"><![CDATA[irc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>MARC, Perl and Unicode</title>
		<link>http://inkdroid.org/2005/05/05/marc-and-unicode/</link>
		<pubDate>Fri, 06 May 2005 03:33:17 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=5</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I've been doing some work for <a href="http://web.archive.org/web/20080521042856/http://library.tamu.edu:80/portal/site/Library">Texas A&amp;M</a> who need a <a href="http://search.cpan.org/dist/MARC-Record">MARC::Record</a> module that is Unicode safe. Many ILS vendors are moving away from MARC-8 encoded records towards Unicode. No doubt this move is being spurred on by big players like OCLC who are moving (or have moved) their mammoth <a href="http://www.oclc.org/worldcat/default.htm">WorldCat</a> database to Unicode.</p>

<p>At any rate Texas A&amp;M have workflows that use MARC::Record for transforming records in their catalog and they need the Unicode support for their new Voyager system. Technically there were very few places where MARC::Record needed to be adjusted. The <a href="http://rt.cpan.org/NoAuth/Bug.html?id=3707">problem</a> is that the antiquated transmission format for MARC records uses byte lengths in the so called directory, as offsets into the record. MARC::Record uses length() and substr() to create and work with the directory...which works fine when 1 character equals 1 byte. However, Unicode characters can have multiple bytes per character...so the character oriented length() will create faulty record directories, and substr() will extract data from the rest of the record incorrectly.</p>

<p>Fortunately there is the bytes pragma which alters the behavior of various character oriented Perl functions. Unfortunately these functions were added to Perl relatively recently, so this new version of MARC::Record will require Perl >= v5.8.2. Technically it could run on 5.8.1, however I found that the 5.8.1 that ships with OS X 10.3 lacks the bytes::substr(). Not only that but if you <a href="http://groups-beta.google.com/group/perl.unicode/browse_frm/thread/87bc31c03bbaa305/5fcbeb78c003164e?q=ed+summers&rnum=2#5fcbeb78c003164e">try</a>  to call a non existent function in the bytes namespace you'll go into an infinite loop. This is even the case with Perl 5.8.6 as well.</p>

<p>All in all I really have come to dislike Perl's Unicode support. The magical utf8 flag on scalars has a tendency to pop on and off for obscure reasons. And I've found the behavior of bytes::length() to be a bit unpredictable. Surely this is because I don't fully understand the mechanics involved, but judging from the traffic on perl-unicode I'm not the only one who has struggled with it.  My experience using unicode in Java and Python has been much more pleasant, and really confirms my decision to move towards doing new work in these languages.  Perl has served me well, and there are some things I really love about the language, but these nasty corners are a bit scary.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5</wp:post_id>
		<wp:post_date>2005-05-05 20:33:17</wp:post_date>
		<wp:post_date_gmt>2005-05-06 03:33:17</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>marc-and-unicode</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="category" nicename="perl"><![CDATA[perl]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=2825</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2825</guid>
		<description></description>
		<content:encoded><![CDATA[In 2010 I hope to:

<ol>
	<li>Listen more, especially to those that are close to me.</li>
	<li>Volunteer, to help make the world a better place.</li>
	<li>Meditate.</li>
</ol>

Happy New Year!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2825</wp:post_id>
		<wp:post_date>2013-05-10 01:29:47</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>surrogates</title>
		<link>http://inkdroid.org/?p=2997</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2997</guid>
		<description></description>
		<content:encoded><![CDATA[author card : author :: html : author]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2997</wp:post_id>
		<wp:post_date>2013-05-10 01:29:45</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=3599</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3599</guid>
		<description></description>
		<content:encoded><![CDATA[I heard via <a href="http://twitter.com/#!/ostephens/status/99412020557918208">Owen Stephens</a> this morning that BBC Music has (recently?) discontinued access to their RDF representations.

You can see from the Beach House

http://web.archive.org/web/20100421151331/http://www.bbc.co.uk/music/artists/d5cc67b8-1cc4-453b-96e8-44487acdebea

<pre>
&lt;link rel="alternate" type="application/rdf+xml" href="http://web.archive.org/web/20100421151331/http://www.bbc.co.uk/music/artists/d5cc67b8-1cc4-453b-96e8-44487acdebea.rdf" /&gt;
</pre>

http://web.archive.org/web/20100421151331/http://www.bbc.co.uk/music/artists/d5cc67b8-1cc4-453b-96e8-44487acdebea.rdf

<blockquote>
The site has been developed against the principles of linked open data and RESTful architecture where the creation of persistent URLs is a primary objective. The initial sources of data are somewhat limited to the artist page but this will hopefully be extended over time.

Here's our mini-manifesto:

Peristence
Linked open data
RESTful
One web
At present this is limited to the provision of JSON and XML representations of the artist page at /music/artists/:mbz_guid.[xml|json] We hope soon to extend this to offer reviews in a similar manner, and to expose RDF formatted data for these resources.
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3599</wp:post_id>
		<wp:post_date>2013-05-10 01:29:04</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=5806</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5806</guid>
		<description></description>
		<content:encoded><![CDATA[<h3>Defense:</h3>
<ul>
	<li>manning honor pfc correct government wikileaks information defense court didn exhibit computer witness don</li>
	<li>ma machine logs information gal active agent objection directory agree dot centcom shaver classified</li>
	<li>correct ma wikileaks agree document prosecution machine agent page johnson talked doesn honor dot</li>
	<li>documents secret shortcut news cds prove american basis benkler prior expect classified force specifications</li>
	<li>information access put analyst siprnet database computer drive sigacts enemy based time open scif</li>
</ul>

<h3>prosecution</h3>
<ul>
<li><ma information government evidence exhibit time classified accused agent prosecution product training remember cables</li>
<li>honor evidence information manning computer show logs pfc prosecution agent exhibit case file wikileaks</li>
<li>cable telegram affairs state classified assistant deputy eo secretary embassy department bureau official confidential</li>
<li>ma information honor states united sir exhibit court prosecution testimony inaudible stipulation defense classified</li>
<li>manning pfc honor information prosecution training computer long exhibit mr fob talk position hammer
</li>
</ma></li></ul>

<h3>Witnesses</h3>
	<li>sir correct file computer files dot inaudible don user line number manning time log
</li>
	<li>information correct document intelligence time documents military wikileaks website don basically security open department
</li>
	<li>sir system network user inaudible army mail access software server program gal systems don
</li>
	<li>wikileaks journalism organization public materials network report people things research media article source news
</li>
	<li>sir ma information don time correct remember intelligence shift didn training computer day back</li>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5806</wp:post_id>
		<wp:post_date>2013-07-18 17:57:05</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>harvesting</title>
		<link>http://inkdroid.org/?p=6957</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6957</guid>
		<description></description>
		<content:encoded><![CDATA[<ul>
<li>trove one-off harvesters</li>
<li>supplejack - scraping vs harvesting</li>
<li>implications for dpla</li>
<li>harvesting web splash pages is always needed anyhow</li>
</ul>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6957</wp:post_id>
		<wp:post_date>2013-12-02 12:45:56</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=6984</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6984</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A few days ago my friend and colleague <a href="https://twitter.com/footnotesrising">Susan Garfinkel</a> let me know about a new article by <a href="https://twitter.com/alexismadrigal">Alexis Madrigal</a> about Netflix's genre categories:</p>

<blockquote class="twitter-tweet" lang="en">
  <p>
    .<a href="https://twitter.com/edsu">@edsu</a> would love your thoughts on how <a href="https://twitter.com/alexismadrigal">@alexismadrigal</a>'s netflix discovery relates to lc subject headings. <a href="http://t.co/o47djGEZuw">http://t.co/o47djGEZuw</a>
  </p>— susan garfinkel (@footnotesrising) 
  
  <a href="https://twitter.com/footnotesrising/statuses/418850443209355264">January 2, 2014</a>
</blockquote>

<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> 
Discussion about it then popped up on a few discussion lists I'm on. If you haven't <a href="http://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/">read it already</a> Alexis does a really nice job of telling a computational detective story involving a bot, linguistic analysis, <a href="http://www.theatlantic.com/static/front/html/netflix-generator/netflix.html">humor</a>, a hidden algorithm and its creator. If you prefer to listen he was <a href="http://www.npr.org/blogs/alltechconsidered/2014/01/02/259128268/netflix-built-its-microgenres-by-staring-into-the-american-soul">recently on NPR</a> as well.</p>

<p>What I didn't know until reading the article was that <a href="http://www.bogost.com/">Ian Bogost</a> was involved in the work as well. This added an extra juicy dimension to the article since I read (and enjoyed) his <a href="http://www.bogost.com/books/alien_phenomenology_1.shtml">Alien Phenomenology</a> last year.</p>

<p>So Susan asked about the relation to the Library of Congress Subject Headings (LCSH). A few years ago I created a prototype that made LCSH available on the Web, that went on to form part of the service at <a href="http://id.loc.gov">id.loc.gov</a>. I cataloged dissertations between classes at Rutgers, but I do not consider myself a cataloger, let alone a subject cataloging expert...so please apply grains of salt.</p>

<p>Just as Netflix has a 36 page document (Netflix Quantum Theory) that describes how they assign "microtags", and rules for generating genres from these microtags, the Library of Congress has its own <a href="http://id.loc.gov/authorities/subjects.html">list of subjects</a> and <a href="http://www.loc.gov/aba/publications/FreeSHM/freeshm.html">rules</a> for combining them. Both LCSH and Netflix have people assign the subject tags.</p>

<ul>
<li><p>Scale: There are currently 412,798 LCSH headings, and 76,897 Netflix genre terms. The LCSH number doesn't include all the possible combinations as the Netflix number does. So in a way the difference in scale is even larger than the numbers reflect. The difference makes sense given how long LCSH has been used, and the larger universe of things that it is used to describe. But then again, Netflix has limited the number of possible genre terms based on usability requirements ... possibly something that should've been done more with LCSH.</p></li>
<li><p>Rules: Historically the rules to create a LC subject heading have been applied by people (catalogers around the world). The rules that generate Netflix's genres are applied automatically by an algorithm.</p></li>
<li><p>he accidentally ran into the contours</p></li>
</ul>

<iframe border="0" class="no-rescale" height="300" scrolling="no" src="http://www.theatlantic.com/static/front/html/netflix-generator/netflix.html" style="width: 100%"></iframe>

<p>generative</p>

<p>lower." http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html How did they connect these?</p>

<p>funny that some are empty http://movies.netflix.com/WiAltGenre?agid=49999 http://movies.netflix.com/WiAltGenre?agid=59999</p>

<p>max that i found: http://movies.netflix.com/WiAltGenre?agid=93116</p>

<p>how did the sketch of the american soul table get generated?</p>

<p>how many microtags are assigned for each movie?</p>

<p>rules for generating genres</p>

<p>And nothing highlights their personalization like throwing you a very, very specific altgenre. being able to say why is important</p>

<p>"Imagine if Facebook broke down individual websites according to a 36-page tagging document that let the company truly understand what it was people liked about Atlantic or Popular Science or 4chan or ViralNova?" Do we ever understand why?</p>

<p>"The vexing, remarkable conclusion is that when companies combine human intelligence and machine intelligence, some things happen that we cannot understand." Same is true in the human world too."</p>

<p>The eye of software staring into the American soul. haha, nsa</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6984</wp:post_id>
		<wp:post_date>2014-01-04 17:17:26</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=7240</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7240</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I've been thinking recently about expressing search results as JSON-LD. JSON-LD makes lists so much easier. Dave Reynolds pointed me at the Linked-Data-API's use of the OpenSearch namespace with their collection responses</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7240</wp:post_id>
		<wp:post_date>2014-02-27 03:53:34</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>web25</title>
		<link>http://inkdroid.org/?p=7304</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7304</guid>
		<description></description>
		<content:encoded><![CDATA[<p>25 years</p>

<ul>
<li>ndf video / text</li>
<li>not just traditional web archiving (heritrix)</li>
<li>web archive packages (twitter, facebook)</li>
<li>ndf talk, opportunity to think about where I am</li>
<li>stung a bit that it was old news</li>
<li>work on openwayback, ghost, pywb</li>
<li>commercial companies doing webarchiving</li>
<li>video demo of running wget to get an warc file</li>
</ul>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7304</wp:post_id>
		<wp:post_date>2014-03-11 08:37:08</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=7340</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7340</guid>
		<description></description>
		<content:encoded><![CDATA[<pre language="python">#!/usr/bin/env python

import os
import re
import sqlite3

home = os.path.expanduser("~")
dbfile = home + "/Library/Application Support/Google/Chrome/Default/History"
db = sqlite3.connect(dbfile)

q = """
    SELECT url, title, visit_count, datetime(last_visit_time/1000000-11644473600,'unixepoch','localtime')
    FROM urls
    ORDER BY visit_count DESC
    """

for url, title, visit_count, last_visit in db.execute(q):
    if re.match('http://(.+)\.wikipedia\.org/wiki', url):
        print ("* [%s](%s) - %s - %s" % (title, url, last_visit, visit_count)).encode('utf-8')

db.close()
</pre>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7340</wp:post_id>
		<wp:post_date>2014-03-30 07:15:18</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_post_restored_from</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:20:"restored_revision_id";i:7342;s:16:"restored_by_user";i:2;s:13:"restored_time";i:1396188809;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>MayOne</title>
		<link>http://inkdroid.org/?p=7479</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7479</guid>
		<description></description>
		<content:encoded><![CDATA[
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7479</wp:post_id>
		<wp:post_date>2014-05-01 11:21:29</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=7636</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7636</guid>
		<description></description>
		<content:encoded><![CDATA[<p>George:</p>

<ul>
<li>convo w/ dan, david and michael about disruption</li>
<li>semweb dc</li>
<li>getting id.loc.gov</li>
<li>dcat / egov at w3c</li>
<li>ckan at hhs</li>
<li>github / json-ld</li>
<li>india</li>
</ul>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7636</wp:post_id>
		<wp:post_date>2014-06-20 04:00:10</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=7949</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7949</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Tweets w/ links from within a 5 mile radius of</p>

<p>1632 2014-08-18
   3906 2014-08-17
   2753 2014-08-16
   3434 2014-08-15
   3857 2014-08-14
   4648 2014-08-13
   3749 2014-08-12
   3295 2014-08-11
    183 2014-08-10</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7949</wp:post_id>
		<wp:post_date>2014-08-18 10:52:16</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=8104</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8104</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Over at <a href="http://mith.umd.edu">MITH</a> we have a fair number of WordPress sites for events and projects that are now finished, but which are still of high value to the organization. MITH has been doing neat stuff in the digital humanities for 15 years, so there is quite a bit of this content.</p>

<p>The things that make Wordpress such an awesome content management platform (editing, publishing, commenting, etc) are no longer needed for this content. However we do need to keep the content itself, since it documents what we did, learned and achieved as a group, and as a community. In short we want to archive these Wordpress websites, and make this archive available on the Web for people to use.</p>

<p>WordPress is a piece of software. <a href="https://medium.com/@mkirschenbaum/software-its-a-thing-a550448d0ed3">It's a thing</a>! As a piece of software there is an overhead to running a WordPress site, in terms of keeping Wordpress itself up to date, keeping PHP up to date, keeping the database up to date, backing up the database and media, managing the configuration, user accounts, etc. So computationally Wordpress and its environment are almost like a living thing, or a niche within an ecosystem...and life can get complicated. How can we decrease the complexity of this content, so that it is suitable for an archive, without killing it altogether?</p>

<p>One way of archiving these websites is to let the Internet Archive crawl them, and to point to the archived version. We can also package up the WordPress site (.php, media files, database dump, etc) and keep it somewhere, perhaps in an institutional digital repository, where it will hopefully survive as data urn, if not as a readily usable record of MITH's activity.</p>

<p>But another way of approaching this is to try to capture the content of the website, as static files, and then make those available on the Web for people to use.</p>

<p>The beautiful thing about the RESTful architecture of the Web, is that browsers request resources, but get back representations of those resources in generally understood formats like HTML, JPEG, PNG, etc. What's more the Web's predominant representation, HTML, contains <a href="https://en.wikipedia.org/wiki/HATEOAS">hypertext links</a> to other resources, which you can fetch representations of as well. Machine agents can find these links, and save the representations, and do stuff with them. It's what Google does when indexing your website, it's what the Internet Archive does to save pieces of the Web for viewing from their mammoth <a href="http://archive.org/web">archive</a>.</p>

<p>Traditional web archiving software like the Heretrix crawler will crawl a website given certain parameters, and save the crawled data as a WARC file. This WARC file is then made available using a playback mechanism, the most popular of which is the Wayback. Think of the WARC file as a single file that contains many HTTP requests and responses concatenated together. It's great that you can do this with open source software. But Wayback and Heretrix are used by a pretty small community, at least compared with Wordpress.</p>

<p>From a preservation perspective I'm not sure that it makes sense to swap out Wordpress site for Heretrix and Wayback instance that we run at MITH. If there were a Web archiving effort at the University of Maryland that we could piggyback on then perhaps that approach would make more sense. Simply relying on the Internet Archive's snapshot also seems a bit risky. Not that IA is going anywhere, and don't do great work. But it would be nice if there were something minimal we could do to create a snapshot of the site that we could make available ourselves.</p>

<p>Fortunately there is a relatively simple thing you can do if you find yourself in this situation -- you can use <a href="https://www.gnu.org/software/wget/">wget</a>. wget is a venerable piece of software from the Free Software Foundation that can fetch a single page or an entire website. Given its age and the runaway popularity of the Web, wget has accumulated a lot of options over the years. So it can be a bit intimidating to use. But it's a very handy tool, and worth getting to know.</p>

<p>While wget crawls a website it can be instructed to pull down images, stylesheets and JavaScript referenced in the pages, and rewrite the HTML to use those local URLs, rather than the ones on the Web. This is incredibly important from a preservation perspective because the crawled data is made internally consistent. If an image file was hosted externally, and suddenly it went away, the archived content would be damaged. By rewriting these links wget makes it possible for the snapshot to be self contained, and much more resilient.</p>

<pre>wget --mirror --page-requisites --html-extension --convert-links --wait 1 --execute robots=off --no-parent http://example.com/foo/
</pre>

<p>The trouble is that the Web isn't so simple anymore. JavaScript often runs in your browser to fetch content dynamically from APIs in order to assemble the page that you see. If web crawlers aren't executing this JavaScript then they won't find this data.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8104</wp:post_id>
		<wp:post_date>2014-09-12 17:40:50</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=8340</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8340</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The W3C is poised to publish a new document <a href="https://w3ctag.github.io/web-https/">Transitioning the Web to HTTPS</a>. The <a href="http://www.w3.org/2001/tag/">Technical Architecture Group</a> or TAG helpes guide the architecture of the Web, an d provides</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8340</wp:post_id>
		<wp:post_date>2014-12-19 08:22:13</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=8414</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8414</guid>
		<description></description>
		<content:encoded><![CDATA[<p>At yesterday's coding workshop we segued from topics of programming and software development to talking about data collections and services. I won't get into the particulars here, but at a high level the topic at hand was: how much do we as librarians and archivists get in the business of providing services around our data collections? Specifically we were talking about data services such as software that visualizes particular aspects of the collection.</p>

<p>I trotted out a (perhaps familiar) analogy that I first heard a few years ago from my colleague <a href="http://twitter.com/dbrunton">David Brunton</a>. I should check with David to learn where he heard it from. If you've heard it before I'd be interested to hear where you ran across it. Anyway, the analogy asks you to consider a <a href="https://en.wikipedia.org/wiki/User_story">user story</a> from the traditional library:</p>

<ol>
<li>patron enters the library</li>
<li>patron consults with a reference librarian</li>
<li>patron examines some reference material (bibliographies, indexes, the catalog)</li>
<li>patron locates some items</li>
<li>patron serendipitously browses the stacks to find some more items of interest</li>
<li>patron relaxes into comfy chair to read some items</li>
<li>patron decides to check out some books to read at home</li>
</ol>

<p>Notice that at no point in that user story does someone a librarian <em>read the book</em> for the patron. So why do we presume to create services to read data for them? Rather than trying to imagine all the ways someone might want to read and interpret a dataset (using a given programming language, operating system, application, statistical technique, research methodology) shouldn't librarians focus on getting the data to them so they can do those things?</p>

<p>Since I first heard it I've held fast to this analogy as a useful thought experiment, or at least a conversation piece, for examining what librarians and archivists do. But yesterday it struck me that perhaps it's become more of a platitude for me.</p>

<p>The analogy is focused on what librarians and archivists don't do, namely the intellectual work of reading and interpreting material. It quietly demeans the work of the librarian or archivist -- or at least sweeps this work under the carpet--it self effaces. Pay no attention to the librarian/archivist behind the curtain. Most of all it over simplifies all the intellectual and experimental work that has gone into the design of libraries, and technologies like the book.</p>

<p>Many of the people who work in libraries and archives tend to be prolific readers, listeners and creators. They don't perform these activities for library users, but they do recognize fellow travelers on the same road of <a href="http://www.amazon.com/Seeking-Meaning-Information-Libraries-Unlimited/dp/1591580943">seeking meaning</a>, and help them on their way by providing <em>services</em> like reference interviews, comfy chairs, indexes, catalogs and <em>collections</em>. Why should data be any different?</p>

<p>The catalog, the indexes, the discussion, the environment ... of course these are needed for digital content.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8414</wp:post_id>
		<wp:post_date>2015-01-27 10:18:17</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title></title>
		<link>http://inkdroid.org/?p=8631</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8631</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote class="twitter-tweet" lang="en">
  <p lang="en" dir="ltr">
    I have a dedicated troll base -- they literally sit in my mentions. And as of today, I've blocked 13,251 folks on here.
  </p>— deray mckesson (@deray) 
  
  <a href="https://twitter.com/deray/status/604782399906418688">May 30, 2015</a>
</blockquote>

<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> This is a sad and terrible reality public activists and thinkers like DeRay who engange openly in social media. I didn't quite understand the full dimensions of this till I heard Professor Brittany Cooper <a href="https://twitter.com/professorcrunk">@professorcrunk</a> speak about her activism as an academic at the <a href="http://bsos.umd.edu/event/2nd-annual-congressman-parren">Parren Mitchell Symposium on Intellectual Actiism</a> last month.</p>

<iframe src="http://livestream.com/accounts/949619/events/3991891/videos/85534077/player?autoPlay=false&height=360&mute=false&width=640" width="640" height="360" frameborder="0" scrolling="no"></iframe>

<p>Between May 22 and May 31 there were 15,191 of the 17,801</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8631</wp:post_id>
		<wp:post_date>2015-06-07 11:58:12</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Communication</title>
		<link>http://inkdroid.org/2005/05/06/tools/</link>
		<pubDate>Fri, 06 May 2005 20:55:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=6</guid>
		<description></description>
		<content:encoded><![CDATA[At my day job I've spent the better part of a month working on a nasty performance tuning problem in some software that I didn't actually write. Without going into much detail we have a distributed application that provides cover images (a la Amazon) to the websites and other applications at various divisions with Follett. There are multiple caching layers, and heavy use of 3rd party software such as <a href="http://lucene.apache.org">lucene</a> and <a href="http://jakarta.apache.org/tomcat/">tomcat</a>. The problem was the image query service would ocasionally take 10 times as long (or more) to service a request.

Initially I used a tool called <a href="http://jrat.sourceforge.net/">jrat</a> to profile the application in question to see where it was spending its time. jrat is a neat little application that uses the <a href="http://jakarta.apache.org/bcel/">Byte Code Engineering Library</a> to instrument Java class files so that they write timing information to a log file. jrat then has a visualization tool that lets you open the log and view timings for the various methods. After doing this it became clear that a large amount of time was being spent in searching the Lucene index.

So I isolated the searching component of the code and replicated the timeout behavior outside of the web container. Once I could replicate the behavior at will I was able to start turning knobs and switching switches to try to get better performance. One of the first obvious things I tried was to create one IndexSearcher object and share it across the threads. This helped a great deal and I was happy. Thinking that it was the creation of the Searchers which slowed things down I created a pool of IndexSearchers which the application drew from, and a worker thread that kept the pool full.  This change also worked well outside of Tomcat; however once it ran under Tomcat I saw the same delays. The test outside of Tomcat pushed the searching much harder that our web traffic ever did...so extrapolating from one to the other wasn't appropriate. I had fixed *a* problem but not *the* problem.

This is when depression set in...

After I had started to think clearly again I happened to have lunch with <a href="http://www.stresscafe.com/">Mike</a> who asked if JVM garbage collection could have anything to do with it. I practically slapped myself on the forehead. This is what all those articles warned me about when discussing Java and embedded software! I went back, turned on garbage collection logging and sure enough, every 10-20 seconds the JVM was spending sometimes around 2 seconds collecting a huge amount of memory.  I had a little log analysis tool that told me when the response times were exceeding 2 seconds, and sure enough these popped up while the full GC was running. What objects were chewing up that amount of space?

This is when <a href="http://www.jroller.com/page/bdaug">Bob</a> suggested giving the commercial Java tuning app <a href="http://www.yourkit.com">YourKit</a> a try. They have a fully functional 14 day demo which I got to run under RH Fedora Core 2 in fairly short order. YourKit can talk to a host of J2EE servers including Tomcat. On startup it asks what type of server you want it to attach to. After selecting Tomcat it goes and creates a new Tomcat startup script based on the existing one. After resetarting Tomcat YourKit is able to selectively log a ton of data from the running JVM, including memory usage.

<a href="/images/your_kit.jpg"><img src="/images/your_kit.jpg"  border="0"/ width="500"/></a>

This screen alone (click on it for a more readable version) showed that a large chunk of memory was being used up by all the IndexSearcher objects that were being created. So I had been right to focus on the IndexSearcher after all, but it wasn't that they were expensive to create, but that they resulted in a great deal of memory being used which caused the JVM to stall out while garbage collection was being done. I confirmed this by hacking the app to keep one IndexSearcher around and stress testing again, which performed nicely.

While I don't have a solution in code yet, this whole exercise has made it clear to me how important communication is in programming.  I always seem to get better results when talking to people I work with. It's so easy to get stuck in one way of looking at a problem, and discussion has a way of dislocating my perspective, challenging my assumptions, and bringing humor into a problem.  In addition good tools are worth their weight in gold. I spent far too much time guessing and testing when I could have used something like YourKit from the start.  One thing that has impressed me a lot about Java are the high quality development tools that are available.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6</wp:post_id>
		<wp:post_date>2005-05-06 13:55:03</wp:post_date>
		<wp:post_date_gmt>2005-05-06 20:55:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>tools</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="java"><![CDATA[java]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>20</wp:comment_id>
			<wp:comment_author><![CDATA[Bob Bamford]]></wp:comment_author>
			<wp:comment_author_email>bbamford@san.rr.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>66.27.104.187</wp:comment_author_IP>
			<wp:comment_date>2005-12-18 22:41:25</wp:comment_date>
			<wp:comment_date_gmt>2005-12-19 05:41:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I am keenly interested in how you proceeded with your IndexSearcher management.  We are experiencing the exact same issue that you seem to be dealing with.  We started with one IndexSearcher being shared across requests. What we found was that this IS continued to grow in size, incrementally, as it performed more and more searches.  Almost like it was retaining "hints" about its previous searches.  Well, the IS was inside a singleton object, that never was Garbage Collected.  The growing index and the fact that it was never GC'd caused the Index to grow until it consumed enough available memory to cause GC to fire more often, which caused a CPU spike/request processing delay, and eventually, an OOM error on Tomcat.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>CPAN Module Wins Library Award</title>
		<link>http://inkdroid.org/2005/05/11/cpan-module-wins-library-award/</link>
		<pubDate>Wed, 11 May 2005 12:15:22 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=7</guid>
		<description></description>
		<content:encoded><![CDATA[Jane Jacobs (the brains behind <a href="http://search.cpan.org/dist/MARC-Detrans">MARC::Detrans</a>) let me know that the CPAN module won the <a href="http://nylink.suny.edu/amtg05.htm">NYLink Achievement Award</a> for "Innovation in Technology" (search for Detrans, it's a big page). Good going Jane, Elizabeth and Stuart!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7</wp:post_id>
		<wp:post_date>2005-05-11 05:15:22</wp:post_date>
		<wp:post_date_gmt>2005-05-11 12:15:22</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>cpan-module-wins-library-award</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="perl"><![CDATA[perl]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>code4lib sprint</title>
		<link>http://inkdroid.org/2005/05/16/code4lib-face2face/</link>
		<pubDate>Mon, 16 May 2005 15:34:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=8</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A bunch of <a href="irc://irc.freenode.net/code4lib">#code4lib</a> folks will be converging on Chicago this summer for the annual American Library Association conference. Several of us thought it would be fun to get together for a <a href="http://c2.com/cgi/wiki?XpCodeSprint">sprint</a> on a project that has yet to be decided.  A potential project is building a framework for metadata translation similar to  <a href="http://www.scripps.edu/~cdputnam/software/bibutils/bibutils.html">bibutils</a> or perhaps <a href="http://gondolin.hist.liv.ac.uk/~cheshire/">Cheshire</a>. I worked on creating a bibutils wrapper for Python a few months ago, and decided it would be better to have a pure python framework instead. The wrapper worked ok, but only on particular platforms, and the API felt kludgy in that bibutils is oriented towards command line tools.  There's also some interest in having a discussion and possibly some hacking on mirroring OPACs that Art Rhyno and Ross Singer have been working on.</p>

<p>I called <a href="http://www.chipublib.org/">Chicago Public Library</a> to reserve some of their rooms but they're already all booked up. Fortunately the <a href="http://www.chipublib.org/002branches/lincoln/lincoln.html">Lincoln Park Branch</a> has a nice room (with wifi) which <a href="http://www.chipy.org">chipy</a> used for their pypi sprint a few months ago...and I just reserved the space for the entire day of Friday June 24th, 2005. My friend <a href="http://web.archive.org/web/20101127052904/http://brianray.chipy.org:80/">Brian Ray</a> from chipy kindly offered to stop by his local branch to fill out the paper work to make it official.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8</wp:post_id>
		<wp:post_date>2005-05-16 08:34:03</wp:post_date>
		<wp:post_date_gmt>2005-05-16 15:34:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>code4lib-face2face</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="irc"><![CDATA[irc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>one billion</title>
		<link>http://inkdroid.org/2005/05/16/oclc/</link>
		<pubDate>Mon, 16 May 2005 16:26:53 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=9</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://outgoing.typepad.com/outgoing/2005/05/_of_records_in_.html">Thom Hickey</a> mentioned a new <a href="http://www.oclc.org/worldcat/grow.htm">page</a> at OCLC which lists some real time stats for worldcat: total holdings, last record added, etc. Perhaps this is in honor of the total holdings getting very close to crossing the 1 billion mark. 

So of course I had to add a plugin for panizzi to scrape the page. Rather than writing yet another state machine for parsing html I decided to try out Frederik Lundh's <a href="http://effbot.org/zone/element-tidylib.htm">ElementTree Tidy HTML Tree Builder</a>, which works out very well when you want to walk a datastructure representing possibly invalid HTML. 

<pre lang="python">
    url = "http://www.oclc.org/worldcat/grow.htm"
    tree = TidyHTMLTreeBuilder.parse( urlopen( self.url ) )
</pre>

That's all there is to getting nice elementtree object which you can dig into for a page of HTML.

So, predictably:

<pre>
10:53 < edsu> @worldcat
10:53 < panizzi> edsu: [May 16, 2005 11:49 AM EDT #981,277,234] 
                      El senor de los anillos. Tolkien, J. R. R. ... 
                      uploaded by OEL - EUGENE PUB LIBR
</pre>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>9</wp:post_id>
		<wp:post_date>2005-05-16 09:26:53</wp:post_date>
		<wp:post_date_gmt>2005-05-16 16:26:53</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oclc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="irc"><![CDATA[irc]]></category>
		<category domain="post_tag" nicename="oclc-worldcat"><![CDATA[oclc worldcat]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>MARC::Record v2.0 RC1</title>
		<link>http://inkdroid.org/2005/05/20/marcrecord-v20-rc1/</link>
		<pubDate>Fri, 20 May 2005 20:52:39 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=10</guid>
		<description></description>
		<content:encoded><![CDATA[Thanks to the support of Anne Highsmith at Texas A&M MARC::Record v2.0 RC1 was released today to <a href="http://www.sf.net/projects/marcpm">sourceforge</a>. This new version of MARC::Record addresses the use of Unicode in MARC records. There has been a long standing <a href="http://rt.cpan.org/NoAuth/Bug.html?id=3707">bug</a> in MARC::Record which caused it to calculate record directories incorrectly when the records contained Unicode. This isn't hitting CPAN yet so that the people who want Unicode handling can take it for a test drive first. As noted previously this Perl/Unicode stuff is pretty tricky since most of the time the encoding of a scalar variable is sort of hidden from view. I'd much prefer to be in a situation like in Java where all strings are UTF-8.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>10</wp:post_id>
		<wp:post_date>2005-05-20 13:52:39</wp:post_date>
		<wp:post_date_gmt>2005-05-20 20:52:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>marcrecord-v20-rc1</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>pybibutils</title>
		<link>http://inkdroid.org/2005/06/09/pybibutils/</link>
		<pubDate>Thu, 09 Jun 2005 10:39:36 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=11</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The #code4lib sprint is coming up soon and (alas) we still don't really have a firm grasp on what we will be sprinting on. After pycon dchud had some ideas for a metadata wrangling framework for python. Around the same time I was working on <a href="http://www.swig.org/">SWIG</a> wrapper for the <a href="http://www.scripps.edu/~cdputnam/software/bibutils/bibutils.html">bibutils</a> library.  So one idea we had was to create this python utility that would enable converting between many of the popular metadata/citation formats:</p>

<ul>
<li><a href="http://www.loc.gov/standards/mods/">MARC</a></li>
<li><a href="http://www.loc.gov/standards/mods/">MODS</a></li>
<li><a href="http://en.wikipedia.org/wiki/BibTeX">BibTex</a></li>
<li><a href="http://www.endnote.com/">EndNote</a></li>
<li><a href="http://www.refman.com/">RIS</a></li>
<li><a href="http://www.isinet.com/">ISI</a></li>
</ul>

<p>Emerging details are available on the <a href="http://web.archive.org/web/20050827220707/http://wiki.inkdroid.org:80/code4lib/pybibutils">wiki</a>.  If you have <i>any</i> ideas for the sprint please note them on the <a href="http://wiki.inkdroid.org/code4lib">wiki</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>11</wp:post_id>
		<wp:post_date>2005-06-09 03:39:36</wp:post_date>
		<wp:post_date_gmt>2005-06-09 10:39:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pybibutils</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>pylucene</title>
		<link>http://inkdroid.org/2005/06/09/pylucene/</link>
		<pubDate>Thu, 09 Jun 2005 11:16:15 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=12</guid>
		<description></description>
		<content:encoded><![CDATA[I'm going to be doing a lightning talk tonight at the <a href="http://www.chipy.org">Chicago Python Group</a>  about <a href="http://pylucene.osafoundation.org/">pylucene</a>. pylucene essentially lets you use the popular <a href="http://lucene.apache.org">Lucene</a> indexing library (Java) in Python. No time limit has been set for the lightning talks (and <a href="http://husk.org/pics/x/trips/yapc_eu_paris_2003-07/people_and_places/mjd_and_lighting_talk_gong.jpg">mjd</a> won't be there with his gong) but I hope to quickly cover how to index an mbox with pylucene in 5 minutes. There are <a href="http://www.inkdroid.org/talks/pylucene">slides</a>, which are there mainly as cue cards.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>12</wp:post_id>
		<wp:post_date>2005-06-09 04:16:15</wp:post_date>
		<wp:post_date_gmt>2005-06-09 11:16:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pylucene</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="java"><![CDATA[java]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>govtrack</title>
		<link>http://inkdroid.org/2005/06/09/govtrack/</link>
		<pubDate>Thu, 09 Jun 2005 17:08:05 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=13</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.govtrack.us">GovTrack</a> has done some awesome work generating publicly available <a href="http://web.archive.org/web/20080517173534/http://www.govtrack.us/source.xpd">machine readable data</a> for US government information. After the last election I decided that I really wanted to get involved in some sort of volunteer technology/political activity, so I started googling and found GovTrack pretty much just starting up. Now there is a loose affiliation of similar sites (including GovTrack) called <a href="http://www.ogdex.com/">Ogdex</a> who are attempting to foster the collection of publicly available government information. In particular there has been some <a href="http://groups.yahoo.com/group/govtrack/message/138">talk</a> on the govtrack discussion list about local efforts to add state data to the collection of federal data...and even bounties for getting state data collection going.  I'm going to take a stab at writing some scraping utilities for gathering together Illinois data and will report back with how it goes. If you are interested in helping out details are <a href="http://www.ogdex.com/warehouse/space/Wish+List">available</a>.</p>

<p><i>Update</i>: Joshua just set up a new drupal <a href="http://web.archive.org/web/20060220120807/http://www.govtrack.us:80/dev/">site</a> for govtrack development.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>13</wp:post_id>
		<wp:post_date>2005-06-09 10:08:05</wp:post_date>
		<wp:post_date_gmt>2005-06-09 17:08:05</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>govtrack</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>lightning strikes</title>
		<link>http://inkdroid.org/2005/06/10/chipy/</link>
		<pubDate>Fri, 10 Jun 2005 15:05:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=14</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Chris has a nice <a href="http://weblog.lonelylion.com/?p=512">writeup</a> about last nights ChiPy lightning talks. There were tons of interesting people there with very interesting projects. Apart from the announcement that we might be hosting <a href="http://www.python.org/pycon/">PyCon</a> next year in Chicago, the highlight of the evening for me was hearing about the <a href="http://web.archive.org/web/20050707011548/http://www.cnn.com:80/2005/TECH/internet/06/09/google.map.hacks.ap/">amazing</a> data hack that is <a href="http://www.chicagocrime.org/">ChicagoCrime</a>.  <a href="http://www.holovaty.com/">Adrian</a> is a journalist/programmer who managed to glue together GoogleMaps with publicly available data from the Chicago Police Department. The main (perhaps unintended) things I took from his enthusiastic and humorous talk were:</p>

<ul>
<li>screen scraping is fragile but it's an important lever for fostering more elegant/robust information sharing.</li>
<li>screen scraping is fragile but it's important for building new <i>public</i> applications that aren't run behind closed doors at the Department of Homeland Security</li>
</ul>

<p>I really, really want to get going on the GovTrack data scraping now.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>14</wp:post_id>
		<wp:post_date>2005-06-10 08:05:07</wp:post_date>
		<wp:post_date_gmt>2005-06-10 15:05:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>chipy</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>tiger</title>
		<link>http://inkdroid.org/2005/06/14/tiger/</link>
		<pubDate>Tue, 14 Jun 2005 16:11:12 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=15</guid>
		<description></description>
		<content:encoded><![CDATA[Well, I took the plunge and installed the latest version of OS X. I'm actually posting this blog entry with a WordPress dashboard <a href="http://www.firewire.org/downloads/dashboard/blogs_forums/wordpressdash.html">plugin</a>. I backed up my mail, addressbook and calendar and did a clean install. I was a bit nervous that I forgot to back up everything I needed...but it was also kind of refreshing starting with a clean slate.  I've got the latest versions of Perl and Python building in the background now, and everything so far seems pretty smooth. I hope to take a closer look at dashboard widgets sometime soon.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>15</wp:post_id>
		<wp:post_date>2005-06-14 09:11:12</wp:post_date>
		<wp:post_date_gmt>2005-06-14 16:11:12</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>tiger</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>google&#039;s map api</title>
		<link>http://inkdroid.org/2005/06/29/googles-map-api/</link>
		<pubDate>Thu, 30 Jun 2005 01:58:15 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=16</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://web.archive.org/web/20110215085610/http://www.holovaty.com/images/2005-06-18_tribune_article.jpg">Adrian</a> pointed out at the last <a href="http://chipy.org">chipy</a> meeting that a formal API for GoogleMaps was in the works...but I had no idea it was <a href="http://www.google.com/apis/maps/">this</a> close.</p>

<p>After you've got an authentication key for your site directory, all you need to do to embed a map in your page is include a javascript library source URL directly from google, create a &lt;div&gt; tag with an id (say "map") and add some javascript to your page.</p>

<pre lang="javascript">
    var map = new GMap( document.getElementById("map") );
    map.addControl( new GSmallMapControl() );
    map.centerAndZoom( new GPoint(-88.316385,42.247090), 4);                                                                    
</pre>

<p>This took <a href="http://web.archive.org/web/20061012034456/http://www.inkdroid.org/google.html">literally</a> 2 minutes to do, if that. It's a bit tedious that the token is only good on a per-directory basis, but I guess this is because of partitioned blogging sites where different users have different directories with the same hostname.</p>

<p><i>update: I guess I'm not the only one who finds the per-directory limit to be kind of a <a href="http://groups-beta.google.com/group/Google-Maps-API/browse_thread/thread/a70d545eeed46d40/c7e8b0ad540232ae#c7e8b0ad540232ae">hassle</a>.</i></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>16</wp:post_id>
		<wp:post_date>2005-06-29 18:58:15</wp:post_date>
		<wp:post_date_gmt>2005-06-30 01:58:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>googles-map-api</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Net::OAI::Harvester v1.0</title>
		<link>http://inkdroid.org/2005/07/27/netoaiharvester/</link>
		<pubDate>Thu, 28 Jul 2005 04:06:42 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=17</guid>
		<description></description>
		<content:encoded><![CDATA[I got an email from Thorsten Schwander at <a href="http://lanl.gov">LANL</a> about a bug in <a href="http://search.cpan.org/dist/OAI-Harvester">Net::OAI::Harvester</a> when using a custom metadata handler with the auto-resumption token handling code. This was the first I'd heard about anyone using the custom metadata handling feature in N:O:H, so I was pleased to hear about it. Thorsten was kind enough to send a patch, so a new version is on its way around the CPAN mirrors. While it's hardly a major change, this is bumping the version from 0.991 to 1.0. It's been over 2 years since N:O:H was first released, and it's been pretty stable for the past year. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>17</wp:post_id>
		<wp:post_date>2005-07-27 21:06:42</wp:post_date>
		<wp:post_date_gmt>2005-07-28 04:06:42</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>netoaiharvester</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="perl"><![CDATA[perl]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Update to SRU and CQL::Parser</title>
		<link>http://inkdroid.org/2005/08/10/update-to-sru-and-cqlparser/</link>
		<pubDate>Wed, 10 Aug 2005 16:37:00 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=18</guid>
		<description></description>
		<content:encoded><![CDATA[<p>If you are tracking it you might be interested to know that <a href="http://web.archive.org/web/20111008014054/http://use.perl.org:80/~LTjake/journal/">Brian Cassidy</a> added a <a href="http://web.archive.org/web/20060924104603/http://search.cpan.org/dist/SRU/lib/Catalyst/Plugin/SRU.pm">Catalyst plugin</a> to the <a href="http://search.cpan.org/dist/SRU/">SRU</a> CPAN module. <a href="http://search.cpan.org/dist/Catalyst/">Catalyst</a> is a MVC framework that is getting quite a bit of mindshare in the Perl community (at least the small subset I hang out with in <a href="irc://irc.freenode.net/code4lib">#code4lib</a>). And if that wasn't enough Brian also committed some changes to <a href="http://search.cpan.org/dist/CQL-Parser/">CQL::Parser</a> that provides toLucene() functionality for converting CQL queries to queries that can be passed off to Lucene. Thanks Brian!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>18</wp:post_id>
		<wp:post_date>2005-08-10 09:37:00</wp:post_date>
		<wp:post_date_gmt>2005-08-10 16:37:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>update-to-sru-and-cqlparser</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="perl"><![CDATA[perl]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>chipy bookclub</title>
		<link>http://inkdroid.org/2005/08/10/chipy-bookclub/</link>
		<pubDate>Wed, 10 Aug 2005 19:27:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=19</guid>
		<description></description>
		<content:encoded><![CDATA[<p>So the Chicago Python Group has started up a <a href="http://www.chipy.org/bookclub.html">bookclub</a> about a month ago. The first book we're reading as a group is <a href="http://web.archive.org/web/20080723215313/http://www2.info.ucl.ac.be/people/PVR/book.html">Concepts, Techniques, and Models of Computer Programming</a> which is fortunately available <a href="http://web.archive.org/web/20090407061429/http://www.ulb.ac.be:80/di/rwuyts/INFO020_2003/vanRoyHaridi2003-book.pdf">online</a> for free. The aim of the bookclub (as with many bookclubs) is to work through a text together, and hopefully get to hear different perspectives during discussion which will happen online and after our monthly meetings.  Also, a bit of peer pressure can help make it through certain types of books...</p>

<p>And this first book is a doozy at 939 pages. It covers all sorts of territory from computer science using a multi-paradigm  language called <a href="http://www.mozart-oz.org/">Oz</a>. I've made it through the preface, and into Chapter 1, which starts out teaching some fundamental concepts behind functional programming. The jury is still out, but so far I'm finding the content refreshingly clear and stimulating. I like the fact that mathematical notation (so far)  is explained and not taken for granted.  Calculating factorial with recursion is a bit predictable, but chapter 1 quickly moved on to an algorithm that calculates a given row in a Pascal's Triangle.</p>

<p>The sheer magnitude of the book is a bit intimdating, however reading it on my ibook makes it easy to ignore that. I'm thinking of it as a sort of thematic encyclopedia of computer programming with handy illustrations. Hopefully I'll find the time to drop my thoughts here as I work my way through each chapter. Please feel free to <a href="http://web.archive.org/web/20081120181130/http://lonelylion.com/mailman/listinfo/bookclub">join</a> us (whether you're from Chicago or not) if you are interested.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>19</wp:post_id>
		<wp:post_date>2005-08-10 12:27:03</wp:post_date>
		<wp:post_date_gmt>2005-08-10 19:27:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>chipy-bookclub</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>pascal&#039;s triangle in python</title>
		<link>http://inkdroid.org/2005/08/10/pascals-triangle-in-python/</link>
		<pubDate>Thu, 11 Aug 2005 02:53:07 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=20</guid>
		<description></description>
		<content:encoded><![CDATA[I mentioned Pascal's Triangle in the previous post, and after typing in the Oz code decided to make a Pascal's Triangle pretty printer in python. 

<pre lang="python">
from sys import argv

def pascal(n):
    if n == 1:
        return [ [1] ]
    else:
        result = pascal(n-1)
        lastRow = result[-1]
        result.append( [ (a+b) for a,b in zip([0]+lastRow, lastRow+[0]) ] )
        return result

def pretty(tree):
    if len(tree) == 0: return ''
    line = '  ' * len(tree)
    for cell in tree[0]:
        line += '  %2i' % cell
    return line + &quot;\n&quot; + pretty(tree[1:])

if __name__ == '__main__':
    print pretty( pascal( int(argv[1]) ) ) 
</pre>

Which, when run with can generate something like this:

<pre>

biblio:~/Projects/bookclub ed$ python pascal.py 9
                     1
                   1   1
                 1   2   1
               1   3   3   1
             1   4   6   4   1
           1   5  10  10   5   1
         1   6  15  20  15   6   1
       1   7  21  35  35  21   7   1
     1   8  28  56  70  56  28   8   1 
</pre>

It's been fun reading up on the <a href="http://mathforum.org/dr.math/faq/faq.pascal.triangle.html">uses</a> for Pascal's triangle, although I imagine this is old hat for people more familiar with math than I. Still I think getting through this tome will be time well spent in the long run.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>20</wp:post_id>
		<wp:post_date>2005-08-10 19:53:07</wp:post_date>
		<wp:post_date_gmt>2005-08-11 02:53:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pascals-triangle-in-python</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>pypi over xmlrpc</title>
		<link>http://inkdroid.org/2005/08/11/pypi-over-xmlrpc/</link>
		<pubDate>Thu, 11 Aug 2005 20:44:16 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=21</guid>
		<description></description>
		<content:encoded><![CDATA[<p>It's great to see that our <a href="http://web.archive.org/web/20050404011108/http://chipy.org:80/sprint.html">ChiPy sprint</a> bore some fruit for the PyPI service. There's now decent <a href="http://wiki.python.org/moin/PyPiXmlRpc">XMLRPC support</a> in PyPI for querying the packages. This will hopefully open up the door for lots of PyPI utilities that abound in the Perl/CPAN world...like this very simple client for listing packages:</p>

<pre lang="python">
#!/usr/bin/env python

import xmlrpclib
import sys

pypi = xmlrpclib.ServerProxy('http://python.org/pypi')
for hit in pypi.search( { 'name':sys.argv[1] } ):
    urls = pypi.package_urls( hit['name'], hit['version'] )
    if urls:
        hit['url'] = urls[0]['url']
    else:
        hit['url'] = 'None'
    print 'NAME: %(name)s [%(version)s]\nDESC: %(summary)s\nURL: %(url)s' % hit
    print

</pre>

<p>which if you put in your PATH allows you to:</p>

<pre>
biblio:~ ed$ pypi-search sql
NAME: dbsql [0.7.1]
DESC: A tool to sync pilot-db databases with an SQL database.
URL: None

NAME: ll-orasql [0.6]
DESC: Utilities for working with cx_Oracle
URL: http://cheeseshop.python.org/packages/source/l/ll-orasql/ll-orasql-0.6.tar.bz2
  
NAME: ll-sql [0.7.1]
DESC: A module for generating SQL queries
URL: http://cheeseshop.python.org/packages/source/l/ll-sql/ll-sql-0.7.1.tar.gz

NAME: MySQL-python [1.2.0]
DESC: Python interface to MySQL
URL: None

NAME: pSQL [0.9.2]
DESC: SQL statement wrapper class
URL: None

NAME: pymssql [0.5.2]
DESC: A simple database interface to MS-SQL for Python.
URL: None

NAME: pymssql [0.6.0]
DESC: A simple database interface to MS-SQL for Python.
URL: None

NAME: pymssql [0.7.1]  
DESC: A simple database interface to MS-SQL for Python.
URL: None

NAME: pyPgSQL [2.3]
DESC: pyPgSQL - A Python DB-API 2.0 compliant interface to PostgreSQL.
URL: None

NAME: pysqlite [2.0.3]
DESC: DB-API 2.0 interface for SQLite 3.x
URL: None

NAME: SnakeSQL [0.5.2 Alpha]
DESC: Pure Python SQL database supporting NULLs, foreign keys and simple joins
URL: None

NAME: SQLEdit [1.0]
DESC: An sql table editor and query viewer.
URL: None

NAME: SQLObject [0.6.1]
DESC: Object-Relational Manager, aka database wrapper
URL: None 
</pre>

<p>As you can see there is something odd going on with the package URL...it requires an extra trip to the XMLRPC service to get it, and it is rarely there.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>21</wp:post_id>
		<wp:post_date>2005-08-11 13:44:16</wp:post_date>
		<wp:post_date_gmt>2005-08-11 20:44:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pypi-over-xmlrpc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>3</wp:comment_id>
			<wp:comment_author><![CDATA[brianray]]></wp:comment_author>
			<wp:comment_author_email>bray@sent.com</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>216.159.232.124</wp:comment_author_IP>
			<wp:comment_date>2005-08-11 14:17:49</wp:comment_date>
			<wp:comment_date_gmt>2005-08-11 21:17:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[nice.  Now  let's Sprint for a get,config,make, and install util... then let somebody else finish it.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>7</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>296</wp:comment_id>
			<wp:comment_author><![CDATA[Jorge Godoy]]></wp:comment_author>
			<wp:comment_author_email>jgodoy@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>201.22.40.73</wp:comment_author_IP>
			<wp:comment_date>2006-04-23 06:11:18</wp:comment_date>
			<wp:comment_date_gmt>2006-04-23 13:11:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[It looks like the URL problem were solved in the last 10 months ;-)

The URL for the sprint has changed to http://chipy.org/Sprint

Bice piece of code.  Intresting, useful and fast.  Congratulations!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>backdoors</title>
		<link>http://inkdroid.org/2005/08/12/backdoors/</link>
		<pubDate>Fri, 12 Aug 2005 17:30:46 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=22</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The FCC is <a href="http://web.archive.org/web/20061212134150/https://www.eff.org//Privacy/Surveillance/CALEA/FCC_voip_wiretaps.pdf">mandating</a> that Internet providers and network appliance manufacturers build in backdoors so that the spooks can monitor electronic communication between, well you know, terrorists and stuff. What a profoundly bad idea...do they <em>really</em> think that the secret access mechanism will stay a secret? And when it leaks out, what sort of access will the crooks have, and will they ever know it was leaked?</p>

<p>At least the effort to tap is on the table, unlike the <a href="http://web.archive.org/web/20081201090832/http://www.totse.com/en/privacy/encryption/des_prob.html">Data Encryption Standard</a> which was supposedly introduced with a deliberately small keysize to ease decoding by the NSA.</p>

<p>Thanks to the ever vigilant <a href="http://web.archive.org/web/20080706013823/http://www.eff.org/news/archives/2005_08.php">EFF</a> for reporting on this. Which reminds me, my membership is up for renewal.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>22</wp:post_id>
		<wp:post_date>2005-08-12 10:30:46</wp:post_date>
		<wp:post_date_gmt>2005-08-12 17:30:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>backdoors</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="security"><![CDATA[security]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Intelligent Design</title>
		<link>http://inkdroid.org/2005/08/12/intelligent-design/</link>
		<pubDate>Sat, 13 Aug 2005 02:31:22 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=23</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.venganza.org/"><img src="http://www.inkdroid.org/photodex/image/2677" border=0 align="left"/></a>
I was so pleased to <a href="http://www.venganza.org">read</a> recently that there are others who find it appalling that the Kansas School Board isn't considering teaching the solid scientific evidence for the Flying Spaghetti Monster. If the <a href="http://www.venganza.org/">statistics</a> on pirates and global warming  aren't enough to convince you, I have a little story to relate. One night I was driving along the road, and I saw some strange lights in the clouds. At first I thought it might be an airplane, but when I pulled over and got out I caught the aroma of spaghetti sauce and cooked pasta. I looked at my shirt, and didn't see any spaghetti stains so I immediately thought that it had been the Flying Spaghetti Monster in the clouds. The next day I decided to hire an artist to draw what I imagined the Flying Spaghetti Monster to look like, lurking in the clouds. The result is above, and it's *exactly* like the one I have seen reported elsewhere! Since I paid for this artist to do the picture, it is obviously scientifically accurate. I hope that this leaves no doubt in your mind, that FSM is not only a reality, but a very cool one indeed.

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>23</wp:post_id>
		<wp:post_date>2005-08-12 19:31:22</wp:post_date>
		<wp:post_date_gmt>2005-08-13 02:31:22</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>intelligent-design</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="category" nicename="religion"><![CDATA[religion]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A Tale of Two Searches</title>
		<link>http://inkdroid.org/2005/08/19/a-tale-of-two-searches/</link>
		<pubDate>Fri, 19 Aug 2005 21:46:20 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=24</guid>
		<description></description>
		<content:encoded><![CDATA[<p>There's been some interesting discussion about <a href="http://www.loc.gov/z3950/agency/zing/srw/">SRW/U</a> vs <a href="http://opensearch.a9.com/">OpenSearch</a> on <a href="http://listserv.loc.gov/cgi-bin/wa?A2=ind0506&L=zng&T=0&F=&S=&P=6898">some</a> library <a href="http://web.archive.org/web/20100115140731/http://lists.webjunction.org:80/wjlists/web4lib/2005-July/thread.html">email lists</a>, <a href="http://dilettantes.blogspot.com/2005/08/librarians-are-arrogant-asses.html">blogs</a>, and over in <a href="irc://irc.freenode.net/code4lib">#code4lib</a>. I worked on the <a href="http://search.cpan.org/dist/SRU">SRU</a> and <a href="http://search.cpan.org/dist/CQL-Parser">CQL::Parser</a> modules for the <a href="http://www.ockham.org/">Ockham Initiative</a>, and have watched the comparisons to A9's RSS based OpenSearch with great interest. It's amazing how similar the goals of these two search technologies are, and yet how different the implementations and developer communities are.</p>

<p>At their most basic both SRW/U and OpenSearch aim to make it easy to conduct searches over the web. They both want to spread distributed searching over the web for the masses. SRW/U grew up before OpenSearch at the Library of Congress, mainly on a small implementors list. It allows you to use SOAP as a transport layer, or simple XML over HTTP using a RESTful interface. The results can really be any type of XML, and there is no lingua franca of DublinCore like in OAI-PMH. SRW/U comes with an attendent query specification known as the Common Query Language (CQL). So there are a fair amount of moving pieces in building even a barebones SRW/U server.</p>

<p>OpenSearch on the other hand is relatively new, and was developed by A9 (a subsidiary of Amazon who know a thing or two about building robust easy to use web services). Really it's just a RESTful interface for obtaining search results in RSS 2.0. There is <a href="http://blog.a9.com/blog/2005/07/27/opensearch-11-preview/">talk</a> that v1.1 might have some extensions to support more refined queries and xmlnamespaces for bundling different types of XML results...but at the moment there's no need to parse queries, or to be able to handle any flavor of XML other than RSS 2.0.</p>

<p>When comparing the two sites one thing makes itself clear: the SRW/U <a href="http://www.loc.gov/z3950/agency/zing/srw/">site</a> is a shambles--the specification itself is fragmented, and as a result there's information all over the place.  The OpenSearch <a href="http://opensearch.a9.com/">page</a> is neatly laid out with examples, rationale and even has a developers blog. The key here I think is that OpenSearch started simple and is slowly adding functionality that might be needed. SRW/U started out trying to simplify an existing standard and is slowly trying to make it simpler (there's even been suggestions to drop the SOAPy SRW altogether and focus on the RESTful SRU). They're moving in opposite directions.  I don't really have any doubts about which standard will see the widest deployment. The virtues of keeping things simple have been noted (very eloquently) by <a href="http://web.archive.org/web/20080517174900/http://www.adambosworth.net/archives/000031.html">Adam Bosworth</a>.</p>

<p>There is hope for library technologists though. OCLC is doing some really good work like their <a href="http://www.oclc.org/worldcat/open/isbnissnlinking/">Open WorldCat</a> program which allows you to link directly to local holdings for a book with a URL like:</p>

<p><a href="http://worldcatlibraries.org/wcpa/isbn/0670032506&loc=60014">http://worldcatlibraries.org/wcpa/isbn/0670032506&amp;loc=60014</a></p>

<p>Yeah, that's an ISBN and a ZIP code. Oh, and I installed Chris Fairbanks' nice <a href="http://www.williamsburger.com/wb/archives/opensearch-v-1-0">OpenSearch/WordPress plugin</a> in like 5 minutes. Here's an example:</p>

<p><a href="http://www.inkdroid.org/os-query?s=code4lib">http://www.inkdroid.org/journal/os-query?s=code4lib</a>.</p>

<p>Drop it in an RSS reader and you can see whenever I write about code4lib. Not that you would really want to do that. Hmm but maybe it would be useful with say an <a href="http://web.archive.org/web/20110726164412/http://open-ils.org/blog/?p=34">online catalog</a> or bibliographic database!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>24</wp:post_id>
		<wp:post_date>2005-08-19 14:46:20</wp:post_date>
		<wp:post_date_gmt>2005-08-19 21:46:20</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-tale-of-two-searches</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="post_tag" nicename="worldcat"><![CDATA[worldcat]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>File under m for megalomania</title>
		<link>http://inkdroid.org/2005/09/01/file-under-m-for-megalomania/</link>
		<pubDate>Thu, 01 Sep 2005 15:26:21 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=28</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.theonion.com/content/node/40076">Google Announces Plan To Destroy All Information It Can't Index</a>. 

<blockquote>
Although Google executives are keeping many details about Google Purge under wraps, some analysts speculate that the categories of information Google will eventually index or destroy include handwritten correspondence, buried fossils, and private thoughts and feelings.
</blockquote>

Seriously, many a truth is said in jest. With the <a href="http://www.pbs.org/cringely/pulpit/pulpit20050825.html">news</a> that Google is going to be selling another 4 billion dollars worth of shares it makes sense that they would be thinking of a purging program to balance out their binging. What are they going to do with 4 billion dollars? I can't even begin to imagine. It is frankly, a bit frightening, and seems like behavior one might read about in the <a href="http://www.psychologynet.org/dsm.html">DSM IV</a>.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>28</wp:post_id>
		<wp:post_date>2005-09-01 08:26:21</wp:post_date>
		<wp:post_date_gmt>2005-09-01 15:26:21</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>file-under-m-for-megalomania</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Open Documents </title>
		<link>http://inkdroid.org/2005/09/06/open-documents/</link>
		<pubDate>Tue, 06 Sep 2005 10:14:52 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=29</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Have you ever had trouble importing one type of word processing document into your current word processor? Perhaps you're using the same word processor, but are trying to import a document you created with an earlier version. Imagine for a moment what this will mean for a historian who is trying to research some correspondence fifty years from now. How about five hundred years from now? Are historians going to have to be computer hackers who have superhuman reverse engineering talents? Will there be mystical emulators that let you convert your modern computer into an insanely slow Pentium processor CPU running Windows 95 and Word 7? How will you even know what format a document is in?</p>

<p>There's been some inspiring <a href="http://www.groklaw.net/article.php?story=20050831202118904">developments</a> in Massachusetts who have decided to use the <a href="http://en.wikipedia.org/wiki/OpenDocument">OpenDocument Format</a> instead of Mircrosoft's <a href="http://web.archive.org/web/20070125232957/http://www.microsoft.com:80/office/xml/default.mspx">OpenXML</a>. David Wheeler does a really nice <a href="http://www.dwheeler.com/essays/why-opendocument-won.html"> job</a> of summarizing what this means for open source development, and how Microsoft can choose to recover. I had no idea (but was not surprised to learn) that the <a href="http://web.archive.org/web/20060203143300/http://www.microsoft.com:80/mscorp/ip/format/xmlpatentlicense.asp">royalty free license</a> that Microsoft is using to distribute its "open" document format is incompatable with the popular <a href="http://www.gnu.org/licenses/licenses.html">GNU</a> open source license. Ironically this seems to have been a calculated move by Microsoft to exclude open source developers from working with the open formats. Isn't the whole point of an open document format to be open?</p>

<p>Thank goodness the folks in Massachusetts are on the ball, asking the right questions, and not simply following the money, power and status quo. At the same time they're not exclusively endorsing the GPL; but have wisely decided to include as many possible development environments as possible. This sounds like the best way to making a truly archivable document format that will be good for "long haul institutions" like libraries and archives. Hopefully other public organizations will consider taking a similiar approach. Thanks <a href="http://web.archive.org/web/20070814015719/http://netapps.muohio.edu:80/blogs/darcusb/darcusb/archives/2005/09/03/win-for-opendocument">Bruce</a> for writing about this.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>29</wp:post_id>
		<wp:post_date>2005-09-06 03:14:52</wp:post_date>
		<wp:post_date_gmt>2005-09-06 10:14:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>open-documents</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Lockheed Martin and NARA</title>
		<link>http://inkdroid.org/2005/09/10/lockheed-martin-to-build-a-national-electornic-archive/</link>
		<pubDate>Sat, 10 Sep 2005 19:22:44 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=30</guid>
		<description></description>
		<content:encoded><![CDATA[<p>After 7 years of consultation Lockheed-Martin has been <a href="http://web.archive.org/web/20100601162705/http://archives.gov/era/acquisition/option-award.html">selected</a> to build the Electronic Records Archives for the National Archives and Records Administration for 38 million dollars.</p>

<blockquote>
ERA will provide NARA with the capability to authentically preserve and provide access to any kind of electronic record free from dependence on any specific hardware or software.
</blockquote>

<p>There are some aging exploratory papers on the NARA site, along with what appears to be a copy of the RFP...but I can't seem to find any specific information on how Lockheed Martin is planning to do this. I wonder what sort of track record L-M has in building electronic archiving software. Do they have an existing system which they are going to modify for NARA, or are they going to be building a new system from scratch?  It sure would be interesting to hear some more details.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>30</wp:post_id>
		<wp:post_date>2005-09-10 12:22:44</wp:post_date>
		<wp:post_date_gmt>2005-09-10 19:22:44</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lockheed-martin-to-build-a-national-electornic-archive</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On Lateral Thinking </title>
		<link>http://inkdroid.org/2005/10/02/on-lateral-thinking/</link>
		<pubDate>Sun, 02 Oct 2005 20:59:21 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=32</guid>
		<description></description>
		<content:encoded><![CDATA[I recently checked out <a href="http://www.amazon.com/exec/obidos/tg/detail/-/0060958324">Zen and the Art of Motorcycle Maintenance</a> after reading Kevin's <a href="http://www.kevinclarke.info/weblog/2005/05/02/the-practice-of-cataloging/">piece</a> about how the book informed his practice of library cataloging. I am enjoying it a lot more this time around, and have found it really informs my practice of computer programming as well. Unfortunately I only made it half way through before it needed to be returned to the library, and the local superbookstores oddly enough don't seem to carry it...So, I've got a copy on order from a used bookstore I found through Amazon. Anyhow here's one nice quote I jotted down before I had to return the book:

<blockquote>
At first the truths Phaedrus began to pursue were lateral truths; no longer the frontal truths of science, those toward which the discipline pointed, but the kind of truth you see laterally, out of the corner of your eye. In a laboratory situation, when your whole procedure goes haywire, when everything goes wrong or is indeterminate or is so screwed up by unexpected results you can't make head or tail out of anything, you start looking laterally. That's a word he later used to describe a growth of knowledge that doesn't move forward like an arrow in flight, but expands sideways, like an arrow enlarging in flight, or like the archer, discovering that although he has hit the bull's eye and won the prize, his head is on a pillow and the sun is coming in the window. Lateral knowledge is knowledge that's from a wholly unepected direction, from a direction that's not even understood as a direction until the knowledge forces itself upon one. Lateral truths point to the falseness of axioms and postulates underlying one's existing system of getting at truth.
</blockquote>

I'm not entirely sure why this resonated with me. I think the idea of "lateral thinking" reminds me of how IRC and web surfing often informs my craft of writing software.  While many universities offer computer "science" programs, I've found a large component of writing software is more artistic than scientific. Of course I'm hardly the <a href="http://www.literateprogramming.com/">first</a> <a href="http://www.dreamsongs.com/">person</a> to <a href="http://www.paulgraham.com/hp.html">comment</a> on this...but Zen and the Art of Motorcycle Maintenance is full of good advice for writing and tuning your programs. Hopefully I'll get to write more about them in here when I get my copy in the mail.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>32</wp:post_id>
		<wp:post_date>2005-10-02 13:59:21</wp:post_date>
		<wp:post_date_gmt>2005-10-02 20:59:21</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-lateral-thinking</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>5</wp:comment_id>
			<wp:comment_author><![CDATA[Eric]]></wp:comment_author>
			<wp:comment_author_email>eric_childress@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://www.oclc.org/research/staff/childress.htm</wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.6</wp:comment_author_IP>
			<wp:comment_date>2005-10-29 16:29:56</wp:comment_date>
			<wp:comment_date_gmt>2005-10-29 23:29:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[It's a wonderful book. And I too have found the seamless way Robert Pirsig seems to address engineering, ethics, and life in the same breath very engaging. Nice post.  
BTW: if you'd like to cite the Open WorldCat record  Amazon, the URL is http://www.worldcatlibraries.org/wcpa/isbn/0060958324]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>10</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>delicious json</title>
		<link>http://inkdroid.org/2005/09/21/delicious-json/</link>
		<pubDate>Wed, 21 Sep 2005 12:35:42 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=34</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I just noticed over on the <a>del.icio.us blog</a> that my data is available as <a href="http://del.icio.us/doc/feeds/json/">JavaScript Object Notation (JSON)</a> by following a simple URL like: http://del.icio.us/feeds/json/inkdroid.</p>

<p>Essentially you just load  the URL as javascript source in your HTML:</p>

<p><code>
&lt;script type="text/javascript" src="http://del.icio.us/feeds/json/inkdroid?count=20"&gt;&lt;/script&gt;
</code></p>

<p>and voila you've magically got a new javascript array variable <code>Delicious.posts</code>, each element of which is a hash describing your link on delicious. It's a very elegant (and simple) technique...<i>much</i> more elegant than that taken in the <a href="http://search.cpan.org/dist/XML-RSS-JavaScript">XML::RSS::JavaScript</a> module which I helped create.  It's so elegant in fact that I got it working off to the side of this page in 2 minutes. I downloaded the python and ruby extensions for working with JSON just to take a look. The <a href="https://sourceforge.net/projects/json-py/">python</a> version is a pleasant read, especially the unittests! The <a href="http://rubyforge.org/snippet/detail.php?type=snippet&id=29">ruby version</a> is a lesson in minimalism:</p>

<pre lang="ruby">
jsonobj = eval(json.gsub(/(["'])\s*:\s*(['"0-9tfn\[{])/){"#{$1}=>#{$2}"})
</pre>

<p>Now, if I were to use this I'd probably put a wrapper around it :-) Although it's less minimalistic I think I prefer the explicitness of the python code. I've been digging into Ruby a bit more lately as I work on <a href="http://www.inkdroid.org:7000/textualize/browser/ruby-marc/trunk/">ruby-marc</a>, and while I'm really enjoying the language I tend to shy away from one line regex hacks like this...which more often than not turn out to be a pain to extend and maintain.</p>

<p>I first heard of JSON from <a href="http://web.archive.org/web/20111122231134/http://open-ils.org/blog/?p=31">Mike Rylander</a> at the <a href="http://open-ils.org">open-ils</a> project who are using JSON heavily in the opensource library catalog that they are developing for the state of Georgia.  It is nice to see library technologists leading the curve.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>34</wp:post_id>
		<wp:post_date>2005-09-21 05:35:42</wp:post_date>
		<wp:post_date_gmt>2005-09-21 12:35:42</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>delicious-json</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87479</wp:comment_id>
			<wp:comment_author><![CDATA[pumpkin pie bonethugs smart and final digital blasphemy sniper games dan brown usweekly brooklyn decker housewives afro ninja two step 92.3 the big lebowski california state university chico meridian high school yondaime wii stock loon wes anderson lament]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.fipsascaserta.it/web/index.php?option=com_phocaguestbook&amp;view=guestbook&amp;id=1&amp;Itemid=215&amp;lang=it</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-08-24 01:35:20</wp:comment_date>
			<wp:comment_date_gmt>2015-08-24 08:35:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<strong>pumpkin pie bonethugs smart and final digital blasphemy sniper games dan brown usweekly brooklyn decker housewives afro ninja two step 92.3 the big lebowski california state university chico meridian high school yondaime wii stock loon wes anderson l&#8230;</strong>

delicious json | inkdroid
]]></wp:comment_content>
			<wp:comment_approved>0</wp:comment_approved>
			<wp:comment_type>trackback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87480</wp:comment_id>
			<wp:comment_author><![CDATA[tracking help desk bachelor degree teaching burglary lawyer auto crash lawyer lemon law bmw online master degree social work home insurance virgin little tykes mission finance remortgage dwi pennsylvania human resources degree program california high risk]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.rsstip.com/deal-with-the-devil-the-fbis-s-1002615485.html</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-08-24 13:31:22</wp:comment_date>
			<wp:comment_date_gmt>2015-08-24 20:31:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<strong>tracking help desk bachelor degree teaching burglary lawyer auto crash lawyer lemon law bmw online master degree social work home insurance virgin little tykes mission finance remortgage dwi pennsylvania human resources degree program california high&#8230;</strong>

delicious json | inkdroid
]]></wp:comment_content>
			<wp:comment_approved>0</wp:comment_approved>
			<wp:comment_type>trackback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>trackbacks at arXiv </title>
		<link>http://inkdroid.org/2005/09/22/trackbacks-at-arxiv/</link>
		<pubDate>Thu, 22 Sep 2005 15:01:15 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=35</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I just <a href="http://web.archive.org/web/20080724005305/http://endlesshybrids.com/2005/09/21/research-tools-enter-the-blogosphere/">read</a>  (thanks jeff) about how arXiv.org has <a href="http://arxiv.org/help/trackback/">implemented</a> experimental <a href="http://www.sixapart.com/pronet/docs/trackback_spec">trackback</a> support. Essentially this allows researchers who maintain online journals to simply reference an abstract like <a href="http://arxiv.org/abs/cs.DL/0503016">File-based storage of Digital Objects and constituent datastreams: XMLtapes and Internet Archive ARC files</a> (a great article by the way) and arXiv will receive a trackback ping at http://arxiv.org/trackback/0503016 that lets them know someone referenced the abstract. If you've followed this so far you might be wondering how the blogging software (wordpress, moveabletype, blosxom, etc) figure out where to ping arxiv.org. Take a look in the source code for the arXiv abstract and you'll see a chunk of RDF:</p>

<pre>

&lt;rdf:RDF xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:dc='http://purl.org/dc/elements/1.1/'
  xmlns:trackback='http://madskills.com/public/xml/rss/module/trackback/'&gt;
&lt;rdf:Description
  rdf:about='http://arxiv.org/abs/cs/0503016'
  dc:identifier='http://arxiv.org/abs/cs/0503016'
  dc:title='File-based storage of Digital Objects and constituent datastreams: XMLtapes and Internet Archive ARC files'
  trackback:ping='http://arxiv.org/trackback/cs/0503016' /&gt;
&lt;/rdf:RDF&gt; 
</pre>

<p>So when you are finished composing a blog entry (like this one), wordpress will look at outbound links in blog entries, follow the URLs, look for trackback RDF in the HTML source, and then actually ping the respective trackback server. Pretty fancy to have all this stuff just happening automatically...and it's great to see how arXiv is continuing to blur the lines between electronic and traditional publishing models.</p>

<p>The fact that trackback autodiscovery uses RDF is a nice illustration for folks who are skeptical about the <a href="http://www.tantek.com/presentations/2004etech/realworldsemanticspres.html">semantic web</a> (note case).  I'm no expert, but I do think that the semantic web revolution will be a quiet one, and will not be televised (easily viewable).</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>35</wp:post_id>
		<wp:post_date>2005-09-22 08:01:15</wp:post_date>
		<wp:post_date_gmt>2005-09-22 15:01:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>trackbacks-at-arxiv</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>geocoder and rdf</title>
		<link>http://inkdroid.org/2005/10/05/geocoder-and-rdf/</link>
		<pubDate>Thu, 06 Oct 2005 04:05:24 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=36</guid>
		<description></description>
		<content:encoded><![CDATA[While <a href="http://mail.pm.org/pipermail/chicago-talk/2005-October/002532.html">fielding </a> a question on a local Perl list this weekend I ran across some more RDF alive and kicking in the very useful <a href="geocoder.us">geocoder.us</a> service. They have a nice RESTful web service, which allows you to drop an address or intersection into a URL like:

<a href="http://rpc.geocoder.us/service/rest?address=1340%20Ridgeview%20Drive%20McHenry%2C%20Illinois%2060050">http://rpc.geocoder.us/service/rest?address=<br />1340%20Ridgeview%20Drive%20McHenry%2C%20Illinois%2060050</a>

and get back the longtitude and latitude in a chunk of RDF like:

<pre>
<code>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;rdf:RDF
  xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot;
  xmlns:geo=&quot;http://www.w3.org/2003/01/geo/wgs84_pos#&quot;
  xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;&gt;

&lt;geo:Point rdf:nodeID=&quot;aid87293465&quot;&gt;
    
    &lt;dc:description&gt;899 Ridgeview Dr, McHenry IL 60050&lt;/dc:description&gt;
    &lt;geo:long&gt;-88.291658&lt;/geo:long&gt;
    &lt;geo:lat&gt;42.314936&lt;/geo:lat&gt;

&lt;/geo:Point&gt;


&lt;/rdf:RDF&gt;  
</code>
</pre>

Of course this data could be encoded in comma-separated-values, in fact they have a similar RESTful service that does just that:

<a href="http://rpc.geocoder.us/service/csv?address=1340%20Ridgeview%20Drive%20McHenry%2C%20Illinois%2060050">http://rpc.geocoder.us/service/csv?address=<br />1340%20Ridgeview%20Drive%20McHenry%2C%20Illinois%2060050</a>

which returns:

<pre>
<code>
42.314936,-88.291658,899 Ridgeview Dr,McHenry,IL,60050
</code>
</pre>

Does this mean RDF isn't necessary? For someone who is just querying geocoder.us directly and knows what the output is I guess the RDF doesn't really add that much value. My coworker Bill likes to talk about being explicit in code whenever possible, and the RDF in this case is more explicit. Until there are programs that follow lines of inference using this data it's largely a matter of taste. It's nice that geocoder supports both world views. 

And hats off to geocoder: they give away their <a href="http://search.cpan.org/dist/Geo-Coder-US/">software</a> and how they built the service to anyone who wants it. They provide expertise in using the data, and also offer commercial access to their web services which have the 10 second or so pause between requests disabled. What an interesting model for a company. Heck, wouldn't it be nice if <a href="http://www.oclc.org">OCLC</a> operated this way?]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>36</wp:post_id>
		<wp:post_date>2005-10-05 21:05:24</wp:post_date>
		<wp:post_date_gmt>2005-10-06 04:05:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>geocoder-and-rdf</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>4</wp:comment_id>
			<wp:comment_author><![CDATA[ksclarke]]></wp:comment_author>
			<wp:comment_author_email>ksclarke@princeton.edu</wp:comment_author_email>
			<wp:comment_author_url>http://www.kevinclarke.info/weblog/</wp:comment_author_url>
			<wp:comment_author_IP>128.112.202.209</wp:comment_author_IP>
			<wp:comment_date>2005-10-06 06:51:19</wp:comment_date>
			<wp:comment_date_gmt>2005-10-06 13:51:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[We're not so far apart on this issue.  That next to the last paragraph is all I was saying :-)

RDF, or any XML, is more explicit.  Until there are programs that follow lines of reference (and they are used in the wild), RDF is just another standardized XML... a matter of taste, as you say.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>9</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>ruby-zoom</title>
		<link>http://inkdroid.org/2005/10/07/ruby-zoom/</link>
		<pubDate>Fri, 07 Oct 2005 22:06:15 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=37</guid>
		<description></description>
		<content:encoded><![CDATA[If you ever need to do z39.50 from ruby and have successfully built and installed <a href="http://ruby-zoom.rubyforge.org/">ruby-zoom</a> only to see:
<pre><code>   biblio:~ ed$ irb   irb(main):001:0> require 'zoom'   dyld: NSLinkModule() error   dyld: Symbol not found: _ZOOM_connection_search     Referenced from: /usr/lib/ruby/site_ruby/1.8/powerpc-darwin8.0/zoom.bundle     Expected in: flat namespace </code></pre>
or a similar error about missing symbols...never fear! The <a href="http://www.indexdata.dk/yaz/">YAZ</a> toolkit doesn't build a shared library by default. It's confusing because the ruby-zoom package builds fine with header files. When building YAZ you'll need to:
<pre>biblio:/usr/src/yaz-2.1.8 ed$ ./configure --enable-shared</pre>
Submitted here to help similar users who are flailing wildly in Google.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>37</wp:post_id>
		<wp:post_date>2005-10-07 15:06:15</wp:post_date>
		<wp:post_date_gmt>2005-10-07 22:06:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>ruby-zoom</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>7856</wp:comment_id>
			<wp:comment_author><![CDATA[Walter McGinnis]]></wp:comment_author>
			<wp:comment_author_email>walter@katipo.co.nz</wp:comment_author_email>
			<wp:comment_author_url>http://blog.katipo.co.nz/?p=26</wp:comment_author_url>
			<wp:comment_author_IP>58.28.158.48</wp:comment_author_IP>
			<wp:comment_date>2006-11-10 21:47:35</wp:comment_date>
			<wp:comment_date_gmt>2006-11-11 04:47:35</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I've outlined how to install Zebra, YAZ, and ruby-zoom on Debian Testing (etch).

You can find it here:

http://blog.katipo.co.nz/?p=26

Cheers,
Walter]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>quite a patch</title>
		<link>http://inkdroid.org/2005/10/13/quite-a-patch/</link>
		<pubDate>Thu, 13 Oct 2005 19:10:42 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=38</guid>
		<description></description>
		<content:encoded><![CDATA[Since starting to use <a href="http://lucene.apache.org">lucene</a> heavily at work about a year ago I've been watching the lucene list out of the corner of my eye for tips and tricks. Today I saw an email go by that referenced a <a href="http://issues.apache.org/jira/browse/LUCENE-454">recent patch</a> that lazily creates SegmentMergeInfo.docMap objects. I guess the point isn't so much what the object is, but the mere change in lazily creating the object yielded some pretty impressive performance gains:

<pre>
<blockquote>
Performance Results:
A simple single field index with 555,555 documents, and 1000 random 
deletions was queried 1000 times with a PrefixQuery matching a single document.

Performance Before Patch:
  indexing time = 121,656 ms
  querying time = 58,812 ms

Performance After Patch:
  indexing time = 121,000 ms
  querying time = 598 ms

A 100 fold increase in query performance! 
</blockquote>
</pre>

Umm, 100 fold increase in performance. That's quite a patch!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>38</wp:post_id>
		<wp:post_date>2005-10-13 12:10:42</wp:post_date>
		<wp:post_date_gmt>2005-10-13 19:10:42</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>quite-a-patch</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="java"><![CDATA[java]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>a citation microformat - when worlds collide</title>
		<link>http://inkdroid.org/2005/11/03/a-citation-microformat-when-worlds-collide/</link>
		<pubDate>Thu, 03 Nov 2005 18:55:54 +0000</pubDate>
		<dc:creator><![CDATA[admin]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=41</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://web.archive.org/web/20081118135651/http://www.tjameswhite.com/blog/archives/2005/11/citation-microformat/">Tim White</a> has taken the time to <a href="http://microformats.org/discuss/mail/microformats-discuss/2005-November/001813.html">prod </a> the microformats list about the <a href="http://microformats.org/wiki/cite">citation microformat</a> that's been floating around for a few months. It's really encouraging that a developer at <a href="http://www.galegroup.com/">Gale</a> is thinking of using a citation microformat.  While I <a href="http://www.follettebooks.com">also</a> work in the industry I've been coming at the citation microformat from a slightly different angle.  For the past few months I've been monitoring activity in microformat land while watching another group of library technologists. Recently, Bill Burcham's <a href="http://lesscode.org/2005/10/21/baby-steps-to-synergistic-web-apps/">Baby Steps to Synergistic Web Apps</a> and <a href="http://lesscode.org/2005/11/02/half-a-baby-step/">Half a Baby Step</a> confirmed a nagging feeling I was having that the two communities were converging.</p>

<p>The "other" group are library/programmer friends of mine in  <a href="irc://irc.code4lib.org/code4lib">#code4lib</a>. These guys have been brainstorming about adapting the widely used <a href="http://web.archive.org/web/20071009055221/http://www.niso.org:80/committees/committee_ax.html">OpenURL</a> for use in HTML. OpenURL is used extensively in the academic library environment to enable linking to licensed content from online indexes. OpenURL essentially provides guidelines for encoding citation metadata in URLs, which has given birth to an ecosystem of vendors/developers who can provide resolver and content services. <a href="http://ocoins.info/">Context Object in Spans (COinS)</a> provides a microformatty way to put openurls (without reference to an openurl resolver) into HTML.  I'm not doing this work justice, so if you're curious to see how COinS got started there's lots of content in the <a href="http://web.archive.org/web/20070204041210/http://cipolo.med.yale.edu:80/mailman/listinfo/gcs-pcs-list">gather-create-share</a> discussion list. COinS exists in the wild at <a href="http://www.citeulike.org/">citeulike</a>, <a href="http://hublog.hubmed.org/archives/001163.html">hubmed</a>, <a href="http://law.wlu.edu/library/CLJC/">Current Law Journal Content</a>.</p>

<p>Now after reading up about microformats and <a href="http://microformats.org/discuss/mail/microformats-discuss/2005-August/000653.html">posting</a> to the discussion list, and talking to <a href="http://suda.co.uk/">Brian Suda</a> it became clear that COinS as it stands now isn't really usable as a microformat. Microformats center around marking up human readable data with semantic HTML, whereas COinS hides citation data encoded as a query string in HTML. However it is possible to encode openurl's as XML, so there's still hope I suppose. I want to sketch out what this could look like for the microformat wiki.</p>

<p>Before Tim's post I'd never even heard of the <a href="http://web.archive.org/web/20060928101302/http://www.niso.org/standards/resources/z3980-3.pdf">Standard Format for Downloading Bibliographic Records z39.80</a>. While it's only a draft it's used by Gale for providing downloadable citations, can be imported by <a href="http://www.refworks.com/">RefWorks</a> and most likely others. It bears a lot of resemblance to other citation formats that I've come across, but is obviously pre XML. The microformats brain storming that Brian has done has centered around DublinCore, BibTeX, MODS. At the moment I'm thinking BibTeX, Z39.80 and OpenURL stand the best chance of working. Honestly I think we could debate formats till the cows come home (and have left their cow paths ;-), but what microformats needs is some workable solution like semantic-html for OpenURL or Z39.80 and get some examples out there ane people using it while there's momentum.  It feels like there's a swell here and a wave to ride.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>41</wp:post_id>
		<wp:post_date>2005-11-03 11:55:54</wp:post_date>
		<wp:post_date_gmt>2005-11-03 18:55:54</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-citation-microformat-when-worlds-collide</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>ruby-chicago</title>
		<link>http://inkdroid.org/2005/11/08/ruby-chicago/</link>
		<pubDate>Tue, 08 Nov 2005 16:40:55 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=42</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Last night I took the train into <a href="http://www.thoughtworks.com">ThoughtWorks</a> to check out the <a href="http://web.archive.org/web/20060505062043/http://ruby.meetup.com/55/">Chicago Area Ruby Group</a> meeting. There wasn't a planned talk, so I wasn't sure what to expect (apart from getting a chance to chat with <a href="http://www.multiply.org/notebook/">Jason</a> and <a href="http://weblog.lonelylion.com/">Chris</a>). One thing I definitely didn't expect was seeing close to 30 people there.</p>

<p>The room we met in was kind of an atrium type of space. Everyone arranged themselves into a circle, and at the center of the room there was a smaller table with 6 chairs. After everyone went round an introduced themselves the function of the central table was revealed by <a href="http://web.archive.org/web/20080516072711/http://blog.objo.com/">Joe O'Brien</a> who got everyone to play this discussion game called fishbowl. Basically there are 6 seats, and any 5 people can sit in them at any time (always leaving one chair open). People start talking about stuff, and if at any point someone wants to join the discussion then they sit down in the empty seat, and someone who no longer wants to talk can get up and leave. At all times the 5 seats needed to stay filled.</p>

<p>This fishbowl actually worked out really well. The conversation ranged from ruby's performance, to comparisons with java, rails, the community, the "Ruby Way", joy, and practical examples of Ruby in the workplace. All in all it was a very pleasant meeting, and it was really interesting to see a good cross section of Chicago technology in an informal environment. Afterwards Jason, Chris and I went and had a few drinks at a nearby bar with <a href="http://conio.net/">Sam Stephenson</a> and <a href="http://web.archive.org/web/20130113064310/http://www.vernix.org/marcel/">Marcel Molina</a> of <a href="http://www.37signals.com">37Signals</a>. Sam and Marcel are both core developers for the Rails project, and recently moved to Chicago to join 37Signals. Sam is the developer behind <a href="http://web.archive.org/web/20080708185736/http://prototype.conio.net/">prototype</a> which I've been meaning to learn more about. Hopefully Sam can be coerced into doing a prototype talk at some point.</p>

<p>It's been interesting watching local perl, python, ruby and java groups and how they regard each other. Chris mentioned that it's unfortunate when discussion borders on digging at the other guy, and that the real thing that unites these groups is that everyone enjoys programming, and works on stuff on their own time. If the focus could brought to be that level I think there could very well be occasional cross language meetings. I mentioned to <a href="http://wiseheartdesign.com/">John Long</a> that perhaps a meeting about javascript could bring folks from other languages together. He had a great idea of having someone like Sam talk about javascript, and then break off into smaller groups that talked about integration with various languages. Anyhow it was well worth the train ride in. Thanks for putting up with me being away for an evening Kesa :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>42</wp:post_id>
		<wp:post_date>2005-11-08 09:40:55</wp:post_date>
		<wp:post_date_gmt>2005-11-08 16:40:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>ruby-chicago</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="chicago"><![CDATA[chicago]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>10</wp:comment_id>
			<wp:comment_author><![CDATA[Brian Ray]]></wp:comment_author>
			<wp:comment_author_email>bray@sent.com</wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20101127052904/http://brianray.chipy.org:80/</wp:comment_author_url>
			<wp:comment_author_IP>216.80.126.112</wp:comment_author_IP>
			<wp:comment_date>2005-11-30 21:00:14</wp:comment_date>
			<wp:comment_date_gmt>2005-12-01 04:00:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>It is interesting to see programmers of any sort get together and have some fun.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>access2005 presentations</title>
		<link>http://inkdroid.org/2005/11/08/access2005-presentations/</link>
		<pubDate>Tue, 08 Nov 2005 19:01:36 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=43</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Unfortunately I wasn't able to make it to <a href="http://web.archive.org/web/20110610010758/http://access2005.library.ualberta.ca:80/">Access</a> this year where lots of library developer types I respect and learn from were presenting and <a href="http://web.archive.org/web/20080531162547/http://access2005.library.ualberta.ca:80/hackfest.php">hacking</a>. Fortunately the <a href="http://web.archive.org/web/20080517040408/http://access2005.library.ualberta.ca:80/presentations/">audio and slides</a> are now available. Combined with the <a href="http://web.archive.org/web/20071018163909/http://access2005.library.ualberta.ca:80/planetaccess/">collected blogging</a>  and snippets in <a href="irc://irc.freenode.net/code4lib">irc</a> I almost feel like I was there...but I imagine the real brain storming and fun happened outside of these artifacts. Inspiring stuff, and highly recommended if you're into writing software for libraries/archives.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>43</wp:post_id>
		<wp:post_date>2005-11-08 12:01:36</wp:post_date>
		<wp:post_date_gmt>2005-11-08 19:01:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>access2005-presentations</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>search @ delicious and the bbc</title>
		<link>http://inkdroid.org/2005/11/09/search-delicious-and-the-bbc/</link>
		<pubDate>Thu, 10 Nov 2005 04:55:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=44</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I just noticed that <a href="http://del.icio.us/">del.icio.us</a> now has full, fast search across all content (not just your own bookmarks). This is something that Dan's <a href="http://www.unalog.com">unalog</a> has had on delicious for a while (apart from the delightful content). Dan uses <a href="http://pylucene.osafoundation.org/">pylucene</a> as his search engine, which still has some interesting <a href="http://web.archive.org/web/20080716061301/http://unalog.com:80/about/search">features</a>. It's pretty wild being able to search across all the delicious content, given their volume.</p>

<p>When delicious was really ramping up I saw the occasional <a href="http://www.masonhq.com/">mason</a> error page, so I know that they are (or were) using Perl. This makes me really curious to know what search technology they are using...but I couldn't find any details in the <a href="http://web.archive.org/web/20071011170242/http://blog.del.icio.us:80/blog/2005/11/find_the_url_of.html">announcement</a>.</p>

<p>Likewise, the <a href="http://web.archive.org/web/20081025142021/http://www.hackdiary.com:80/archives/000071.html">news</a> about the BBC Programme Catalogue being built with <a href="http://www.rubyonrails.org/">RubyOnRails</a>. I've really come to appreciate Lucene and PyLucene and am in search of similar search tools for Ruby. I've got an email out to Matt Biddulph to see if he can provide any details about the BBC effort.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>44</wp:post_id>
		<wp:post_date>2005-11-09 21:55:06</wp:post_date>
		<wp:post_date_gmt>2005-11-10 04:55:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>search-delicious-and-the-bbc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>6</wp:comment_id>
			<wp:comment_author><![CDATA[Eric Sinclair]]></wp:comment_author>
			<wp:comment_author_email>esinclai@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.kittyjoyce.com/eric/log/</wp:comment_author_url>
			<wp:comment_author_IP>64.1.133.235</wp:comment_author_IP>
			<wp:comment_date>2005-11-10 05:31:27</wp:comment_date>
			<wp:comment_date_gmt>2005-11-10 12:31:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>How about Ferret:</p>

<p>http://web.archive.org/web/20110514072656/http://ferret.davebalmain.com/trac</p>

<p>which seems to be a Ruby port of Lucene.</p>

<p>I've been lurking on the Ruby Weekly news page (and various others) trying to figure out if I want to invest the time learning.  So far, a thumbs up.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>7</wp:comment_id>
			<wp:comment_author><![CDATA[jonvw]]></wp:comment_author>
			<wp:comment_author_email>jonvw@jonvw.com</wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20080317143551/http://jonvw.com:80/</wp:comment_author_url>
			<wp:comment_author_IP>24.14.90.72</wp:comment_author_IP>
			<wp:comment_date>2005-11-10 08:40:28</wp:comment_date>
			<wp:comment_date_gmt>2005-11-10 15:40:28</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>I've been looking for a similar search tool for Ruby.  Have you seen ferret?  It is a port of Lucene to Ruby.  I guess the performance isn't great right now, and it's not terribly finished, but it should be very useful as development continues.</p>

<p>http://web.archive.org/web/20110514072656/http://ferret.davebalmain.com/trac</p>

<p>I've also been meaning to play around with ruby/odeum.  I understand that performance is pretty good, but it's only for indexing; it doesn't do any of the lexical parsing that I understand Lucene performs.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>8</wp:comment_id>
			<wp:comment_author><![CDATA[Administrator]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>12.47.12.131</wp:comment_author_IP>
			<wp:comment_date>2005-11-10 09:12:30</wp:comment_date>
			<wp:comment_date_gmt>2005-11-10 16:12:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Many thanks Eric and jonvw -- I'll take a look at Ferret. I missed you guys at the ruby meetup on Monday. Hopefully we can get together next time.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>jython niceties</title>
		<link>http://inkdroid.org/2005/11/11/jython-niceties/</link>
		<pubDate>Fri, 11 Nov 2005 19:29:56 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=45</guid>
		<description></description>
		<content:encoded><![CDATA[While playing around with the Java <a href="http://www.jdom.org/">JDOM</a> library, I found myself resorting to jython to experiment with the API. It's just so much easier this way for me:

<pre lang="python">
#!/usr/bin/env jython

from java.io import StringReader

from org.jdom import Document
from org.jdom.input import SAXBuilder
from org.jdom.xpath import XPath

xml = '<foo><bar>foobar</bar></foo>'

builder = SAXBuilder()
document = builder.build(StringReader(xml))
xpath = XPath.newInstance('//foo/bar')
node = xpath.selectSingleNode(document)
print node.getText()
</pre> 

In case it's of interest I've got a little jython startup script which automatically makes .jar files I drop in a particular directory available to the interpreter. So when testing jdom all i had to do was drop jdom.jar in my /usr/local/jython/jars and it's immediately available the next time I start up jython.

<pre lang="sh">
#!/bin/bash

JYTHON_HOME=/usr/local/jython

for jar in $JYTHON_HOME/jars/*.jar
do
    jars=&quot;$jars:$jar&quot;
done

CLASSPATH=&quot;$JYTHON_HOME/dist/jython.jar$jars&quot;

java -cp $CLASSPATH \
	$JYTHON_JAVA_ARGS -Dpython.home=$JYTHON_HOME \
	org.python.util.jython &quot;$@&quot;

</pre>

Pretty handy, especially for interactive sessions.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>45</wp:post_id>
		<wp:post_date>2005-11-11 12:29:56</wp:post_date>
		<wp:post_date_gmt>2005-11-11 19:29:56</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>jython-niceties</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="java"><![CDATA[java]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>buddhism and spimes</title>
		<link>http://inkdroid.org/2005/11/13/buddhism-and-spimes/</link>
		<pubDate>Sun, 13 Nov 2005 14:19:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=46</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The Dalai Lama has an <a href="http://www.nytimes.com/2005/11/12/opinion/12dalai.html">op-ed</a> on science and faith in yesterdays New York Times. There are some delightful descriptions of his encounters with science as a child, which I imagine are excerpts from his recent <a href="http://www.amazon.com/exec/obidos/tg/detail/-/076792066X">book</a>. I also like how he <a href="http://www.mozilla.org/blue-sky/misc/199805/">intertwingles</a> religion and science--not making one higher up in a hierarchy.</p>

<blockquote>If science proves some belief of Buddhism wrong, then Buddhism will have to change. In my view, science and Buddhism share a search for the truth and for understanding reality. By learning from science about aspects of reality where its understanding may be more advanced, I believe that Buddhism enriches its own worldview.</blockquote>

<p>And the converse:</p>

<blockquote>Just as the world of business has been paying renewed attention to ethics, the world of science would benefit from more deeply considering the implications of its own work. Scientists should be more than merely technically adept; they should be mindful of their own motivation and the larger goal of what they do: the betterment of humanity.</blockquote>

<p>The impact of science and our way of life on our environment is something I've been reading about in Bruce Sterling's <a href="http://www.amazon.com/exec/obidos/tg/detail/-/0262693267/qid=1131890069/sr=8-2/ref=pd_bbs_2/102-2855265-7932924">Shaping Things</a>. I haven't finished it yet but the essential message so far is that we need to design objects in our environment so that they can reveal information about how they fit into the environment. This information amounts to links to databases that can track the history of the object, how to get customer support, history of ownership, manufacturing origins, internal components, details on customizing and interfacing, etc. Sterling calls these objects <a href="http://en.wikipedia.org/wiki/Spime">spimes</a> and if you are interested his <a href="http://web.archive.org/web/20110629073810/http://www.boingboing.net:80/images/blobjects.htm">speech at SIGGRAPH</a> has more details.</p>

<p>I'm not entirely sure why I'm mentioning both Spimes, Buddhism and Ted Nelson in the same breath. I suppose all three focus the attention on just how deeply interconnected we all are with each other and with the world around us. Sometimes these interconnections can be overwhelming. Meditating on this inter-connectedness, and building tools to manage the connections responsibly are two worthwhile things I'd like to work on.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>46</wp:post_id>
		<wp:post_date>2005-11-13 07:19:49</wp:post_date>
		<wp:post_date_gmt>2005-11-13 14:19:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>buddhism-and-spimes</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="category" nicename="religion"><![CDATA[religion]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>BBC Catalogue&#039;s Search</title>
		<link>http://inkdroid.org/2005/11/26/bbc-catalogue-engine/</link>
		<pubDate>Sat, 26 Nov 2005 13:21:02 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=47</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I did end up hearing back from <a href="http://www.hackdiary.com/">Matt Biddulph</a> about the search technology that he's using with <a href="http://www.rubyonrails.org">RubyOnRails</a> to build the BBC Programme Catalogue.</p>

<blockquote>
The core of the search is nothing more than mysql 4.1's fulltext indexer. I used to think very poorly of it until I discovered how to turn off its automatic stoplist and minimum indexable word length, and started using its boolean mode. Having the database manage the indexing without having to keep a separate index in sync is very valuable, and of course it's portable to any client language.

The nice thing with a dataset the size and quality of the BBC's is that you're not solely dependent on the quality of the freetext indexer. I've done a little statistical analysis on the data to help with scoring the results. For example, programme contributors can be ranked according to how many shows they've contributed to, and commonly co-occurring contributors can be easily calculated with a bit of overnight batch processing. This kind of stuff contributes to a pretty good set of search results.
</blockquote>

<p>Given the visibility of the BBC Catalogue and that it has nearly a million records this says good things to me about the scalability of MySQL's <a href="http://dev.mysql.com/doc/refman/5.0/en/fulltext-search.html">fulltext search</a>. I'll definitely consider it along with <a href="http://web.archive.org/web/20110514072656/http://ferret.davebalmain.com/trac">Ferret</a> for Rails experiments that need search functionality.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>47</wp:post_id>
		<wp:post_date>2005-11-26 06:21:02</wp:post_date>
		<wp:post_date_gmt>2005-11-26 13:21:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bbc-catalogue-engine</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>code4lib 2006</title>
		<link>http://inkdroid.org/2005/12/02/code4lib-2006/</link>
		<pubDate>Fri, 02 Dec 2005 07:54:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=48</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Some #code4lib regulars (who also help put on <a href="http://web.archive.org/web/20110610010758/http://access2005.library.ualberta.ca:80/">Access</a> up north) have managed to get some space at the <a href="http://www.oregonstate.edu">Oregon State University</a> in February for <a href="http://www.code4lib.org/2006">code4lib 2006</a>:</p>

<blockquote>
code4lib 2006 is a loosely structured conference for library technologists to commune, gather/create/share ideas and software, be inspired, and forge collaborations. It is also an outgrowth of the Access HackFest, wrapped into a conference-ish format. It is *the* event for technologists building digital libraries and digital information systems, tools, and software.
</blockquote>

<p>A call for <a href="http://web.archive.org/web/20060104184731/http://www.code4lib.org:80/proposals">proposals</a> is out. The nice thing about this conference is that there will be different levels of involvement: from keynote speakers, to shorter presentations, to lightning talks, and with space/time to actually hack at stuff/brainstorm with colleagues.</p>

<p>We're hoping to attract both library professionals who use computers, and computer professionals who have an interest in libraries. The registration is now open as well at a discounted price. If you are interested in computers and libraries please submit a proposal or register to attend!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>48</wp:post_id>
		<wp:post_date>2005-12-02 00:54:52</wp:post_date>
		<wp:post_date_gmt>2005-12-02 07:54:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>code4lib-2006</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>14</wp:comment_id>
			<wp:comment_author><![CDATA[ablog &raquo; code4lib 2006]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://ablog.apress.com/?p=770</wp:comment_author_url>
			<wp:comment_author_IP>65.19.150.103</wp:comment_author_IP>
			<wp:comment_date>2005-12-05 12:07:48</wp:comment_date>
			<wp:comment_date_gmt>2005-12-05 19:07:48</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid &#8212; code4lib 2006 [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>snakes and rubies</title>
		<link>http://inkdroid.org/2005/12/04/snakes-and-rubies/</link>
		<pubDate>Mon, 05 Dec 2005 04:09:45 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=49</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="https://web.archive.org/web/20060213060312/http://snakesandrubies.com/images/snakeandruby.gif" align="left" /> I managed to attend <a href="https://web.archive.org/web/20060212172955/http://snakesandrubies.com/event">Snakes and Rubies</a> yesterday where Adrian Holovaty and David Heinemeier Hansson talked about their respective web frameworks: <a href="http://www.djangoproject.com/">Django</a> and <a href="http://www.rubyonrails.org/">Rails</a>.</p>

<p>The event started at 2PM and went to 6:30PM or so, and was attended by over 100 people! I watched this little event take shape out of the mists of the local python and ruby mailing lists and was just amazed to see how vibrant the Chicago software development scene is, or has become in the 3 years I've lived in the area.</p>

<p>What's more Adrian and David did a great job promoting both of their projects, while remaining amiable and respectful of the other camp. It's hard to imagine a similar event between two commercial frameworks. Both were given about 45 minutes or so to talk about their software in any way they wanted. They both had extremely different yet effective presentation styles, and their projects had one important thing in common: disillusionment with PHP.</p>

<p>Rather than talking technical details Adrian spent most of his time focused on how Django came to be down in Kansas at <a href="http://www.lawrence.com/">lawrence.com</a>. lawrence.com began it's life as a PHP application which served as a community site for all sorts of goings on in Lawrence, Kansas. The site interleaved all sorts of local entertainment content: music, dining, art, movies...and it encouraged user participation. For example you can listen to mp3s from local musicians, but here's the twist, you listen to them as they are playing in town...so if you like a song you can jump over to the venue later that week to see them live. Another example was a full on sports site for the local little leagues which posted details of games, scores, weather conditions, etc. All of this was detailed to show how deeply intertwined all the data was.</p>

<p>The really interesting stuff for me was when Adrian described how journalism informed his software development practices...and how Django fed into this process. In the same way that journalists work against the clock to get news stories out quickly and accurately Adrian and his team worked to get software projects done often on similar deadlines (sometimes like 4 hours). They quickly found that their PHP infrastructure wasn't allowing them to respond quickly enough without introducing side effects, and decided that they needed new tools...which is how Django was born. In fact the little league application mentioned above was the first Django application.</p>

<p>Adrian has since moved on to the Washington Post, where he is their resident web technology mad scientist. Apparently they are using Django in some form at the Post, or are planning to since he mentioned Django's caching can scale to the 9 million odd requests the Post gets in a single day.</p>

<p>Unfortunately my lead pencil ran out of lead just a bit of the way into David's talk, so I don't have as much written down from the Rails presentation. David dropped some wonderful one liners that I wish I could have written down. Much unlike Adrian, David let actual code do most of the talking for him.</p>

<p>Early on he had a screen with a quote from Richard Feynman on the importance of finding beautiful solutions to problems (if you remember the quote please let me know). This quote kind of guided the rest of the talk where David showed off beautiful Model, View and Controller code from RubyOnRails...and it really was beautiful stuff. David's thesis was that beautiful things make you happy, and happiness makes you more productive...so beautiful code will make for happy, productive programmers. Much of this comes back to the essential philosophy of Ruby--to give joy to programmers. At any rate, the lights were dimmed and David gave us a tour of what RubyOnRails code looks like, while highlighting some of the strengths of the project and the Ruby language. On one of the pages there was some code to set a cookie expiration, and the date was created like so:</p>

<pre lang="ruby">20.years.from_now
</pre>

<p>How cool is that! I wasn't sure if this was part of Ruby proper until I fired up my ruby interpreter to check:</p>

<pre lang="ruby">biblio:~ ed$ irb
irb(main):001:0> 20.years.from_now
NoMethodError: undefined method `years' for 20:Fixnum
        from (irb):1   
</pre>

<p>Whereas from the Rails console it works fine:</p>

<pre lang="ruby">biblio:~/Projects/cheap ed$ script/console 
Loading development environment.
>> 20.years.from_now
=> Thu Dec 04 15:38:48 CST 2025 
</pre>

<p>So Rails decorates the Fixnum class with the years method. Pretty awesome :-) Another thing David highlighted was that Ruby is used everywhere, from configuration, to writing XML, to writing JavaScript. I was even surprised to hear him argue for full on Ruby in view templates. His argument is that even when a framework offers only a limited set of tags, it's still offering logic, and rather than creating some bastardized tag language why not just use tried and true Ruby.</p>

<p>The two presentations were followed by a few (not many) moderated questions, and some questions from the audience. The highlight for me was when Why the Lucky Stiff's question was asked:</p>

<blockquote>
  <p>Looking a bit beyond web frameworks, how do you envision the world coming to an end? David responded by "scoping" the world to mean the world of software development and said that this world would come to an end if the layers of Java "sedimentation" continue to accrue. He went on to predict that we're at a crossroads in software development, and that a paradigm shift is underway...intentionally provocative, and pretty much right on as far as web development goes if you ask me. Adrian responded "Yoko Ono".</p>
</blockquote>

<p>So, as you can tell I'm still digesting the presentations and discussion. There was so much good stuff, and I was really struck by the collegiality between the two guys: open source software development at its finest. The two main things I took away were embracing the boundaries between software development and a particular industry like Journalism, or in my case Libraries; and always trying to strive for the beautiful in software, "boiling down" a thorny problem into its most simple and elegant expression.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>49</wp:post_id>
		<wp:post_date>2005-12-04 21:09:45</wp:post_date>
		<wp:post_date_gmt>2005-12-05 04:09:45</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>snakes-and-rubies</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="chicago"><![CDATA[chicago]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>12</wp:comment_id>
			<wp:comment_author><![CDATA[import this. &raquo; Blog Archive &raquo; Snakes and Rubies roundup]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://thinkhole.org/wp/2005/12/04/snakes-and-rubies-roundup/</wp:comment_author_url>
			<wp:comment_author_IP>65.254.1.1</wp:comment_author_IP>
			<wp:comment_date>2005-12-04 21:51:46</wp:comment_date>
			<wp:comment_date_gmt>2005-12-05 04:51:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Finally, Indroid was struck by the collegiality between the two speakers a very detailed writeup of the event. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>13</wp:comment_id>
			<wp:comment_author><![CDATA[Pat Eyler]]></wp:comment_author>
			<wp:comment_author_email>pat.eyler@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>12.110.19.201</wp:comment_author_IP>
			<wp:comment_date>2005-12-05 11:40:42</wp:comment_date>
			<wp:comment_date_gmt>2005-12-05 18:40:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Andy,
nice write up of an event I wish I could have gone to.  It's nice to see the cross-pollinization of the various dynamic libraries out there (highlighted by reading a Perl guy's view of a meeting between Python and Ruby).

Good to read your words again.
-pate]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>15</wp:comment_id>
			<wp:comment_author><![CDATA[Gheorghe Milas]]></wp:comment_author>
			<wp:comment_author_email>gmilas@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.emilas.com/george</wp:comment_author_url>
			<wp:comment_author_IP>68.20.90.66</wp:comment_author_IP>
			<wp:comment_date>2005-12-05 12:33:53</wp:comment_date>
			<wp:comment_date_gmt>2005-12-05 19:33:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm from the python crowd. I was there too and also noticed 20.years.from_now and that is exactly what I don't like in ruby. While it reads nice, the danger I see here is that someone does not only need to learn ruby to understand the code in rails they also need to learn all the rails languages to understand rails. Someone may argue that this is no diffrenet then learning a projects api's, I hope that to be right but my feeling is that it is not right, expecialy when someone changes builtins to do what they think they should do.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>16</wp:comment_id>
			<wp:comment_author><![CDATA[Pat Eyler]]></wp:comment_author>
			<wp:comment_author_email>pat.eyler@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>12.110.19.97</wp:comment_author_IP>
			<wp:comment_date>2005-12-05 13:04:47</wp:comment_date>
			<wp:comment_date_gmt>2005-12-05 20:04:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[*sigh*
I hate Mondays ... Ed, it's a nice write up.   Sorry to cross wires on your name]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>17</wp:comment_id>
			<wp:comment_author><![CDATA[jonvw]]></wp:comment_author>
			<wp:comment_author_email>jonvw@jonvw.com</wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20080317143551/http://jonvw.com:80/</wp:comment_author_url>
			<wp:comment_author_IP>24.14.90.72</wp:comment_author_IP>
			<wp:comment_date>2005-12-05 14:31:46</wp:comment_date>
			<wp:comment_date_gmt>2005-12-05 21:31:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Ed, I'm curious to hear more of your thoughts about embracing the boundaries between software development and libraries.  I was thinking about that a lot after hearing the presentation, as well.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>140</wp:comment_id>
			<wp:comment_author><![CDATA[St.]]></wp:comment_author>
			<wp:comment_author_email>stephen.yearl@yale.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>130.132.146.234</wp:comment_author_IP>
			<wp:comment_date>2006-03-14 08:56:28</wp:comment_date>
			<wp:comment_date_gmt>2006-03-14 15:56:28</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Not sure about the Feynman, but Fuller suggested that 'if the solution is not beautiful, I know it is wrong.']]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Washington Post U.S. Congress Votes Database</title>
		<link>http://inkdroid.org/2005/12/05/washington-posts-us-congress-votes-database/</link>
		<pubDate>Mon, 05 Dec 2005 22:18:22 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2005/12/05/washington-posts-us-congress-votes-database/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Umm, wow! Adrian Holovaty <a href="http://www.holovaty.com/blog/archive/2005/12/05/1513">announced</a> the Washington Post <a href="http://projects.washingtonpost.com/congress/">Congressional Votes Database</a>.  This site is of important for at least two reasons:</p>

<ul>
<li>it offers <a href="http://projects.washingtonpost.com/congress/rss/">RSS feeds</a> for  tracking the voting of your house/senate representative</li>
<li>it is powered by <a href="http://www.python.org">Python</a> and the <a href="http://www.djangoproject.com/">Django</a> web framework.
</li></ul>

<p>As far as the RSS goes I just pulled up Dick Durbin's <a href="http://projects.washingtonpost.com/congress/members/d000563/">recent votes</a> and there were over 20 events since 11/17/2005, whereas the comparable service from <a href="http://www.govtrack.us">GovTrack</a> had only one event since then.</p>

<p>After the election I daydreamed about somehow getting involved in the political process in a technical way...which is how I found my way to GovTrack, who are essentially doing very elaborate screen scraping of the <a href="http://thomas.loc.gov/">Thomas</a> database at the Library of Congress. One thing I really like about the GovTrack project is they are making their <a href="http://web.archive.org/web/20080517173534/http://www.govtrack.us/source.xpd">data</a> available as RDF, for downstream applications. Adrian's work seems to draw on a richer data source, as I imagine is the case at a place like the Post.  All I can say is well done, and damn...you've only been there for a couple months right? Talk about hitting the ground running.</p>

<p>At the recent Snakes and Rubies Adrian indicated that there was going to be some huge Django related news. When the voting db hit my Instant Messenger, IRC client and RSS aggregator I thought that this was it. But according to Adrian there's something bigger in the works...</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>50</wp:post_id>
		<wp:post_date>2005-12-05 15:18:22</wp:post_date>
		<wp:post_date_gmt>2005-12-05 22:18:22</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>washington-posts-us-congress-votes-database</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>gmail + atom</title>
		<link>http://inkdroid.org/2005/12/06/gmail-atom/</link>
		<pubDate>Tue, 06 Dec 2005 15:56:21 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=51</guid>
		<description></description>
		<content:encoded><![CDATA[I imagine this is old hat to long time <a href="http://gmail.com">gmail</a> users but I just noticed that my gmail is available via atom...and very easily at that with Mark Pilgrim's <a href="http://feedparser.org/">Universal Feed Parser</a>.

<pre lang="python">
    import feedparser

    feed = feedparser.parse(
        'https://username:password' + 
        '@gmail.google.com/gmail/feed/atom')

    for entry in feed.entries:
        print entry.title
</pre>

 
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>51</wp:post_id>
		<wp:post_date>2005-12-06 08:56:21</wp:post_date>
		<wp:post_date_gmt>2005-12-06 15:56:21</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>gmail-atom</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>selenium/ruby sprint</title>
		<link>http://inkdroid.org/2005/12/07/seleniumruby-sprint/</link>
		<pubDate>Wed, 07 Dec 2005 19:49:10 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=52</guid>
		<description></description>
		<content:encoded><![CDATA[In case you missed it, or aren't subscribed the <a href="http://rubyforge.org/pipermail/chicagogroup-members-list">chicago-ruby</a> group is getting together for a <a href="http://selenium.thoughtworks.com">Selenium</a> demo/sprint <a href="http://rubyforge.org/pipermail/chicagogroup-members-list/2005-December/000135.html">session</a> on Dec 13th. I've seen Selenium demo'd before at a <a href="chipy.org">chipy</a> meeting, and look forward to a more in depth look since several of my friends really like this testing tool. I think Jason is particularly interested in getting some Ruby driver support.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>52</wp:post_id>
		<wp:post_date>2005-12-07 12:49:10</wp:post_date>
		<wp:post_date_gmt>2005-12-07 19:49:10</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>seleniumruby-sprint</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="chicago"><![CDATA[chicago]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>lib4code</title>
		<link>http://inkdroid.org/2005/12/08/lib4code/</link>
		<pubDate>Thu, 08 Dec 2005 16:14:33 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=53</guid>
		<description></description>
		<content:encoded><![CDATA[Thanks to <a href="http://www.librarian.net/stax/1572">Jessamyn</a> I found <a href="http://scilib.typepad.com/science_library_pad/2005/12/librarians_20_d.html">Librarians 2.0 don't need to be coders 2.0</a> where Richard Ackerman has an interesting take on just how important programming skills are for a library technologist. Richard cites a <a href="http://www-128.ibm.com/developerworks/webservices/library/ws-soad1/">paper</a> from IBM on Service Oriented Architectures to make a compelling point that there are many roles to play when building technology solutions, particularly the web services that comprise such a big part of "library 2.0" efforts...and that coding skills really aren't that important when you can just get a student, consultant, or vendor (heh) to do it.

It's unfortunate I think that the <a href="http://www.code4lib.org">code4lib 2006</a> conference <i>name</i> seems to emphasize "coding" so much over the ideas.  I totally agree that the most important aspect of our work as library technologists are the service ideas, and that the code is simply a machine readable description of these ideas.  Some high level languages are actually really, really nice for expressing ideas, and I would argue that often times learning a good computer language can help you express your technology ideas better. As Martin Fowler says:

<blockquote>
Any fool can write code that a computer can understand. Good programmers write code that humans can understand.
</blockquote>

Let me go on record, as someone who has helped organize the code4lib conference, to say non-coders are more than welcome...there will be plenty of people who can program there...we want ideas, mindshare and collaboration. Please don't let the computer programming jargon dissuade you from participating.

Also, a few things stood out to me eye:

<blockquote>
If your goals and architecture are clear enough, the coders don't need to be library experts in order to deliver the functions you need.
</blockquote>

The coders don't need to be experts, but wouldn't it be nice if they were, and you didn't have to go into great detail about certain things? Wouldn't it also be nice if the coders didn't start from scratch and were aware of good reusable components from the library software community which could be leveraged to make the software construction phase that much faster?  Indeed, architectural decisions often have a direct effect on programming decisions that are made, and it helps if those who are architecting things have at least a general understanding of how software is built so that designs stay doable and sane...and so that they'll know when things are drifting of course.

<blockquote>
Also, don't try to build big complex systems.  Live in the beta world.  Get some chunk of functionality out quickly so that people can play with it.  The hardest part is having the initial idea, and the good news is I see lots of great ideas out in the library blogosphere.  I can understand the frustrations in the gap between the idea and running code, but I hope I've presented a bunch of areas above in which you can work to turn the idea into the next hot beta, without necessarily needing to code it yourself.
</blockquote>

The one danger to moving to a formal process like the one described in the article by IBM is that it may encourage you to build big complex systems on a slow time scale. If you need to thoroughly describe a software solution before beginning to program (the so called <a href="http://en.wikipedia.org/wiki/Waterfall_model">waterfall model</a>) you will be spend a lot of time trying to get the design right before even beginning to code to see what actually works. I've found the more that the design and the coding can be intermingled the better...since it lets them both inform each other as they go on.  This intermingling is easy if you are a small shop and you have a handful of people (1-8) that need to communicate on a regular basis. I imagine most software development groups in libraries are around this size.  That being said I think that Richard is right, it's good to be aware of the different roles that are being played, perhaps by one individual.

After seeing Adrian talk about software development and journalism at <a href="http://snakesandrubies.com">Snakes and Rubies</a> I've been thinking off and on about the space between libraries and software development. I'm particularly interested in how one informs the other...and found Richard's post to be a good catalyst.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>53</wp:post_id>
		<wp:post_date>2005-12-08 09:14:33</wp:post_date>
		<wp:post_date_gmt>2005-12-08 16:14:33</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lib4code</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>18</wp:comment_id>
			<wp:comment_author><![CDATA[Richard Akerman]]></wp:comment_author>
			<wp:comment_author_email>scilib@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://scilib.typepad.com/</wp:comment_author_url>
			<wp:comment_author_IP>70.25.91.244</wp:comment_author_IP>
			<wp:comment_date>2005-12-08 10:12:43</wp:comment_date>
			<wp:comment_date_gmt>2005-12-08 17:12:43</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks, I'm glad the posting raised some useful points.  The diagram is really there to show all the different roles and slices you can take as part of a systems development effort.  It does look kind of waterfall-y, that was kind of a tradeoff.  I think there is a whole separate software engineering discussion that you can have, and you have raised a lot of the important points (small teams, good communication etc.)  In terms of actual development process I very much agree with you that you need to use an iterative, incremental, rapid design process.  

I wrote about one that CISTI has tried in "having a RAD time with GRAPPLE".

http://scilib.typepad.com/science_library_pad/2005/02/software_develo.html

I do believe as stated in that post that you need a strong requirements process and architecture in order to be able to do rapid development successfully.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>opensearch and autodiscovery</title>
		<link>http://inkdroid.org/2005/12/14/opensearch-and-autodiscovery/</link>
		<pubDate>Wed, 14 Dec 2005 07:50:38 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=55</guid>
		<description></description>
		<content:encoded><![CDATA[I just noticed that a9 has released a second draft of <a href="http://opensearch.a9.com/spec/changelog.jsp">opensearch v1.1</a>. This draft includes details on opensearch <a href="http://opensearch.a9.com/spec/1.1/description/#autodiscovery">autodiscovery</a> for providing a reference to the opensearch description file in an HTML page. This could have a lot of potential for browser plugins. Also, they've added a Query element that can be used for echoing back the query that was used to generate results...kinda like the  echoedRequest in <a href="http://www.loc.gov/z3950/agency/zing/srw/sru.html">SRU</a>. These are the things that popped out at me. Of course the big news in the first draft was that Atom can now be used in responses.

At any rate it was nice to see that they link to my <a href="http://www.textualize.com/opensearch">opensearch python library</a> from their <a href="http://opensearch.a9.com/docs/readers.jsp">tools</a> page. Once 1.1 moves from draft I'm going to work on upgrading it from 1.0 right away.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>55</wp:post_id>
		<wp:post_date>2005-12-14 00:50:38</wp:post_date>
		<wp:post_date_gmt>2005-12-14 07:50:38</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>opensearch-and-autodiscovery</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>on delicious</title>
		<link>http://inkdroid.org/2005/12/15/on-delicioius/</link>
		<pubDate>Thu, 15 Dec 2005 13:28:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=56</guid>
		<description></description>
		<content:encoded><![CDATA[<img src="/images/delicious.jpg" align="left" style="border: thin dashed; margin: 10px;"/>

Ari Paparo has some interesting <a href="http://www.aripaparo.com/archive/001456.html">notes</a> on what made delicious succeed where his company (doing essentially the same thing in 1999 with 13 million dollars) failed. Two things that really resonated with me were: <i>defaults matter</i> and <i>folders suck</i>. 

Ari's site blink.com made bookmarks private by default but allowed users to share them--whereas delicious makes them public. This encouraged looking at the bookmarks globally as the purpose of the service, whereas with blink it was an afterthought.

What Ari says about why folders suck was so good I'm going to take the liberty of just quoting a big chunk of it.

<blockquote>
We believed that users would not only make their folders public, but also would categorize those folders into a directory structure. We called this the "Public Library" and created a Yahoo-like node structure on which users could post. This could have made sense since categorizing folders would be less work than categorizing individual bookmarks &#8211; after all, the folders were already "categories" of a sort.

There were several severe problems with this folder-based approach. First, people are very bad and inconsistent at organizing things. One day etrade.com will go into the "finance" folder and another day it will go into the "favorite links" folder. We were taking this fundamental flaw and squaring it &#8211; asking users to use graph their existing categorization onto a second arbitrary structure within the public library. Does my "finance" folder go into the "Business" directory or the "Personal" directory?

Then there was the issue of how deep to go when categorizing folders. If I&rsquo;ve got a folder of "online brokerages" do I put it in the directory at the level of "Finance" since my folder is in a sense a sub-category of finance, or do I put it within the pre-existing "Finance -> Brokerage" directory? Users were confused, and with good reason. 
</blockquote>

Even librarians aren't always that great at categorizing things. We need lots of rules, and don't always remember to follow all of them. It's a tricky business :-)

Case in point I don't even use the tag "taxonomy" consistently (see screenshot above). delicious has a new feature that provides guidance on what tags you've used, and how many times -- when you are typing in a tag. This is extremely handy for displaying just how inconsistent I am...and how to improve the quality of my tags.]]></content:encoded>
		<excerpt:encoded><![CDATA[ ]]></excerpt:encoded>
		<wp:post_id>56</wp:post_id>
		<wp:post_date>2005-12-15 06:28:16</wp:post_date>
		<wp:post_date_gmt>2005-12-15 13:28:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-delicioius</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="business"><![CDATA[business]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>22</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>12.47.12.131</wp:comment_author_IP>
			<wp:comment_date>2005-12-22 06:58:15</wp:comment_date>
			<wp:comment_date_gmt>2005-12-22 13:58:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Chris: yes good point, search is actually way more important than tagging. When delicious and unalog added search it was so much fun to search across all the content.

Roy: The mere fact that people are collecting links to content and assigning terms to them is extremely useful from a data mining perspective. What's more, if people want their content to be found (as is often the case) there is a tendency to use the same tags. Perhaps the tags are used in different and incorrect ways, but let's face it, catalogers aren't perfect either. I think your dismissal of these technologies is premature.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>19</wp:comment_id>
			<wp:comment_author><![CDATA[Chris McAvoy]]></wp:comment_author>
			<wp:comment_author_email>chris.mcavoy@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://weblog.lonelylion.com</wp:comment_author_url>
			<wp:comment_author_IP>63.240.137.252</wp:comment_author_IP>
			<wp:comment_date>2005-12-15 08:25:25</wp:comment_date>
			<wp:comment_date_gmt>2005-12-15 15:25:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah...tagging is better than directories.  On my pc's, I've started to use the various indexers out there (Google Desktop / Tiger Spotlight) rather than trying to maintain a strict hierarchy of document folders.  I'm wondering which OS will bring tags to files first.  I'm guessing it'll be OSX.  Because they're more clever.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>21</wp:comment_id>
			<wp:comment_author><![CDATA[Roy Tennant]]></wp:comment_author>
			<wp:comment_author_email>roy.tennant@ucop.edu</wp:comment_author_email>
			<wp:comment_author_url>http://roytennant.com/</wp:comment_author_url>
			<wp:comment_author_IP>71.139.111.147</wp:comment_author_IP>
			<wp:comment_date>2005-12-21 12:56:32</wp:comment_date>
			<wp:comment_date_gmt>2005-12-21 19:56:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is why I think the idea that people tagging stuff can result in anything remotely useful in a community-wide sense is ridiculous. "Folksonomies" is an oxymoron.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Fear Itself</title>
		<link>http://inkdroid.org/2005/12/28/fear-itself/</link>
		<pubDate>Wed, 28 Dec 2005 17:26:33 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=61</guid>
		<description></description>
		<content:encoded><![CDATA[I'm glad I'm not the only one who was immediately reminded of <a href="http://www.commondreams.org/views05/1227-26.htm">this</a> when the NSA spying <a href="http://www.nytimes.com/2005/12/21/politics/21nsa.html">story</a> broke.  

If you are interested in the perspective of a computer security specialist definitely take a look at what Bruce Scheneier has been <a href="http://www.schneier.com/blog/archives/2005/12/nsa_and_bushs_i.html">writing</a>. Schneier's theory on why Bush needed to bypass the Foreign Intelligence Security Court is pretty harrowing.  

<blockquote>
The NSA's ability to eavesdrop on communications is exemplified by a technological capability called Echelon. Echelon is the world's largest information "vacuum cleaner," sucking up a staggering amount of voice, fax, and data communications -- satellite, microwave, fiber-optic, cellular and everything else -- from all over the world: an estimated 3 billion communications per day. These communications are then processed through sophisticated data-mining technologies, which look for simple phrases like "assassinate the president" as well as more complicated communications patterns.

Supposedly Echelon only covers communications outside of the United States. Although there is no evidence that the Bush administration has employed Echelon to monitor communications to and from the U.S., this surveillance capability is probably exactly what the president wanted and may explain why the administration sought to bypass the FISA process of acquiring a warrant for searches.
</blockquote>

Honestly, this kind of behavior from the Bush Administration isn't at all surprising given their "go it alone" attitude.  However I'm really dissapointed that the ranking members of the House and Senate Intelligence Committees didn't make noise--any noise. I imagine they are bound by some oath or whatnot...but what good are checks and balances if they don't work properly?

Indeed, a recent <a href="http://www.nytimes.com/2005/12/24/politics/24spy.html">article</a> from the NYTimes indicates that Schenier's theory may in fact be, umm fact:

<blockquote>
The National Security Agency has traced and analyzed large volumes of telephone and Internet communications flowing into and out of the United States as part of the eavesdropping program that President Bush approved after the Sept. 11, 2001, attacks to hunt for evidence of terrorist activity, according to current and former government officials.

The volume of information harvested from telecommunication data and voice networks, without court-approved warrants, is much larger than the White House has acknowledged, the officials said. It was collected by tapping directly into some of the American telecommunication system's main arteries, they said.

As part of the program approved by President Bush for domestic surveillance without warrants, the N.S.A. has gained the cooperation of American telecommunications companies to obtain backdoor access to streams of domestic and international communications, the officials said. 
</blockquote>

I'm really worried that we're not teetering on a slippery slope but are actually in free fall.  It appears that telecommunications companies are helping feed data mining operations at the NSA in real time. Perhaps they have a googlish front end where 'professionals' can type in 'keywords' and hit "I'm feeling lucky" and get a list of phone conversations or emails.

The Bush Administration's prolific use of "fear" as an policy wedge is extremely dangerous. As Roosevelt famously <a href="http://historymatters.gmu.edu/d/5057/">said</a> in a time of national crisis:

<blockquote>
So, first of all, let me assert my firm belief that the only thing we have to fear is fear itself: nameless, unreasoning, unjustified terror which paralyzes needed efforts to convert retreat into advance.
</blockquote>

On a somewhat lighter note, Schneier linked to a little <a href="http://www.computerbytesman.com/privacy/emailsnooping.htm">trick</a> devised by Richard M. Smith which allows you to detect if the NSA is monitoring your email communications.  As my friend <a href="http://weblogs.litmusgreen.com/ed/">Ed Silva</a> pointed out in IM:

<blockquote>
I wouldn't try it if you are planning on flying.
</blockquote> 

Uh, yeah I was planning on going to <a href="http://www.code4lib.org/2006">code4lib 2006</a> in a few months....maybe I'll wait.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>61</wp:post_id>
		<wp:post_date>2005-12-28 10:26:33</wp:post_date>
		<wp:post_date_gmt>2005-12-28 17:26:33</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>fear-itself</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="category" nicename="security"><![CDATA[security]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>good fences and the frankenweb</title>
		<link>http://inkdroid.org/2006/01/03/good-fences-and-the-frankenweb/</link>
		<pubDate>Tue, 03 Jan 2006 18:09:02 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=62</guid>
		<description></description>
		<content:encoded><![CDATA[<img src="/images/frankenstein.jpg" width="200" align="left" /> Ian Bicking has some interesting <a href="http://blog.ianbicking.org/theres-so-much-more-than-rails.html">notes</a> about competing web development technologies--mainly in response to <a href="http://blogs.law.harvard.edu/ivan/2005/12/20#a42">some</a> <a href="http://blogs.law.harvard.edu/ivan/2005/07/12#a9">posts</a> from Ivan Krystic.  The discussion is definitely recommended, especially if you find yourself looking at web application frameworks for Python and Ruby. I found the pivot point of the discussion to be around a new term (for me) -- the "frankenweb".

My understanding is that like Frankenstein (a being created by stitching together random body parts from dead humans) the frankenweb is an unholy mixture of MVC components pulled from different projects, when put together result in an ugly partially functional whole. I think this characterization of Ian's work is really unpleasant, but strangely compelling. I think that this is mainly because of Ian's response:

<blockquote>
The "Frankenweb" is a feature, and it describes the web we have, the software we have, and the future that is inevitable. The world was never all J2EE, or ASP(.NET), or PHP, and it won't be all Rails either.
</blockquote>

I think Ian is right on about this: "frankenweb" does describe the web we have, and hopefully the web we will continue to have--and the degree to which we can all interoperate is the degree to which the web will succeed. Perhaps I'm seeing the frankenweb through Weinberger-Colored-Glasses, having just finished <a href="http://www.smallpieces.com/">Small Pieces Loosely Joined</a> (which I thoroughly enjoyed and plan to write about later if there is time).  Weinberger does an excellent job of distilling the essence of the web, and how its architecture enabled it to pull itself up from it's own bootstraps, grow and adapt:

<blockquote>
In the real world, I can't just put in a door from my apartment to my neighbor's so that anyone can go through. But that's exactly how the web was built. Tim Berners-Lee orginally created the web so that scientists could link to the work of other scientists without having to ask their permission. If I put a page into the public Web, you can link to it without having to ask to do anything special, without asking me if it's alright with me, and without even letting me know that you've done it...The web couldn't have been built if everyone had to ask permission first.
</blockquote>

Of course I'm conflating links between pages, and API links between software components...but what Ian says about embracing the frankenweb seems to resonate with this somehow. 

It's also quite disorienting to hear Ivan and others lauding tight coupling: 

<blockquote>
You don't see the Ruby on Rails guys modularizing Rails to the point of pain. You see them delivering a single, high-polish, tightly coupled product that does its job well.
</blockquote>

Given the various pluggable modules that <a href="http://dev.rubyonrails.org/browser/trunk">make up</a> Rails I think "tightly coupled" is largely an overstatement. Granted they are available in the same code base, and I haven't tried to use one of them in isolation--but I imagine it could be done if someone wanted to say, use a activerecord model in a script or something. <a href="http://www.pragmaticprogrammer.com/ppbook/index.shtml">The Pragmatic Programmer</a> has a really nice chapter on decoupling, and the authors are actually heavily involved in the Ruby/Rails community. The chapter starts out with a nice quote from Robert Frost's poem <a href="http://www.writing.upenn.edu/~afilreis/88/frost-mending.html">The Mending Wall</a>:

<blockquote>
Good fences make good neighbors.
</blockquote>

It seems to me that Ian is doing the hard work of patching some of these fences, and building a few and deserves a lot of credit for the effort and cat herding.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>62</wp:post_id>
		<wp:post_date>2006-01-03 11:09:02</wp:post_date>
		<wp:post_date_gmt>2006-01-03 18:09:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>good-fences-and-the-frankenweb</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>23</wp:comment_id>
			<wp:comment_author><![CDATA[Walter Lewis]]></wp:comment_author>
			<wp:comment_author_email>lewisw@hhpl.on.ca</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>70.49.12.12</wp:comment_author_IP>
			<wp:comment_date>2006-01-06 20:14:29</wp:comment_date>
			<wp:comment_date_gmt>2006-01-07 03:14:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[So is it Frankenweb or Service Oriented Architecture, when a block of Ruby code calls an SQL database before generated a response to an request from a Perl RESTful service that another site calls from an asp page, before passing the results through an XSLT transformation to handoff a valid XHTML page?  What matters is that a) it works b) in a timely fashion.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>code4lib conference shaping up</title>
		<link>http://inkdroid.org/2006/01/11/code4lib-conference-shaping-up/</link>
		<pubDate>Wed, 11 Jan 2006 15:41:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/01/11/code4lib-conference-shaping-up/</guid>
		<description></description>
		<content:encoded><![CDATA[The votes are in, and the a <a href="http://www.code4lib.org/2006/schedule">tentative schedule</a> is up. There were a remarkable amount of <a href="http://inkcow.backpackit.com/pub/354440">wonderful</a> presentation ideas submitted, and unfortunately there wasn't the time/space for all of them. Fortunately there will be lightning talks and breakout sessions that will hopefully pick up some of the slack.

The talks were voted on by anyone who planned on attending. That's right *anyone*. This was like a breath of fresh air for me. The voting mechanism was a genius javascript <a href="http://www.code4lib.org/2006/voting/start">hack</a> at the 11th hour by <a href="http://dilettantes.blogspot.com/">Ross Singer</a> which allowed drupal users on code4lib.org to annotate a <a href="http://inkcow.backpackit.com/pub/354440">backpack page</a>, which stored results in a database at gatech.edu. We even hooked up our resident bot in <a href="irc://chat.freenode.net/code4lib">#code4lib</a> to be able to talk to the database and get up to the minute polling results. 

Anyhow, things are looking really good for the conference. If you were waiting for the presentation to firm up before registering take a look at the schedule. And if you need anymore convincing checkout Lorcan Dempsey's <a href="http://orweblog.oclc.org/archives/000917.html">blog</a> which says it all.

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>63</wp:post_id>
		<wp:post_date>2006-01-11 08:41:49</wp:post_date>
		<wp:post_date_gmt>2006-01-11 15:41:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>code4lib-conference-shaping-up</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>2 bookmarklets </title>
		<link>http://inkdroid.org/2006/01/16/2-bookmarklets/</link>
		<pubDate>Mon, 16 Jan 2006 15:27:51 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=67</guid>
		<description></description>
		<content:encoded><![CDATA[Here are a couple of handy bookmarklets that you can drag to your toolbar:

<ul>
<li>
<a href="javascript:void(q=prompt('Web-Archive:',location));if(q)void(location='http://web.archive.org/*/'+q)">archive</a> - I got this one from <a href="http://miscoranda.com/">sbp</a> in <a href="irc://irc.freenode.net/swhack">#swhack</a>. If you are on a page and you'd like to see what it looked like in the past just click on this one and it'll display the archived versions in the <a href="http://archive.org">Internet Archive</a>.
</li>

<li>
<a href="javascript:void(q=prompt('Backrub:',location));if(q)void(location='http://www.google.com/search?as_lq='+q)">backrub</a> - I created this one after reading in <a href="http://www.amazon.com/gp/product/1591840880">The Search</a> about the <a href="http://www-db.stanford.edu/~backrub/google.html">Backrub application</a> which gave birth to Google. Backrub was all about revealing the backlinks to a page, and likewise this bookmarklet uses google to display links that link back to the page you are on.
</li>
</ul>

I've been enjoying <i>The Search</i> quite a bit so far. I was really surprised to learn how much bibliometrics informed Larry Page as he thought about <a href="http://www.webworkshop.net/pagerank.html">PageRank</a>.  In particular Eugene Garfield's <a href="http://www.garfield.library.upenn.edu/essays/V1p527y1962-73.pdf">Citation analysis as a tool in journal evaluation</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>67</wp:post_id>
		<wp:post_date>2006-01-16 08:27:51</wp:post_date>
		<wp:post_date_gmt>2006-01-16 15:27:51</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>2-bookmarklets</wp:post_name>
		<wp:status>private</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>cdbaby</title>
		<link>http://inkdroid.org/2006/01/17/cdbaby/</link>
		<pubDate>Tue, 17 Jan 2006 19:11:29 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/01/17/cdbaby/</guid>
		<description></description>
		<content:encoded><![CDATA[Thanks to a ping from <a href="http://onebiglibrary.net">Dan</a> I just finished listening to an inteview with David Sivers of  <a href="http://www.cdbaby.com">CDBaby</a> over on <a href="http://www.venturevoice.com/2005/11/vv_show_19_derek_sivers_of_cd.html">Venture Voice</a>. Sivers talks about how being a musician and working briefly in the music industry informed his decision to build CDBaby. 

The interview spans a ton of subjects from what's wrong with the music industry and what to do about it; how being in a circus informs your business acumen; why rejecting venture capital can help you focus on what really matters; and how a lot of heart and hubris can get you really far. Really refreshing (and funny) stuff.

CDBaby is now the single largest digital catalog in the world, and provides a gateway for independent musicians into distribution chains like <a href="http://www.apple.com/itunes/">iTunes</a>. They started with three people sharing a 56k modem in Woodstock, NY. What a fun story.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>68</wp:post_id>
		<wp:post_date>2006-01-17 12:11:29</wp:post_date>
		<wp:post_date_gmt>2006-01-17 19:11:29</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>cdbaby</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="business"><![CDATA[business]]></category>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>openurl as microformat  </title>
		<link>http://inkdroid.org/2006/01/18/openurl-as-microformat/</link>
		<pubDate>Thu, 19 Jan 2006 06:11:54 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=74</guid>
		<description></description>
		<content:encoded><![CDATA[	<div class='hreview x-wpsb-review-book'>		<div>			<h3 class='item fn'><span class='btitle'>The Search</span></h3>						                        <p><b>Author</b>: <span class='au'>John Battelle</span></p>                                                <p><b>Year</b>: <span class='date'>2005</span></p>			                        <p><b>Publisher</b>: <span class='pub'>Portfolio Hardcover</span></p>						<p><b>ISBN</b>: <span class='Z3988 isbn' title='ctx_ver=Z39.88-2004&amp;rft_val_fmt=info:ofi/fmt:kev:mtx:book&amp;rft.isbn=1591840880'>1591840880</span></p>		</div>		<div class='description'><p>Ok, so The Search is a great book so far...but I'm really just testing some <a href="/docs/review-book.xml">local modifications</a> I made to the <a href="http://structuredblogging.org">structured blogging</a> tool to use <a href="http://alcme.oclc.org/openurl/servlet/OAIHandler/extension?verb=GetMetadata&metadataPrefix=mtx&identifier=info:ofi/fmt:kev:mtx:book">Book OpenURL KEV</a> parameter names as a microformat. Take a look in the HTML and you should see them hiding there.

Here's a somewhat prettified version as an image since I couldn't get my syntax highlighter plugin to do a nice enough job with the HTML.

<img src="/images/openurl-microformat.png" style="border: thin dashed black; padding: 10px;"/>

Pretty simple stuff right? Notice the COinS in there too? That's thanks to Dan's <a href="http://onebiglibrary.net/node/14">hacking</a> at structured blogging. Actually getting openurl KEV support into structured blogging is another idea of Dan's. Go Chudnov.

<i>Update 01/19/2006 09:39 CST</i>: Dan <a href="http://onebiglibrary.net/project/coins/openurl-microformat-for-journals">got</a> similar support for journal articles. If this stuff caught on it could really revolutionize academic blogging...and more.</p></div>			</div>
<script type="application/x-subnode; charset=utf-8">
       <!-- the following is structured blog data for machine readers. -->
       <subnode xmlns:data-view="http://www.w3.org/2003/g/data-view#" data-view:transformation="http://structuredblogging.org/subnode-to-rdf-interpreter.xsl" xmlns="http://www.structuredblogging.org/xmlns#subnode">
       	    <xml-structured-blog-entry xmlns="http://www.structuredblogging.org/xmlns">
       		    <generator id="wpsb-1" type="x-wpsb-post" version="1"/><review type="review/book"><subject name="The Search" author="John Battelle" year="2005" publisher="Portfolio Hardcover" isbn="1591840880"/><rating max="5" min="0"/><description>Ok, so The Search is a great book so far...but I'm really just testing some &lt;a href=&quot;/docs/review-book.xml&quot;&gt;local modifications&lt;/a&gt; I made to the &lt;a href=&quot;http://structuredblogging.org&quot;&gt;structured blogging&lt;/a&gt; tool to use &lt;a href=&quot;http://alcme.oclc.org/openurl/servlet/OAIHandler/extension?verb=GetMetadata&amp;metadataPrefix=mtx&amp;identifier=info:ofi/fmt:kev:mtx:book&quot;&gt;Book OpenURL KEV&lt;/a&gt; parameter names as a microformat. Take a look in the HTML and you should see them hiding there.

Here's a somewhat prettified version as an image since I couldn't get my syntax highlighter plugin to do a nice enough job with the HTML.

&lt;img src=&quot;/images/openurl-microformat.png&quot; style=&quot;border: thin dashed black; padding: 10px;&quot;/&gt;

Pretty simple stuff right? Notice the COinS in there too? That's thanks to Dan's &lt;a href=&quot;http://onebiglibrary.net/node/14&quot;&gt;hacking&lt;/a&gt; at structured blogging. Actually getting openurl KEV support into structured blogging is another idea of Dan's. Go Chudnov.

&lt;i&gt;Update 01/19/2006 09:39 CST&lt;/i&gt;: Dan &lt;a href=&quot;http://onebiglibrary.net/project/coins/openurl-microformat-for-journals&quot;&gt;got&lt;/a&gt; similar support for journal articles. If this stuff caught on it could really revolutionize academic blogging...and more.</description></review>
       	    </xml-structured-blog-entry>
       </subnode>
       </script>
]]></content:encoded>
		<excerpt:encoded><![CDATA[	<div class='hreview x-wpsb-review-book'>		<div>			<h3 class='item fn'><span class='btitle'>The Search</span></h3>						                        <p><b>Author</b>: <span class='au'>John Battelle</span></p>                                                <p><b>Year</b>: <span class='date'>2005</span></p>			                        <p><b>Publisher</b>: <span class='pub'>Portfolio Hardcover</span></p>						<p><b>ISBN</b>: <span class='Z3988 isbn' title='ctx_ver=Z39.88-2004&amp;rft_val_fmt=info:ofi/fmt:kev:mtx:book&amp;rft.isbn=1591840880'>1591840880</span></p>		</div>		<div class='description'><p>Ok, so The Search is a great book so far...but I'm really just testing some <a href="/docs/review-book.xml">local modifications</a> I made to the <a href="http://structuredblogging.org">structured blogging</a> tool to use <a href="http://alcme.oclc.org/openurl/servlet/OAIHandler/extension?verb=GetMetadata&metadataPrefix=mtx&identifier=info:ofi/fmt:kev:mtx:book">Book OpenURL KEV</a> parameter names as a microformat. Take a look in the HTML and you should see them hiding there.

Here's a somewhat prettified version as an image since I couldn't get my syntax highlighter plugin to do a nice enough job with the HTML.

<img src="/images/openurl-microformat.png" style="border: thin dashed black; padding: 10px;"/>

Pretty simple stuff right? Notice the COinS in there too? That's thanks to Dan's <a href="http://onebiglibrary.net/node/14">hacking</a> at structured blogging. Actually getting openurl KEV support into structured blogging is another idea of Dan's. Go Chudnov.

<i>Update 01/19/2006 09:39 CST</i>: Dan <a href="http://onebiglibrary.net/project/coins/openurl-microformat-for-journals">got</a> similar support for journal articles. If this stuff caught on it could really revolutionize academic blogging...and more.</p></div>			</div>]]></excerpt:encoded>
		<wp:post_id>74</wp:post_id>
		<wp:post_date>2006-01-18 23:11:54</wp:post_date>
		<wp:post_date_gmt>2006-01-19 06:11:54</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>openurl-as-microformat</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>sb_rss_content</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>structuredblogging_xml</wp:meta_key>
			<wp:meta_value><![CDATA[<script type="application/x-subnode; charset=utf-8">
       <!-- the following is structured blog data for machine readers. -->
       <subnode xmlns:data-view="http://www.w3.org/2003/g/data-view#" data-view:transformation="http://structuredblogging.org/subnode-to-rdf-interpreter.xsl" xmlns="http://www.structuredblogging.org/xmlns#subnode">
       	    <xml-structured-blog-entry xmlns="http://www.structuredblogging.org/xmlns">
       		    <generator id="wpsb-1" type="x-wpsb-post" version="1"/><review type="review/book"><subject name="The Search" author="John Battelle" year="2005" publisher="Portfolio Hardcover" isbn="1591840880"/><rating max="5" min="0"/><description>Ok, so The Search is a great book so far...but I'm really just testing some &lt;a href=&quot;/docs/review-book.xml&quot;&gt;local modifications&lt;/a&gt; I made to the &lt;a href=&quot;http://structuredblogging.org&quot;&gt;structured blogging&lt;/a&gt; tool to use &lt;a href=&quot;http://alcme.oclc.org/openurl/servlet/OAIHandler/extension?verb=GetMetadata&amp;metadataPrefix=mtx&amp;identifier=info:ofi/fmt:kev:mtx:book&quot;&gt;Book OpenURL KEV&lt;/a&gt; parameter names as a microformat. Take a look in the HTML and you should see them hiding there.

Here's a somewhat prettified version as an image since I couldn't get my syntax highlighter plugin to do a nice enough job with the HTML.

&lt;img src=&quot;/images/openurl-microformat.png&quot; style=&quot;border: thin dashed black; padding: 10px;&quot;/&gt;

Pretty simple stuff right? Notice the COinS in there too? That's thanks to Dan's &lt;a href=&quot;http://onebiglibrary.net/node/14&quot;&gt;hacking&lt;/a&gt; at structured blogging. Actually getting openurl KEV support into structured blogging is another idea of Dan's. Go Chudnov.

&lt;i&gt;Update 01/19/2006 09:39 CST&lt;/i&gt;: Dan &lt;a href=&quot;http://onebiglibrary.net/project/coins/openurl-microformat-for-journals&quot;&gt;got&lt;/a&gt; similar support for journal articles. If this stuff caught on it could really revolutionize academic blogging...and more.</description></review>
       	    </xml-structured-blog-entry>
       </subnode>
       </script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>31</wp:comment_id>
			<wp:comment_author><![CDATA[Edward Vielmetti]]></wp:comment_author>
			<wp:comment_author_email>emv@superpatron.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.superpatron.com</wp:comment_author_url>
			<wp:comment_author_IP>70.229.253.126</wp:comment_author_IP>
			<wp:comment_date>2006-01-23 11:37:43</wp:comment_date>
			<wp:comment_date_gmt>2006-01-23 18:37:43</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hm, interesting.

I wrote a little XSLT and was able to generate a first pass at openurl microformatted pages from the Ann Arbor District Library's catalog.

Grab http://www.superpatron.com/aadl-openurl.xsl as the stylesheet, and then apply it to http://www.superpatron.com/aadlexample.xml to see what it looks like.   I didn't get the COINS stuff working on a first go-around, but it should be straightforward enough.

A few things were funky.  The AADL REST interface (see Blyberg's blog for details) doesn't have a separate "publication year" field, rather putting it formatted in the "pubinfo" field.  The "author" field I get easily is a string like "Vielmetti, Edward 1964-" rather than "Edward Vielmetti".  But it looks like it generates something reasonable.

A second pass at doing this against a bigger catalog would be to do a similar transform against SRU so that you could snag these records from e.g. the Library of Congress.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>ical and outlook</title>
		<link>http://inkdroid.org/2006/01/19/ical-and-outlook/</link>
		<pubDate>Thu, 19 Jan 2006 20:23:27 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/01/19/ical-and-outlook/</guid>
		<description></description>
		<content:encoded><![CDATA[I got to talking to <a href="http://suda.co.uk/">Brian Suda</a> about why his <a href="http://microformats.org/wiki/hcalendar">hCalendar</a> extracting application <a href="http://suda.co.uk/projects/X2V/">x2v</a> works like a dream with iCal but doesn't seem to work with Microsoft Outlook 2002.

<blockquote>
vCalendar/iCalendar Import failed. The input file may be corrupt.
</blockquote>

Here's the event that Outlook doesn't like, but iCal does:

<pre>
BEGIN:VCALENDAR
PRODID:-//suda.co.uk//X2V 0.6.7 (BETA)//EN
X-ORIGINAL-URL: http://www.code4lib.org/
VERSION:2.0
BEGIN:VEVENT
SUMMARY;LANGUAGE=en:The Portland Jazz Festival
DTSTART:20060217
DTEND:20060219
URL:http://www.travelportland.com/jazz/index.html
END:VEVENT
END:VCALENDAR
</pre>

After quite a bit of experimentation we determined that Outlook demands that the METHOD, UID and DTSTAMP fields be defined.

<pre>
BEGIN:VCALENDAR
PRODID:-//suda.co.uk//X2V 0.6.7 (BETA)//EN
X-ORIGINAL-URL: http://www.code4lib.org/
VERSION:2.0
METHOD:PUBLISH
BEGIN:VEVENT
UID:&lt;abc123&gt
DTSTAMP:20060119T184157Z
SUMMARY;LANGUAGE=en:The Portland Jazz Festival
DTSTART:20060217
DTEND:20060219
URL:http://www.travelportland.com/jazz/index.html
END:VEVENT
END:VCALENDAR
</pre>

Just thought I'd mention it in here in case someone ends up googling for that error. Brian said he's going to look into providing this support for those of us who have Microsoft Outlook inflicted on us.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>77</wp:post_id>
		<wp:post_date>2006-01-19 13:23:27</wp:post_date>
		<wp:post_date_gmt>2006-01-19 20:23:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>ical-and-outlook</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>29</wp:comment_id>
			<wp:comment_author><![CDATA[Tim]]></wp:comment_author>
			<wp:comment_author_email>tjameswhite@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url>http://tjameswhite.com/blog</wp:comment_author_url>
			<wp:comment_author_IP>140.244.105.113</wp:comment_author_IP>
			<wp:comment_date>2006-01-20 05:26:56</wp:comment_date>
			<wp:comment_date_gmt>2006-01-20 12:26:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[What a great coincidence that I was bugging Brian about the same thing! I'm glad you found the culprit. And here I was afraid I'd have to contact our help desk to have Outlook patched... (phew).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>30</wp:comment_id>
			<wp:comment_author><![CDATA[Brian Ray]]></wp:comment_author>
			<wp:comment_author_email>bray@sent.com</wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20101127052904/http://brianray.chipy.org:80/</wp:comment_author_url>
			<wp:comment_author_IP>216.80.126.112</wp:comment_author_IP>
			<wp:comment_date>2006-01-21 22:04:32</wp:comment_date>
			<wp:comment_date_gmt>2006-01-22 05:04:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Do not get me started with the vCard, vCalendar, ...</p>

<p>I am so annoyed in the differences, it makes me ill.</p>

<p>Because mac's iCal is a great GUI and because Firefoxs Calandar allows edits on DAV server, I stick with this format. M$ is just wrong.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>63</wp:comment_id>
			<wp:comment_author><![CDATA[Mark Mansour]]></wp:comment_author>
			<wp:comment_author_email>mark@lifelint.com</wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20061205062951/http://www.lifelint.com:80/blog/</wp:comment_author_url>
			<wp:comment_author_IP>202.173.156.87</wp:comment_author_IP>
			<wp:comment_date>2006-02-09 08:16:40</wp:comment_date>
			<wp:comment_date_gmt>2006-02-09 15:16:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>You may want to check out the UPDATED lifelint parser.  I've added support for Outlook.</p>

<p>http://web.archive.org/web/20060820190512/http://lifelint.net/</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>5198</wp:comment_id>
			<wp:comment_author><![CDATA[Anton]]></wp:comment_author>
			<wp:comment_author_email>antonmos@hotmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>198.175.55.5</wp:comment_author_IP>
			<wp:comment_date>2006-09-28 07:12:42</wp:comment_date>
			<wp:comment_date_gmt>2006-09-28 14:12:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for posting this, it did save me the pain! :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>7585</wp:comment_id>
			<wp:comment_author><![CDATA[Leroy]]></wp:comment_author>
			<wp:comment_author_email>leroy@leeseit.com</wp:comment_author_email>
			<wp:comment_author_url>http://leeseit.com</wp:comment_author_url>
			<wp:comment_author_IP>132.228.195.206</wp:comment_author_IP>
			<wp:comment_date>2006-11-06 12:12:04</wp:comment_date>
			<wp:comment_date_gmt>2006-11-06 19:12:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank you - very helpful info - was struggling with Outlook's useless error message and incompatibility.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>22286</wp:comment_id>
			<wp:comment_author><![CDATA[KluverBucy.com]]></wp:comment_author>
			<wp:comment_author_email>mike@kluverbucy.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.kluverbucy.com</wp:comment_author_url>
			<wp:comment_author_IP>70.141.133.73</wp:comment_author_IP>
			<wp:comment_date>2007-04-17 15:49:56</wp:comment_date>
			<wp:comment_date_gmt>2007-04-17 22:49:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Lifesaver!  frustration central over here before reading this - Thanks!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>26662</wp:comment_id>
			<wp:comment_author><![CDATA[Mike]]></wp:comment_author>
			<wp:comment_author_email>oristian@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>71.147.5.171</wp:comment_author_IP>
			<wp:comment_date>2007-05-04 16:57:52</wp:comment_date>
			<wp:comment_date_gmt>2007-05-04 23:57:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Argh!  I've tried the iCalendar gem and I'm getting nowhere on the ability to get .ics or .vcs files into Outlook.  This is the 


BEGIN:VCALENDAR
METHOD:PUBLIC
VERSION:2.0
CALSCALE:GREGORIAN
PRODID:iCalendar-Ruby
BEGIN:VEVENT
PRIORITY:5
CLASS:PUBLIC
URL:http://finance.yahoo.com
UID:2007-05-04T16:54:13-0700_110934668@mo1.local
DESCRIPTION:this is the event test description
SUMMARY:This is the Summary
DTSTART:20070412T000000
TRANSP:OPAQUE
DTSTAMP:20070504T165413
SEQ:0
END:VEVENT
END:VCALENDAR
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>44421</wp:comment_id>
			<wp:comment_author><![CDATA[Steve]]></wp:comment_author>
			<wp:comment_author_email>sarowe@syr.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>128.230.84.100</wp:comment_author_IP>
			<wp:comment_date>2007-11-02 14:00:39</wp:comment_date>
			<wp:comment_date_gmt>2007-11-02 21:00:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I switched from Mozilla Lightning to Outlook, and Outlook refused to import a Lightning-produced calendar.  

This web page is the only one returned by a Google search for the exact error message that Outlook gives, so I thought I'd add my two bits here.

All I had to do to get Outlook to import the calendar was add the line "METHOD:PUBLISH" under BEGIN:VCALENDAR.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>testing, testing, is this thing on?</title>
		<link>http://inkdroid.org/2006/01/26/testing-testing-is-this-thing-on/</link>
		<pubDate>Thu, 26 Jan 2006 18:55:27 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=78</guid>
		<description></description>
		<content:encoded><![CDATA[	<div class='vevent x-wpsb-simple-event'>		<p></p>		<h3 class='summary'>Marcel Molina on testing in Rails</h3>		<p><b>Begins</b>: <abbr class='dtstart' title='2006-02-06T18:30:00'>Mon, 06 Feb 2006 at  6:30 PM</abbr></p>		<p><b>Ends</b>: <abbr class='dtend' title='2006-02-06T18:30:00'>Mon, 06 Feb 2006 at 10:00 PM</abbr></p>				<p>			<b>Location</b>:			<span class='location'><p>Thoughtworks</p>			<p>651 W. Washington Blvd</p>			<p>Chicago, 			IL			60661</p>			<p>USA</p>		</span></p>					<p><b>Link</b>: <a href='http://ruby.m eetup.com/55/events/4824278/'>meetup page</a></p>				<div>I'm looking forward to attending this talk by <a href="http://vernix.org/marcel/">Marcel Molina</a> of <a href="http://37signals.com">37Signals</a> on <a href="http://manuals.rubyonrails.com/read/book/5">testing</a> in Rails. One of the things that has impressed me the most about Rails so far is how test stubs are automatically written out for you when you generate classes. I also really like how fixtures can load test data for each test...and the custom assertions rock. Anyhow, hopefully I will find the time to attend this the week before I head out to Oregon.</div>			</div>
<script type="application/x-subnode; charset=utf-8">
       <!-- the following is structured blog data for machine readers. -->
       <subnode xmlns:data-view="http://www.w3.org/2003/g/data-view#" data-view:transformation="http://structuredblogging.org/subnode-to-rdf-interpreter.xsl" xmlns="http://www.structuredblogging.org/xmlns#subnode">
       	    <xml-structured-blog-entry xmlns="http://www.structuredblogging.org/xmlns">
       		    <generator id="wpsb-1" type="x-wpsb-post" version="1"/><event type="event/generic"><name>Marcel Molina on testing in Rails</name><location address="Thoughtworks" subaddress="651 W. Washington Blvd" city="Chicago" state="IL" postcode="60661" country="USA"/><description>I'm looking forward to attending this talk by &lt;a href=&quot;http://vernix.org/marcel/&quot;&gt;Marcel Molina&lt;/a&gt; of &lt;a href=&quot;http://37signals.com&quot;&gt;37Signals&lt;/a&gt; on &lt;a href=&quot;http://manuals.rubyonrails.com/read/book/5&quot;&gt;testing&lt;/a&gt; in Rails. One of the things that has impressed me the most about Rails so far is how test stubs are automatically written out for you when you generate classes. I also really like how fixtures can load test data for each test...and the custom assertions rock. Anyhow, hopefully I will find the time to attend this the week before I head out to Oregon.</description><link url="http://ruby.m eetup.com/55/events/4824278/">meetup page</link><begins>2006-02-06T18:30:00</begins><ends>2006-02-06T22:00:00</ends></event>
       	    </xml-structured-blog-entry>
       </subnode>
       </script>
]]></content:encoded>
		<excerpt:encoded><![CDATA[	<div class='vevent x-wpsb-simple-event'>		<p></p>		<h3 class='summary'>Marcel Molina on testing in Rails</h3>		<p><b>Begins</b>: <abbr class='dtstart' title='2006-02-06T18:30:00'>Mon, 06 Feb 2006 at  6:30 PM</abbr></p>		<p><b>Ends</b>: <abbr class='dtend' title='2006-02-06T18:30:00'>Mon, 06 Feb 2006 at 10:00 PM</abbr></p>				<p>			<b>Location</b>:			<span class='location'><p>Thoughtworks</p>			<p>651 W. Washington Blvd</p>			<p>Chicago, 			IL			60661</p>			<p>USA</p>		</span></p>					<p><b>Link</b>: <a href='http://ruby.m eetup.com/55/events/4824278/'>meetup page</a></p>				<div>I'm looking forward to attending this talk by <a href="http://vernix.org/marcel/">Marcel Molina</a> of <a href="http://37signals.com">37Signals</a> on <a href="http://manuals.rubyonrails.com/read/book/5">testing</a> in Rails. One of the things that has impressed me the most about Rails so far is how test stubs are automatically written out for you when you generate classes. I also really like how fixtures can load test data for each test...and the custom assertions rock. Anyhow, hopefully I will find the time to attend this the week before I head out to Oregon.</div>			</div>]]></excerpt:encoded>
		<wp:post_id>78</wp:post_id>
		<wp:post_date>2006-01-26 11:55:27</wp:post_date>
		<wp:post_date_gmt>2006-01-26 18:55:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>testing-testing-is-this-thing-on</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>structuredblogging_xml</wp:meta_key>
			<wp:meta_value><![CDATA[<script type="application/x-subnode; charset=utf-8">
       <!-- the following is structured blog data for machine readers. -->
       <subnode xmlns:data-view="http://www.w3.org/2003/g/data-view#" data-view:transformation="http://structuredblogging.org/subnode-to-rdf-interpreter.xsl" xmlns="http://www.structuredblogging.org/xmlns#subnode">
       	    <xml-structured-blog-entry xmlns="http://www.structuredblogging.org/xmlns">
       		    <generator id="wpsb-1" type="x-wpsb-post" version="1"/><event type="event/generic"><name>Marcel Molina on testing in Rails</name><location address="Thoughtworks" subaddress="651 W. Washington Blvd" city="Chicago" state="IL" postcode="60661" country="USA"/><description>I'm looking forward to attending this talk by &lt;a href=&quot;http://vernix.org/marcel/&quot;&gt;Marcel Molina&lt;/a&gt; of &lt;a href=&quot;http://37signals.com&quot;&gt;37Signals&lt;/a&gt; on &lt;a href=&quot;http://manuals.rubyonrails.com/read/book/5&quot;&gt;testing&lt;/a&gt; in Rails. One of the things that has impressed me the most about Rails so far is how test stubs are automatically written out for you when you generate classes. I also really like how fixtures can load test data for each test...and the custom assertions rock. Anyhow, hopefully I will find the time to attend this the week before I head out to Oregon.</description><link url="http://ruby.m eetup.com/55/events/4824278/">meetup page</link><begins>2006-02-06T18:30:00</begins><ends>2006-02-06T22:00:00</ends></event>
       	    </xml-structured-blog-entry>
       </subnode>
       </script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>sb_rss_content</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>53</wp:comment_id>
			<wp:comment_author><![CDATA[Kesa Summers]]></wp:comment_author>
			<wp:comment_author_email>missmccowan@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>67.163.0.83</wp:comment_author_IP>
			<wp:comment_date>2006-02-05 13:01:04</wp:comment_date>
			<wp:comment_date_gmt>2006-02-05 20:01:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[BEBE!!!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>WeibelNumber 3</title>
		<link>http://inkdroid.org/2006/02/12/weibelnumber-3/</link>
		<pubDate>Mon, 13 Feb 2006 06:17:54 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=79</guid>
		<description></description>
		<content:encoded><![CDATA[So I have succumbed to infection (thanks <a href="http://dilettantes.code4lib.org/2006/02/11/tagged-in/">Ross</a>), but I'm not entirely sure how this is supposed to work...and Kesa said "Don't Do It!". After reading Morbus <a href="http://lists.swhack.com/swhack/2006-February/000039.html">rail</a> about it I'm almost afraid that I won't be able to lurk on #swhack anymore if I do spread this any further. But it's already gutted the interwebs pretty well by now, so here it goes anyhow. I'm just pleased to have a WeibelNumber of 3.

Jobs I've had:
- pizza delivery man
- food coop worker
- used bookstore employee
- newspaper kiosk attendant

Movies I can watch over and over:
- Brazil
- Finding Nemo
- Eternal Sunshine of the Spotless Mind
- Lord of the Rings

TV shows I love to watch:
- Jim Lehrer News Hour
- Mystery
- Nova
- Frontline

Places I have lived:
- Princeton Jct, NJ
- New York, New York
- Brighton, England
- Urbana, IL

Places I have been on holiday:
- Cinque Terre
- Montreal
- Kalymnos
- Amsterdam

4 of my favorite dishes:
- Italian Wedding Soup
- Any kind of curry
- Cracklin' Oat Bran
- Oranges in winter

4 books (just 4 that I'm currently reading--favorite pshaw!)
- <a href="http://www.amazon.com/gp/product/097669400X">Agile Web Development with Rails</a>
- <a href="http://www.amazon.com/gp/product/0471407348">A Mathematical Mystery Tour: Discovering the Truth and Beauty of the Cosmos</a>
- <a href="http://www.amazon.com/gp/product/1841492299">The Algebraist</a>
- <a href="http://www.amazon.com/gp/product/0120884070">Data Mining</a>

Websites I visit daily:
- gmail
- google
- delicious
- unalog

Places I would rather be right now:
- at home with kesa and chloe
- on vacation
- outside of the United States of America
- outside of the United States of America

Bloggers I am tagging (and hope that they miss this entry):
- <a href="http://use.perl.org/~LTjake/journal/">Brian Cassidy</a>
- <a href="http://www.multiply.org/notebook/">Jason Gessner</a> 
- <a href="http://interoperating.info/mark/">Mark Jordan</a>
- <a href="http://endlesshybrids.com/">Jeff Barry</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>79</wp:post_id>
		<wp:post_date>2006-02-12 23:17:54</wp:post_date>
		<wp:post_date_gmt>2006-02-13 06:17:54</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>weibelnumber-3</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>68</wp:comment_id>
			<wp:comment_author><![CDATA[stu]]></wp:comment_author>
			<wp:comment_author_email>stuart.weibel@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://weibel-lines.typepad.com</wp:comment_author_url>
			<wp:comment_author_IP>66.127.145.2</wp:comment_author_IP>
			<wp:comment_date>2006-02-16 22:35:15</wp:comment_date>
			<wp:comment_date_gmt>2006-02-17 05:35:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[weibelnumber?  are you kidding?  bacon... sure... britany spear, maybe... ranganathannumber... gatesnumber... ok.... but weibelnumber???

jesus, let me buy you a beer sometime!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>off to corvallis</title>
		<link>http://inkdroid.org/2006/02/13/off-to-corvallis/</link>
		<pubDate>Tue, 14 Feb 2006 05:52:34 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=80</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://code4lib.org"><img src="http://www.code4lib.org/themes/panizzi/logo.png" border=0 style="padding: 10px;" align="left" /></a> So tomorrow I'm headed for Corvallis, Oregon to attend the first ever <a href="http://code4lib.org/2006">code4lib conference</a>. It's been amazing to watch this conference start as a glimmer in the eye of a handful of people in IRC and turn into a real event attended by 80 library technologists from all over the place. 

I'm planning on doing a lightning talk or two, and had spent some time preparing some slides which I tossed in the end. I'm going to talk about using eclipse, microformats and object-relational-mapping -- hopefully by just doing some live coding. We'll see how it goes.

I plan on taking some copious notes, so keep your eye on <a href="http://planet.code4lib.org">planet.code4lib.org</a> for the play by play.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>80</wp:post_id>
		<wp:post_date>2006-02-13 22:52:34</wp:post_date>
		<wp:post_date_gmt>2006-02-14 05:52:34</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>off-to-corvallis</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>code4lib day 1</title>
		<link>http://inkdroid.org/2006/02/16/code4lib-day-1/</link>
		<pubDate>Thu, 16 Feb 2006 15:17:02 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=81</guid>
		<description></description>
		<content:encoded><![CDATA[So the first day of "the conference" was a lot of fun. It is just great to see all these people who care about the same stuff in the same place. The lightning talks and the breakout sessions built in some breathing space between the presentations which worked pretty well I thought. Memorable moments for me included:

- hearing people in the audience shout "OPA!" like we were in Greek Town during Dan's "Connecting Everything to Everything" talk.
- being able to ask Jeff Young to do a lightning talk about <a href="http://info-uri.info/">Info URIs</a> and then hear him do it later. (jyoung++)
- picking Rob Sanderson's brain during break about the fine details of CQL.
- having beers with tholbroo and calvinm at the crowbar
- being able to ask Eric Hellman about the guts of openly's <a href="http://www.openly.com/uhf/">data collection efforts</a>.
- chilling at jaf's comfy house in the hills of corvallis]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>81</wp:post_id>
		<wp:post_date>2006-02-16 08:17:02</wp:post_date>
		<wp:post_date_gmt>2006-02-16 15:17:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>code4lib-day-1</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>code4lib days 2-3</title>
		<link>http://inkdroid.org/2006/02/20/code4lib-days-2-3/</link>
		<pubDate>Tue, 21 Feb 2006 04:25:51 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=82</guid>
		<description></description>
		<content:encoded><![CDATA[So I didn't have time to journal about the 2nd and 3rd days of the conference since there was so much good stuff going on. I'm on the plane back to Chicago now so I've got a few moments to jot down some notes about those days and some general thoughts about the conference.

To be honest the 2nd and 3rd days kind of blur together for me because I really didn't get much sleep between them. I was pretty much blown away by the variety and quality of the presentations. <a href="http://outgoing.typepad.com/outgoing/">Thom</a> provided a detailed look at how he builds nimble, high-powered applications using short n'sweet python code on a beowulf cluster using techniques like map-reduce. 

While they did separate talks on different topics I found some common strands between Devon Smith's talk about metadata processing and Rob Sanderson's talk about indexing in <a href="http://cheshire3.sourceforge.net/">Cheshire3</a>. Both of them had interesting workflows which they illustrated with neat diagrams which I should be able to link to from here soon. It wasn't UML or anything boring like that. Rob's illustration was more an overlayed animation over a bunch of slides showing the full lifecycle of a document being busted apart, indexed, a query coming in, triggering retrieval and then reconstitution of the document. Devon used interesting shaped objects to represent components in his metadata management framework. It was so much more fun than a dry description of what the software was doing, and really evoked what's so much fun about building software--metaphor creation and architecture. Similarly Colleen Whitney of the California Digital Library had some really neat ways of visualizing search results which I wish I could link to as well.

Ryan Chute's talk about the aDORe archiving framework from Los Alamos was interesting, but it largely seemed like a verbalization of the series of articles about aDORe that have been published. Don't get me wrong, it's fascinating stuff, and perhaps I just had super-high expectations--but I was hoping to hear more details of how they are actually using the aDORe framework at Los Alamos.  It was good to hear Aaron Krowne talk about his experiments with quality metrics at Emory--especially after hearing a bit about it months ago in IRC. It turns out he was able to layer his new metrics over lucene without having to dive into the lucene code itself. I'm looking forward to seeing the code once it is released. I knew Aaron was a smart dude from talking to him IRC, but was surprised to see he is a confident and articulate public speaker as well.

Of course Roy Tennant is so at home at public speaking he was probably the only person that could easily tackle the "future of code4lib" in a presentation. He talked for 20 minutes about a variety of options that could be in the cards to make code4lib into a more formal organization; and then afterwards he did a breakout session on the topic. Unfortunately I wasn't able to attend this because I was sitting in on Ross's openurl Ruby library discussion. I heard that the basic consensus at the end was that things will stay much as they are now, but there might be a niche for code4lib to provide educational training for libraries. I think this idea came from Dan, and I think it's a great idea. Hopefully we'll get a chance to discuss over the coming months.

One of the neatest things I witnessed was Ross Singer spontaneously suggesting a breakout session about designing an openurl library for ruby...and something like 20 people showed up. Not that just any 20 people were there: we had Jeff Young (who wrote OCLCs <a href="http://www.oclc.org/research/software/openurl/default.htm">openurl library</a>), Eric Hellman (who helped write the openurl spec and who just sold his company to OCLC), Todd Holbrook (the software developer behind <a href="http://cufts.lib.sfu.ca/">CUFTS</a>) and Jay (?) one of the software developers beind ExLibris' <a href="http://www.exlibrisgroup.com/sfx.htm">SFX</a> product. We had a good discussion, which Ross was able to fascilitate, and I think we came away with some good ideas on how to improve the existing library, and perhaps think about providing a common DOMlike api for openurl implementations.

I could go on and on. Like how great the lightning talks were...for example Terry Reese's five minute laid back demo of his <a href="http://oregonstate.edu/~reeset/marcedit/html/">MARCEdit</a> software that was so polished and amazing I couldn't believe it. It can query z39.50/SRU targets, and crosswalk to MODs and other metadata formats. Casey Bisson finished the conference on the right note encouraging library software developers to get involved in the technology world outside libraries and to look outwards for cowpaths to pave rather than navel gazing and using only standards developed by libraries. I think he definitely has a point, and that the converse is also true--we should be promoting library standards such as sru/cql in the outside world and encouraging them to pave some of our cowpaths. I was hoping to follow Casey's talk with my lightning talk about microformats but alas we ran out of time.

All in all I had a great time, and got a chance to meet some really interesting folks (some of whom I got to hang out in Portland with afterwards: Gabriel, Devon, Rob, Aaron). I don't think it would've been possible without the support of people like Art Rhyno, Roy Tennant, Dan Chudnov and of course Jeremy Frumkin who managed to make it just happen. The most important feature of the conference was the size, which was big enough to make it interesting, but small enough to make it easily experienced as a whole, and relaxed enough to be fun. I think that it's pretty clear that it hit a sweet spot, and that it is highly likely that it will happen again.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>82</wp:post_id>
		<wp:post_date>2006-02-20 21:25:51</wp:post_date>
		<wp:post_date_gmt>2006-02-21 04:25:51</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>code4lib-days-2-3</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>a new type of journal</title>
		<link>http://inkdroid.org/2006/02/22/a-new-type-of-journal/</link>
		<pubDate>Wed, 22 Feb 2006 21:53:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/02/22/a-new-type-of-journal/</guid>
		<description></description>
		<content:encoded><![CDATA[In the unlikely event that you haven't seen it there is an interesting <a href="http://dewey.library.nd.edu/mailing-lists/code4lib/archive/2005/0347.html">thread</a> over on the code4lib discussion list about establishing a code4lib journal. I think Mark Jordan has the right <a href="http://dewey.library.nd.edu/mailing-lists/code4lib/archive/2005/0356.html">idea</a>:

<blockquote>
...would creating a section at http://code4lib.org/ that was reserved for formal, maybe even peer-reviewed articles do what you're describing? The articles would be the starting point, but the Web 1.9-compliant features that are already appearing on the site (comments, attachments, microformat links, etc.) may satisfy what you're describing. Heck, maybe we could write a module for http://code4lib.org/ that would pull some of these things together (drupal already has a publishing module). In other words, http://code4lib.org/ could _be_ the journal but it could be a new type of journal.
</blockquote>

<a href="http://dewey.library.nd.edu/mailing-lists/code4lib/archive/2005/0357.html">Dan</a> followed up with a +1 and I think he is right. The drupal instance running on code4lib.org was thrown together at the last minute and rejiggered by lots of people to serve as a place to put conference information. I've been wondering what might be in the cards for the site as we move post-conference and I think this "new kind of journal" idea might be where it can go. While there are lots of people with administrator access via the web, there's not many people with shell access.  I'd like to get where mjordan and others can have shell access (if they want it) so that we can make hardcore changes if necessary. Perhaps we just need another plugin, and we can go to town...or as Ross <a href="http://dewey.library.nd.edu/mailing-lists/code4lib/archive/2005/0359.html">says</a>

<blockquote>
It still sounds like there'd still need to be /a/ process (and we need
to work that out), but the overhead is very low.

And I like that.
</blockquote>

I like it too.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>83</wp:post_id>
		<wp:post_date>2006-02-22 14:53:19</wp:post_date>
		<wp:post_date_gmt>2006-02-22 21:53:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-new-type-of-journal</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>xbib in subversion</title>
		<link>http://inkdroid.org/2006/02/22/xbib-in-subversion/</link>
		<pubDate>Thu, 23 Feb 2006 04:31:32 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=84</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://web.archive.org/web/20080517164303/http://netapps.muohio.edu/blogs/darcusb/darcusb/">Bruce D'Arcus</a> has started putting his <a href="http://xbiblio.sourceforge.net/">XBib project</a> into <a href="http://svn.sourceforge.net/viewcvs.cgi/xbiblio/">subversion</a> at sourceforge. The code is going to include ruby, python and styling libraries. I didn't realize that sourceforge was offering up svn now...which is a welcome change. I want to set aside some time to get familiar with Bruce's model and code.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>84</wp:post_id>
		<wp:post_date>2006-02-22 21:31:32</wp:post_date>
		<wp:post_date_gmt>2006-02-23 04:31:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>xbib-in-subversion</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>hit sh</title>
		<link>http://inkdroid.org/2006/02/24/hit-sh/</link>
		<pubDate>Fri, 24 Feb 2006 16:14:53 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/02/24/hit-sh/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://flickr.com/photos/66726692@N00/sets/72057594065491946/">
<img src="/images/hit-sh.jpg" align="left" style="margin: 10px" alt="hit SH"/>
</a> It's <a href="http://technology.guardian.co.uk/news/story/0,,1716842,00.html">great</a> when our system of government <a href="http://www.outragedmoderates.org/2006/02/dod-staffers-notes-from-911-obtained.html">works</a> like it is supposed to. <a href="http://web.archive.org/web/20090927003157/http://www.usdoj.gov:80/04foia/">freedom_of_information_act++</a>.</p>

<p>Not only does the Internet <a href="http://maisonbisson.com/blog/post/11100/">change everything</a>, but it makes some good things work even better.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>85</wp:post_id>
		<wp:post_date>2006-02-24 09:14:53</wp:post_date>
		<wp:post_date_gmt>2006-02-24 16:14:53</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>hit-sh</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>(py)?lucene 1.9</title>
		<link>http://inkdroid.org/2006/03/02/pylucene-19/</link>
		<pubDate>Fri, 03 Mar 2006 05:11:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=86</guid>
		<description></description>
		<content:encoded><![CDATA[So on March 1st <a href="http://mail-archives.apache.org/mod_mbox/lucene-java-user/200603.mbox/%3c4405FBCE.9090506@apache.org%3e">lucene v1.9</a> was released and the *next day* <a href="http://lists.osafoundation.org/pipermail/pylucene-dev/2006-March/000873.html">pylucene v1.9</a> is released. Nice work!

I guess there are a bunch of methods that are deprecated in 1.9 which will dissappear entirely in v2.0. Now would be a good time to update usage...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>86</wp:post_id>
		<wp:post_date>2006-03-02 22:11:06</wp:post_date>
		<wp:post_date_gmt>2006-03-03 05:11:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pylucene-19</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="java"><![CDATA[java]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>learning programming w/ python</title>
		<link>http://inkdroid.org/2006/03/14/learning-programming-w-python/</link>
		<pubDate>Tue, 14 Mar 2006 18:43:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/03/14/87/</guid>
		<description></description>
		<content:encoded><![CDATA[	<div class='vevent x-wpsb-simple-event'>		<p></p>		<h3 class='summary'>Programming For Newbies With Python</h3>		<p><b>Begins</b>: <abbr class='dtstart' title='2006-03-25T13:00:00'>Sat, 25 Mar 2006 at  1:00 PM</abbr></p>		<p><b>Ends</b>: <abbr class='dtend' title='2006-03-25T13:00:00'>Sat, 25 Mar 2006 at  3:30 PM</abbr></p>				<p>			<b>Location</b>:			<span class='location'><p>Loyola University</p>						<p>Chicago, 			IL			</p>					</span></p>					<p><b>Link</b>: <a href='http://mail.python.org/pipermail/chicago/2006-March/000516.html'>details</a></p>				<div><p>Here's an interesting event for promoting python as a first-language. Below is an excerpt from mtobis' <a href="http://mail.python.org/pipermail/chicago/2006-March/000516.html">email announcement</a>:

<blockquote>
The Chicago Python User Group (ChiPy) with the kind cooperation of the
Computer Science Department at Loyola University of Chicago will be
offering a free introduction to computer programming using the Python
language.

I'm looking for people who would be interested in taking up computing
as a serious hobby. The final impetus to present this was presented by
a father-son team who want to learn to program together. I would
welcome teenagers or adults. Parent/teen pairs are especially welcome.

Children under the age of 13 may attend if accompanied by an adult but
for most pre-teens this may prove too challenging.

On the other hand, professional programmers will find the pace too slow.

You need no coding experience at all, but you shouldn't be unfamiliar
with a computer altogether. A small amount of exposure to HTML would
be helpful.

The first meeting will be an introduction to the power of Python, and
an organizational meeting. By the time you leave you will have written
a small and amusing piece of working software.

We'll also poll the group about your interests, and decide on where
and how often we should meet in the future, and set up some online
communication to keep each other in contact. We will probably meet
every second Saturday.
</blockqoute></p></div>		<p><i>Tags: chicago python</i></p>	</div>
<script type="application/x-subnode; charset=utf-8">
       <!-- the following is structured blog data for machine readers. -->
       <subnode xmlns:data-view="http://www.w3.org/2003/g/data-view#" data-view:transformation="http://structuredblogging.org/subnode-to-rdf-interpreter.xsl" xmlns="http://www.structuredblogging.org/xmlns#subnode">
       	    <xml-structured-blog-entry xmlns="http://www.structuredblogging.org/xmlns">
       		    <generator id="wpsb-1" type="x-wpsb-post" version="1"/><event type="event/generic"><name>Programming For Newbies With Python</name><location address="Loyola University" city="Chicago" state="IL"/><description>Here's an interesting event for promoting python as a first-language. Below is an excerpt from mtobis' &lt;a href=&quot;http://mail.python.org/pipermail/chicago/2006-March/000516.html&quot;&gt;email announcement&lt;/a&gt;:

&lt;blockquote&gt;
The Chicago Python User Group (ChiPy) with the kind cooperation of the
Computer Science Department at Loyola University of Chicago will be
offering a free introduction to computer programming using the Python
language.

I'm looking for people who would be interested in taking up computing
as a serious hobby. The final impetus to present this was presented by
a father-son team who want to learn to program together. I would
welcome teenagers or adults. Parent/teen pairs are especially welcome.

Children under the age of 13 may attend if accompanied by an adult but
for most pre-teens this may prove too challenging.

On the other hand, professional programmers will find the pace too slow.

You need no coding experience at all, but you shouldn't be unfamiliar
with a computer altogether. A small amount of exposure to HTML would
be helpful.

The first meeting will be an introduction to the power of Python, and
an organizational meeting. By the time you leave you will have written
a small and amusing piece of working software.

We'll also poll the group about your interests, and decide on where
and how often we should meet in the future, and set up some online
communication to keep each other in contact. We will probably meet
every second Saturday.
&lt;/blockqoute&gt;</description><tags>chicago python</tags><link url="http://mail.python.org/pipermail/chicago/2006-March/000516.html">details</link><begins>2006-03-25T13:00:00</begins><ends>2006-03-25T15:30:00</ends></event>
       	    </xml-structured-blog-entry>
       </subnode>
       </script>
]]></content:encoded>
		<excerpt:encoded><![CDATA[	<div class='vevent x-wpsb-simple-event'>		<p></p>		<h3 class='summary'>Programming For Newbies With Python</h3>		<p><b>Begins</b>: <abbr class='dtstart' title='2006-03-25T13:00:00'>Sat, 25 Mar 2006 at  1:00 PM</abbr></p>		<p><b>Ends</b>: <abbr class='dtend' title='2006-03-25T13:00:00'>Sat, 25 Mar 2006 at  3:30 PM</abbr></p>				<p>			<b>Location</b>:			<span class='location'><p>Loyola University</p>						<p>Chicago, 			IL			</p>					</span></p>					<p><b>Link</b>: <a href='http://mail.python.org/pipermail/chicago/2006-March/000516.html'>details</a></p>				<div><p>Here's an interesting event for promoting python as a first-language. Below is an excerpt from mtobis' <a href="http://mail.python.org/pipermail/chicago/2006-March/000516.html">email announcement</a>:

<blockquote>
The Chicago Python User Group (ChiPy) with the kind cooperation of the
Computer Science Department at Loyola University of Chicago will be
offering a free introduction to computer programming using the Python
language.

I'm looking for people who would be interested in taking up computing
as a serious hobby. The final impetus to present this was presented by
a father-son team who want to learn to program together. I would
welcome teenagers or adults. Parent/teen pairs are especially welcome.

Children under the age of 13 may attend if accompanied by an adult but
for most pre-teens this may prove too challenging.

On the other hand, professional programmers will find the pace too slow.

You need no coding experience at all, but you shouldn't be unfamiliar
with a computer altogether. A small amount of exposure to HTML would
be helpful.

The first meeting will be an introduction to the power of Python, and
an organizational meeting. By the time you leave you will have written
a small and amusing piece of working software.

We'll also poll the group about your interests, and decide on where
and how often we should meet in the future, and set up some online
communication to keep each other in contact. We will probably meet
every second Saturday.
</blockqoute></p></div>		<p><i>Tags: chicago python</i></p>	</div>]]></excerpt:encoded>
		<wp:post_id>87</wp:post_id>
		<wp:post_date>2006-03-14 11:43:41</wp:post_date>
		<wp:post_date_gmt>2006-03-14 18:43:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>learning-programming-w-python</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="chicago"><![CDATA[chicago]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>structuredblogging_xml</wp:meta_key>
			<wp:meta_value><![CDATA[<script type="application/x-subnode; charset=utf-8">
       <!-- the following is structured blog data for machine readers. -->
       <subnode xmlns:data-view="http://www.w3.org/2003/g/data-view#" data-view:transformation="http://structuredblogging.org/subnode-to-rdf-interpreter.xsl" xmlns="http://www.structuredblogging.org/xmlns#subnode">
       	    <xml-structured-blog-entry xmlns="http://www.structuredblogging.org/xmlns">
       		    <generator id="wpsb-1" type="x-wpsb-post" version="1"/><event type="event/generic"><name>Programming For Newbies With Python</name><location address="Loyola University" city="Chicago" state="IL"/><description>Here's an interesting event for promoting python as a first-language. Below is an excerpt from mtobis' &lt;a href=&quot;http://mail.python.org/pipermail/chicago/2006-March/000516.html&quot;&gt;email announcement&lt;/a&gt;:

&lt;blockquote&gt;
The Chicago Python User Group (ChiPy) with the kind cooperation of the
Computer Science Department at Loyola University of Chicago will be
offering a free introduction to computer programming using the Python
language.

I'm looking for people who would be interested in taking up computing
as a serious hobby. The final impetus to present this was presented by
a father-son team who want to learn to program together. I would
welcome teenagers or adults. Parent/teen pairs are especially welcome.

Children under the age of 13 may attend if accompanied by an adult but
for most pre-teens this may prove too challenging.

On the other hand, professional programmers will find the pace too slow.

You need no coding experience at all, but you shouldn't be unfamiliar
with a computer altogether. A small amount of exposure to HTML would
be helpful.

The first meeting will be an introduction to the power of Python, and
an organizational meeting. By the time you leave you will have written
a small and amusing piece of working software.

We'll also poll the group about your interests, and decide on where
and how often we should meet in the future, and set up some online
communication to keep each other in contact. We will probably meet
every second Saturday.
&lt;/blockqoute&gt;</description><tags>chicago python</tags><link url="http://mail.python.org/pipermail/chicago/2006-March/000516.html">details</link><begins>2006-03-25T13:00:00</begins><ends>2006-03-25T15:30:00</ends></event>
       	    </xml-structured-blog-entry>
       </subnode>
       </script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>sb_rss_content</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>#code4lib logging</title>
		<link>http://inkdroid.org/2006/03/17/code4lib-logging/</link>
		<pubDate>Fri, 17 Mar 2006 16:07:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/03/17/code4lib-logging/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Since the code4lib <a href="http://www.code4lib.org/2006">conference</a> the <a href="irc://irc.freenode.net/code4lib">#code4lib</a> irc channel has gained a lot of new voices with new ideas. In fact <a href="http://web.archive.org/web/20081012074740/http://cavlec.yarinareth.net/archives/2006/03/12/fifty-ways-to-lose-your-techies/">dsalo</a> and <a href="http://www.kevinclarke.info/weblog/archives/56">ksclarke</a> have already said all I can think of saying on the topic of the changing culture of the #code4lib channel.</p>

<p>Some people have suggested that some of the ideas, such as a journal, doing outreach/consulting work, etc will require the irc channel to clean up its act. I know that I've had moments in irc where I've lost my cool, or said something that I regret later...yesterday in fact.</p>

<p>So, for my own selfish purposes I would like to see a public log of the channel. It's been kind of a defacto rule that there are no public logs for the two years #code4lib has been going...but I think it might be time to add them. My reasoning is:</p>

<ol>
<li>it would require me to clean up my act, or at least be conscious of when I'm being erratic and telling the logging bot to go "off the record". This is how <a href ="http://swhack.com">#swhack logs</a> operate with some success.</li>
<li>it would allow people with lives to scan the logs looking for stuff and get other work done</li>
<li>it would encourage international participation by people who aren't online with the (Eastern|Central|Mountain|Pacific) Standard Tribe</li>
<li>and most importantly it would allow for <a href="http://web.archive.org/web/20090411123446/http://backchannel.stamen.com:80/">interesting</a> <a href="http://onebiglibrary.net/node/27">consuming</a> applications</li>
</ol>

<p>At any rate I think that code4lib changing is a good thing. Afterall, an organism that has ceased changing is umm, dead. I guess we should have a vote or something :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>88</wp:post_id>
		<wp:post_date>2006-03-17 09:07:57</wp:post_date>
		<wp:post_date_gmt>2006-03-17 16:07:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>code4lib-logging</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="irc"><![CDATA[irc]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>reading 2.0</title>
		<link>http://inkdroid.org/2006/03/20/reading-20/</link>
		<pubDate>Mon, 20 Mar 2006 21:47:05 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/03/20/reading-20/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://web.archive.org/web/20070623084839/http://ono.cdlib.org/archives/shimenawa/000219.html">Reading 2.0</a> slipped under my radar, but I guess that was the idea: to let people from O'Reilly, Los Alamos National Labs, OCLC, The Internet Archive, Adobe, Yahoo, Harvard and Elsevier hobnob away from prying eyes. I haven't seen any audio/video for the event but Tim O'Reilly has a nice fly on the wall <a href="http://web.archive.org/web/20080705050657/http://radar.oreilly.com/archives/2006/03/link_list_reading_20_1.html">summary</a> of what went on.</p>

<p>It's refreshing to see library technologies/concepts such as <a href="http://web.archive.org/web/20071009055221/http://www.niso.org:80/committees/committee_ax.html">OpenURL</a>, <a href="http://ocoins.info">OCOinS</a>, <a href="http://www.openarchives.org">OAI-PMH</a>, <a href="http://www.oclc.org/research/projects/frbr/">FRBR</a>, <a href="http://www.loc.gov/standards/mets/">METS</a> and <a href="http://dublincore.org">Dublin Core</a> starting to be talked about in the context of a larger information environment. For example I had no idea that Yahoo is harvesting data from the Internet Archive using the OAI-PMH protocol. And I didn't know Yahoo is starting to leverage <a href="http://microformats.org">microformats</a>, but should've guessed considering the recent <a href="http://microformats.org/discuss/mail/microformats-discuss/2006-March/003287.html">news</a> about <a href="http://flickr.com">Flickr</a> starting to use <a href="http://microformats.org/wiki/hcard">hCard</a>.</p>

<p>All in all these are exciting "lowercase semantic web" times we're living in. And it's interesting to watch some of the things people you know have worked on starting to catch on. Hopefully Reading 2.0 was just the start of this ongoing collaboration. Case in point, I just heard Robert Sanderson say in #code4lib that he's visiting the <a href="http://a9.com">a9</a> folks to talk about <a href="http://opensearch.a9.com">opensearch</a> and <a href="http://www.loc.gov/standards/sru/">sru</a>. This is just the sort of cross-fertilization we need going on in library land.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>89</wp:post_id>
		<wp:post_date>2006-03-20 14:47:05</wp:post_date>
		<wp:post_date_gmt>2006-03-20 21:47:05</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>reading-20</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="business"><![CDATA[business]]></category>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Standard</title>
		<link>http://inkdroid.org/2006/03/21/standard/</link>
		<pubDate>Tue, 21 Mar 2006 16:39:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/03/21/standard/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.amazon.com/gp/product/1841492299"><img src="http://images.amazon.com/images/P/1841492299.01._SCMZZZZZZZ_PB_PU_PU-5_.jpg" border=0 style="margin-right: 15px" align="left"/></a>

When I find the time I'm enjoying reading <a href="http://www.amazon.com/gp/product/1841492299">The Algebraist</a> by Iain M. Banks which (so far) details a many-species galactic civilization in 4034 AD. The milieu includes an amorphous ancient species known as the Dwellers who live for millions of years on gas giant planets (like Jupiter) and have very, very long memories...and the best archives which other beings ocassionaly 'delve' into. It's the usual Banksian genius. Last night on pages 100-101 I couldn't help laughing at this segment that discusses standards bodies in the future. Apologies to Mr Banks for the extended quote...   

<blockquote>
The official was speaking the human version of Standard, the galaxy's lingua franca. Standard had been chose as an inter-species, pan-galactic language over eight billion years ago. Dwellers had been the main vector in its spread, though they made a point of emphasising that it was not theirs originally. They had one very ancient, informal vernacular and another even more ancient formal language of their own, plus lots that had survived somehow from earlier times or been made up in the meantime. These latter came and went in popularity as such things tended to.

'Oh no, there was a competition,' the Dweller guide/mentor Y'sul had explained to Fassin on his first delve, hundreds of years ago. 'Usual thing; lots of competing so-called universal standards. There was a proper full-scale war after one linguistic disagreement -- a grumous and a p'Liner species, if memory serves -- and after that came the usual response: inquiries, missions, meetings, reports, conferences, summits.'

'What we now know as Standard was chosen after centuries of research, study and argument by a vast and unwieldly committee composed of representatives of thousands of species., at least two of which became effectively extinct during the course of the deliberations. It was chosen, astonishingly, on its merits, because it was an almost perfect language: flexible, descriptive, uncoloured (whatver that means, but apparently it's important), precise but malleable, highly, elegantly complete yet primed for external-term-adoption and with an unusually free but logical link between the written form and the pronounced which could easily and plausibly embrace almost any set of phonemes, scints, glyphs or pictals and still make translatable sense.'

'Best of all, it didn't belong to anybody, the species which had invented it having safely extincted itself themselves millions of years earlier without leaving either any proven inheritors or significant mark on the greater galaxy, save this sole linguistic gem. Even more amazingly, the subsequent conference to endorse the decision of the mega-committee went smoothly and agreed all the relevan recommendations. Take-up and acceptance were swift and widespread. Standard became the first and so far only true universal language within just a few Quick-mean generations. Set a standard for pan-species cooperation that everybody's been trying to live up to ever since.'
</blockquote>

Too funny. I love how the 'perfect' language was created by a race that extincted themselves. Just goes to show that perfection ain't everything...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>90</wp:post_id>
		<wp:post_date>2006-03-21 09:39:41</wp:post_date>
		<wp:post_date_gmt>2006-03-21 16:39:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>standard</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Translation and a Citation Microformat</title>
		<link>http://inkdroid.org/2006/04/01/translation-and-a-citation-microformat/</link>
		<pubDate>Sat, 01 Apr 2006 10:25:42 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=91</guid>
		<description></description>
		<content:encoded><![CDATA[<div><a href="http://b3ta.com/challenge/google/"><img width="400" align="left" style="margin-right: 15px" src="http://www.inkdroid.org/images/translate.jpg" /></a>I can think of only one <a href="http://www.google.com">company</a> that has the resources to embed translation links into the world's existing body of printed material. What's more, while they are at it they are going to markup the title page with a <a href="http://microformats.org/wiki/citation">citation microformat</a>...and get this...the microformat is based on a <a href="http://web.archive.org/web/20071009055221/http://www.niso.org:80/committees/committee_ax.html">OpenURL</a> <a href="http://gmpg.org/xmdp/description">XMDP</a> profile so that it'll interoperate with existing citation resolvers in use in libraries around the world...niiiice.</div>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>91</wp:post_id>
		<wp:post_date>2006-04-01 03:25:42</wp:post_date>
		<wp:post_date_gmt>2006-04-01 10:25:42</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>translation-and-a-citation-microformat</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>80072</wp:comment_id>
			<wp:comment_author><![CDATA[Recent Links Tagged With "xmdp" - JabberTags]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.jabbertags.com/popular/xmdp</wp:comment_author_url>
			<wp:comment_author_IP>67.228.47.154</wp:comment_author_IP>
			<wp:comment_date>2009-01-05 04:34:02</wp:comment_date>
			<wp:comment_date_gmt>2009-01-05 11:34:02</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] public links &gt;&gt; xmdp   Translation and a Citation Microformat Saved by FishingGuy13 on Mon 22-12-2008   XHTML Friends Network Saved by igaum on Sun 14-12-2008   [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Graham Patrick Summers</title>
		<link>http://inkdroid.org/2006/04/02/graham-patrick-summers/</link>
		<pubDate>Mon, 03 Apr 2006 00:19:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=92</guid>
		<description></description>
		<content:encoded><![CDATA[<div><img src="http://static.flickr.com/37/122218006_b789d5cfae_m.jpg" style="margin-right: 15px;" /><br /><br />
Graham Patrick Summers<br />
Born: April 2, 2006 in McHenry, IL at 11:51 AM <br />
Weight: 8lb 8oz<br />
Length: 22 inches<br />
Mother and Baby Healthy and Happy :-)<br /><br />
Details to follow!
</div>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>92</wp:post_id>
		<wp:post_date>2006-04-02 17:19:49</wp:post_date>
		<wp:post_date_gmt>2006-04-03 00:19:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>graham-patrick-summers</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="family"><![CDATA[family]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>185</wp:comment_id>
			<wp:comment_author><![CDATA[Brian Cassidy]]></wp:comment_author>
			<wp:comment_author_email>brian.cassidy@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>70.25.33.230</wp:comment_author_IP>
			<wp:comment_date>2006-04-05 18:51:56</wp:comment_date>
			<wp:comment_date_gmt>2006-04-06 01:51:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Congrats! Summers++ =)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>180</wp:comment_id>
			<wp:comment_author><![CDATA[Robert Lasch]]></wp:comment_author>
			<wp:comment_author_email>robertalasch@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>12.47.12.131</wp:comment_author_IP>
			<wp:comment_date>2006-04-04 11:12:28</wp:comment_date>
			<wp:comment_date_gmt>2006-04-04 18:12:28</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Congratulations Ed!!  Very cute little boy!!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>175</wp:comment_id>
			<wp:comment_author><![CDATA[Chris McAvoy]]></wp:comment_author>
			<wp:comment_author_email>chris.mcavoy@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>12.47.30.10</wp:comment_author_IP>
			<wp:comment_date>2006-04-03 07:25:31</wp:comment_date>
			<wp:comment_date_gmt>2006-04-03 14:25:31</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey!  Congratulations!  Very cute picture!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>176</wp:comment_id>
			<wp:comment_author><![CDATA[Graham Fawcett]]></wp:comment_author>
			<wp:comment_author_email>fawcett@uwindsor.ca</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>137.207.200.153</wp:comment_author_IP>
			<wp:comment_date>2006-04-03 08:12:50</wp:comment_date>
			<wp:comment_date_gmt>2006-04-03 15:12:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[On behalf of Grahams everywhere, thanks for your +1. ;-)
Congratulations! He's a handsome boy.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>178</wp:comment_id>
			<wp:comment_author><![CDATA[Gabriel Farrell]]></wp:comment_author>
			<wp:comment_author_email>gsf@panix.com</wp:comment_author_email>
			<wp:comment_author_url>http://un.ctuo.us</wp:comment_author_url>
			<wp:comment_author_IP>69.112.253.5</wp:comment_author_IP>
			<wp:comment_date>2006-04-03 12:14:36</wp:comment_date>
			<wp:comment_date_gmt>2006-04-03 19:14:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey, wow, awesome. Congratulations, Ed!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>215</wp:comment_id>
			<wp:comment_author><![CDATA[Stuart Weibel]]></wp:comment_author>
			<wp:comment_author_email>stuart.weibel@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://weibel-lines.typepad.com</wp:comment_author_url>
			<wp:comment_author_IP>24.18.164.218</wp:comment_author_IP>
			<wp:comment_date>2006-04-12 20:04:16</wp:comment_date>
			<wp:comment_date_gmt>2006-04-13 03:04:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Congratulations, Ed... hang on tight!

stu]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>224</wp:comment_id>
			<wp:comment_author><![CDATA[Tamara Beeler (Snihurowycz)]]></wp:comment_author>
			<wp:comment_author_email>tamarabeeler@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>65.12.181.4</wp:comment_author_IP>
			<wp:comment_date>2006-04-13 20:23:01</wp:comment_date>
			<wp:comment_date_gmt>2006-04-14 03:23:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[He's beautiful!  Congratulations to your family.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>hGoogle</title>
		<link>http://inkdroid.org/2006/04/13/hgoogle/</link>
		<pubDate>Thu, 13 Apr 2006 15:12:02 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=93</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.google.com/calendar/"><img src="http://www.google.com/calendar/images/calendar_sm2.gif" border="0" align="left" style="margin-right:15px" /></a>So it's been <a href="http://microformats.org/discuss/mail/microformats-discuss/2006-April/thread.html#3661">noted</a> elsewhere that the latest ajaxy application out of google labs (<a href="http://www.google.com/calendar">Google Calendar</a>) lacks support for the <a href="http://microformats.org/wiki/hcalendar">hCalendar</a> microformat.</p>

<p>Perhaps it's an oversight--but with all the high profile <a href="http://radar.oreilly.com/archives/2006/03/etech_ray_ozzie.html">exposure</a> microformats have been getting lately it's kind of hard to imagine. But people have deadlines and some things just can't make it into the first release--even at Google. The main thing, as Mark Pilgrim says is:</p>

<blockquote>
Sniping from the sidelines makes us look petty and insular.  Instead
of making assumptions about big bad evil Google ignoring open
standards and locking users in, have we tried opening a dialogue?
</blockquote>

<p>I don't know anyone at google so I feel like I'm doing my part by just blogging about how <em>awesome</em> it would be if they marked up their calendar data using hCalendar. As a full featured calendaring application on the web, Google Calendar could really enable  downstream applications like the <a href="http://web.archive.org/web/20060720020831/http://spaces.msn.com:80/editorial/rayozzie/demo/liveclip/liveclipsample/clipboardexample.html">LiveClipboard</a> if they simply added some class attributes and spans to the data they are already displaying.</p>

<p>In the long run I imagine it's in Google's best interests to promote microformats since their infrastructure would allow them to take best advantage of a system of distributed metadata. Here's to hoping that it'll be layered in sometime soon. In the meantime <a href="http://microformats.org/discuss/mail/microformats-discuss/2006-April/003669.html">Scott and Mark</a> have the right idea!</p>

<p>By the way, being able to enter a quick event in free text and have the time/location/description parsed as opposed to tabbing around in a complicated form is very nice.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>93</wp:post_id>
		<wp:post_date>2006-04-13 08:12:02</wp:post_date>
		<wp:post_date_gmt>2006-04-13 15:12:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>hgoogle</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="business"><![CDATA[business]]></category>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>219</wp:comment_id>
			<wp:comment_author><![CDATA[Bruce]]></wp:comment_author>
			<wp:comment_author_email>bdarcus@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>24.210.249.184</wp:comment_author_IP>
			<wp:comment_date>2006-04-13 16:05:54</wp:comment_date>
			<wp:comment_date_gmt>2006-04-13 23:05:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I've only JUST looked at the new calendar stuff, but Google does support iCal and some sort of XML feed (Atom?). Surely those are standards, and have proven more useful to date than microformats? That's not to say they can't do better of course.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>221</wp:comment_id>
			<wp:comment_author><![CDATA[Administrator]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>24.12.123.44</wp:comment_author_IP>
			<wp:comment_date>2006-04-13 17:07:11</wp:comment_date>
			<wp:comment_date_gmt>2006-04-14 00:07:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah, totally -- that's why I thought the initial reaction about hCalendar was a bit premature.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>23893</wp:comment_id>
			<wp:comment_author><![CDATA[Tom]]></wp:comment_author>
			<wp:comment_author_email>tmbdev@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>88.134.217.1</wp:comment_author_IP>
			<wp:comment_date>2007-04-24 09:48:23</wp:comment_date>
			<wp:comment_date_gmt>2007-04-24 16:48:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[At this point, microformats are just a bunch of wiki pages, not an "open standard" (they're open, just not standards).  Furthermore, Google offers RSS, iCal, and configurable HTML feeds for the calendars--how much more open do you want them to get?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>23931</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.241.133</wp:comment_author_IP>
			<wp:comment_date>2007-04-24 10:50:24</wp:comment_date>
			<wp:comment_date_gmt>2007-04-24 17:50:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I thought it was evident how much more open I wanted them to get :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>oai/sru and ruby</title>
		<link>http://inkdroid.org/2006/04/20/oaisru-and-ruby/</link>
		<pubDate>Thu, 20 Apr 2006 19:59:45 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=94</guid>
		<description></description>
		<content:encoded><![CDATA[<pre>
biblio:~/Projects/ruby-oai ed$ ruby test.rb
Loaded suite test
Started
..........
Finished in 171.247595 seconds.

10 tests, 280 assertions, 0 failures, 0 errors
</pre>

<p>So after about 4 hours of hacking I've got <a href="http://oai.rubyforge.org">ruby-oai</a> which is a <a href="http://openarchives.com">OAI-PMH</a> client library for ruby. Included is a test suite that puts all 6 oai-pmh verbs through their paces using OAI-PMH servers at the <a href="http://loc.gov">loc.gov</a>, <a href="http://lanl.gov">lanl.gov</a>, <a href="http://pubmedcentral.gov">pubmedcentral.gov</a>.</p>

<p>Just a few days before I did something with <a href="http://sruby.rubyforge.org">sruby</a> which is a <a href="http://www.loc.gov/standards/sru">SRU</a> client library for Ruby. Now these are just the initial versions, and I'm sure there are ways that they can be improved. But after doing a few years of solid Java coding it's just another reminder of how dynamic languages such as Ruby and Python can really help catapault productivity.</p>

<p>I have to admit, I do miss javac sitting on my shoulder reminding me of things I'm doing wrong at compile time compared with discovering bugs/errors at run time in python and ruby. But I've really come around to Bruce Eckel's point about <a href="http://web.archive.org/web/20100209072034/http://mindview.net:80/WebLog/log-0025">Strong Typing vs Strong Testing</a>. Building sruby and ruby-oai using test driven development really makes me more confident that my code is working properly...and what's more it makes me much happier with the look and feel of the API. This look n' feel aspect is something that the mechanical javac can't really help with. Doing test driven development in Java with Eclipse approaches this level--but somehow isn't as fun--but I imagine it scales better to larger teams. However, I don't work on any of these uber large teams anyhow. Hopefully I'll find a moment to talk about the new work I'm doing in the coming weeks.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>94</wp:post_id>
		<wp:post_date>2006-04-20 12:59:45</wp:post_date>
		<wp:post_date_gmt>2006-04-20 19:59:45</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oaisru-and-ruby</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Fedora/SOAP and Ruby</title>
		<link>http://inkdroid.org/2006/04/25/fedorasoap-and-ruby/</link>
		<pubDate>Tue, 25 Apr 2006 20:31:09 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=96</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://fedora.info"><img src="http://web.archive.org/web/20060811065005/http://www.fedora.info:80/images/index/logo.gif" border="none" style="margin-right: 15px" align="left" /></a></p>

<p>I've been playing around getting <a href="http://ruby-lang.org">ruby</a> to talk to the <a href="http://fedora.info">fedora</a> framework for building digital repositories. Fedora makes its api available by different sets of SOAP services, defined in WSDL files. What follows is a brief howto on getting Ruby to talk to the <a href="http://www.fedora.info/definitions/1/0/api/Fedora-API-A.html">API-A</a> and <a href="http://www.fedora.info/definitions/1/0/api/Fedora-API-M.html">API-M</a></p>

<p>To get basic API-A and API-M clients working you'll need the following:</p>

<ul><li>A modern ruby: probably >= 1.8.2</li><li>The latest <a href="http://web.archive.org/web/20100819082659/http://dev.ctor.org:80/soap4r">soap4r</a>: the one that comes standard in 1.8.4 may work but emits some warnings when processing the fedora wsdl files.</li><li>The latest <a href="http://raa.ruby-lang.org/project/http-access2/">http-access2</a> if you plan on doing API-M with basic authentication.</li><li>A <a href="http://www.textualize.com/files/ruby-fedora.tar.gz">tarball</a> of ruby classes I generated with wsdl2ruby using the wsdl files in the latest fedora distribution.</li></ul>

<p>So assuming you've unpacked the ruby-fedora.tar.gz you should be able to go in there and write the following program which will attempt to connect to a fedora server at localhost:8080 and retrieve the PDF datastream for an object with PID 'biblio:2' and write it out to disk. I guess to get it working right you should change the datastream label and PID to something relevant in your repository.</p>

<pre lang="ruby">
#!/usr/bin/env ruby

require 'Fedora-API-A-WSDLDriver'

fedora = FedoraAPIA.new
ds = fedora.getDatastreamDissemination('biblio:2', 'PDF', nil)

File.open('shannon.pdf', 'w') {|f| f.write ds.stream}
</pre>

<p>To talk API-M it's just a little bit more work since you have to tell the SOAP client what the username and password are. In this example we simply ask for the next PID in the 'biblio' namespace.</p>

<pre lang="ruby">
#!/usr/bin/env ruby

require 'Fedora-API-M-WSDLDriver'

host = 'http://localhost:8080/fedora/services/management'
user = 'fedoraAdmin'
pass = 'fedoraAdmin'

fedora = FedoraAPIM.new
fedora.options['protocol.http.basic_auth'] &lt;&lt; [host, user, pass]

print fedora.getNextPID(SOAP::SOAPNonNegativeInteger.new(1), 'biblio')   
</pre>

<p>Obviously there's a lot more depth to go into as far as exploring the fedora api. But these are the basics. Tomorrow I'm going to explore FOXML some more and look at what's involved in doing injest with ruby.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>96</wp:post_id>
		<wp:post_date>2006-04-25 13:31:09</wp:post_date>
		<wp:post_date_gmt>2006-04-25 20:31:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>fedorasoap-and-ruby</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>17364</wp:comment_id>
			<wp:comment_author><![CDATA[Micah]]></wp:comment_author>
			<wp:comment_author_email>mwedeme@emory.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>170.140.210.152</wp:comment_author_IP>
			<wp:comment_date>2007-03-20 14:35:50</wp:comment_date>
			<wp:comment_date_gmt>2007-03-20 21:35:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Whenever I try to connect to my fedora server (on localhost:8080) via SOAP, I get a 401 authentication error, and it returns a bunch of HTML .  I have tried turning off XACML (ie. ENFORCE-MODE="permit-all-requests") and removing all the deny* XACML policies, but nothing has worked.

Note: This is for API-A.  I have not even tried API-M.

Setup:
Linux
ruby 1.8.5 (using included version of soap4r)
Fedora 2.2

Here is the code:
===========================
require 'soap/wsdlDriver'

factory = SOAP::WSDLDriverFactory.new('http://localhost:8080/fedora/wsdl?api=API-A')
driver = factory.create_rpc_driver
response = driver.getObjectProfile(:pid =&gt; 'changeme:1', :asOfDateTime =&gt; '')
====================================

Any ideas?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Cataloging at the BBC with RubyOnRails</title>
		<link>http://inkdroid.org/2006/04/26/bbc-archive/</link>
		<pubDate>Wed, 26 Apr 2006 14:37:55 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=97</guid>
		<description></description>
		<content:encoded><![CDATA[<p>It's nice to see that <a href="http://web.archive.org/web/20061208071320/http://open.bbc.co.uk:80/catalogue/infax/">BBC Programme Catalogue</a> (built with <a href="http://rubyonrails.org">RubyOnRails</a> and <a href="http://mysql.com">MySQL</a>) has gone live. Here is some historical background from the <a href="http://web.archive.org/web/20061206204544/http://open.bbc.co.uk:80/cataloguemeta/2005/11/about_this_prototype.html">about</a> page:</p>

<blockquote>
The BBC has been cataloguing and indexing its programmes since the 1920s. The development of the programme catalogue has reflected the changes in the BBC and in broadcasting over the last seventy five years. For example, in the early days of broadcasting, for both Radio and TV, the majority of programmes were broadcast live and were never recorded. There was therefore little point at the time to do extensive cataloguing and indexing of material that did not exist. As you will see, the number of catalogue entries for a day in the 1990s, far exceeds the entries for a day from the 1950s.

As recording technology developed in both mediums, the requirement to keep material for re-use also grew. If material was going to be re-used, it had to be catalogued and indexed. The original records of radio programmes were handwritten into books; over time, card catalogues were developed, and from the mid-1980s onwards there have been computer based catalogues.

This experimental catalogue database holds over 900,000 entries. It is a sub-set of the data from the internal BBC database created and maintained by the BBC&rsquo;s Information and Archives department. This public version is updated daily as new records are added and updated in the main catalogue. This figure is so high because, for example, each TV news story now has an individual entry in the catalogue. 
</blockquote>

<p>Talk about sexy retrospective conversion eh? Hats off to <a href="http://feeds.feedburner.com/hackdiary?m=16">Matt Biddulph</a> and his colleagues. I wish I was going to <a href="http://web.archive.org/web/20071011033934/http://railsconf.org/talks/selected/show/122">RailsConf</a> to hear more of the technical details. Actually, if you haven't already take a look at the RailsConf <a href="http://web.archive.org/web/20061114095809/http://railsconf.org:80/talks">program</a>--it looks like it's going to be a great event.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>97</wp:post_id>
		<wp:post_date>2006-04-26 07:37:55</wp:post_date>
		<wp:post_date_gmt>2006-04-26 14:37:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bbc-archive</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>building and ingesting</title>
		<link>http://inkdroid.org/2006/04/28/building-and-ingesting/</link>
		<pubDate>Fri, 28 Apr 2006 18:29:31 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=98</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I prefer using an XML generating mini-language (<a href="http://effbot.org/zone/element-index.htm">elementtree</a>, <a href="http://search.cpan.org/dist/XML::Writer/">XML::Writer</a>, <a href="http://www.germane-software.com/software/rexml/">REXML</a>, <a href="http://web.archive.org/web/20101208214148/http://www.kieranholland.com:80/code/documentation/nevow-stan/">Stan</a>, etc) to actually writing raw XML. It's just too easy for me to forget or misstype an end tag, or forget to encode strings properly--and I find all those inline strings or even here-docs make a mess of an otherwise pretty program.</p>

<p>Recently I wanted some code to write FOXML for ingesting digital objects into my <a href="http://fedora.info">Fedora</a> test instance. I'm working in Ruby so REXML seemed like the best place to start...but after I finished I ran across <a href="http://www.xml.com/pub/a/2006/01/04/creating-xml-with-ruby-and-builder.html">Builder</a>. The Builder code turned out to be somewhat shorter, much more expressive and consequently a bit easier to read (for my eyes). Here's a quick example of how Builder's API improves on REXML when writing this little chunk of XML:</p>

<pre>&lt;dc xmlns='http://purl.org/dc/elements/1.1/'&gt;
  &lt;title&gt;Communication in the Presence of Noise&lt;/title&gt;
&lt;/dc&gt;
</pre>

<p>So here's the REXML code:</p>

<pre lang="ruby">
dc = REXML::Element.new 'dc'
dc.add_attributes 'xmlns' => 'http://purl.org/dc/elements/1.1/'
title = REXML::Element.new 'title', dc
title.text 'Communication in the Presence of Noise'
</pre>

<p>and the Builder code:</p>

<pre lang="ruby">
x = Builder::XmlMarkup.new 
x.dc 'xmlns' => 'http://purl.org/dc/elements/1.1' do
  x.title 'Communication in the Presence of Noise'
end
</pre>

<p>So both are four lines, but look at how the Builder::XmlMarkup object infers the name of the element based on the message that is passed to it? Element attributes and content can be set when the element is created--something I wasn't able to do w/ REXML. My favorite though is Builder's use of blocks so that the hierarchical structure of the code directly mirrors that of the XML content!</p>

<p>So anyway, if you read this far you might actually like to see how a FOXML document can be built and ingested into Fedora--so hear goes building the document:</p>

<pre lang="ruby">
x = Builder::XmlMarkup.new :indent => 2
  
x.digitalObject 'xmlns' => 'info:fedora/fedora-system:def/foxml#' do
  
  x.objectProperties do
    x.property 'NAME' => 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type',
      'VALUE' => 'FedoraObject'
    x.property 'NAME' => 'info:fedora/fedora-system:def/model#state',
      'VALUE' => 'A'
  end

  x.datastream 'ID' => 'DC', 'STATE' => 'A', 'CONTROL_GROUP' => 'X' do
    x.datastreamVersion 'ID' => 'DC.0', 'MIMETYPE' => 'text/xml' do
      x.xmlContent do
        x.tag! 'oai_dc:dc',
          'xmlns:oai_dc' => 'http://www.openarchives.org/OAI/2.0/oai_dc/',
          'xmlns:dc' => 'http://purl.org/dc/elements/1.1/' do
          x.tag! 'dc:title', 'Communication in the Presence of Noise'
          x.tag! 'dc:creator', 'Claude E Shannon'
          x.tag! 'dc:subject', 'Information Science'
        end
      end
    end
  end

end

</pre>

<p>And here's some code to fire the foxml at Fedora in a SOAP call:</p>

<pre lang="ruby">
require 'Fedora-API-M-WSDLDriver'

# configure api_m soap client for 
host = 'http://localhost:8080/fedora/services/management'
user = 'fedoraAdmin'
pass = 'fedoraAdmin'
fedora = FedoraAPIM.new
fedora.options['protocol.http.basic_auth'] &lt;&lt; [host, user, pass]

fedora.ingest SOAP::SOAPBase64.new(x.to_s), 'foxml1.0', 'added test object' 
</pre>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>98</wp:post_id>
		<wp:post_date>2006-04-28 11:29:31</wp:post_date>
		<wp:post_date_gmt>2006-04-28 18:29:31</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>building-and-ingesting</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>27359</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Lux]]></wp:comment_author>
			<wp:comment_author_email>peter@echotech.ca</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>24.215.108.37</wp:comment_author_IP>
			<wp:comment_date>2007-05-08 14:25:49</wp:comment_date>
			<wp:comment_date_gmt>2007-05-08 21:25:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great posts and a lot of work put into this. I was looking for some help getting a Ruby implementation of a invocation tool for the Fedora API. This is a great start. 

I am having difficulty however with Builder. When I run this last bit of code, I'm getting a commandline error  stating "uninitialized constant Builder (NameError)"


Builder  2.1.1 is installed running on Ruby 1.8.6. 


Thanks for the work on the classes.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>27519</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.241.133</wp:comment_author_IP>
			<wp:comment_date>2007-05-09 06:09:37</wp:comment_date>
			<wp:comment_date_gmt>2007-05-09 13:09:37</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Peter, how did you install Builder? Did you install the gem? Try putting a:

<pre>
  require 'builder'
</pre>

at the top of your script. Also, a bunch of people hacking ruby and things like fedora hang out in irc://chat.freenode.net/#code4lib so please stop by and we can figure this stuff out interactively :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>info-uris and opening up library data</title>
		<link>http://inkdroid.org/2006/05/16/info-uris-and-opening-up-library-data/</link>
		<pubDate>Wed, 17 May 2006 03:34:17 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=99</guid>
		<description></description>
		<content:encoded><![CDATA[I had a few moments to read the info-uri spec during a short flight from DC to Chicago this past weekend. info-uri aka <a href="ftp://ftp.rfc-editor.org/in-notes/rfc4452.txt">RFC 4452</a> is a spec that allows you to create URIs for identifiers in public namespaces.

So what does this mean in practice and why would you want to use one?

If you have a database of stuff you make available on the web, and you have ids for the stuff (say a primary_key on a Stuff table) you essentially have an identifier in a public namespace. Go <a href="http://info-uri.info/registry/register.html">register</a> the namespace!

So, the LoC assigns <a href="http://www.loc.gov/marc/lccn-namespace.html">identifiers</a> called Library of Congress Control Numbers (LCCN) to each of its metadata records. Here's the personal-name authority record (expressed as <a href="http://www.loc.gov/standards/mads/">MADS</a>) that allows works by Tim Berners-Lee to be grouped together:
<pre>&lt;?xml version='1.0' encoding='UTF-8'?&gt;
&lt;madsCollection
xmlns:xlink='http://www.w3.org/1999/xlink'
xmlns='http://www.loc.gov/mods/v3'
xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance'
xsi:schemaLocation='http://www.loc.gov/mads
http://www.loc.gov/standards/mads/mads.xsd'&gt;
&lt;mads version='beta'&gt;
&lt;authority&gt;
&lt;name type='personal' authority='naf'&gt;
&lt;namePart&gt;Berners-Lee, Tim&lt;/namePart&gt;
&lt;/name&gt;
&lt;titleInfo authority='naf'&gt;
&lt;title/&gt;
&lt;/titleInfo&gt;
&lt;/authority&gt;
&lt;variant type='other'&gt;
&lt;name type='personal'&gt;
&lt;namePart&gt;Lee, Tim Berners-&lt;/namePart&gt;
&lt;/name&gt;
&lt;titleInfo&gt;
&lt;title/&gt;
&lt;/titleInfo&gt;
&lt;/variant&gt;
&lt;variant type='other'&gt;
&lt;name type='personal'&gt;
&lt;namePart&gt;Berners-Lee, Timothy J&lt;/namePart&gt;
&lt;/name&gt;
&lt;titleInfo&gt;
&lt;title/&gt;
&lt;/titleInfo&gt;
&lt;/variant&gt;
&lt;note type='source'&gt;The WWW virtual library Web site,
Feb. 15, 1999 about the virtual library (Tim Berners-Lee; creator
of the Web)&lt;/note&gt;
&lt;note type='source'&gt;OCLC, Feb. 12, 1999 (hdg.: Berners-Lee,
Tim; usage: Tim Berners-Lee)&lt;/note&gt;
&lt;note type='source'&gt;Gaines, A. Tim Berners-Lee and the
development of the World Wide Web, 2001: CIP galley
(Timothy J. Berners-Lee; b. London, England, June 8, 1955)&lt;/note&gt;
&lt;recordInfo&gt;
&lt;identifier&gt;no 99010609 &lt;/identifier&gt;
&lt;recordContentSource authority='marcorg'&gt;NBL&lt;/recordContentSource&gt;
&lt;recordCreationDate encoding='marc'&gt;990216&lt;/recordCreationDate&gt;
&lt;recordChangeDate encoding='iso8601'&gt;20010716094452.0&lt;/recordChangeDate&gt;
&lt;recordIdentifier&gt;1851704&lt;/recordIdentifier&gt;
&lt;languageOfCataloging&gt;
&lt;languageTerm authority='iso639-2b' type='code'&gt;eng&lt;/languageTerm&gt;
&lt;/languageOfCataloging&gt;
&lt;/recordInfo&gt;
&lt;/mads&gt;
&lt;/madsCollection&gt;</pre>
In the record/recordInfo/identifier element you can find the LCCN:
<pre>no 99010609</pre>
Which can be represented as an info-uri:
<pre>info:lccn/no9910609</pre>
Now why would you *ever* want to express a LCCN as an info-uri? The LoC has spent a lot of <a href="http://www.loc.gov/catdir/pcc/naco/personnamefaq.html">time and effort</a> establishing these personal name and subject authorities. You might want to use a URI like info:lccn/no9910609 to identify Tim Berners-Lee as an individual in your data so that other people will know who you are talking about and be able to interoperate with you. For example you can now unambiguously say that Tim Berners-Lee created <a href="http://www.amazon.com/gp/product/006251587X">Weaving the Web</a>
<pre>&lt;info:lccn/no9910609&gt; &lt;http://purl.org/dc/elements/1.1/creator&gt;
&lt;info:lccn/99027665&gt;</pre>
That was for you ksclarke :-) Pretty nifty eh? Now what's really cool is that while info-uris aren't necessarily resolvable (by design) OCLC does have the <a href="http://alcme.oclc.org/laf/index.html">Linked Authority File</a>, which allows you to look up these records. So tbl's record can be found here:
<pre><a href="http://errol.oclc.org/laf/no99-10609.html">http://errol.oclc.org/laf/no99-10609.html</a></pre>
I imagine that this is part of the joint OCLC/LoC/Die Deutsche Bibliothek project to build a <a href="http://www.oclc.org/research/projects/viaf/default.htm">Virtual International Authority File</a>...but I'm not totally sure. At any rate there's currently no way to drop a lccn info-uri in there and have it resolve to the XML--but that looks like an easy thing to add.

It feels like there is a real opportunity for libraries and archives to offer up their data to the larger web community. How can we make it easy for non-library folks to find and repurpose this data we've so assiduously collected over the years?

tbl is encouraging people to <a href="http://dig.csail.mit.edu/breadcrumbs/node/71">give themselves a URI</a>...I wonder if he knew that he (and millions of others) already have one!
<ul>
	<li>Nikola Tesla: <a href="http://errol.oclc.org/laf/n78-86404.html">info:lccn/n7887404</a></li>
	<li>Madonna: <a href="http://errol.oclc.org/laf/n84-156128.html">info:lccn/n84156128</a></li>
	<li>Notorious B.I.G.: <a href="http://errol.oclc.org/laf/no96-31850.html">info:lccn/no9631850</a></li>
	<li>Henriette Avram: <a href="http://errol.oclc.org/laf/n50-29954.html">info:lccn/n5029954</a></li>
</ul>
<h2>Addendum:</h2>
If you are interested section 6 of the RFC details the subtle rationale behind why the authors chose to create a new URI scheme rather than:
<ol>
	<li>using an existing <a href="http://www.iana.org/assignments/uri-schemes.html">URI scheme</a></li>
	<li>creating a new <a href="http://www.iana.org/assignments/urn-namespaces">URN namespace</a></li>
</ol>
In essence they didn't want to use an existing URI scheme because they all assume that you should be able to dereference the URI. An example of dereferencing in action can be found when clicking on a link like <a href="http://www.yahoo.com/">http://www.yahoo.com</a> where the magic of DNS allows you to find yahoo's web server and talk to it on port 80 in a predictable way. info-uris are designed to be agnostic as to whether or not the identifier can be dereferenced through a resolver of some kind.

Using URNs was thrown out since URNs are intended to persistently identify information resources and info-uris are designed to identify persistent namespaces not the resources themselves. Also the process of establishing a URN namespace isn't for the faint of heart, which is evidenced by the short list of them. info-uris by contrast have a <a href="http://info-uri.info/registry/">registrar</a> who will expedite the process of registering a namespace, and have set up a framework for publishing validation/normalization rules. The current registrar is run by OCLCRLGBORG^w OCLC on behalf of NISO. So basically you don't have to write an RFC to register your namespace.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>99</wp:post_id>
		<wp:post_date>2006-05-16 20:34:17</wp:post_date>
		<wp:post_date_gmt>2006-05-17 03:34:17</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>info-uris-and-opening-up-library-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_b4169104a31afcf2248aefcbc91510af</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_6386a1caed5a526a582dac837e33af78</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_d556f5ae2f2a257cd127d7ed967e14f6</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_469fbc89e664fe21a51b3c86d4222564</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_5f3132032979a44b8b8bdb5b4b915bc3</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_5446ec6a4fe030cb1edef531d0936bd9</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_cc030d85b254494370e5df71336f6d00</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>1146</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &raquo; more on web identifiers]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org/2006/07/11/more-on-web-identifiers/</wp:comment_author_url>
			<wp:comment_author_IP>66.187.134.52</wp:comment_author_IP>
			<wp:comment_date>2006-07-11 04:25:28</wp:comment_date>
			<wp:comment_date_gmt>2006-07-11 11:25:28</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] I monitor the www-tag discussion list, but half of it goes right over my head&#8211;so I was pleased when a colleague forwarded URNs, Namespaces and Registries to me. Don&#8217;t let the 2001 in the URL fool you, it has been updated quite recently. This finding provides an interesting counterpoint to rfc 4452 which I wrote about earlier. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>1149</wp:comment_id>
			<wp:comment_author><![CDATA[PeteJ]]></wp:comment_author>
			<wp:comment_author_email>pete.johnston@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>195.188.238.252</wp:comment_author_IP>
			<wp:comment_date>2006-07-11 04:30:29</wp:comment_date>
			<wp:comment_date_gmt>2006-07-11 11:30:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I don't think it is correct to say that "info-uris are designed to identify persistent namespaces not the resources themselves". RFC 4452 refers to the use of info URIs to describe "information assets", and says

When referencing an information asset by means of its "info" URI, the asset SHALL be considered a "resource" as defined in RFC 3986

And your examples above refer to info URIs for people, i.e. resources/things other than "persistent namespaces".

The examples of info URIs from the LCCN namespace does raise an interesting question. According to http://www.loc.gov/marc/lccn-namespace.html and http://info-uri.info/registry/OAIHandler?verb=GetRecord&amp;metadataPrefix=reg&amp;identifier=info:lccn/

An LCCN is an identifier assigned by the Library of Congress for a
metadata record (e.g., bibliographic record, authority record)

which seems quite unambiguous that an LCCN (and an info URI in the LCCN namespace?) is an identifier for LoC's metadata record. If that is the case, then I think using that same identifier for the subject of the metadata record (the person etc) contradicts that statement by LoC and introduces ambiguity about what asset/resource is identified. The person who created the LoC authority record describing the Notorious B.I.G.is a different person from the one who created the Notorious B.I.G. (Probably.)

But I'm really of the school that says anything the info URI scheme provides could be achieved more easily and cheaply - still without writing an RFC to refister my namespace ;-) - using the http URI scheme e.g. as suggested here

http://lists.w3.org/Archives/Public/www-rdf-interest/2003Oct/0000

Cheers
PeteJ]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>37649</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.241.132</wp:comment_author_IP>
			<wp:comment_date>2007-07-11 06:27:49</wp:comment_date>
			<wp:comment_date_gmt>2007-07-11 13:27:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Total agreement Pete :-) One of the benefits of writing down ones thoughts is to see how much they change over time.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>professionalism in the age of discontent</title>
		<link>http://inkdroid.org/2006/05/19/professionalism-in-the-age-of-discontent/</link>
		<pubDate>Fri, 19 May 2006 18:56:13 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=100</guid>
		<description></description>
		<content:encoded><![CDATA[After seeing him speak and meeting him a couple times I'm a big fan of <a href="http://www.holovaty.com/blog">Adrian's</a> work. He was one of the first people to "mash up" google maps at <a href="http://chicagocrime.org">chicagocrime.org</a>; has set the bar for local online media content at <a href="http://lawrence.com">lawrence.com</a>; created the <a href="http://projects.washingtonpost.com/congress/">Congressional Votes Database</a> at the washingtonpost which allows you to (among other things) get an RSS feed for your representatives votes; and has created probably the most popular <a href="http://djangoproject.com">web framework</a> for python.

But the thing that really impresses me the most about him is how he mixes the role of technologist and journalist. If you are curious take a look at the <a href="http://www.holovaty.com/blog/archive/2006/05/15/0110">commencement speech</a> he just gave at his alma matter, University of Missouri's School of Journalism. Now if you work in/for a libraries/archives (which is likely given this blogs focus) just substitute 'journalism' for 'libraries' as you read the piece. You may be surprised to learn that the field of Journalism finds itself in much the same dire straits that Librarianship is in:

<blockquote>
Then there's this whole Internet thing -- which is clearly evil. Some guy in San Francisco runs a Web site, Craigslist, that lets anybody post a classified ad for free -- completely bypassing the newspaper classifieds and, therefore, chipping away at one of newspapers' most important sources of revenue. Why would I post a classified ad in a newspaper, which charges me money for a tiny ad in which I'm forced to use funky abbreviations just to fit within the word limit, when I can post a free ad to Craigslist, with no space limitation and the ability to post photos, maps and links? Google lets anybody place an ad on search results. Why would I, the consumer, place an ad on TV, radio or in a newspaper, if I can do the same on Google for less money and arguably more reach?
</blockquote>

Ahem, <a href="http://scholar.google.com">Google Scholar</a> or <a href="http://amazon.com">Amazon</a> anyone?

<blockquote>
The foundation that you got here is important because it will guide you for the rest of your journalism career. It's important because, no matter what you do in this industry, it all comes back to that foundation. No matter how the industry changes, no matter how your jobs may change, it all comes back to the core journalism values you've learned here at Missouri.

But, most of all, the foundation is important because you need to understand the rules before you can break them. And now, more than ever, this industry needs to break some rules.

...

You're going to be the people breaking the rules. You're going to be the people inventing new ones. You'll be the person who says, "Hey, let's try this new way of getting our journalism out to the public." You'll be the PR person who says, "Let's try this new way of public relations that takes advantage of the Internet." You'll be the photographer who says, "Wow, quite a few amateur photographers are posting their photos online. Let's try to incorporate that into our journalism somehow."

...

So think about how exciting that is. Rarely is an entire industry in a position such that it needs to completely reinvent itself.

</blockquote>

What are the rules of the library profession that we need to break? In my conversations with fellow library technologists we often talk about how the profession needs to be advanced, like we are uniquely effected by the massive changes in media/information in the last 10 years. I think we should draw some comfort from the fact that we're not the only ones dealing with this new terrain--as we kick ourselves in the pants. Perhaps some new professions are being born out of this melange.

Adrian is the type of professional I'd like to be, that's for sure.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>100</wp:post_id>
		<wp:post_date>2006-05-19 11:56:13</wp:post_date>
		<wp:post_date_gmt>2006-05-19 18:56:13</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>professionalism-in-the-age-of-discontent</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="chicago"><![CDATA[chicago]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>xml spelunking</title>
		<link>http://inkdroid.org/2006/05/27/xml-spelunking/</link>
		<pubDate>Sat, 27 May 2006 13:04:38 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=101</guid>
		<description></description>
		<content:encoded><![CDATA[<p>As part of my day job I've been rifling through large foreign XML files--learning the rhyme and reason of tags used, looking at content, etc... I opened files in Firefox and vim and that was OK--but I like working from the command line. After minimal searching I wasn't able to find a suitable tool that would simply outline the structure of an xml document in the way I wanted--although <a href="http://librarycog.uwindsor.ca:8087/artblog/librarycog/">artunit</a> pointed out <a href='http://simile.mit.edu/gadget/'>Gadget</a> from MIT which looks like a really wonderful GUI tool to try out.  So (predictably) I wrote <a href='http://textualize.com/svn/tilde_bin/xmltree'>my own</a>:</p>
<pre>
biblio:~ ed$ xmltree
Usage: xmltree foo.xml [--depth=n] [--xpath=/foo/bar] [--content]

Specific options:
    -d, --depth n                    max levels
    -x, --xpath /foo/bar             xpath to apply
    -c, --content                    include tag content
    -n, --namespaces                 include namespace information
    -h, --help                       show this message
</pre>
<p>You can use it to list all the elements in a document like this:</p>
<pre>
biblio:~ ed$ xmltree pmets.xml

PorticoMETS
 metsHdr
  agent
   name
 structMapContent
  div
   mdGroup
    descMDcurated
     mdWrap
      xmlData
       PorticoArticleMetadata
        article
... many lines of content removed
</pre>
<p>Maybe it's a huge file and you only want to see a few levels in:</p>
<pre>
biblio:~ed$ xmltree --depth=3 pmets.xml 

PorticoMETS
 metsHdr
  agent
   name
 structMapContent
  div
   mdGroup
   div
   div
   div
 structMapMetadata
  div
   mdGroup
   fileGrp
</pre>
<p>And if you just want to explore a particular node you can use an xpath:</p>
<pre>
biblio:~ed$ xmltree --xpath .//PorticoMETS/structMapContent/div/mdGroup/descMDextracted/mdWrap/xmlData sample.pmets

xmlData
 PorticoArticleMetadata
  article
   front
    journal-meta
     journal-id
     journal-title
     issn
     issn
    article-meta
     article-id
     article-id
     title-group
      article-title
     contrib-group
      contrib
     pub-date
      day
      month
      year
      string-date
     volume
     issue
     fpage
     page-range
     product
     copyright-year
     copyright-holder
     self-uri
</pre>
<p>And finally if you want to eyeball the content of the fields you can use the --content option:</p>
<pre>
biblio:~ ed$ xmltree --xpath .//PorticoMETS/structMapContent/div/mdGroup/
  descMDextracted/mdWrap/xmlData --content sample.pmets

xmlData
 PorticoArticleMetadata
  article
   front
    journal-meta
     journal-id='bull'
     journal-title='Bulletin of the American Mathematical Society'
     issn='0273-0979'
     issn='1088-9485'
    article-meta
     article-id='S0273-0979-00-00866-1'
     article-id='10.1090/S0273-0979-00-00866-1'
     title-group
      article-title='Book Review'
     contrib-group
      contrib='David Marker'
     pub-date
      day='02'
      month='03'
      year='2000'
      string-date='02 March 2000'
     volume='37'
     issue='03'
     fpage='351'
     page-range='351-357'
     product='Tame topology and o-minimal structures, by Lou van den Dries,
       Cambridge Univ. Press, New York (1998), x + 180 pp., $39.95,
       ISBN 0-521-59838-9'
     copyright-year='2000'
     copyright-holder='American Mathematical Society'
     self-uri='http://www.ams.org/jourcgi/jour-getitem?pii=S0273-0979-
       00-00866-1'</pre>
<p>Anyhow, if you have a favorite tool for doing this sort of stuff please let me know. If you want to try out xmltree you can <a href='http://textualize.com/svn/tilde_bin/xmltree'>grab it</a> out of my subversion repository. You'll just need a modern Ruby.
</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>101</wp:post_id>
		<wp:post_date>2006-05-27 06:04:38</wp:post_date>
		<wp:post_date_gmt>2006-05-27 13:04:38</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>xml-spelunking</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>xquery</title>
		<link>http://inkdroid.org/2006/06/12/xquery/</link>
		<pubDate>Mon, 12 Jun 2006 23:22:01 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=102</guid>
		<description></description>
		<content:encoded><![CDATA[<p>So my new <a href="http://loc.gov">employer</a> was kind enough to send me to <a href="http://jcdl2006.org">Joint Conference on Digital Libraries</a> this year. The JCDL program has caught my eye for a few years now, but my previous employer didn't really see the value in being involved in the digital library community. It's nice to be back listening to new people with good ideas again. I plan on taking sparse free-form notes here just so I have a record of what I attended and what I learned--rather than waiting till the end to write up a report.</p>

<p>I spent the morning in David Durand's XQuery tutorial. David has worked on the XML and XLink <a href="http://w3c.org">w3c</a> working groups, teaches at Brown, has over 20 years experience with SGML/XML technologies, and is currently running a startup out of the third floor of his house. He gave a nice hands on demonstration of XQuery using the <a href="http://exist.sf.net">eXist</a> xml database.</p>

<p>About the first half was spent going over the syntax of XQuery which included a nice mini-tutorial on XPath. I've been interested in XQuery since hearing Kevin Clarke talk about it and native xml databases quite a bit on #code4lib, so I really was looking forward to learning more about it from a practical perspective.</p>

<p>I was blown away by how easy it is to actually set up <a href="http://exist.sourceforge.net">eXist</a> and start adding content and querying it. While David was talking I literally downloaded it, set it up and imported a body of test xml data in 5 minutes. The setup amounts to downloading a jar file and running it. A nice feature is the webdav interface which allows you to mount the eXist database as an editable filesystem, which is very handy. In addition eXist provides REST and XMLRPC interfaces. David used the snazzy XQuery Sandbox web interface for exploring XQuery.</p>

<p><img width="400" border="0" src="/images/exist.png" /></p>

<p>I found the functional aspects of XQuery to be really interesting. David nicely summarized the XQuery type system in and covered enough of the basic flow constructs (let, for, where, return, order by) to start experimenting right away. I must admit that I found the mixture of templating functionality (like that in PHP) with the functional style was a little bit jarring--but that's normally the case in an environment that supports templating:</p>

<pre>&lt;hits&gt;
{
for $speech in //SPEECH[LINE &= 'love']
return &lt;hit&gt;{$speech}&lt;/hit&gt;
}
&lt;/hits&gt;</pre>

<p>which can generate:</p>

<p>&lt;HITS&gt;
&lt;HIT&gt;
&lt;SPEECH&gt;
&lt;SPEAKER&gt;KING CLAUDIUS&lt;/SPEAKER&gt;
&lt;LINE&gt;'Tis sweet and commendable in your nature, Hamlet,&lt;/LINE&gt;
&lt;LINE&gt;To give these mourning duties to your father:&lt;/LINE&gt;
&lt;LINE&gt;But, you must know, your father lost a father;&lt;/LINE&gt;
&lt;LINE&gt;That father lost, lost his, and the survivor bound&lt;/LINE&gt;
&lt;LINE&gt;In filial obligation for some term&lt;/LINE&gt;
&lt;LINE&gt;To do obsequious sorrow: but to persever&lt;/LINE&gt;
&lt;LINE&gt;In obstinate condolement is a course&lt;/LINE&gt;
&lt;LINE&gt;Of impious stubbornness; 'tis unmanly grief;&lt;/LINE&gt;
&lt;LINE&gt;It shows a will most incorrect to heaven,&lt;/LINE&gt;
&lt;LINE&gt;A heart unfortified, a mind impatient,&lt;/LINE&gt;
&lt;LINE&gt;An understanding simple and unschool'd:&lt;/LINE&gt;
&lt;LINE&gt;For what we know must be and is as common&lt;/LINE&gt;
&lt;LINE&gt;As any the most vulgar thing to sense,&lt;/LINE&gt;
&lt;LINE&gt;Why should we in our peevish opposition&lt;/LINE&gt;
&lt;LINE&gt;Take it to heart? Fie! 'tis a fault to heaven,&lt;/LINE&gt;
&lt;LINE&gt;A fault against the dead, a fault to nature,&lt;/LINE&gt;
&lt;LINE&gt;To reason most absurd: whose common theme&lt;/LINE&gt;
&lt;LINE&gt;Is death of fathers, and who still hath cried,&lt;/LINE&gt;
&lt;LINE&gt;From the first corse till he that died to-day,&lt;/LINE&gt;
&lt;LINE&gt;'This must be so.' We pray you, throw to earth&lt;/LINE&gt;
&lt;LINE&gt;This unprevailing woe, and think of us&lt;/LINE&gt;
&lt;LINE&gt;As of a father: for let the world take note,&lt;/LINE&gt;
&lt;LINE&gt;You are the most immediate to our throne;&lt;/LINE&gt;
&lt;LINE&gt;And with no less nobility of love&lt;/LINE&gt;
&lt;LINE&gt;Than that which dearest father bears his son,&lt;/LINE&gt;
&lt;LINE&gt;Do I impart toward you. For your intent&lt;/LINE&gt;
&lt;LINE&gt;In going back to school in Wittenberg,&lt;/LINE&gt;
&lt;LINE&gt;It is most retrograde to our desire:&lt;/LINE&gt;
&lt;LINE&gt;And we beseech you, bend you to remain&lt;/LINE&gt;
&lt;LINE&gt;Here, in the cheer and comfort of our eye,&lt;/LINE&gt;
&lt;LINE&gt;Our chiefest courtier, cousin, and our son.&lt;/LINE&gt;
&lt;/SPEECH&gt;
&lt;/HIT&gt;
&lt;HIT&gt;
&lt;SPEECH&gt;
&lt;SPEAKER&gt;HAMLET&lt;/SPEAKER&gt;
&lt;LINE&gt;For God's love, let me hear.&lt;/LINE&gt;
&lt;/SPEECH&gt;
&lt;/HIT&gt;
&lt;HIT&gt;
&lt;SPEECH&gt;
&lt;SPEAKER&gt;OPHELIA&lt;/SPEAKER&gt;
&lt;LINE&gt;My lord, he hath importuned me with love&lt;/LINE&gt;
&lt;LINE&gt;In honourable fashion.&lt;/LINE&gt;
&lt;/SPEECH&gt;
&lt;/HIT&gt;
&lt;HIT&gt;
&lt;SPEECH&gt;
&lt;SPEAKER&gt;Ghost&lt;/SPEAKER&gt;
&lt;LINE&gt;I am thy father's spirit,&lt;/LINE&gt;
&lt;LINE&gt;Doom'd for a certain term to walk the night,&lt;/LINE&gt;
&lt;LINE&gt;And for the day confined to fast in fires,&lt;/LINE&gt;
&lt;LINE&gt;Till the foul crimes done in my days of nature&lt;/LINE&gt;
&lt;LINE&gt;Are burnt and purged away. But that I am forbid&lt;/LINE&gt;
&lt;LINE&gt;To tell the secrets of my prison-house,&lt;/LINE&gt;
&lt;LINE&gt;I could a tale unfold whose lightest word&lt;/LINE&gt;
&lt;LINE&gt;Would harrow up thy soul, freeze thy young blood,&lt;/LINE&gt;
&lt;LINE&gt;Make thy two eyes, like stars, start from their spheres,&lt;/LINE&gt;
&lt;LINE&gt;Thy knotted and combined locks to part&lt;/LINE&gt;
&lt;LINE&gt;And each particular hair to stand on end,&lt;/LINE&gt;
&lt;LINE&gt;Like quills upon the fretful porpentine:&lt;/LINE&gt;
&lt;LINE&gt;But this eternal blazon must not be&lt;/LINE&gt;
&lt;LINE&gt;To ears of flesh and blood. List, list, O, list!&lt;/LINE&gt;
&lt;LINE&gt;If thou didst ever thy dear father love--&lt;/LINE&gt;
&lt;/SPEECH&gt;
&lt;/HIT&gt;
&lt;HIT&gt;
&lt;SPEECH&gt;
&lt;SPEAKER&gt;HAMLET&lt;/SPEAKER&gt;
&lt;LINE&gt;Haste me to know't, that I, with wings as swift&lt;/LINE&gt;
&lt;LINE&gt;As meditation or the thoughts of love,&lt;/LINE&gt;
&lt;LINE&gt;May sweep to my revenge.&lt;/LINE&gt;
&lt;/SPEECH&gt;
&lt;/HIT&gt;
&lt;/HITS&gt;</p>

<p>Apart from the nitty gritty of XQuery David also provided an interesting look at some tricks that eXist uses to make it possible to join tree based structures. Basically the algorithm creates a tree structure and then indexes the nodes with identifiers making an assumption about the number of children beneath a particular node. Practically this means it's easy to do math to traverse the tree, and join subtrees--but a side effect is that lots of 'ghost nodes' are created.</p>

<p>Ghost nodes are gaps in the identifier space, and if you are working with irregularly structured XML documents you can actually easily exceed the available resources on a 64bit machine. An example of a irregularly structured document could be a dictionary that has hundreds of thousands of entries, which on average have 2-3 definitions, but a handful have like 60 definitions...this causes the identifier space padding to get bloated with tons of ghost nodes.</p>

<p>If you are interested about any of this take a look at <a href="http://web.archive.org/web/20090419221752/http://www.exist-db.org:80/webdb.pdf">eXist: An Open Source XML Database</a> by Wolfgang Meier. David also recommended <a href="http://www.amazon.com/gp/product/0321165810/">XQuery - The XML Query Language</a> by Micael Brundage for learning more about XQuery. In the future David said there is work going on at W3C on extensions to search and update: XQuery Search and Update, which will be good to keep an eye on.</p>

<p>All in all I like XQuery and I'm glad that I finally seem to understand it enough to consider it part of my tool set. I'd like to see XQuery used in say a Java program much like SQL is used via JDBC--and be able to get back results say as JDOM or XOM objects. I must admit I'm not so interested in using XQuery as a general programming language though.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>102</wp:post_id>
		<wp:post_date>2006-06-12 16:22:01</wp:post_date>
		<wp:post_date_gmt>2006-06-12 23:22:01</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>xquery</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>oai-pmh tut</title>
		<link>http://inkdroid.org/2006/06/13/oai-pmh-tut/</link>
		<pubDate>Tue, 13 Jun 2006 13:12:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=103</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I couldn't pass up the opportunity to hear Simeon Warner talk about <a href="http://www.openarchives.org">oai-pmh</a> in the second group of tutorials. I've implemented data and service providers before--so I consider myself fairly knowledgeable about the protocol. But Simeon works with this stuff constantly at Cornell since <a href="http://arxiv.org">arXiv</a> so I was certain there would be things to learn from him...he did not disappoint.</p>

<p>Using some recent work at <a href="http://web.archive.org/web/20070702003636/http://oai-best.comm.nsdl.org:80/cgi-bin/wiki.pl?">NSDL</a>, and his experience with the protocol Simeon provided some really useful advice on sing oai-pmh. Here are the things I picked up on:</p>

<ul>

<li>avoid using sets--especially overloading sets to do searches. There is an interesting edge case in the protocol when a record moves from a particular set A to set B, which makes harvesters who are harvesting set A totally miss the update.</li>

<li>pay attention to datestamps. Make sure datestamps are assigned to records when they actually change in the repository or else harvesters can miss updates. The protocol essentially is a way of exposing updates, so getting the datestamps right is crucial.</li>

<li>resumption tokens need to be idempotent. This means a harvester should be able to use the resumption token more than once and get the same result (barring updates to the repository). This is essential so that harvesters engaged in a lengthy harvest can recover from network failure and other exceptions.</li>

<li>pay attention to character encoding. Use a parser that decodes character entities in XML and store the utf8. This will make your live simpler as you layer new services over harvested data. Make sure that HTML entities aren't used in oai-pmh responses. <a href="http://www.cs.cornell.edu/people/simeon/software/utf8conditioner/">utf8conditioner</a> is Simeon's command line app for debugging utf8 data.</li>

<li>be aware of the two great myths of oai-pmh: the myth that oai-php only allows exposure of DC records, and that oai_pmh only allows a single metadata format to be exposed.</li>

</ul>

<p>There are lots more recommendations at NSDL, but it was useful to have this overview and have the chance to ask Simeon questions. For example even though oai-pmh requires records to have an XML schema, it would be possible to create a wrapping schema for freeform data like RDF.</p>

<p>The main reason I was interested in this tutorial was to hear more about using oai-pmh to distribute not only resource metadata records, but also the resources themselves. There were a couple of initial problems with using the protocol to provide access to the actual resources.</p>

<p>The first is that identifiers such as URLs in metadata records which point to the resource for capture had too much ambiguity. Some of the URLs point at splash screens where someone could download a particular flavor of the resource, others went directly to a PDF, etc. This made machine harvesting data-provider specific. In addition there is a problem with the datestamp semantics when a remote resource changes--when the resource is updated but the metadata stays the same the datestamp is not required to change. This makes it impossible for harvesters to know when it needs to download the resource again.</p>

<p>Fortunately there was a solution that is detailed more fully in a <a href="http://dlib.org/dlib/december04/vandesompel/12vandesompel.html">paper</a> written by Simeon and the usual suspects. It boils down to actually distributing the resource as a metadata format. This plays a little bit with what metadata itself is...but it makes the two previously mentioned problems disappear. Simeon gave a brief overview of MPEG21 DIDL but was keen to point out METS and other packaging formats can work the same. Using oai-pmh in this way is <em>really</em> interesting to me since it enables respositories to share actual objects with each other--with OAI-PMH working almost like an ingestion protocol.</p>

<p>I asked about mechanisms to autodiscover oai-pmh metadata in HTML, like <a href="http://unapi.info">unAPI</a>. Simeon pointed out that the usual suspects are actually extending/refining this idea a bit in some recent work done w/ the Andrew Mellon Foundation on <a href="http://msc.mellon.org/Meetings/Interop/presentations">interoperability</a>. Apparently they've experimented with the LiveClipboard idea in support of some of this work. More on this later.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>103</wp:post_id>
		<wp:post_date>2006-06-13 06:12:07</wp:post_date>
		<wp:post_date_gmt>2006-06-13 13:12:07</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oai-pmh-tut</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>1040</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &raquo; oai-pmh revisited]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org/2006/06/21/oai-pmh-revisited/</wp:comment_author_url>
			<wp:comment_author_IP>66.187.134.52</wp:comment_author_IP>
			<wp:comment_date>2006-07-07 15:15:56</wp:comment_date>
			<wp:comment_date_gmt>2006-07-07 22:15:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] The other rubs were datestamps and resumption tokens which I got to hear about extensively in Simeon&#8217;s OAI-PMH tutorial. The point being that there are these little wrinkles to creating a data provider which don&#8217;t sound like much; but when multiplied out to many nodes can result in an explosion of administrative emails (170 messages per provider, per year) for the central hub. This amounts to a lot of (wo)man hours lost for both the central hub and the data providers. It makes one wonder at how lucky/brialliant tbl was in creating protocols which scaled massively to the web we know today&#8230;with a little help from his friends. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>citation graphs</title>
		<link>http://inkdroid.org/2006/06/16/citation-graphs/</link>
		<pubDate>Sat, 17 Jun 2006 06:03:40 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=104</guid>
		<description></description>
		<content:encoded><![CDATA[JCDL2006 was chock-a-block full of good content. A set of papers presented on the first day in the <a href="http://jcdl2006.org/program/sessions/named-entities-1/">Named Entities</a> track explored a common theme of applying <a href="http://en.wikipedia.org/wiki/Graph_theory">graph theory</a> to citation networks in order to cluster works by the same author. For example an author name may appear as Daniel Chudnov, D Chudnov, Dan Chudnov. There is also a similar problem when two authors with the same name are actually two different people. Being able to group all the works by an author is very important for good search interfaces...and also for calculating citation counts and impact factors.

The most interesting paper in the bunch (for me) was <a href="http://eprints.ecs.soton.ac.uk/12704/">Also By The Same Author: AKTiveAuthor, A Citation Graph Approach To Name Disambiguation</a>. This paper revolves around the hypothesis that authors tend to cite their own works more frequently than others--so called 'self-citation'. Self-citation isn't the result of navel gazing or self-promotion so much as it is the result of researchers building on the work that they've done previously. In addition to self-citation graphs co-authorship and source URL graphs are also used to build a graph of a particular authors works.

The paper concludes some good precision/recall figures (.997/.818) which points to the value in using self-citation for name clustering. This paper made and some growing interest I have in RDF and Jena have made me realize that I'd like to spend a bit of time over the coming year learning about graph theory.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>104</wp:post_id>
		<wp:post_date>2006-06-16 23:03:40</wp:post_date>
		<wp:post_date_gmt>2006-06-17 06:03:40</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>citation-graphs</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>archiving the web</title>
		<link>http://inkdroid.org/2006/06/19/archiving-the-web/</link>
		<pubDate>Mon, 19 Jun 2006 22:22:38 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=105</guid>
		<description></description>
		<content:encoded><![CDATA[Folks at Cornell are doing some fun stuff with Internet Archive data. William Arms presented <a href="http://www.cs.cornell.edu/wya/papers/JCDL-2006.doc">Building a Research Library for the History of the Web</a> at JCDL last week which summarized some of the architectural decisions they had to make in designing a system for mirroring and providing access to 240 terabytes of web content. Their goal is to function as both a full mirror of IA, and to build tools that allow social science and computer science researchers to use this data. 

A few interesting tidbits include:

<ul>
<li>Rather than building a distributed system for processing the data (which is what IA and Google have) they went with a symmetric multi-processor. Not just any kind of multi-processor mind you but two dedicated Unisys ES7000/430 each with 16 Itanium2 processors running at 1.5 Gigahertz with 16GB of RAM. The argument was that the very high data rates made this architecture more palatable. The kicker for me was that they are using Microsoft Windows Server 2003 as the operating system. But it gets weirder.</li>

<li>The system's pre-load system extracts useful metadata from ARC files and then stores this in a relational database, while saving off the actual content to a separate Page Store. The Page Store has some intelligence in it which uses an MD5 checksum to figure out if the content has changed; it also provides a layer of abstraction that will allow some content to be stored offline on tapes, etc. Apparently IA stores redundant data quite a bit, and Cornell will be able to save a significant amount of disk space if they de-dupe. Arms detailed the trade offs with using a relational db, namely that they had to get the schema right because if they decided to change it down the road it would require a complete pass over the content again. Ok, so the weirder part is that they are using SQLServer 2000 as the RDBMS.</li>

<li>They have created web-service and high-performance clients for extracting data from the archive so that cpu-intensive research operations can be performed locally instead of on the main server. I'd be interested to learn more about the high-performance clients since we've been keen to have file-system-like clients in the repository we are building at the LoC. Among the more interesting things the extractors can do is extracting the sub-graph of a particular node on the web.</li>

<li>They have a retro-browser which (from the paper) sounds like an interesting http-proxy which turns any old browser into a time-machine. It performs a similar function as the way-back machine, but sounds a lot cooler.</li>

<li>Full-text indexing is initially being done using Nutch on an extracted subset of nodes. However Cornell is investigating the use <a href="http://archive-access.sourceforge.net/projects/nutch/">NutchWAX</a> for providing fulltext indexes. NutchWAX was written by Doug Cutting for working directly with IA <a href="http://www.archive.org/web/researcher/ArcFileFormat.php">ARC Files</a>. It also has the ability to distribute indexing--which seems counter to the non-distributed nature of this system at Cornell...but there you go.</li>

</ul>

I've learned from my colleague <a href="http://andy.boyko.net/">Andy Boyko</a> that the Library of Congress has been doing similar work with IA...and have been doing other work archiving the <a href="http://www.911digitalarchive.org/">world wild web</a>. I imagine my other team members already have been exposed to the work Cornell has been doing in this area, but it was useful for me to learn more. It's important work--as Arms said:

<blockquote>
Everyone with an interest in the history of the Web must be grateful to <a href="http://en.wikipedia.org/wiki/Brewster_Kahle">Brewster Kahle</a> for his foresight in preserving the content of the Web for future generations...
</blockquote>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>105</wp:post_id>
		<wp:post_date>2006-06-19 15:22:38</wp:post_date>
		<wp:post_date_gmt>2006-06-19 22:22:38</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>archiving-the-web</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>oai-pmh revisited</title>
		<link>http://inkdroid.org/2006/06/21/oai-pmh-revisited/</link>
		<pubDate>Wed, 21 Jun 2006 19:59:15 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=106</guid>
		<description></description>
		<content:encoded><![CDATA[<p>So the big news for me at JCDL was Carl Lagoze's presentation on the state of the NSDL in his paper <a href="http://arxiv.org/abs/cs.DL/0601125">Metadata Aggregation and "Automated Digital Libraries": A Retrospective on the NSDL Experience</a>. It ended up winning the <a href="http://web.archive.org/web/20120304165945/http://www.jcdl.org/awards.shtml">Vannevar Bush Best Paper Award</a> so I guess it was of interest to some other folks. I highly recommend printing out the paper for a train ride home if you are at all interested in digital libraries and metadata.</p>

<p>The paper is essentially a review of a three year effort to build a distributed digital library using the OAI-PMH protocol. Lagoze's pain in relating some of the findings was audible and understandable given his involvement in OAI-PMH...and the folks seated before him.</p>

<p>The findings boil down to a few key points:</p>

<h3>The OAI-PMH isn't low barrier enough</h3>

<p>Overall success rate for OAI-PMH harvesting has been 64%. I took the liberty of pulling out this graphic that Carl put up on the big screen at JCDL:</p>

<p><img width="450" src="http://www.inkdroid.org/images/harvest.png" /></p>

<p>The reason for harvest failure varied but centered around XML encoding problems (character sets, well-formedness, schema validation) as well as improper use of datestamps and resumption tokens. These errors are often unique to a particular harvest request--so that a repository that has passed "validation" can often turn around and randomly emit bad XML. So perhaps more than OAI-PMH not being low-barrier enough we are really talking about XML not being low barrier enough eh?</p>

<p>This issue actually came up in the Trusted Repositories workshop after JCDL (which I'll write about eventually) when Simeon Warner stated directly (in Mackenzie Smith's direction) that off the shelf repositories like DSpace should be engineered in such a way that they are unable to emit invalid/ill-formed XML. Mackenzie responded by saying that as far as she knew the issues had been resolved, but that she is unable to force people to upgrade their software. This situation sounds familiar to the bind that M$ finds themselves in--but at least with Windows you are prompted to upgrade your software...I wonder if DSpace has anything like that.</p>

<p>So anyway--bad XML. Perhaps on the flipside brittle XML tools are part of the problem...nd the fact that folks are generating XML by hand instead of using tools such as <a href="http://www.xml.com/pub/a/2006/01/04/creating-xml-with-ruby-and-builder.html">Builder</a>.</p>

<p>The other rubs were datestamps and resumption tokens which I got to hear about extensively in <a href="http://www.inkdroid.org/2006/06/13/oai-pmh-tut/">Simeon's OAI-PMH tutorial</a>. The point being that there are these little wrinkles to creating a data provider which don't sound like much; but when multiplied out to many nodes can result in an explosion of administrative emails (170 messages per provider, per year) for the central hub. This amounts to a lot of (wo)man hours lost for both the central hub and the data providers.  It makes one wonder at how lucky/brialliant tbl was in creating protocols which scaled massively to the web we know today...with a little help from his friends.</p>

<h3>Good metadata is hard</h3>

<p>...or at least harder than expected. Good metadata requires domain expertise, metadata expertise and technical expertise--and unfortunately the NSDL data providers typically lacked people or a team with these skills (aka library technologists).</p>

<p>Essentially the NSDL was banking that the successful Union Catalog model (WorldCat) could be re-expressed using OAI-PMH and a minimalist metadata standard (oai_dc)...but WorldCat has professinally trained librarians and NSDL did not. oai_dc was typically too low-resolution to be useful, and only 50% of the collections used the recommended nsdl_dc qualified DC...which made it less than useful at the aggregator level. Furthermore only 10% of data providers even provided another type of richer metadata.</p>

<p>The NSDL team expended quite a bit of effort building software for scrubbing and transforming the oai_dc but:</p>

<blockquote>In the end, all of these transforms don't enhance the richness of the information in the metadata. Minimally descriptive metadata, like Dublin Core, is still minimally descriptive even after multiple quality repairs. We suggest that the time spent on such format-specific transforms might be better spent on analysis of the resource itself--the source of all manner of rich information.</blockquote>

<p>which brings us to...</p>

<h3>Resource-centric (rather than metadata-centric) DL systems are the future</h3>

<p>OAI-PMH by definition is essentially a protocol for sharing metadata. The system that the NSDL built is centered around a Metadata Repository which is essentially a RDBMs with a Dublin Core metadata record at it's core. Various services are built up around the MR including a service for Search and an Archiving service.</p>

<p>However as the NSDL has started to build out other digital library services they've discovered problems such as multiple records for the same resource. But more importantly they want to build services that provide context to resources, and the current MR model puts metadata at the center rather than the actual resource.</p>

<blockquote>In the future, we also want to express the relationships between resources and other information, such as annotations and standards alignments...we wish to inter-relate resources themselves, such as their co-existence within a lesson plan or curriculum.</blockquote>

<p>So it seems to me the NSDL folks have decided that the Union Catalog approach just isn't working out, and that the resource itself needs to be moved into the center of the picture. At the end of the day, the same is true of libraries--where the most important thing is the content that is stored there--not the organization of it. More information about this sea change can be found in <a href="http://arxiv.org/abs/cs.DL/0501080">An Information Network Overlay Architecture for the NSDL</a>, and in the work on Pathways, which was discussed and I hope to summarize here in the coming days.</p>

<h3>So...</h3>

<p>I think it's important to keep these findings in the context of the NSDL enterprise. For example the folks at lanl.gov are using OAI-PMH in their aDORe framework heavily. Control of data providers is implicit in the aDORe framework--so XML and metadata quality problems are mitigated. Furthermore aDORe is resource centric since MPEG21 surrogates for the resources themselves are being sent over OAI-PMH. But it does seem somehow odd that a metadata protocol is being overridden to transfer objects...but that begs a philosophical question about metadata which I'm not even going to entertain right now.</p>

<p>I think it's useful to compare the success of oai-pmh with the success of the www. Consider this thought experiment...Imagine that early web browsers (lynx/mosaic/netscrape/etc) required valid HTML in order to display a page. If the page was invalid you would get nothing but a blank page. If you really needed to view the page you'd have to email the document author and tell them to fix an unclosed tag, or a particular character encoding. Do you think that the web would've still propagated at the speed that it did?</p>

<p>fault_tolerant_xml_tools + using_xml_generator_libraries == happiness</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>106</wp:post_id>
		<wp:post_date>2006-06-21 12:59:15</wp:post_date>
		<wp:post_date_gmt>2006-06-21 19:59:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oai-pmh-revisited</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>nice work atlanta</title>
		<link>http://inkdroid.org/2006/06/27/nice-work-atlanta/</link>
		<pubDate>Tue, 27 Jun 2006 10:06:43 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=107</guid>
		<description></description>
		<content:encoded><![CDATA[It's nice to see that Dr. King's papers <a href="http://www.nytimes.com/2006/06/27/us/27king.html">found a home</a> at his alma mater--and won't be locked away in somebody's safe.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>107</wp:post_id>
		<wp:post_date>2006-06-27 03:06:43</wp:post_date>
		<wp:post_date_gmt>2006-06-27 10:06:43</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>nice-work-atlanta</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>code4libcon 2007</title>
		<link>http://inkdroid.org/2006/07/02/code4libcon-2007/</link>
		<pubDate>Sun, 02 Jul 2006 20:46:09 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/07/02/code4libcon-2007/</guid>
		<description></description>
		<content:encoded><![CDATA[Here's to making sure that code4libcon 2007 is a <a href="http://www.librarywebchic.net/wordpress/2006/07/01/on-being-the-library-web-chic/">watershed</a> moment for women library technologists. 

code4libcon 2006 in Corvallis wasn't all male, but it was largely...and I can only remember two women speaking to the audience. To a large extent code4libcon was modeled after technology conferences like yapc, pycon, oscon, barcamp, etc--which have much the same sort of ratio. But libraries are different because the majority of people who work in libraries are women. So it was a bit surprising that more women didn't end up at code4libcon 2006.

2006 did get organized practically overnight with a very small (male) clique in an irc room (that's not always well behaved, but mean well--hey it's IRC). When people actually started signing up and sending in papers to the more formal discussion list I think we were all kind of surprised. I seriously thought we were just going to be hanging out in some random space with free wifi, and it turned into this really successful event.

Some folks like Dan Chudnov, Art Rhyno, Jeremy Frumkin and Roy Tennant started thinking and talking early about making the conference appeal to women library technologists. But it seems that either the voting (open to all, but all men for some reason) somehow subconsciously counteracted this.

AFAIK the keynote voting is still going on, and I imagine you can still suggest speakers. There will only be more voting to do as we get into selecting presenters. If you'd like to participate just email <a href="mailto://radl@georgialibraries.org">Brad LaJeunesse</a> and he'll hook you up with a backpack login. Also, sign up for the <a href="http://dewey.library.nd.edu/mailing-lists/code4lib/">code4lib</a> and <a href="https://lists.gatech.edu/sympa/info/code4libcon">code4libcon</a> discussion lists.  Luckily Dorothea Salo is involved and vocal and I'm hoping that other women technologists will get involved too. This is a grassroots thing after all, not some sort of LITA top-tech trends panel. It'll become whatever we want it to be.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>108</wp:post_id>
		<wp:post_date>2006-07-02 13:46:09</wp:post_date>
		<wp:post_date_gmt>2006-07-02 20:46:09</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>code4libcon-2007</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="irc"><![CDATA[irc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>repositories and domain-specific-languages</title>
		<link>http://inkdroid.org/2006/07/06/repositories-and-domain-specific-languages/</link>
		<pubDate>Thu, 06 Jul 2006 10:28:53 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/07/06/repositories-and-domain-specific-languages/</guid>
		<description></description>
		<content:encoded><![CDATA[At work I've been doing some experiments with the <a href="http://fedora.info">Fedora</a> repository software. One of the strengths of Fedora is that it is fundamentally designed as a set of extensible web services. At first I set about becoming familiar with the set of web services and decided that Ruby would be a useful and lightweight language to do this from. Sure enough, Ruby was <a href="http://www.inkdroid.org/2006/04/25/fedorasoap-and-ruby/">plenty capable</a> of putting stuff into Fedora and getting stuff back out again.

As time went on it became clear that what was really needed was a layer of abstraction around this Fedora web services API that would allow it (or another repository framework) to be used in a programatic way without having to make SOAP calls and building FOXML all over the place. Typically in software pattern lingo this is referred to as a <a href="http://en.wikipedia.org/wiki/Facade_pattern">facade</a>. 

So I worked on creating a facade, and ended up with something I half-jokingly called 'bitbucket' which looks something like this:

<pre lang="ruby">
  require 'bitbucket'

  # create a repository
  repository = BitBucket::Repository.new

  # create a new repository object
  o = BitBucket::RepositoryObject.new
  o.dc.title = 'Automatic for the People'
  o.dc.creator = 'REM'

  # add a datastream to the object
  o &lt;&lt; BitBucket::DataStream.new_from_file('The Sidewinder Sleeps Tonight.mp3')

  # ingest it!
  id = repository.ingest(o)
</pre>

Now this code is pretty basic: it creates an object for a CD, associates an mp3 with it, and then adds it to a repository. This is the typical 'ingest' process but notice that the ingest format, the SOAP requests, mime-types, and the actual type of the repository are unspecified. The truth is even more could be hidden such as the Dublin Core. Some things could use better names: 'Resource' instead of 'RepostoryObject' perhaps. If you have interest in using this code (yes it works!) let me know--I imagine it could be liberated from a private subversion repository.

Just after finishing this up it struck me that while I was trying to build a facade around Fedora I was at the same time striving for a <a href="http://www.martinfowler.com/bliki/DomainSpecificLanguage.html">domain specific language</a> for repositories. 

<blockquote>
The basic idea of a domain specific language (DSL) is a computer language that's targeted to a particular kind of problem, rather than a general purpose language that's aimed at any kind of software problem.
</blockquote>

As Martin Fowler goes on to describe there are two different types of DSLs: external and internal. External DSLs are custom languages such as regular expressions, postscript, ant configuration files, etc. Typically a syntax for the mini-language is determined and a small (hopefully) interpreter is written which parses and processes the DSL. Internal DSLs on the other hand use the constructs of a host programming language to define the DSL. There is a strong tradition of using DSLs in Lisp and Smalltalk...and it seems to also be a growing tradition in the <a href="http://www.artima.com/rubycs/articles/ruby_as_dsl.html">Ruby</a> community as well.

So a DSL for repositories would provide a mini-lanugage, if you will, for interacting with a repository. I think that the <a href="http://msc.mellon.org/Meetings/Interop/">efforts</a> underway to build models for interoperability across scholarly repositories are in a way groping after this same thing--an unambiguous language for interacting with repositories.

The <a href="http://public.lanl.gov/herbertv/papers/pathways_core_poster_submit.pdf">Pathways Core</a> poster session at JCDL was very exciting. While the ideas were compelling enough, Jeroen Bekaert created some absolutely beautiful diagrams which really sold some of the concepts. I wish I could find some to put here. I got a chance to pick <a href="http://lxming.blogspot.com/">Xiaoming Liu's</a> brain a lot at the conference and over beers and I am really looking forward to their upcoming papers on this topic.

What I'd like to see is how easy it would be to use this emerging pathways model to create a Ruby DSL that uses the <a href="http://bitworking.org/projects/atom/">Atom Publishing Protocol</a> as a backend. I'd also like to take a look at <a href="http://www.jcp.org/en/jsr/detail?id=170">JSR 170</a>. My main purpose in this is to see how well the aims of the scholarly community map to the content management solutions being developed outside the digital library community.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>109</wp:post_id>
		<wp:post_date>2006-07-06 03:28:53</wp:post_date>
		<wp:post_date_gmt>2006-07-06 10:28:53</wp:post_date_gmt>
		<wp:comment_status>closed</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>repositories-and-domain-specific-languages</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>iraq</title>
		<link>http://inkdroid.org/2006/07/10/iraq/</link>
		<pubDate>Mon, 10 Jul 2006 20:09:21 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/07/10/iraq/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://web.archive.org/web/20070609175343/http://www.msnbc.msn.com:80/id/7245228/site/newsweek/"><img src="http://web.archive.org/web/20120108075322/http://bop.nppa.org/2006/thumbnails/512/00010547-INS-63710/124345.jpg" width="350" border="0" /></a></p>

<p>I saw this in yesterday's Washington Post and just learned it won the <a href="http://web.archive.org/web/20120619234105/http://bop.nppa.org:80/2006/still_photography/winners/INS/63710/124345.html">Best of Photo Journalism Award</a> for 2006. The picture says it all, but the <a href="http://web.archive.org/web/20070609175343/http://www.msnbc.msn.com:80/id/7245228/site/newsweek/">story</a> is just as harrowing. What a sad mess.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>110</wp:post_id>
		<wp:post_date>2006-07-10 13:09:21</wp:post_date>
		<wp:post_date_gmt>2006-07-10 20:09:21</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>iraq</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>more on web identifiers</title>
		<link>http://inkdroid.org/2006/07/11/more-on-web-identifiers/</link>
		<pubDate>Tue, 11 Jul 2006 11:25:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/07/11/more-on-web-identifiers/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I monitor the <a href="http://web.archive.org/web/20090525091637/http://www.nabble.com:80/w3.org---www-tag-f11733.html">www-tag</a> discussion list, but more than half of it goes right over my head--so I was pleased when a colleague forwarded <a href="http://www.w3.org/2001/tag/doc/URNsAndRegistries-50">URNs, Namespaces and Registries</a> to me. Don't let the 2001 in the URL fool you, it has been updated quite recently. This finding provides an interesting counterpoint to <a href="http://www.ietf.org/rfc/rfc4452.txt">rfc 4452</a> which I <a href="http://www.inkdroid.org/2006/05/16/info-uris-and-opening-up-library-data/">wrote</a> about earlier.</p>

<p>Essentially the authors go about examining the reasons why folks want to have URNs (persistence) and info-uris (non-dereferencability) and showing how URIs actually satisfy the requirements of these two communities.</p>

<p>I have to admit, it sure would be nice if (for example) LCCNs and OCLCNUMs resolved using the existing the infrastructure of http and dns. Let's say I run across an info-uri in a XML document identifying tbl as info:lccn/no9910609. What does that really tell me? Wouldn't it be nice if instead it was http://lccn.info/no9910609 and I could use my <a href="http://www.ruby-doc.org/stdlib/libdoc/net/http/rdoc/index.html">net/http</a> library of choice to fetch tbl's MADS record? Amusingly Henry Thompson (one of the authors of the finding) is holding <a href="http://lccn.info">http://lccn.info</a> and <a href="http://oclcnum.info">http://oclcnum.info</a> for ransom :-)</p>

<p>Instead, in the case of info-uri, OCLC is tasked with building a <a href="http://info-uri.info">registry</a> of these namespaces, and even when this is built the identifiers won't necessarily be resolvable in any fashion. This is the intent behind info-uris of course--that they need not be resolvable or persistent. But this finding raises some practical issues that are worth taking a look at, which seem to point to the ultimate power of the web-we-already-have.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>111</wp:post_id>
		<wp:post_date>2006-07-11 04:25:23</wp:post_date>
		<wp:post_date_gmt>2006-07-11 11:25:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>more-on-web-identifiers</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>the librarian&#039;s store</title>
		<link>http://inkdroid.org/2006/08/01/the-librarians-store/</link>
		<pubDate>Tue, 01 Aug 2006 19:41:33 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/08/01/the-librarians-store/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>While working at <a href="http://follett.com">Follett</a> I always thought it was just a matter of time till Amazon turned it's eye on the library market. Much of the web development that went on at Follett was done with an eye towards what Amazon was doing...while tailoring the experience for librarians and library book ordering/processing. The management I expressed this idea to seemed to think that Amazon wouldn't be interested in Follett's business. It was my opinion at the time that it would be better to have Amazon as a partner than a competitor. This is really just common sense right? No leap of intuition there.</p>

<p>...time passes...</p>

<p>Now it looks like (thanks <a href="http://blog.ryaneby.com/">eby</a>) that Follett has some <a href="http://www.amazon.com/gp/browse.html/104-4592833-0688700?node=13753131">company</a>. When a web savvy company like Amazon notices your niche in the ecosystem it's definitely important to pay attention. Amazon has decided to partner with TLC and Marcive for MARC data and with OCLC to automatically update holdings. This is big news.</p>

<p>Somewhat related and even more interesting in some ways <a href="http://dilettantes.code4lib.org/">rsinger</a> and eby report in #code4lib that they've seen Library of Congress Subject Headings and Dewey Decimal Classification Numbers in <a href="http://aws.amazon.com">Amazon Web Service</a> responses. For an example splice your Amazon Token in here:</p>

<pre>
http://web.archive.org/web/20120913073818/http://webservices.amazon.com/onca/xml
?Service=AWSECommerceService
&amp;Version=2006-06-28
&amp;Operation=ItemLookup
&amp;ContentType=text%2Fxml
&amp;SubscriptionId=YOUR_TOKEN_HERE
&amp;ItemId=097669400X
&amp;IdType=ASIN
&amp;ResponseGroup=ItemAttributes,Large,Subjects
</pre>

<p>scan for:</p>

<blockquote>
Ruby (Computer program language)
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>112</wp:post_id>
		<wp:post_date>2006-08-01 12:41:33</wp:post_date>
		<wp:post_date_gmt>2006-08-01 19:41:33</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-librarians-store</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_e6f8b0297229b68c5bc6ed7b5ada334b</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_c3966a3313002b2eb9106b9909d77d8d</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>gems...on ice</title>
		<link>http://inkdroid.org/2006/08/12/gemson-ice/</link>
		<pubDate>Sun, 13 Aug 2006 00:14:31 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/08/12/gemson-ice/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.rubygems.org"><img src="http://web.archive.org/web/20070628193008/http://rubygems.org/images/rubygems-125x125t.png" align="left" border="0" /></a>
When developing and deploying <a href="http://rubyonrails.org">RubyOnRails</a> applications you've often got to think about the <a href="http://www.rubygems.org/">gem</a> dependencies your project might have. It's particularly useful to freeze a version of rails in your <i>vendor</i> directory so that your app uses that version of rails rather than a globally installed (or not installed) one. It's easy to do this by simply invoking:</p>

<pre>
  rake freeze_gems
</pre>

<p>Which will unpack all the rails gems into vendor, and your application will magically use these instead of the globally installed rails gems.</p>

<p>The cool thing is that with a little bit of plugin help you can freeze your other gems in <i>vendor</i> as well. Simply install Rick Olson's elegantly simple <a href="http://web.archive.org/web/20070729055327/http://svn.techno-weenie.net/projects/plugins/gems/">gem plugin</a> into vendor/plugins. Then assuming you are using let's say my <a href="http://www.textualize.com/ruby-oai">oai-pmh</a> gem you can simply:</p>

<pre>
  rake gems:freeze GEM=oai
</pre>

<p>and the gem will be unpacked in vendor, and the $LOAD_PATH for your application will automatically include the library path for the new gem. Very useful, thanks Rick!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>113</wp:post_id>
		<wp:post_date>2006-08-12 17:14:31</wp:post_date>
		<wp:post_date_gmt>2006-08-13 00:14:31</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>gemson-ice</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>oai2rdf</title>
		<link>http://inkdroid.org/2006/08/24/oai2rdf/</link>
		<pubDate>Thu, 24 Aug 2006 20:10:37 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/08/24/oai2rdf/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://simile.mit.edu/images/logo.png" align="left" style="margin-right: 10px;" /></p>

<p>Amidst the flurry of commit messages and the like on the <a href="http://simile.mit.edu/mail/BrowseList?listName=Dev">simile development discussion list</a> I happened to see the <a href="http://simile.mit.edu/">Simile Project</a> includes a <a href="http://simile.mit.edu/repository/RDFizers">RDFizer</a> project which has a component called oai2rdf.</p>

<p>oai2rdf is a command line program that happens to use Jeff Young's <a href="http://web.archive.org/web/20090920021516/http://www.oclc.org/research/software/oai/harvester2.htm">OAIHarvester2</a> and some XSLT magic to harvest an entire <a href="http://www.openarchives.org">oai-pmh</a> archive and covert it to <a href="http://www.w3.org/RDF/">rdf</a>.</p>

<pre>
  % oai2rdf.sh http://cogprints.ecs.soton.ac.uk/perl/oai2 cogprints
</pre>

<p>This will harvest the entire <a href="http://cogprints.org">cogprints eprint archive</a> and convert it on the fly to rdf which is saved in a directory called cogprints. Just in case you are wondering--yes it handles resumption tokens. In fact you can also give it date ranges to harvest, and tell it to only harvest particular metadata formats. By default it actually grabs all possible metadata formats.</p>

<p>As part of my day job I've been looking at some rdf technologies like <a href="http://jena.sourceforge.net/">jena</a> and while there are lots of chunks of rdf around on the web to play with oai2rdf suddenly opens up the possibilities quite a bit.</p>

<p>Getting oai2rdf up and running is pretty easy. First get the oai2rdf code:</p>

<pre>
  svn co http://simile.mit.edu/repository/RDFizers/oai2rdf/ oai2rdf
</pre>

<p>Next make sure you have <a href="http://maven.apache.org">maven</a>. If you don't have it maven is very easy to install. Just <a href="http://maven.apache.org/download.html">download</a>, unpack, and make sure the maven/bin directory is in your path. Then you can:</p>

<pre>
  mvn package
</pre>

<p>The magic of maven will pull down dependencies and compile the code. Then you should be able to run <em>oai2rdf</em>. Art Rhyno has been talking about the work the Simile folks are doing for quite a while now, and only recently have I started to see what a rich set of tools they are developing.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>114</wp:post_id>
		<wp:post_date>2006-08-24 13:10:37</wp:post_date>
		<wp:post_date_gmt>2006-08-24 20:10:37</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oai2rdf</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="java"><![CDATA[java]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86922</wp:comment_id>
			<wp:comment_author><![CDATA[saniya chawla]]></wp:comment_author>
			<wp:comment_author_email>chawlasaniya21@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-04 03:23:12</wp:comment_date>
			<wp:comment_date_gmt>2014-04-04 10:23:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey... 
I tried to use oai2rdf... the installation fine ..but i am getting an error when i tried to run
% oai2rdf.sh http://cogprints.ecs.soton.ac.uk/perl/oai2 cogprints

the error says : could not not find the transformer for schema http://standars.iso.org/ittf/publiclyAvailableStandards-21_schema_files/did/didl.xsd

Please help me on this !
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2191</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1396606992.923511028289794921875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:13:"saniya chawla";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1396880909.946916103363037109375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>set your data free ... with unapi</title>
		<link>http://inkdroid.org/2006/08/28/set-your-data-free-with-unapi/</link>
		<pubDate>Mon, 28 Aug 2006 16:58:01 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/08/28/set-your-data-free-with-unapi/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.onebiglibrary.net/">Dan</a>, <a href="http://digitallibrarian.org/">Jeremy</a>, <a href="http://www.wallandbinkley.com/quaedam/">Peter</a>, <a href="http://purl.org/net/leftwing/blog">Michael</a>, <a href="http://web.archive.org/web/20130502121736/http://open-ils.org/blog/">Mike</a>, <a href="http://dilettantes.code4lib.org/">Ross</a> and I wrote an <a href="http://www.ariadne.ac.uk/issue48/chudnov-et-al/">article</a> in the latest Ariadne introducing the lightweight web protocol <a href="http://unapi.info">unAPI</a>. Essentially unAPI is an easy way to include references to digital objects in your HTML which can then be predictably retrieved by a machine...yes 'machine' includes JavaScript running in a browser :-) Dan and a really nice cross section of developers around the world have been working on this spec for over a year now and I think it could be poised to play an important role in the emerging <a href="http://www.tbray.org/ongoing/When/200x/2006/07/28/Open-Data">open data</a> movement.</p>

<p>Imagine you have a <a href="http://canarydatabase.org/" hover="for example dan's :)">citation database</a> which is searchable via the web. The search results include hits. Wouldn't it be nice to align your human viewable results with machine readable representations so that people could write <a href="http://www.liveclipboard.org">browser hacks</a> and the like to remix your application data?</p>

<p>As far as I can tell there are a few options available to help you do this (apart from doing something ad-hoc).</p>

<ol>

<li>use a <a href="http://microformats.org/wiki/citation">citation microformat</a> and mark up your HTML predictably so that it can be recognized and parsed</li>

<li>use <a href="http://www.w3.org/2004/01/rdxh/spec">GRDDL</a> to map your HTML to RDF via an XLST profile.</li>

<li><a href="http://research.talis.com/2005/erdf/wiki/Main/RdfInHtml">embed RDF in your HTML</a> essentially using an RDF microformat.</li>

<li><a href="http://en.wikipedia.org/wiki/OpenURL">OpenURL</a> and/or <a href="http://ocoins.info/">COinS</a> to link in page IDs to OpenURL servers.</li>

<li>use <a href="http://unapi.info">unAPI</a> and include a unapi server url (familiar autodiscovery like RSS/Atom), and identifiers (simple element attributes) and write a simple server side script that emits xml for a given identifier.</li>

</ol>

<p>I like microformats a lot and I think a citation format will eventually get done. But it's been a long time coming and there's no indication it's going to get done any time soon. What's more unAPI is bigger than just citation data--and it allows you to publish all kinds of rich data objects without waiting for a community to ratify a particular representation in HTML.</p>

<p>Options 2 and 3 use RDF which I actually like quite a bit as well. GRDDL implies a GRDDL aware browser which would be cool but is a bit heavy weight. XSLT will require clean XHTML--or pipelines to clean it. Embedding RDF in HTML using microformat techniques is compelling because you can theoretically process the RDF data similarly--whereas unAPI doesn't require any particular kind of machine readable format (apart from HTML). Actually there's nothing stopping you from using unAPI to link human viewable objects with RDF representations. The advantage unAPI has here is you can learn RDF if you want to, but you don't have to learn RDF to get going with unAPI today.</p>

<p>Option 4 leverages work done in the library community on citation linking. OpenURL routers are widely deployed in libraries around the world and COinS is a quasi-microformat for putting OpenURL context objects into your HTML so that they can be extracted and fired off at an OpenURL server. OpenURL is a relatively complex and subtle standard which can do a lot more than <a href="http://q6.oclc.org/">just citation linking</a>. Compared to OpenURL/COinS unAPI allows for ease of implementation in languages like JavaScript and provides a simple introspection mechanism for discovering what formats a particular resource is available in. AFAIK this can't be done simply using OpenURL/COinS. If I'm wrong, comments should be open. I would argue that the sheer power and flexibility of OpenURL paradoxically make it hard to understand...and that unAPI in Dan's adherence to a one-page-spec is more limited and simple. Less is more...</p>

<p>So if this piques your interest read the <a href="http://www.ariadne.ac.uk/issue48/chudnov-et-al/">article</a>. It does a much better job of describing the origins of the work, where it's headed, has examples and links out to sites/tools that use unAPI today.  I must admit I wrote very little of the article, and mostly contributed text snippets and screenshots of the <a href="http://validator.unapi.info">unAPI validator</a> I wrote, which uses my <a href="http://www.textualize.com/unapi">unapi ruby gem</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>115</wp:post_id>
		<wp:post_date>2006-08-28 09:58:01</wp:post_date>
		<wp:post_date_gmt>2006-08-28 16:58:01</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>set-your-data-free-with-unapi</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>3583</wp:comment_id>
			<wp:comment_author><![CDATA[Bruce]]></wp:comment_author>
			<wp:comment_author_email>bdarcus@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>24.210.249.184</wp:comment_author_IP>
			<wp:comment_date>2006-08-28 11:54:33</wp:comment_date>
			<wp:comment_date_gmt>2006-08-28 18:54:33</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I actually think the centralized community process is the achilles heal of both microformats and OpenURL. 

For hCite, there's really just one semi-large hilll to get over ("monolithic and flat" BibTeX vs. "relational component-based" DC approach), and the rest is easy. 

I guess to me, though, the identifiers are more important than the markup anyway.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>mission haiku</title>
		<link>http://inkdroid.org/2006/09/11/mission-haiku/</link>
		<pubDate>Tue, 12 Sep 2006 02:08:56 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/09/11/mission-haiku/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Following <a href="http://www.lackoftalent.org/michael/blog/2006/09/11/mission-haiku-or-memetic-conformity/">the</a> <a href="http://www.ibiblio.org/bess/?p=42">lead</a> <a href="http://web.archive.org/web/20080720181359/http://cavlec.yarinareth.net/archives/2006/09/07/mission-haiku/">of</a> <a href="http://web.archive.org/web/20081121003554/http://weblog.kevinclarke.info/2006/09/11/mission-haiku/">some</a> <a href="http://web.archive.org/web/20100711134958/http://rochellejustrochelle.typepad.com/copilot/2006/09/exercise_missio.html">others</a>:</p>

<blockquote>
vacant history<br />
crusted burnt bits lost to time<br />
shards of clarity<br />
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>116</wp:post_id>
		<wp:post_date>2006-09-11 19:08:56</wp:post_date>
		<wp:post_date_gmt>2006-09-12 02:08:56</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>mission-haiku</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>open standards</title>
		<link>http://inkdroid.org/2006/09/15/open-standards/</link>
		<pubDate>Fri, 15 Sep 2006 15:30:59 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/09/15/open-standards/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Folks who are interested in libraries and technology are often drawn to the issue of open standards. Using open standards is very important to libraries for a variety of reasons that <a href="http://ecorrado.us">Ed Corrado</a> <a href="http://www.istl.org/05-spring/article2.html">summarizes</a> nicely.</p>

<p>This week my podcast reader picked up an excellent <a href="http://web.archive.org/web/20070712091701/http://osc.gigavox.com/shows/detail1222.html">interview</a> with <a href="http://danesecooper.blogs.com/">Danese Cooper</a> of the <a href="http://www.opensource.org/osr/">Open Source Initiative</a> where she talks about the <a href="http://www.opensource.org/osr/">Open Standard Requirement</a> which was introduced a few months ago. It provides a new perspective on the same issue from outside of the library community.</p>

<p>Essentially the OSR amounts to 5 guidelines for identifying a truly open standard. These guidelines are different though because they focus on what makes a standard open for an <em>implementor</em>. Whether the standard was created by an open process or not is really outside of scope. The important thing is how easy it is for a software developer to write software that uses the standard. A nice feature of the OSR is that the guidelines would fit on an index card. Here's my regurgitation of them:</p>

<ol>
<li>The spec can't omit details needed for implementation</li>
<li>The standard needs to be freely/publicly available</li>
<li>All patents involved in the spec need to be royalty free</li>
<li>Clicking through a license agreement is not necessary</li>
<li>The spec can't be dependent on a standard that is not open as well</li>
</ol>

<p>Danese was quick to point out that these are simply guidelines and not rules. For example Unicode fails on 2. since you have to <a href="http://www.unicode.org/book/bookform.html">pay</a> for a copy of the spec. But in this case printing the standard is a publishing feat--given all the glyphs and their number. It's not unusual that the book would cost money. So this guideline could be waived if the OSI folks agreed.</p>

<p>Rather than the OSI going and applying these rules to all known standards the idea is that standards bodies could claim self-compliance--and as developers implement the standard the compliance will be ascertained.</p>

<p>The guidelines themselves and the process of being fine tuned/hammered on--and they are looking for volunteers...</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>117</wp:post_id>
		<wp:post_date>2006-09-15 08:30:59</wp:post_date>
		<wp:post_date_gmt>2006-09-15 15:30:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>open-standards</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>4521</wp:comment_id>
			<wp:comment_author><![CDATA[John Cowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url>http://www.ccil.org/~cowan</wp:comment_author_url>
			<wp:comment_author_IP>66.108.117.254</wp:comment_author_IP>
			<wp:comment_date>2006-09-15 17:03:10</wp:comment_date>
			<wp:comment_date_gmt>2006-09-16 00:03:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Actually, you have to pay for a <i>hard</i> copy of the standard, but Unicode 4.0 has been free-as-in-beer-ly available online for some time now, and Unicode 5.0 will be as soon as the book is published.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>the importance of making packages</title>
		<link>http://inkdroid.org/2006/09/19/the-importance-of-making-packages/</link>
		<pubDate>Tue, 19 Sep 2006 14:56:12 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/09/19/the-importance-of-making-packages/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://visual.verbaljazz.com/index.php?showimage=66"><img src="/images/mud.jpg" border="0" width="300" align="left" style="margin: 10px;" /></a>If you are interested in such things Ian Bicking has a nice <a href="http://blog.ianbicking.org/why-small-packages-matter.html">posting</a> about why breaking up a project into smaller packages of functionality is important. His key point is that the boundaries between packages actually help in establishing and maintaining decoupled modules within your application.</p>

<blockquote>
...when someone claims their framework is all spiffy and decoupled, but they just don't care to package it as separate pieces... I become quite suspicious. Packaging doesn't fix everything. And it can introduce real problems if you split your packages the wrong way. But doing it right is a real sign of a framework that wants to become a library, and that's a sign of Something I'd Like To Use.
</blockquote>

<p>So why is decoupling important? Creating distinct modules of code with prescribed interfaces helps ensure that a change inside one module doesn't have a huge ripple effect across an entire project codebase. In addition to using packaging to create boundaries between components the <a href="http://www.cmcrossroads.com/bradapp/docs/demeter-intro.html">Law of Demeter</a> is a pretty handy technique for reducing coupling in object oriented systems. It amounts to ensuring that a given method only invokes methods on objects that are: itself, in its parameters, objects that itself creates, or component objects. The LoD seems to be a good practice at the local level, but packaging helps at a macro/design level. One of the most powerful and fun parts of packaging is coming up with good names and metaphors for your packages and components. Having fun and meaningful names for packages provides coherence to a project, and allows developers to talk about an application. Eric Evans has some nice chapters in his <a href="http://domaindrivendesign.org/books/index.html">Domain Driven Design</a> about coming up with what he calls a <em>domain language</em> whose aim is to:</p>

<blockquote>
To create a supple, knowledge-rich design calls for a versatile, shared team language, and a lively experimentation with language that seldom happens on software projects.
</blockquote>

<p>It's important...and naming distinct packages well helps build a good domain language.</p>

<p>I suppose it's implicit in making something a code <em>library</em>--but one of the other major benefits of splitting a larger project up into smaller packages is that you encourage reuse. The bit of functionality that you decided to bundle up separately can be used as a dependency in a different project--perhaps even by a different person or organization. This seems to me to be a hallmark of good open source software.</p>

<p>Most popular languages these days have established ways of making packages available, downloadable and installable while expressing the dependencies between them. Perl has <a href="http://search.cpan.org/">CPAN</a>, PHP has <a href="http://pear.php.net">PEAR</a>, Ruby has <a href="http://www.rubygems.org">gems</a> and <a href="http://rubyforge.org">RubyForge</a>, Python has <a href="http://peak.telecommunity.com/DevCenter/PythonEggs">eggs</a> and <a href="http://peak.telecommunity.com/DevCenter/EasyInstall">EasyInstall</a>, Java has <a href="http://maven.apache.org/">maven</a>, Lisp has <a href="http://www.cliki.net/asdf ">asdf</a>. Even some applications like <a href="http://trac.edgewall.org/wiki/TracDev/ComponentArchitecture">Trac</a>, <a href="http://web.archive.org/web/20121027164843/http://wiki.rubyonrails.org/rails/pages/Plugins">RubyOnRails</a> and <a href="http://drupal.org/project/Modules">Drupal</a> encourage the creation of smaller packages (modules or plugins) by having a well defined api for adding extensions. And that's not even getting into the various ways operating systems make packages available...</p>

<p>The truly hard part about packaging for me isn't really technical. Most packaging solutions allow you to manage dependencies, versioning, installation and removal. As Ian says, its the decision of where to draw the lines between packages that is hard. It's hard because you have to guess before you start coding--and often during the process of coding you realize that the dividing lines between packages begin to blur. This is why having distinct packages is so important because you are forced to stare at the blurriness and encouraged to fix it...instead of creating the infamous <a href="http://en.wikipedia.org/wiki/Big_Ball_Of_Mud">big ball of mud</a>.</p>

<p>An interesting counterpoint to trying to figure out the dividing lines before hand is to try to <a href="http://web.archive.org/web/20080516135520/http://radar.oreilly.com/archives/2005/04/designing_from.html">design from the outside in</a>, and extract reusable components from the result. The very successful RubyOnRails web framework was extracted from a working application (Basecamp). In a lot of ways I think Test Driven Design encourages this sort of outside-in thinking as well. Extracting usable components from a ball of mud is nigh impossible though...at least for me. I would be interested to know how much of the Rails components were anticipated by the designers as they were creating BaseCamp. It takes a great deal of discipline and jazz-like anticipation to be able to improvise a good design. That or, you have to build in time to prototype something with an eye to taking what you've learned to do it right.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>118</wp:post_id>
		<wp:post_date>2006-09-19 07:56:12</wp:post_date>
		<wp:post_date_gmt>2006-09-19 14:56:12</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-importance-of-making-packages</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>4763</wp:comment_id>
			<wp:comment_author><![CDATA[Dan Scott]]></wp:comment_author>
			<wp:comment_author_email>dan@coffeecode.net</wp:comment_author_email>
			<wp:comment_author_url>http://coffeecode.net</wp:comment_author_url>
			<wp:comment_author_IP>142.51.140.233</wp:comment_author_IP>
			<wp:comment_date>2006-09-19 08:34:58</wp:comment_date>
			<wp:comment_date_gmt>2006-09-19 15:34:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[+1

My recent experience mirrors the Rails/Basecamp history. 

While working on the File_MARC proposal for PHP's PEAR package repository, I initially created a basic linked list as part of the File_MARC package. I recognized that it could be its own package, but to be honest I was reluctant to go through the rigorous PEAR package proposal process for both File_MARC and a linked list package at the same time. Especially when File_MARC depends on the linked list package, so a hold-up in approval for the linked list package would delay File_MARC's birth as well.

After a few weeks, I realized that I might as well split the linked list off into its own package as it was likely that a) the linked list package would be of much more general interest than File_MARC, and therefore more likely to have its rough edges (ahem, "bugs") worn down by wider usage -- thereby benefiting File_MARC and 2) by making it a separate package, it would force me to develop, test, and document the linked list functionality much more thoroughly than if it remained a part of the File_MARC "mud".  I'm convinced the results will be worth the extra effort.

Once I factored out the linked list into its own package, I realized that I could go a step further and refactor the doubly-linked list into a singly-linked list as a base class and reimplement the doubly-linked list as a simple extension to the singly-linked list. Benefit to the users, who can just choose a singly-linked list if their application doesn't need the overhead of being able to traverse the list in reverse, and benefit to the development team (me) because of the code reuse.

So yes, nice little reusable packages are good.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>ruby-oai v0.0.3</title>
		<link>http://inkdroid.org/2006/09/19/ruby-oai-v003/</link>
		<pubDate>Wed, 20 Sep 2006 03:01:55 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/09/19/ruby-oai-v003/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://rubyforge.org/projects/oai/">v0.0.3</a> of <a href="http://textualize.com/ruby-oai">ruby-oai</a> was just released to <a href="http://rubyforge.org">RubyForge</a>. The big news is that this release allows you to use <a href="http://libxml.rubyforge.org/">libxml</a> for parsing thanks to the efforts of <a href="http://web.archive.org/web/20090611182302/http://oregonstate.edu:80/~reeset/">Terry Reese</a>. Terry is building a RubyOnRails metasearch application at <a href="http://osulibrary.oregonstate.edu">OSU</a> and, well, felt the need for speed.</p>

<p>After committing the branch he was working on I ran some performance tests of my own. I ran a vanilla ListRecords request against <a href="http://ir.library.oregonstate.edu/dspace-oai/request">dspace</a>, <a href="http://libeprints.open.ac.uk/perl/oai2">eprints</a> and <a href="http://memory.loc.gov/cgi-bin/oai2_0">american memory</a> oai-pmh servers using both the rexml (default) and libxml backend parsers. Here are the results</p>

<table id="bench">
<tr style="font-weight: bold;">
<td>server</td>
<td>parser</td>
<td>real</td>
<td>user</td>
<td>sys</td>
</tr>
<tr>
<td rowspan="2" valign="top">dspace</td>
<td>rexml</td>
<td>0m3.632s</td>
<td>0m2.008s</td>
<td>0m0.044s</td>
</tr>
<tr>
<td>libxml</td>
<td>0m1.900s</td>
<td>0m0.212s</td>
<td>0m0.032s</td>
</tr>
<tr style="color: green">
<td colspan="2">&nbsp;</td>
<td>1.732s (+48%)</td>
<td>1.796s (+89%)</td>
<td>0.012s (+27%)</td>
</tr>
<tr>
<td colspan="5">&nbsp;</td>
</tr>
<tr>
<td rowspan="2" valign="top">eprints</td>
<td>rexml</td>
<td>0m19.807s</td>
<td>0m1.984s</td>
<td>0m0.036s</td>
</tr>
<tr>
<td>libxml</td>
<td>0m19.344s</td>
<td>0m0.236s</td>
<td>0m0.024s</td>
</tr>
<tr style="color: green;">
<td colspan="2">&nbsp;</td>
<td>0.463s (+2%)</td>
<td>1.748s (+88%)</td>
<td>0.012s (+33%)</td>
</tr>
<tr>
<td colspan="5">&nbsp;</td>
</tr>
<tr>
<td rowspan="2" valign="top">american-memory</td>
<td>rexml</td>
<td>0m12.991s</td>
<td>0m5.424s</td>
<td>0m0.052s</td>
</tr>
<tr>
<td>libxml</td>
<td>0m7.420s</td>
<td>0m0.324s</td>
<td>0m0.032s</td>
</tr>
<tr style="color: green;">
<td colspan="2">&nbsp;</td>
<td>5.571s (+43%)</td>
<td>5.104s (+94%)</td>
<td>0.02s (+38%)</td>
</tr>
</table>

<p>Those percentage values are speed improvements. Thanks Terry :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>119</wp:post_id>
		<wp:post_date>2006-09-19 20:01:55</wp:post_date>
		<wp:post_date_gmt>2006-09-20 03:01:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>ruby-oai-v003</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>4829</wp:comment_id>
			<wp:comment_author><![CDATA[Terry&#8217;s Worklog &raquo; ruby-oai 0.0.3 &#8212; thanks ed]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://oregonstate.edu/~reeset/blog/archives/353</wp:comment_author_url>
			<wp:comment_author_IP>128.193.5.21</wp:comment_author_IP>
			<wp:comment_date>2006-09-20 22:06:59</wp:comment_date>
			<wp:comment_date_gmt>2006-09-21 05:06:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Ed posted revisions to the ruby-oai package.&nbsp; It includes the ability to utilize libxml as the xml parser.&nbsp; [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>42307</wp:comment_id>
			<wp:comment_author><![CDATA[Walter McGinnis]]></wp:comment_author>
			<wp:comment_author_email>walter@katipo.co.nz</wp:comment_author_email>
			<wp:comment_author_url>http://blog.katipo.co.nz/</wp:comment_author_url>
			<wp:comment_author_IP>58.28.158.48</wp:comment_author_IP>
			<wp:comment_date>2007-09-27 15:36:16</wp:comment_date>
			<wp:comment_date_gmt>2007-09-27 22:36:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed,

Kete uses rexml to parse results that it gets back from its Zebra server.  It's one area I have definitely marked for speed improvements.  I'm curious about your experience with the libxml tie-in.  I'm installing them gem now...

Just a note, the browsing of the source code on rubyforge appears broken.

Cheers,
Walter]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>a funny way to make a living</title>
		<link>http://inkdroid.org/2006/09/26/a-funny-way-to-make-a-living/</link>
		<pubDate>Wed, 27 Sep 2006 01:17:48 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/09/26/a-funny-way-to-make-a-living/</guid>
		<description></description>
		<content:encoded><![CDATA[gabe discovered that the <a href="http://code4lib.org">code4lib.org</a> drupal instance was littered with comment spam. Someone had actually registered for an account and proceeded to add comments to virtually every story. 

Since there was an email address associated with the account I figured I'd send an email letting them know their account was going to be zapped.

<pre>
From: edsu
To: evgeniy1985@breezein.net
Subject: code4lib.org spam

Excuse me. Why are you posting spam to my drupal 
site? Consider your account removed.
//Ed
</pre>

I really didn't expect a reply, but sure enough a few hours later:

<pre>
From: evgeniy1985@breezein.net
To: edsu 
Subject: Re: code4lib.org spam

sorry please.i am from russia.my family need a money,
but many money i can do only with spam. sorry for 
my english.
</pre>

sigh]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>120</wp:post_id>
		<wp:post_date>2006-09-26 18:17:48</wp:post_date>
		<wp:post_date_gmt>2006-09-27 01:17:48</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-funny-way-to-make-a-living</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>5143</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>128.220.205.186</wp:comment_author_IP>
			<wp:comment_date>2006-09-27 06:38:39</wp:comment_date>
			<wp:comment_date_gmt>2006-09-27 13:38:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm amazed that they wrote back so apologetically instead of ignoring you. They must get hate mail all the time. What a sad job.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>rsinger++</title>
		<link>http://inkdroid.org/2006/09/28/rsinger/</link>
		<pubDate>Thu, 28 Sep 2006 20:23:35 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/09/28/rsinger/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>So <a href="http://dilettantes.code4lib.org/">Ross</a> beat out 11 other projects to <a href="http://www.oclc.org/research/announcements/2006-09-28.htm">win</a> the OCLC Research Software Contest for his next generation OpenURL resolver <a href="http://web.archive.org/web/20070319172523/http://umlaut.library.gatech.edu:80/umlaut">umlaut</a>. Second place went to to Jesse Andrews' <a href="http://bookburro.org/">BookBurro</a>--so the competition was fierce this year. Much more so than last year when there were 4 contestants.</p>

<p>Those of us who hang out in <a href="irc://irc.freenode.net/code4lib">#code4lib</a> got to hear about this project when it was just a glimmer in his eye...and had front row seats for hearing about the development as it progressed. Essentially umlaut is an openurl router that's able to consult online catalogs (via SRU), other OpenURL resolvers (SFX), Amazon, Google, Yahoo, Connotea, CiteULike and OAI-PMH. It's all written in <a href="http://ruby-lang.org">Ruby</a> and <a href="http://rubyonrails.org">RubyOnRails</a>.</p>

<p>I feel particularly proud because Ross is enough of a mad genius to have found a use for some ruby gems I wrote for doing <a href="http://www.textualize.com/sruby">sru</a>, <a href="http://www.textualize.com/ruby-oai">oai-pmh</a> and querying OCLC's <a href="http://www.textualize.com/xisbn">xisbn</a> service.</p>

<p>Speaking of which we've been collaborating recently on a little ruby gem for querying OCLC's <a href="http://www.oclc.org/productworks/urlresolver.htm">OpenURL Resolver Registry</a>. This registry essentially makes it easy to determine what the appropriate OpenURL resolver is given a particular IP address. So you could theoretically rewrite your fulltext URLs so that they were geospatially aware. For example:</p>

<pre lang="ruby">
  require "resolver_registry"

  client = ResolverRegistry::Client.new
  institution = client.find('130.207.50.91')
  print institution.resolver.base_address
  </pre>

<p>If you want to take a look direct your svn client like so:</p>

<pre>
svn co http://rsinger.library.gatech.edu/svn/openurl_registry/
</pre>

<p>I imagine it'll get released to rubyforge sometime shortly.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>121</wp:post_id>
		<wp:post_date>2006-09-28 13:23:35</wp:post_date>
		<wp:post_date_gmt>2006-09-28 20:23:35</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>rsinger</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="irc"><![CDATA[irc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>30576</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan Rochkind]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>128.220.205.186</wp:comment_author_IP>
			<wp:comment_date>2007-05-30 07:59:03</wp:comment_date>
			<wp:comment_date_gmt>2007-05-30 14:59:03</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Has "shortly" happened yet? Trying to get Umlaut2 up and running, and need the resolver_registry package. Doesn't appear to be in the gem repository?  

Can you tell me the best way to install it from SVN?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>30625</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.241.133</wp:comment_author_IP>
			<wp:comment_date>2007-05-30 10:19:31</wp:comment_date>
			<wp:comment_date_gmt>2007-05-30 17:19:31</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Shortly hasn\'t happened yet, although it looks like all Ross needs to do is register a project on rubyforge and upload the gem. I created a Rakefile for creating the gem. You can install from a local gem file in the meantime if you like:
<pre>% svn co http://rsinger.library.gatech.edu/svn/openurl_registry/
% cd openurl_registry
% rake test
% rake package
% gem install pkg/resolver_resgistry-0.0.1.gem</pre>
oughta do it...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>use the source luke</title>
		<link>http://inkdroid.org/2006/10/06/use-the-source-luke/</link>
		<pubDate>Fri, 06 Oct 2006 11:46:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/10/06/use-the-source-luke/</guid>
		<description></description>
		<content:encoded><![CDATA[Can you imagine (back in the day) going to a page like the one at <a href="http://safari.oreilly.com/">O'Reilly's Safari</a> doing a view-source in <a href="http://en.wikipedia.org/wiki/Mosaic_%28web_browser%29">Mosaic</a> and trying to learn HTML and how the web works?

sigh...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>122</wp:post_id>
		<wp:post_date>2006-10-06 04:46:00</wp:post_date>
		<wp:post_date_gmt>2006-10-06 11:46:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>use-the-source-luke</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>6018</wp:comment_id>
			<wp:comment_author><![CDATA[Brian Kennison]]></wp:comment_author>
			<wp:comment_author_email>kennisonb@wcsu.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>149.152.216.210</wp:comment_author_IP>
			<wp:comment_date>2006-10-13 11:31:41</wp:comment_date>
			<wp:comment_date_gmt>2006-10-13 18:31:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Man, you right about that one. This page is pretty dense! This is one of those pages where you have to wonder if even the guys who created it know what's going on.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>&gt;js</title>
		<link>http://inkdroid.org/2006/10/13/js/</link>
		<pubDate>Fri, 13 Oct 2006 21:40:20 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/10/13/js/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.amazon.com/Ajax-in-Action-Dave-Crane/dp/1932394613"><img src="http://ec1.images-amazon.com/images/P/1932394613.01._AA240_SCLZZZZZZZ_V59003887_.jpg" border="0" align="left"/></a>So I've been dabbling with that <a href="http://web.archive.org/web/20080821140921/http://www.adaptivepath.com/publications/essays/archives/000385.php">four letter word</a> at $work to create a hierarchical journal/volume/issue/article browser. <a href="http://rubyonrails.org">Le rails</a> and <a href="http://web.archive.org/web/20130216003249/http://script.aculo.us/">scriptaculous</a> make it pretty easy indeed.</p>

<p>I figured I'd be a good developer and try to understand what's actually going on behind the scenes, so I picked up a copy of <a href="http://www.amazon.com/Ajax-in-Action-Dave-Crane/dp/1932394613">Ajax in Action [Illustrated]</a> and am working through it.</p>

<p>There is so much hype surrounding Ajax that I had pretty low expectations--but the book is actually very well written and a joy to read. I noticed before diving in that there was an appendix on object-oriented JavaScript. I've been around the block enough times to know that JavaScript is actually quite a <a href="http://interglacial.com/hoj/hoj.html">nice</a> functional language; but apart from DHTML I haven't really had the opportunity to dabble in it much. This appendix really made it clear how JavaScript is really quite elegant, and for someone who has done object-oriented-programming in Perl the idioms for doing OOP in JavaScript didn't seem quite that bad.</p>

<p>Anyhow, I quickly wanted to start fiddling around with the language with a JavaScript interpreter so I downloaded <a href="http://www.mozilla.org/rhino/">Rhino</a> and discovered that you can:</p>

<pre>
frizz:~/Projects/rhino1_6R4 edsu$ java -jar js.jar
Rhino 1.6 release 4 2006 09 09
js> print("hello world");
hello world
js> 
</pre>

<p>Pretty sweet :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>124</wp:post_id>
		<wp:post_date>2006-10-13 14:40:20</wp:post_date>
		<wp:post_date_gmt>2006-10-13 21:40:20</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>js</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>6165</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>128.220.205.186</wp:comment_author_IP>
			<wp:comment_date>2006-10-16 08:05:43</wp:comment_date>
			<wp:comment_date_gmt>2006-10-16 15:05:43</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Would you reccommend that AJAX book?  Let us know as you get through it. I need to get an AJAX book one of these days.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>got data?</title>
		<link>http://inkdroid.org/2006/11/20/got-data/</link>
		<pubDate>Mon, 20 Nov 2006 19:17:15 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/11/20/got-data/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Just saw this float by on <a href="http://web.archive.org/web/20100108122311/http://simile.mit.edu:80/mailman/listinfo/general">simile-general</a></p>

<blockquote>... thanks to Ben, we now have permission to publish the barton RDF dump (consisting of 50 million juicy RDF statements from the MIT library catalogue). They are now available at

http://simile.mit.edu/rdf-test-data/</blockquote>

<p>Juicy indeed...it would be nice to see <a href="http://loc.gov">more</a> libraries do this sort of thing.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>125</wp:post_id>
		<wp:post_date>2006-11-20 12:17:15</wp:post_date>
		<wp:post_date_gmt>2006-11-20 19:17:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>got-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_53b2315f72ec424df1d286fe1424645b</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_8c0fcd02fb00548868605b13c31de398</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>John Price-Wilkin Interview</title>
		<link>http://inkdroid.org/2006/12/05/john-price-wilkin-interview/</link>
		<pubDate>Tue, 05 Dec 2006 16:16:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/12/05/john-price-wilkin-interview/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In case you missed it in your overstuffed RSS reader Jon Udell recently <a href="http://weblog.infoworld.com/udell/2006/12/01.html#a1570">interviewed</a> John Price-Wilkin who is coordinating the University of Michigan's joint digitization project with Google.</p>

<p>The interview covers interesting bits of history about the <a href="http://web.archive.org/web/20090401160917/http://www.si.umich.edu:80/UMDL/">University of Michigan Digital Library</a>, 
<a href="http://moa.umdl.umich.edu">Making of America</a>, <a href="http://www.jstor.org/">JSTOR</a> (didn't realize there was a <a href="http://www.amazon.com/JSTOR-History-Roger-C-Schonfeld/dp/0691115311/">book</a>), and of course the project with Google.</p>

<p>The shocker for me was that while the UMDL has been able to digitize 3000 books per year, Google is doing approximately that number <em>a day</em>. Wilkin wasn't able to go into details about just how Google is doing this, but he does talk about details such as resolutions used, destructive vs non-destructive digitization, and how federations of libraries could work with this data.</p>

<p>Wilkin has been at the center of digital library efforts for as long as I've been working with libraries and technology, so it was really fun to hear this interview.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>126</wp:post_id>
		<wp:post_date>2006-12-05 09:16:52</wp:post_date>
		<wp:post_date_gmt>2006-12-05 16:16:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>john-price-wilkin-interview</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>miniature earth</title>
		<link>http://inkdroid.org/2006/12/05/miniature-earth/</link>
		<pubDate>Tue, 05 Dec 2006 16:41:32 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/12/05/miniature-earth/</guid>
		<description></description>
		<content:encoded><![CDATA[If there world's population were reduced to 100, it would look something like <a href="http://www.miniature-earth.com/">this</a>.

(thanks Jeroen)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>127</wp:post_id>
		<wp:post_date>2006-12-05 09:41:32</wp:post_date>
		<wp:post_date_gmt>2006-12-05 16:41:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>miniature-earth</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Imperiled Federal Libraries</title>
		<link>http://inkdroid.org/2006/12/09/imperiled-federal-libraries/</link>
		<pubDate>Sat, 09 Dec 2006 12:43:05 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/12/09/imperiled-federal-libraries/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Tim Reiterman has a good <a href="http://web.archive.org/web/20070124095318/http://www.latimes.com:80/news/nationworld/politics/la-na-libraries8dec08,0,5849200.story?">article</a> about imperiled federal libraries, and their collections...some of which are already ending up in dumpsters.</p>

<blockquote>
I think we are living in a world of digitized information...In the end there will be better access. 

(Linda Travers of the EPA)
</blockquote>

<p>Which makes me wonder what "end" she is talking about. I think there is a real danger as more and more information goes online that people simply assume that paper collections are no longer necessary.</p>

<p>Perhaps it's no coincidence that the libraries that are currently in danger the most belong to the Environmental Protection Agency, whose library budget is being slashed by 80 percent. These collections and others that are in danger (like NASA's Goddard Space Flight Center) have collections that support research into global warming.</p>

<p>If you are interested in learning more and what you can do about it ALA has a useful <a href="http://web.archive.org/web/20080725135717/http://www.ala.org/ala/washoff/WOissues/governmentinfo/epalibraries/epalibraries.htm">resource</a> page that allows you to contact your representative using a service similar to EFF's <a href="http://action.eff.org/">action center</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>128</wp:post_id>
		<wp:post_date>2006-12-09 05:43:05</wp:post_date>
		<wp:post_date_gmt>2006-12-09 12:43:05</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>imperiled-federal-libraries</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>the beeb and file sharing</title>
		<link>http://inkdroid.org/2006/12/22/the-beeb-and-file-sharing/</link>
		<pubDate>Fri, 22 Dec 2006 13:01:10 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/12/22/the-beeb-and-file-sharing/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://news.bbc.co.uk/2/hi/technology/6194929.stm"><img border="0" style="margin-right: 10px; float: left" src="/images/who.jpg" /></a>Recognizing and <a href="http://news.bbc.co.uk/2/hi/technology/6194929.stm">leveraging</a> the benefits of protocols like <a href="http://www.inkdroid.org/bittorrent.org">bittorrent</a> in "legitimate" media distribution seems like a huge step forward. I guess I have to admit I'm also pretty excited about the prospect of .torrent files for <a href="http://www.reddwarf.co.uk/">Red Dwarf</a> and <a href="http://www.bbc.co.uk/doctorwho/">Doctor Who</a> episodes. But, still there will be some kind of digital-rights-management built in, so it won't be totally open.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>129</wp:post_id>
		<wp:post_date>2006-12-22 06:01:10</wp:post_date>
		<wp:post_date_gmt>2006-12-22 13:01:10</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-beeb-and-file-sharing</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>evergreen </title>
		<link>http://inkdroid.org/2006/12/22/evergreen/</link>
		<pubDate>Fri, 22 Dec 2006 17:02:18 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2006/12/22/evergreen/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://open-ils.org/"><img src="/images/pines_book.jpg" border="0" style="float:left; margin-right: 10px" /></a>
In case you missed it linux.com is running an <a href="http://enterprise.linux.com/enterprise/06/12/04/1538214.shtml?tid=101">article</a> by Michael Stutz on Evergreen, an open source integrated library system developed by the state of Georgia to support a consortium of 44 different libraries. (Thanks for the link <a href="http://use.perl.org/~ziggy/journal/">Adam</a>)</p>

<p>Hanging out with miker_ and bradl in <a href="irc://irc.freenode.net/code4lib">irc</a> and having <a href="http://web.archive.org/web/20130502121736/http://open-ils.org/blog/">open-ils</a> in my feed reader makes me take this sort of work for granted sometimes...and Michael's article made me wake up and marvel at how truly remarkable the work they've done is.</p>

<p>The evergreen folks are hosting this years <a href="http://code4lib.org/2007">code4libcon</a> where I'm supposed to be doing a presentation on the <a href="http://code4lib.org/2007/summers">Atom Publishing Protocol</a>. It's a low cost/pragmatic alternative to the usual library technology conference options--and will be a good opportunity to buy these Evergreeners a beer. I hope to see you there.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>130</wp:post_id>
		<wp:post_date>2006-12-22 10:02:18</wp:post_date>
		<wp:post_date_gmt>2006-12-22 17:02:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>evergreen</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>mirror</title>
		<link>http://inkdroid.org/2007/01/02/mirror/</link>
		<pubDate>Wed, 03 Jan 2007 01:36:30 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/01/02/mirror/</guid>
		<description></description>
		<content:encoded><![CDATA[<object width="425" height="350"><param name="movie" value="http://www.youtube.com/v/v7p7FS88Z18"></param><param name="wmode" value="transparent"></param><embed src="http://www.youtube.com/v/v7p7FS88Z18" type="application/x-shockwave-flash" wmode="transparent" width="425" height="350"></embed></object>

<blockquote>
Yes it's almost as though consumers have moved on because mainstream media has abdicated its responsibility...
</blockquote>

hahahahaha]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>131</wp:post_id>
		<wp:post_date>2007-01-02 18:36:30</wp:post_date>
		<wp:post_date_gmt>2007-01-03 01:36:30</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>mirror</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="apple"><![CDATA[apple]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>#9</title>
		<link>http://inkdroid.org/2007/01/04/9/</link>
		<pubDate>Thu, 04 Jan 2007 19:49:02 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/01/04/9/</guid>
		<description></description>
		<content:encoded><![CDATA[<pre>
22:01 < edsu> i would try to separate them now before it's 
      too late :)
22:02 < erikhatcher> it's never too late, but i certainly want 
      to keep this clean from the start
</pre>

New Years Resolution #9 - never underestimate the power of a positive attitude...
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>132</wp:post_id>
		<wp:post_date>2007-01-04 12:49:02</wp:post_date>
		<wp:post_date_gmt>2007-01-04 19:49:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>9</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="irc"><![CDATA[irc]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>DemoCampDC</title>
		<link>http://inkdroid.org/2007/01/05/democampdc/</link>
		<pubDate>Sat, 06 Jan 2007 03:36:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/01/05/democampdc/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://barcamp.org/DemoCampDC1"><img src="http://barcamp.org/f/democamp_dc_logo_1.png" border="0" style="margin-right: 10px; float: left;" /></a>

<a href="http://barcamp.org/DemoCampDC1">DemoCampDC</a> is an adaptation of <a href="http://barcamp.org">BarCamp</a> to provide an informal mechanism for sharing technology shtuff in the DC area. If you are interested and in the DC area please add your name to the list of attendees and stay tuned.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>133</wp:post_id>
		<wp:post_date>2007-01-05 20:36:57</wp:post_date>
		<wp:post_date_gmt>2007-01-06 03:36:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>democampdc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="dc"><![CDATA[dc]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>identifiers and authority records</title>
		<link>http://inkdroid.org/2007/01/06/identifiers-and-authority-records/</link>
		<pubDate>Sat, 06 Jan 2007 12:51:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/01/06/identifiers-and-authority-records/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Authority_file">Authority files</a> are rather important for unambiguously talking about a person, place or thing. In database lingo they essentially amount to a <a href="http://en.wikipedia.org/wiki/Primary_key">primary key</a> for a table. Given the <a href="http://www.loc.gov/catdir/pcc/naco/">time and effort</a> libraries spend in maintaining authority records and assigning control numbers to individuals it makes sense that a <a href="http://www.inkdroid.org/2006/07/11/more-on-web-identifiers/">URI</a> could be assigned to an individual in such authority files. I realize this idea is nothing new, but until recently I hadn't seen it put into practice particularly well.</p>

<p>I imagine this has been there all along but I just noticed that <a href="http://alcme.oclc.org/laf/index.html">OCLC's Linked Authority File</a> includes <a href="http://en.wikipedia.org/wiki/PURL">PURLs</a> for authors now. For example the following URL contains a <a href="http://en.wikipedia.org/wiki/Library_of_Congress_Control_Number">LCCN</a>:</p>

<blockquote>
<a href="http://errol.oclc.org/laf/n79-7035">http://errol.oclc.org/laf/n79-7035</a>
</blockquote>

<p>When you GET this your browser is automatically redirected with an HTTP 302 to:</p>

<blockquote>
<a href="http://alcme.oclc.org/laf/servlet/OAIHandler?verb=GetRecord&metadataPrefix=oai_dc&identifier=n79-7035">http://alcme.oclc.org/laf/servlet/OAIHandler?
verb=GetRecord&metadataPrefix=oai_dc&identifier=n79-7035</a>
</blockquote>

<p>which you'll notice is a <a href="http://www.openarchives.org/OAI/openarchivesprotocol.html">OAI-PMH</a> request to fetch a <a href="http://dublincore.org/">DublinCore</a> record with the identifier n79-7035:</p>

<pre>
&lt;oai_dc:dc 
  xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" 
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
  xmlns:dc="http://purl.org/dc/elements/1.1/" 
  xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ 
    http://openarchives.org/OAI/2.0/oai_dc.xsd"&gt;
  &lt;dc:creator xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"&gt;
    Borges, Jorge Luis,--1899-
  &lt;/dc:creator&gt;
  &lt;dc:description xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"&gt;
    Sua&Igrave;rezLynch, B.--nnnc
  &lt;/dc:description&gt;
&lt;/oai_dc:dc&gt;
</pre>

<p>So now we know who this identifier is for, and the established heading for the individual. But it gets better (or worse depending on your perspective). Since this is an <a href="http://www.openarchives.org/OAI/openarchivesprotocol.html">OAI-PMH</a> server you can issue a ListMetadataFormats request to see what other flavors this record might be available in. If you <a href="http://alcme.oclc.org/laf/servlet/OAIHandler?verb=ListMetadataFormats&identifier=n79-7035">do</a> you'll find out that this record is also available as <a href="http://www.loc.gov/standards/marcxml/">marcxml</a> in all its <a href="http://alcme.oclc.org/laf/servlet/OAIHandler?verb=GetRecord&metadataPrefix=marcxml&identifier=n79-7035">unholy glory</a> (if you follow that link your browser will use a stylesheet to turn the raw xml into something a bit more presentable). Putting aside my snideness about <a href="http://www.loc.gov/marc/authority/ecadhome.html">MARC</a> for a moment, this is a lot of useful data being made available.</p>

<p>You can also search the name authority file and get relevant PURLs via a <a href="http://alcme.oclc.org/eprintsUK/">SOAP/REST service</a>. For example the irc bot panizzi in <a href="irc://chat.freenode.net/code4lib">#code4lib</a> actually has a bit of <a href="http://textualize.com/trac/browser/panizzi/trunk/plugins/OCLC/plugin.py" title="in the naf method">logic</a> that allows it do lookups in the linked authority file:</p>

<pre>
06:56 &lt; edsu&gt; @naf borges, jorge
06:56 &lt; panizzi&gt; edsu: [20 matches] [~1] Borges, Jorge Luis, 1899- 
                 &lt;http://errol.oclc.org/laf/n79-7035&gt;; [~2] Macedo, Jorge 
                 Borges de. &lt;http://errol.oclc.org/laf/n82-149895&gt;; [~3] 
                 Borges, Jorge G. (Jorge Guillermo), 1874-1938 
                 &lt;http://errol.oclc.org/laf/n90-681877&gt;; [~4] Sua?rez Lynch, B.                  
                 &lt;http://errol.oclc.org/laf/n82-21644&gt;; [~5] Borges, Jorge 
                 Wheliton Miranda &lt;http://errol.oclc.org/laf/n92-76758&gt;; [~6] 
                 Canido Borges, Jorge Oscar (3 more messages)
</pre>

<p>All in all it's an impressive mix of technology, standards and practice. It is not entirely clear to me how this work relates to the <a href="http://web.archive.org/web/20100210093731/http://www.oclc.org:80/research/projects/viaf/">Virtual International Authority File</a>. Perhaps LAF wasn't considered a good acronym? If you are interested in such things Thom Hickey had a really interesting talk at <a href="http://web.archive.org/web/20110722013853/http://www.access2006.uottawa.ca/">Access2006</a> which has <a href="http://web.archive.org/web/20070822080628/http://www.access2006.uottawa.ca/2006-10-14-03-hickeyt.mp3">audio</a> available.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>134</wp:post_id>
		<wp:post_date>2007-01-06 05:51:49</wp:post_date>
		<wp:post_date_gmt>2007-01-06 12:51:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>identifiers-and-authority-records</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>enclosure</wp:meta_key>
			<wp:meta_value><![CDATA[http://web.archive.org/web/20070822080628/http://www.access2006.uottawa.ca/2006-10-14-03-hickeyt.mp3
31826585
audio/mpeg
]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>March on Washington</title>
		<link>http://inkdroid.org/2007/01/19/march-on-washington/</link>
		<pubDate>Fri, 19 Jan 2007 21:01:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/?p=136</guid>
		<description></description>
		<content:encoded><![CDATA[<pre><code>&lt;div class='vevent x-wpsb-simple-event'&gt;        &lt;p&gt;&lt;/p&gt;     &lt;h3 class='summary'&gt;MARCH ON WASHINGTON TO END THE WAR&lt;/h3&gt;     &lt;p&gt;&lt;b&gt;Begins&lt;/b&gt;: &lt;abbr class='dtstart' title='2007-01-27T11:00:00'&gt;Sat, 27 Jan 2007 at 11:00 AM&lt;/abbr&gt;&lt;/p&gt;     &lt;p&gt;&lt;b&gt;Ends&lt;/b&gt;: &lt;abbr class='dtend' title='2007-01-27T11:00:00'&gt;Sat, 27 Jan 2007 at  2:00 PM&lt;/abbr&gt;&lt;/p&gt;             &lt;p&gt;         &lt;b&gt;Location&lt;/b&gt;:            &lt;span class='location'&gt;&lt;p&gt;Mall between 3rd and 7th Streets&lt;/p&gt;                      &lt;p&gt;Washington,          DC          20002&lt;/p&gt;           &lt;p&gt;USA&lt;/p&gt;      &lt;/span&gt;&lt;/p&gt;                 &lt;p&gt;&lt;b&gt;Link&lt;/b&gt;: &lt;a href="http://web.archive.org/web/20100708231233/http://www.unitedforpeace.org/article.php?id=3468"&gt;more info&lt;/a&gt;&lt;/p&gt;             &lt;div&gt;Mark your calendars, and let me know if you need a place to stay...&lt;/div&gt;          &lt;/div&gt;
</code></pre>

<script type="application/x-subnode; charset=utf-8">
       <!-- the following is structured blog data for machine readers. -->
       <subnode xmlns:data-view="http://www.w3.org/2003/g/data-view#" data-view:transformation="http://structuredblogging.org/subnode-to-rdf-interpreter.xsl" xmlns="http://www.structuredblogging.org/xmlns#subnode">
            <xml-structured-blog-entry xmlns="http://www.structuredblogging.org/xmlns">
                <generator id="wpsb-1" type="x-wpsb-post" version="1"/><event type="event/generic"><name>MARCH ON WASHINGTON TO END THE WAR</name><location address="Mall between 3rd and 7th Streets" city="Washington" state="DC" postcode="20002" country="USA"/><description>Mark your calendars, and let me know if you need a place to stay...</description><link url="http://www.unitedforpeace.org/article.php?id=3468">more info</link><begins>2007-01-27T11:00:00</begins><ends>2007-01-27T14:00:00</ends></event>
            </xml-structured-blog-entry>
       </subnode>
       </script>
]]></content:encoded>
		<excerpt:encoded><![CDATA[	<div class='vevent x-wpsb-simple-event'>		<p></p>		<h3 class='summary'>MARCH ON WASHINGTON TO END THE WAR</h3>		<p><b>Begins</b>: <abbr class='dtstart' title='2007-01-27T11:00:00'>Sat, 27 Jan 2007 at 11:00 AM</abbr></p>		<p><b>Ends</b>: <abbr class='dtend' title='2007-01-27T11:00:00'>Sat, 27 Jan 2007 at  2:00 PM</abbr></p>				<p>			<b>Location</b>:			<span class='location'><p>Mall between 3rd and 7th Streets</p>						<p>Washington, 			DC			20002</p>			<p>USA</p>		</span></p>					<p><b>Link</b>: <a href='http://www.unitedforpeace.org/article.php?id=3468'>more info</a></p>				<div>Mark your calendars, and let me know if you need a place to stay...</div>			</div>]]></excerpt:encoded>
		<wp:post_id>136</wp:post_id>
		<wp:post_date>2007-01-19 14:01:04</wp:post_date>
		<wp:post_date_gmt>2007-01-19 21:01:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>march-on-washington</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>structuredblogging_xml</wp:meta_key>
			<wp:meta_value><![CDATA[<script type="application/x-subnode; charset=utf-8">
       <!-- the following is structured blog data for machine readers. -->
       <subnode xmlns:data-view="http://www.w3.org/2003/g/data-view#" data-view:transformation="http://structuredblogging.org/subnode-to-rdf-interpreter.xsl" xmlns="http://www.structuredblogging.org/xmlns#subnode">
       	    <xml-structured-blog-entry xmlns="http://www.structuredblogging.org/xmlns">
       		    <generator id="wpsb-1" type="x-wpsb-post" version="1"/><event type="event/generic"><name>MARCH ON WASHINGTON TO END THE WAR</name><location address="Mall between 3rd and 7th Streets" city="Washington" state="DC" postcode="20002" country="USA"/><description>Mark your calendars, and let me know if you need a place to stay...</description><link url="http://www.unitedforpeace.org/article.php?id=3468">more info</link><begins>2007-01-27T11:00:00</begins><ends>2007-01-27T14:00:00</ends></event>
       	    </xml-structured-blog-entry>
       </subnode>
       </script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>sb_rss_content</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>oxford dictionary of national biography</title>
		<link>http://inkdroid.org/2007/02/05/oxford-dictionary-of-national-biography/</link>
		<pubDate>Mon, 05 Feb 2007 19:44:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/02/05/oxford-dictionary-of-national-biography/</guid>
		<description></description>
		<content:encoded><![CDATA[It's interesting to see that the <a href="http://www.oup.com/oxforddnb">Oxford Dictionary of National Biography</a> has created <a href="http://www.w3.org/Provider/Style/URI">Cool URIs</a> for their index of notable people. So for example if you want an identifier for JRR Tolkien you can use:

<blockquote>
<a href="http://www.oxforddnb.com/index/101031766">http://www.oxforddnb.com/index/101031766</a>
</blockquote>

Alas, the full content of the biography isn't available (unless you subscribe), but I guess some publishers still have business models to hold on to. To see all the entries you have to <a href="http://www.oup.com/oxforddnb/info/index/browsing/">browse</a> them.

I think it's a nice simple example of how authority files can be integrated into the web as we know it. Thanks to Caroline Arms for forwarding this on to me...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>137</wp:post_id>
		<wp:post_date>2007-02-05 12:44:52</wp:post_date>
		<wp:post_date_gmt>2007-02-05 19:44:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oxford-dictionary-of-national-biography</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>uri-templates</title>
		<link>http://inkdroid.org/2007/02/05/uri-templates/</link>
		<pubDate>Mon, 05 Feb 2007 21:36:33 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/02/05/uri-templates/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I've been playing with <a href="http://bitworking.org/news/URI_Templates">uri-templates</a> a little bit at $work to help formulate clean urls for a newspaper application. The goal is to provide urls such as:</p>

<ul>
<li>http://example.gov/issn/0362-4331</li>
<li>http://example.gov/issn/0362-4331/1969-05-28</li>
<li>http://example.gov/issn/0362-4331/1969-05-28/1</li>
<li>http://example.gov/issn/0362-4331/1969-05-28/1/31</li>
</ul>

<p>I was hoping something like this would work:</p>

<ul>
<li>http://example.gov/issn/{issn}/{date}/{edition}/{page}</li>
</ul>

<p>But I'd like to indicate that the date, edition and page parameters are optional. After reading the <a href="http://web.archive.org/web/20070208032916/http://www.ietf.org:80/internet-drafts/draft-gregorio-uritemplate-00.txt">spec</a> and some <a href="http://lists.w3.org/Archives/Public/uri/2006Nov/0048">discussion</a> it becomes clear that there is no way to indicate that part of the path is optional. OpenSearch addresses the issue to some extent by making parameters optional with '?':</p>

<ul>
<li>http://example.gov/issn/{issn}/{date?}/{edition?}/{page?}</li>
</ul>

<p>Which seems to be what I want. But there are some wrinkles such as when a page is included without a date. But perhaps these details could be application specific?</p>

<p>The <a href="http://lists.w3.org/Archives/Public/uri/2006Nov/0052">discussion</a> seemed to indicate that the template could be bundled with a written description of how the parameters are to be used. Or instead an additional template specification for optionality could be created which references the URI Template spec. There were also some nods towards <a href="https://wadl.dev.java.net/">WADL</a>, which apparently has some richer conventions for this sort of thing.</p>

<p>I guess for the moment using</p>

<ul>
<li>http://example.gov/issn/{issn}/{date}/{edition}/{page}</li>
</ul>

<p>with some descriptive text will work good enough. But I think it would be useful if the uri-template draft commented on the issue somehow...since it's bound to come up again.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>138</wp:post_id>
		<wp:post_date>2007-02-05 14:36:33</wp:post_date>
		<wp:post_date_gmt>2007-02-05 21:36:33</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>uri-templates</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>75 minutes</title>
		<link>http://inkdroid.org/2007/02/14/75-minutes/</link>
		<pubDate>Wed, 14 Feb 2007 20:33:31 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/02/14/75-minutes/</guid>
		<description></description>
		<content:encoded><![CDATA[The <a href="http://75minutes.com/podcast/75-minutes-epilogue/">worst news</a> so far in 2007 after <a href="http://en.wikipedia.org/wiki/Iraq_War_troop_surge_of_2007">the surge</a>. Can anyone else recommend a good podcast for independent music? I'm going to suffer...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>139</wp:post_id>
		<wp:post_date>2007-02-14 13:33:31</wp:post_date>
		<wp:post_date_gmt>2007-02-14 20:33:31</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>75-minutes</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>14511</wp:comment_id>
			<wp:comment_author><![CDATA[alf]]></wp:comment_author>
			<wp:comment_author_email>alf@hubmed.org</wp:comment_author_email>
			<wp:comment_author_url>http://hublog.hubmed.org</wp:comment_author_url>
			<wp:comment_author_IP>194.129.50.189</wp:comment_author_IP>
			<wp:comment_date>2007-02-15 03:17:08</wp:comment_date>
			<wp:comment_date_gmt>2007-02-15 10:17:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I was wondering the same thing - in the Epilogue Mike implied that there were lots of other podcasts doing the same kind of thing, but I haven't found them.

Contrast Podcast was excellent the week - the 'my favourite 45' edition. So The Wind... is always consistently good too, in a different style. Neither of these cover the same breadth of punk/independent rock that 75 Minutes did though.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>21599</wp:comment_id>
			<wp:comment_author><![CDATA[Claudia]]></wp:comment_author>
			<wp:comment_author_email>clmccowan@mac.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>76.171.29.23</wp:comment_author_IP>
			<wp:comment_date>2007-04-12 19:47:20</wp:comment_date>
			<wp:comment_date_gmt>2007-04-13 02:47:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Not a podcast, but check out the stream at kxlu.org.


The kids, they're coming up from behind.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>exhibit</title>
		<link>http://inkdroid.org/2007/02/16/exhibit/</link>
		<pubDate>Fri, 16 Feb 2007 16:57:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/02/16/exhibit/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://simile.mit.edu/exhibit"><img src="/images/exhibit.png" style="margin-right: 10px; border: 0; float: left;"/></a> If you haven't tried <a href="http://simile.mit.edu/exhibit/">Exhibit</a> out yet the simile folks have created a truly wonderful data publishing framework which runs entirely in your browser with a bit of javascript, html and css.</p>

<p>The remarkable part is that it requires no backend database, but simply operates on a stream of <a href="http://en.wikipedia.org/wiki/JSON">json</a>. If you have a couple minutes take a look at their <a href="http://simile.mit.edu/wiki/Exhibit/Getting_Started_Tutorial">Getting Started Tutorial</a> which shows you how to create a exhibit of MIT related nobel laureates with a tiny bit of HTML, CSS and JavaScript.</p>

<p>Just as an experiment I tried pointing it at my <a href="http://del.icio.us/feeds/json/inkdroid/metadata?raw=1&count=100">delicious json feed</a> for metadata. It turns out that exhibit wants json data to be a hash with a key 'items' that points to a list of items. In addition it also wants each item to have a 'label' key. I quickly <a href="/exhibit/delicious/delicious.json">reformatted</a> the delicious json with <a href="http://cheeseshop.python.org/pypi/simplejson">simplejson</a>, and got <a href="/exhibits/delicious">this</a>.</p>

<p>A few minutes later I <a href="http://simile.mit.edu/mail/ReadMsg?listName=General&msgId=14474">prodded</a> the simile folks to see if there is a way of filtering json data on the way into exhibit so that it can be normalized...time passes (like maybe an hour) and then I <a href="http://web.archive.org/web/20070321011919/http://simile.mit.edu:80/mail/ReadMsg?listName=General&amp;msgId=14476">hear</a> from <a href="http://ecmanaut.blogspot.com/">Johan Sundstr&ouml;m</a> that the latest/greatest exhibit code has this sort of filtering built in!</p>

<p>Tangential to the exhibit code, there has been an interesting <a href="http://simile.mit.edu/mail/BrowseList?listName=General&from=10038&to=10038&count=100&by=thread&paged=false&by=date" title="need to read the thread from the bottom up">discussion</a> recently about how to expose exhibit content to indexing services like google. Since exhibit content is generated with pure javascript, and google (as far as we know) primarily indexes html content--the exhibit content is rendered invisible. This is a problem that digital library applications and repositories have to deal with as well, so it may be of interest.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>140</wp:post_id>
		<wp:post_date>2007-02-16 09:57:41</wp:post_date>
		<wp:post_date_gmt>2007-02-16 16:57:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>exhibit</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>oclc registry</title>
		<link>http://inkdroid.org/2007/02/19/oclc-registry/</link>
		<pubDate>Tue, 20 Feb 2007 04:36:42 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/02/19/oclc-registry/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://worldcat.org/registry/Institutions"><img src="http://worldcat.org/registry/images/masthead_registry_en.png" style="border: none; margin-right: 10px; float: left" /></a>

So <a href="http://worldcat.org/registry/Institutions">OCLC's WorldCat Registry</a> is a nice new addition to OCLCs growing list of web services. Do a search for your library and take a look at the URL: aye that's right it's <a href="http://www.loc.gov/standards/sru/">SRU</a>. In fact do a view source on the results page and you'll see an SRU response in XML--the HTML is being rendered with client side XSLT.

If you drill into a particular institution you'll see a pleasantly <a href="http://www.w3.org/Provider/Style/URI">cool</a> uri:

<blockquote>
<a href="http://worldcat.org/registry/Institutions/89073">http://worldcat.org/registry/Institutions/89073</a>
</blockquote>

...which would serve nicely as an identifier for the Browne Popular Culture Library. The institution pages are HTML instead of XML--however there is a link to an XML representation:

<blockquote><a href="http://worldcat.org/webservices/registry/content/Institutions/89073">http://worldcat.org/webservices/registry/content/Institutions/89073</a>
</blockquote>

This URL isn't bad but it would be rather nice if the former could return XML if the Accept: header had text/xml slotted before text/html. Yeah, I did check:

<pre>
  curl -I "Accept: text/xml" http://worldcat.org/registry/Institutions/89073
</pre>

It's inspiring to see OCLC going the extra mile to make their new services have web friendly machine APIs.

Update: for deeper analysis check out Pete Johnston's <a href="http://efoundations.typepad.com/efoundations/2007/02/worldcat_instit.html">WorldCat Institution Registry and Identifiers</a>. He has some great points on the use of identifiers in the xml responses.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>141</wp:post_id>
		<wp:post_date>2007-02-19 21:36:42</wp:post_date>
		<wp:post_date_gmt>2007-02-20 04:36:42</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oclc-registry</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="post_tag" nicename="worldcat"><![CDATA[worldcat]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>15086</wp:comment_id>
			<wp:comment_author><![CDATA[Chuck Costakos]]></wp:comment_author>
			<wp:comment_author_email>costakoc@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://www.worldcat.org</wp:comment_author_url>
			<wp:comment_author_IP>132.174.21.113</wp:comment_author_IP>
			<wp:comment_date>2007-02-21 07:53:09</wp:comment_date>
			<wp:comment_date_gmt>2007-02-21 14:53:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[OCLC also introduced two web services that you can use to access the new WorldCat Registries. On this web page: http://www.worldcat.org/affiliate/default.jsp, the two are listed. There is one for searching and a companion web service to retrieve the detail records for institutions. Just register via the WorldCat.org affiliate site. 

Let me know if you have any questions.

Best regards,
Chuck Costakos, OCLC]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>US open access petition</title>
		<link>http://inkdroid.org/2007/03/14/open-access-petition-in-the-us/</link>
		<pubDate>Wed, 14 Mar 2007 13:19:48 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/03/14/open-access-petition-in-the-us/</guid>
		<description></description>
		<content:encoded><![CDATA[As <a href="http://www.jiscmail.ac.uk/archives/jisc-repositories.html">announced</a> on the jisc-repositories list there is now a <a href="http://www.publicaccesstoresearch.org/">US counterpart</a> to the <a href="http://www.ec-petition.eu/">EU Petition calling for Open Access</a>. 

<blockquote>
We, the undersigned, believe that broad dissemination of research results is fundamental to the advancement of knowledge. For America&rsquo;s taxpayers to obtain an optimal return on their investment in science, publicly funded research must be shared as broadly as possible. Yet too often, research results are not available to researchers, scientists, or the members of the public. Today, the Internet and digital technologies give us a powerful means of addressing this problem by removing access barriers and enabling new, expanded, and accelerated uses of research findings.
</blockquote>

The petition was put together by the <a href="http://www.taxpayeraccess.org/">Alliance for Taxpayer's Access</a> in response to the 28,000 odd enlightened folks who signed the EU petition. I was encouraged to see prominent sponsor icons for <a href="http://ala.org">American Libraries Association</a>, <a href="http://acrl.org">Association of College & Research Libraries</a> on the US petition.

I haven't been tracking the <a href="http://en.wikipedia.org/wiki/Open_access">Open Access</a> movement as well as I should have--but I did take a few seconds while drinking coffee at the breakfast table this morning to sign the petition. The movement seems to be really making a lot of progress recently. 

Via a bit of synchronicity Caroline Arms sent a message around at $work about the recent <a href="http://www.delange.rice.edu/conferenceVI.cfm">Emerging Libraries</a> conference at Rice. Apparently <a href="http://en.wikipedia.org/wiki/Kahle">Brewster Kahle</a> and <a href="http://en.wikipedia.org/wiki/Paul_Ginsparg">Paul Ginsparg</a> had a meeting of like like minds. I guess it's not surprising considering their roles in bringing libraries and archives into the computing age with <a href="http://archive.org">The Internet Archive</a> and <a href="http://arxiv.org/">arXiv</a>. What is surprising is that it took this long. These two projects are wildly successful, living and breathing examples of Open Access projects. 

The <a href="http://webcast.rice.edu/">audio</a> for all the conference presentations is available from Rice...including the very listenable <a href="http://webcast.rice.edu/index.php?action=details&event=927">Universal Access to Human Knowledge</a> (Kahle) and <a href="http://webcast.rice.edu/webcast.php?action=details&event=919">Read as We May</a> (Ginsparg).]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>142</wp:post_id>
		<wp:post_date>2007-03-14 06:19:48</wp:post_date>
		<wp:post_date_gmt>2007-03-14 13:19:48</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>open-access-petition-in-the-us</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>16584</wp:comment_id>
			<wp:comment_author><![CDATA[Disruptive Library Technology Jester :: Petition for Public Access to Publicly Funded Research in the U.S.]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://dltj.org/2007/03/public-access-petition/</wp:comment_author_url>
			<wp:comment_author_IP>69.36.2.54</wp:comment_author_IP>
			<wp:comment_date>2007-03-15 05:18:08</wp:comment_date>
			<wp:comment_date_gmt>2007-03-15 12:18:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] As others have noted, there is now an online petition in support of public access to publicly funded research in the United States. The text of the petition is short:  We, the undersigned, believe that broad dissemination of research results is fundamental to the advancement of knowledge. For America’s taxpayers to obtain an optimal return on their investment in science, publicly funded research must be shared as broadly as possible. Yet too often, research results are not available to researchers, scientists, or the members of the public. Today, the Internet and digital technologies give us a powerful means of addressing this problem by removing access barriers and enabling new, expanded, and accelerated uses of research findings. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>theory</title>
		<link>http://inkdroid.org/2007/03/28/theory/</link>
		<pubDate>Wed, 28 Mar 2007 20:05:08 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/03/28/theory/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="ttp://www.amazon.com/Agile-Software-Development-Cooperative-Game/dp/0321482751"><img style="border: medium none ; margin-right: 10px; float: left" src="http://alistair.cockburn.us/images/Asd2ecover.jpg" /></a>The second book I checked out of the Library of Congress with my shiny new borrowing card was Alistair Cockburn's <a href="http://www.amazon.com/Agile-Software-Development-Cooperative-Game/dp/0321482751">Agile Software Development: The Cooperative Game</a> (which happened to just win this years <a href="http://ec2.images-amazon.com/images/P/0321482751.01._AA240_SCLZZZZZZZ_.jpg">Jolt Award</a>). Early on Cockburn recommends jumping to an appendix to read <a href="http://en.wikipedia.org/wiki/Peter_Naur">Peter Naur's</a>  article <a href="http://web.archive.org/web/20080612143832/http://www.zafar.se:80/bkz/Articles/NaurProgrammingTheory">"Programming as Theory Building"</a> (thanks ksclarke).</p>

<p>This is my second time reading the article, but this time it is really resonating with me--the idea of writing programs as building theories. Partly I think this is because I was reading it while I attended a recent Haskell <a href="http://notes-on-haskell.blogspot.com/2007/03/haskell-cooks-tour.html">tutorial </a> by coworker <a href="http://notes-on-haskell.blogspot.com">Adam Turoff</a> here in DC (which I will write about shortly).</p>

<p>On the ride to work this morning a particular quote stood out, and I'm just writing it here so I don't forget it:</p>

<blockquote>... the problems of program modification  arise from acting on the assumption that programming consists of program text production, instead of recognizing programming as an activity of theory building.</blockquote>

<p>It seems obvious at first I guess. But it's a powerful statement about what the activity of software development ought to be--instead of a string of hacks that eventually brings a piece of software to its knees.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>143</wp:post_id>
		<wp:post_date>2007-03-28 13:05:08</wp:post_date>
		<wp:post_date_gmt>2007-03-28 20:05:08</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>theory</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>18644</wp:comment_id>
			<wp:comment_author><![CDATA[Ross]]></wp:comment_author>
			<wp:comment_author_email>rossfsinger@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://dilettantes.code4lib.org/</wp:comment_author_url>
			<wp:comment_author_IP>74.224.121.122</wp:comment_author_IP>
			<wp:comment_date>2007-03-28 20:06:43</wp:comment_date>
			<wp:comment_date_gmt>2007-03-29 03:06:43</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[If I didn't write crappy code, what would we need all that RAM and those extra processors for?  Think of the economic implications if my software projects didn't start some Moore's Law arms race!  What would Dell do?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>nekkid</title>
		<link>http://inkdroid.org/2007/04/05/nekkid/</link>
		<pubDate>Thu, 05 Apr 2007 13:30:18 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/04/05/nekkid/</guid>
		<description></description>
		<content:encoded><![CDATA[Yeah, today is <a title="CSS Naked Day" href="http://naked.dustindiaz.com/">CSS Naked Day</a> I just hope I remember to re-enable CSS tomorrow :-)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>144</wp:post_id>
		<wp:post_date>2007-04-05 06:30:18</wp:post_date>
		<wp:post_date_gmt>2007-04-05 13:30:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>nekkid</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>20462</wp:comment_id>
			<wp:comment_author><![CDATA[John Cowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url>http://www.ccil.org/~cowan</wp:comment_author_url>
			<wp:comment_author_IP>66.108.117.254</wp:comment_author_IP>
			<wp:comment_date>2007-04-05 06:57:41</wp:comment_date>
			<wp:comment_date_gmt>2007-04-05 13:57:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Lucky  me, who has nothing to remove and nothing to add back.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>late easter present</title>
		<link>http://inkdroid.org/2007/04/10/pymarc-egg/</link>
		<pubDate>Wed, 11 Apr 2007 02:43:37 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/04/10/pymarc-egg/</guid>
		<description></description>
		<content:encoded><![CDATA[I finally took the time to make <a title="pymarc" href="http://textualize.com/pymarc">pymarc</a> <a title="setuptools" href="http://peak.telecommunity.com/DevCenter/setuptools">setuptools</a> friendly. This basically means that if you've got easy_install handy you can:
<pre>sudo easy_install pymarc</pre>
If you haven't looked at eggs yet, they are pretty much the defacto standard for distributing python code. The <a href="http://www.python.org/pypi">PyPi</a> (Python Package Index, aka Python Cheese Shop) allows <a href="http://peak.telecommunity.com/DevCenter/EasyInstall">easy_install</a> to locate and download packages, which are then unpacked and installed.

pymarc was basically an experiment to make sure I understood how eggs worked with pypi. Next up Rob Sanderson has sent me some code he and a colleague did for parsing Library of Congress Classification Numbers which I'm going to bundle up as an egg as well. Stay tuned.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>145</wp:post_id>
		<wp:post_date>2007-04-10 19:43:37</wp:post_date>
		<wp:post_date_gmt>2007-04-11 02:43:37</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pymarc-egg</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>rda/frbr and the semantic web</title>
		<link>http://inkdroid.org/2007/05/03/rdafrbr-and-the-semantic-web/</link>
		<pubDate>Thu, 03 May 2007 13:21:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/05/03/rdafrbr-and-the-semantic-web/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I was interested to learn from <a href="http://isegserv.itd.rl.ac.uk/blogs/alistair/archives/53">Alistair Miles</a> that folks in the library community are starting to look at expressing models such as <a href="http://www.collectionscanada.ca/jsc/rda.html">RDA</a> and <a href="http://web.archive.org/web/20100805202110/http://www.loc.gov/cds/FRBR.html">FRBR</a> using the semantic web technology stack--including things like <a href="http://dublincore.org/documents/2007/04/02/abstract-model/">Dublin Core Abstract Model</a>. <br /><br />It's exciting and timely to see the luminaries in the field get together to talk about this sort of convergence. I must admit I'm still a little bit hazy about why we need DCAM when we already have RDF, RDFS and OWL ... but I think only good things can come from this interaction. <br /><br />It's particularly heartening that the library community is exploring what RDA and FRBR look like when the rubber meets the road of data representation. Although Ian Davis, Richard Newman and Bruce D'Arcus have arguably <a href="http://vocab.org/frbr/core">already done this</a> for FRBR.<br /><br /></p>

<p class="poweredbyperformancing">Update: official <a href="http://web.archive.org/web/20080607150822/http://www.bl.uk:80/services/bibliographic/meeting.html">announcement</a> from the British Library.<br /></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>146</wp:post_id>
		<wp:post_date>2007-05-03 06:21:57</wp:post_date>
		<wp:post_date_gmt>2007-05-03 13:21:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>rdafrbr-and-the-semantic-web</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>26610</wp:comment_id>
			<wp:comment_author><![CDATA[Mikael Nilsson]]></wp:comment_author>
			<wp:comment_author_email>mikael@nilsson.name</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>84.217.161.181</wp:comment_author_IP>
			<wp:comment_date>2007-05-04 06:44:10</wp:comment_date>
			<wp:comment_date_gmt>2007-05-04 13:44:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I just wanted to clarify that the FRBR-in-RDF work is firmly on the radar of this work. Hopefully, we'll get an RDA vocabulary with well-shaped formal relationships with existing, relevant, RDF vocabularies.

/Mikael]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>miscellaneous talk</title>
		<link>http://inkdroid.org/2007/05/17/miscellaneous-talk/</link>
		<pubDate>Thu, 17 May 2007 11:22:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/05/17/miscellaneous-talk/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://www.everythingismiscellaneous.com/images/cover_medsmall.jpg" style="border: medium none ; margin-right: 10px; float: left;" />If you are reading <a href="http://www.amazon.com/Everything-Miscellaneous-Power-Digital-Disorder/dp/0805080430">Everything is Miscellaneous</a> like me then you might be interested in watching <a href="http://www.youtube.com/watch?v=WHeta_YZ0oE">a talk</a> David Weinberger did a few days ago at Google.<br /><br />I only wish I had more time to ingest all the good content that comes in through the <a href="http://video.google.com/videofeed?type=search&amp;q=type%3Agoogle+engEDU&amp;so=1&amp;num=20&amp;output=rss">GoogleTech Talks feed</a>.<br /><br /></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>149</wp:post_id>
		<wp:post_date>2007-05-17 04:22:07</wp:post_date>
		<wp:post_date_gmt>2007-05-17 11:22:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>miscellaneous-talk</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>the weight of legacy data</title>
		<link>http://inkdroid.org/2007/05/20/the-weight-of-legacy-data/</link>
		<pubDate>Sun, 20 May 2007 19:30:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/05/20/the-weight-of-legacy-data/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>v0.97 of <a href="http://search.cpan.org/dist/marc-charset/">MARC::Charset</a> was just released with an important <a href="http://www.mail-archive.com/perl4lib%40perl.org/msg01143.html">bugfix</a>. If you've had the misfortune of needing to convert from <a href="http://www.loc.gov/marc/specifications/speccharmarc8.html">MARC-8</a> to UTF-8 and have used MARC::Charset &gt;= v0.8 to do it you may very well have null characters (0x00) in your UTF-8 data. Well, only if your MARC-8 data contained either of the following characters:</p>

<ul>
    <li>DOUBLE TILDE, SECOND HALF / COMBINING DOUBLE TILDE RIGHT HALF</li>
    <li>LIGATURE, SECOND HALF / COMBINING LIGATURE RIGHT HALF</li>
</ul>

<p>It turns out that the <a href="http://www.loc.gov/marc/specifications/codetables.xml">mapping file</a> kindly provided by the Library of Congress does not include UCS mapping values for these two characters, and instead relies on alternate values.</p>

<p>v0.97 now uses  the alternate value when  the ucs is not available...which is good going forward. But I am literally sad when I think about how this little bug has added to the noise of erroneous extant MARC data. Please accept my humble apologies--and hear my plea to for bibliographic data that starts in Unicode rather than MARC-8. I'll go further:</p>

<p><strong>Don't build systems that import/export MARC in transmission format anymore unless you absolutely have to. </strong></p>

<p>Use MARCXML, MODS, RDF, JSON, YAML or something else instead. I realize this is hardly news but it feels good to be saying it. If you're not convinced read Bill's <a href="http://www.frbr.org/2007/05/01/">Pride and Prejudice</a> installments. The library world needs to use common formats and encodings (with lots of tried/true tool sets)...and stop painting itself into a corner. <a href="http://web.archive.org/web/20071008115318/http://www.niso.org/standards/resources/Z39-2.pdf">Z39.2</a> has been hella useful for building up vast networks of data sharing libraries, but its time to leverage that data in ways that are more familiar to the networked world at large.</p>

<p>Many thanks to Michael O'Connor and Mike Rylander for discovering and resolving this bug.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>150</wp:post_id>
		<wp:post_date>2007-05-20 12:30:19</wp:post_date>
		<wp:post_date_gmt>2007-05-20 19:30:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-weight-of-legacy-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="perl"><![CDATA[perl]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Angela&#039;s dilemma</title>
		<link>http://inkdroid.org/2007/05/31/angelas-dilemma/</link>
		<pubDate>Thu, 31 May 2007 21:17:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/05/31/angelas-dilemma/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>If you are interested in practical ways to garden in the emerging web-of-data take a look at this <a href="http://www.w3.org/2001/tag/doc/httpRange-14/2007-05-31/HttpRange-14">draft finding</a> that folks in the W3C Technical Architecture Group are considering. Or for a different expression of the same idea look at <a href="http://web.archive.org/web/20100531061447/http://www.dfki.uni-kl.de:80/~sauermann/2006/11/cooluris/">Cool URIs for the Semantic Web</a>.</p>

<p>These two documents describe a simple use of HTTP and URLs to identify resources that are outside of the information space of the web.  Yes, you read that right: <em>resources that are outside the information space of the web</em>. Why would I want to use URLs to address resources that aren't on the web!? The <a href="http://www.w3.org/2001/tag/doc/httpRange-14/2007-05-31/HttpRange-14">finding</a> illustrates this subtlety using Angela's dilemma:</p>

<blockquote>Angela is creating an OWL ontology that defines specific characteristics of devices used to access the Web. Some of these characteristics represent physical properties of the device, such as its length, width and weight. As a result, the ontology includes concepts such as unit of measure, and specific instances, such as meter and kilogram. Angela uses URIs to identify these concepts.Having chosen a URI for the concept of the meter, Angela faces the question of what should be returned if that URI is ever dereferenced. There is general advice that owners of URIs should provide representations [AWWW] and Angela is keen to comply. However, the choices of possible representations appear legion. Given that the URI is being used in the context of an OWL ontology, Angela first considers a representation that consists of some RDF triples that allow suitable computer systems to discover more information about the meter. She then worries that these might be less useful to a human user, who might prefer the appropriate Wikipedia entry. Perhaps, she reasons, a better approach would be to create a representation which itself contains a set of URIs to a range of resources that provide related representations. Perhaps content negotiation can help? She could return different representations based on the content type specified in the request.

Angela's dilemma is, of course, based on the fact that none of the representations she is considering are actually representations of the units of measure themselves. Even if the Web could deliver a platinum-iridium bar with two marks a meter apart at zero degrees celsius, or 1,650,763.73 wavelengths of the orange-red emission line in the electromagnetic spectrum of the krypton-86 atom in a vacuum, or even two marks, a meter apart on a screen, such representations are probably less than completely useful in the context of an information space. The representations that Angela is considering are not representations of the meter itself. Instead, they are representations of information resources related to the meter.

It is not appropriate for any of the individual representations that Angela is considering to be returned by dereferencing the URI that identifies the concept of the meter. Not only do the representations she is considering fail to represent the concept of the meter, they each have a different <em>essence</em> and so they should each have their own URI. As a consequence, it would also be inappropriate to use content negotiation as a way to provide them as alternate representations when the URI for the concept of the meter is dereferenced.</blockquote>

<p>So assuming we are agreed about the problem what's the solution? Basically you can use content negotiation and a 303 See Other <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP status code</a> to redirect to the appropriate resource. For an example of the basic idea in action fire up curl and take a look at how <a href="http://ontoworld.org/wiki/">this</a> instance of the <a href="http://meta.wikimedia.org/wiki/Semantic_MediaWiki">SemanticMediaWiki</a> responds to a GET request:</p>

<pre>%  curl --head http://ontoworld.org/wiki/Special:URIResolver/Ruby
HTTP/1.1 303 See Other
Date: Thu, 31 May 2007 20:03:12 GMT
Server: Apache/2.2.3 (Debian) ...
Location: http://ontoworld.org/wiki/Ruby
Content-Type: text/html; charset=UTF-8</pre>

<p>Nothing too surprising there--basically just got redirected to another URL that serves up some friendly HTML describing the Ruby programming language. But send along an extra Accept header:</p>

<pre>% curl --head  --header 'Accept: application/rdf+xml
http://ontoworld.org/wiki/Special:URIResolver/Ruby
HTTP/1.1 303 See Other
Date: Thu, 31 May 2007 20:04:36 GMT
Server: Apache/2.2.3 (Debian) ...
Location: http://ontoworld.org/wiki/Special:ExportRDF/Ruby
Content-Type: text/html; charset=UTF-8</pre>

<p>Notice how you are redirected to another URL that results in rdf/xml describing Ruby coming down the pipe?  RubyOnRails and other frameworks have good REST support built in for doing content negotiation to provide multiple representations of a single information resource. But the use of the 303 See Other here is a new subtle twist to accommodate the fact that the resource in question isn't really a canonical set of bits on disk somewhere. The good news is that your browser will display the human readable resource when you visit <a href="http://ontoworld.org/wiki/Special:URIResolver/Ruby">http://ontoworld.org/wiki/Special:URIResolver/Ruby</a></p>

<p>Some folks would argue that resources that are outside the web don't deserve URLs and should instead be identified with URIs like <a href="http://info-uri.info">info-uris</a> that are not required to resolve.  My personal feeling is that info-uris do have a great deal of use in the enterprise (where they are most likely resolvable). But in situations like Angela's where she is creating a public RDF document that needs to refer to concepts like "length" and "meter" I think it makes sense that these concepts should resolve to appropriate representations that will guide appropriate usage. Or as  the <a href="http://www.w3.org/TR/webarch/#representation-management">Architecture of the World Wide Web</a> puts it:</p>

<blockquote>A URI owner may supply zero or more authoritative representations of the resource identified by that URI. There is a benefit to the community in providing representations. A URI owner SHOULD provide representations of the resource it identifies</blockquote>

<p>It'll be interesting to see how these issues shake out as more and more structured data is made available on the web.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>154</wp:post_id>
		<wp:post_date>2007-05-31 14:17:03</wp:post_date>
		<wp:post_date_gmt>2007-05-31 21:17:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>angelas-dilemma</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_2829a09b58bd83f0842c5bead96ca562</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_90b6cb06f4d6012b56ba0a47f8dd10c0</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>35386</wp:comment_id>
			<wp:comment_author><![CDATA[Bruce D'Arcus]]></wp:comment_author>
			<wp:comment_author_email>bdarcus@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>24.210.249.184</wp:comment_author_IP>
			<wp:comment_date>2007-06-14 17:17:29</wp:comment_date>
			<wp:comment_date_gmt>2007-06-15 00:17:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Funny I should stumble on this while looking for something on RDF and content negotiation :-)

Would be interested in hearing your thoughts on how this might best line up with Zotero server plans, where resource URIs were the subject of some long recent threads on zotero-dev.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>37847</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &raquo; purl2]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org/2007/07/12/purl2/</wp:comment_author_url>
			<wp:comment_author_IP>208.68.173.106</wp:comment_author_IP>
			<wp:comment_date>2007-07-12 06:45:14</wp:comment_date>
			<wp:comment_date_gmt>2007-07-12 13:45:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] It&#8217;s great to see that OCLC is going to work with Zepheira on a new version of the PURL service and that it&#8217;s going to live at Apache. Other than addressing scalability issues it sounds like Zepheira is going to build in support for resources that are outside of the information space of the web: The new PURL software will also be updated to reflect the current understanding of Web architecture as defined by the World Wide Web Consortium (W3C).&nbsp;This new software will provide the ability to permanently identify networked information resources, such as Web documents, as well as non-networked resources such as people, organizations, concepts and scientific data.&nbsp;This capability will represent an important step forward in the adoption of a machine-processable &#8220;Web of data&#8221; enabled by the Semantic Web. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>ruby-zoom v0.3.0</title>
		<link>http://inkdroid.org/2007/07/10/ruby-zoom-v003/</link>
		<pubDate>Tue, 10 Jul 2007 20:41:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/07/10/ruby-zoom-v003/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.rubygems.org"><img border="0" align="left" src="http://web.archive.org/web/20070628193008/http://rubygems.org/images/rubygems-125x125t.png" /></a>Thanks to some prodding from <a href="http://www.miskatonic.org/">William Denton</a> and <a href="http://rubyforge.org/users/jaron/">Jason Ronallo</a> and the kindness of <a href="http://web.archive.org/web/20090618073317/http://chopine.be:80/lrz/">Laurent Sansonetti</a> I've been added as a developer to the <a href="http://rubyforge.org/projects/ruby-zoom/">ruby-zoom</a> project which provides a Ruby wrapper to the <a href="http://www.indexdata.dk/yaz/">yaz </a>Z39.50 library. I essentially wanted to remove some unused code from the project that was interfering with the <a href="http://rubyforge.org/projects/ruby-marc">ruby-marc</a> gem ... and I also wanted to create gem for ruby-zoom. This was the first time I've tried packaging up a C wrapper as a gem and it was remarkably smooth. I also added a test suite and a Rakefile. So assuming you have yaz installed you can install ruby-zoom with:</p>

<pre>% gem install zoom</pre>

<p>I'll admit, I'm no huge fan of <a href="http://www.loc.gov/z3950/agency/">Z39.50 </a>but the fact remains that it's pretty much the most widely deployed machine API for getting at bibliographic data locked up in online catalogs. It's really nice to see forward thinking systems at Talis, Evergreen and Koha who have (or at least experimented with) <a href="http://opensearch.org">OpenSearch</a> implementations.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>155</wp:post_id>
		<wp:post_date>2007-07-10 13:41:04</wp:post_date>
		<wp:post_date_gmt>2007-07-10 20:41:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>ruby-zoom-v003</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>purl2</title>
		<link>http://inkdroid.org/2007/07/12/purl2/</link>
		<pubDate>Thu, 12 Jul 2007 13:45:10 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/07/12/purl2/</guid>
		<description></description>
		<content:encoded><![CDATA[It's great to <a href="http://www.oclc.org/news/releases/200669.htm">see</a> that OCLC is going to work with <a href="http://zepheira.com">Zepheira</a> on a new version of the PURL service and that it's going to have an Apache license. Other than addressing scalability issues it sounds like Zepheira is going to build in support for <a href="http://www.inkdroid.org/2007/05/31/angelas-dilemma/">resources that are outside of the information space of the web</a>:
<blockquote>The new PURL software will also be updated to reflect the current understanding of Web architecture as defined by the World Wide Web Consortium (W3C). This new software will provide the ability to permanently identify networked information resources, such as Web documents, as well as non-networked resources such as people, organizations, concepts and scientific data. This capability will represent an important step forward in the adoption of a machine-processable "Web of data" enabled by the Semantic Web.</blockquote>
Since <a href="http://www.w3.org/People/EM/">Eric Miller</a> helped start up Zepheira it's not surprising that purl2 will take this on. As part of some experiments I've been doing with <a href="http://www.w3.org/2004/02/skos/">SKOS</a>, and serving up Concepts over HTTP it has become clear that a minimal bit of work for managing these identifiers would be useful. I can definitely see the need for a general solution that helps manage identifiers for people, organizations, concepts, etc. which also fits into how HTTP <a href="http://www.w3.org/2001/tag/doc/httpRange-14/2007-05-31/HttpRange-14">should/could</a> serve up the resources associated with them.

via <a href="http://outgoing.typepad.com/outgoing/2007/07/purl2.html">Thom Hickey</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>156</wp:post_id>
		<wp:post_date>2007-07-12 06:45:10</wp:post_date>
		<wp:post_date_gmt>2007-07-12 13:45:10</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>purl2</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How do 26 Nobel Laureates change a light bulb?</title>
		<link>http://inkdroid.org/2007/07/13/how-do-26-nobel-laureates-change-a-light-bulb/</link>
		<pubDate>Fri, 13 Jul 2007 18:38:44 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/07/13/how-do-26-nobel-laureates-change-a-light-bulb/</guid>
		<description></description>
		<content:encoded><![CDATA[I don't know ... but it sure is nice to <a href="https://arl.org/lists/sparc-oaforum/Message/3858.html">see</a> that 26 Nobel Laureates at least understand the direction libraries ought to be headed:
<blockquote>As scientists and Nobel laureates, we are writing to express our strong support for the House and Senate Appropriations Committees’ recent directives to the NIH to enact a mandatory policy that allows public access to published reports of work supported by the agency. We believe that the time is now for Congress to enact this enlightened policy to ensure that the results of research conducted by NIH can be more readily accessed, shared and built upon ­ to maximize the return on our collective investment in science and to further the public good.
...
The public at large also has a significant stake in seeing that this research (researched funded by the National Institute of Health) is made more widely available. When a woman goes online to find what treatment options are available to battle breast cancer, she will find many opinions, but peer-reviewed research of the highest quality often remains behind a high-fee barrier. Families seeking clinical trial updates for a loved one with Huntington's disease search in vain because they do not have a journal subscription. Librarians, physicians, health care workers, students, journalists, and investigators at thousands of academic institutions and companies are currently hindered by unnecessary costs and delays in gaining access to publicly funded research results.</blockquote>
Exciting times for libraries and the medical profession! I just hope they can convince Congress.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>157</wp:post_id>
		<wp:post_date>2007-07-13 11:38:44</wp:post_date>
		<wp:post_date_gmt>2007-07-13 18:38:44</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>how-do-26-nobel-laureates-change-a-light-bulb</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>app and repositories</title>
		<link>http://inkdroid.org/2007/07/16/app-and-repositories/</link>
		<pubDate>Mon, 16 Jul 2007 13:32:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/07/16/app-and-repositories/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://inkdroid.org/talks/app/images/atom.png" style="float: left; margin-right: 10px;" width="150" /><a href="http://efoundations.typepad.com/efoundations/2007/07/putting-them-wi.html">Pete Johnston</a> blogged recently about a very nice use of the <a href="http://tools.ietf.org/wg/atompub/">Atom Publishing Protocol </a>(APP) to provide digital library repository functionality. The project is supported by <a href="http://www.ukoln.ac.uk/">UKOLN</a> at the University of Bath and is called <a href="http://www.ukoln.ac.uk/repositories/digirep/index/SWORD">Simple Web-service Offering Repository Deposit</a> (SWORD). <br /><br />If you are interested in digital repositories and web services take a look at their <a href="http://www.ukoln.ac.uk/repositories/digirep/index/SWORD_APP_Profile_0.4">APP profile</a>. It's a great example of how APP encourages the use of the Atom XML format and RESTful practices, which can then be extended to suit the particular needs of a community of practice.<br /><br />To understand APP you really only need to grok a handful of concepts from the data model and <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">REST</a>. The data model is basically made up of a <i>service </i>document, which describes a set of <i>collections, </i>which aggregates <i>member entries,</i> which can in turn point to a <i>media entry</i>. All of these types of resources are identified with URLs. Since they are URLs you can interact with the objects with plain old HTTP--just like your web browser. For example you can list the <i>entries </i>in a <i>collection </i>by issuing a GET to the <i>collection </i>URL. Or you can create a <i>member resource </i>by doing a POST to the <i>collection </i>URL. Similarly you can delete a <i>member entry</i> by issuing a DELETE to the <i>member</i> <i>entry</i>. The full details are available in the <a href="http://tools.ietf.org/id/draft-ietf-atompub-protocol-17.txt">latest draft</a> of the RFC--and also in a wide variety of articles including <a href="http://www.xml.com/pub/a/2006/07/19/implementing-atom-publishing-protocol-python-wsgi.html">this one</a>.<br /><br />So to perform a SWORD deposit a program would have to:<br /></p>

<ol><li>get the service document for the repository (GET http://www.myrepository.ac.uk/app/servicedocument</li><li>see what collections it can add objects to</li><li>create some IMS, METS or DIDL metadata to describe your repository object and ZIP it up with any of the objects datastreams<br /></li><li>POST the zip file to the appropriate collection URL with the appropriate <i>X-Format-Namespace</i> to identify the format of the submitted object<br /></li><li>check that you got a <i>201 Created</i> status code and record the <i>Location</i> of the newly created resource<br /></li><li>profit!<br /></li></ol>

<p>1 and 2 are perhaps not even necessary if the URL for the target collection is already known. Some notable things about the SWORD profile of APP:<br /></p>

<ul><li>two levels of conformance (one really minimalistic one)</li><li>the idea that collections imply particular <i>treatments</i> or workflows associated with how the object is ingested<br /></li><li>service documents dynamically change to describe only the collections that a particular user can see<br /></li><li>no ability to edit resources</li><li>no ability to delete resources </li><li>no ability to list collections <br /></li><li>repository objects are POSTed as ZIP files to collections<br /></li><li>HTTP Basic Authentication + TLS for security</li><li>the use of DublinCore to describe collections and their respective policies.</li><li>collections can support <i>mediated</i> deposit which means deposits can include the <i>X-On-Behalf-Of</i> HTTP header to identify the user to create the resource for. <br /></li><li>the use of <i>X-Format-Namespace </i>HTTP header to explicitly identify the format of the submission package that is zipped up: for example IMS, METS or DIDL.<br /></li></ul>

<p>While I understand why update and delete would be disabled for deposited packages I don't really understand why the listing of collections would be disabled. An atom feed for a collection would essentially enable harvesting of a repository, much like ListRecords in OAI-PMH.<br /><br />I'm not quite sure I completely understand X-On-Behalf-Of and sword:mediation either. I could understand X-On-Behalf-Of in an environment where there is no authentication. But if a user is authenticated couldn't their username be used to identify who is doing the deposit? Perhaps there are cases (as the doc suggests) where a deposit is done for another user?<br /><br />All in all this is really wonderful work. Of particular value for me was seeing the list of <a href="http://www.ukoln.ac.uk/repositories/digirep/index/SWORD_APP_Profile_0.4#SWORD_Extensions_to_the_APP">SWORD extensions</a> and also the <a href="http://www.ukoln.ac.uk/repositories/digirep/index/SWORD_APP_Profile_0.4#5._Protocol_Operations">use of HTTP status codes.</a> If I have the time I'd like to throw together a sample repository server and client to see just how easy it is to implement SWORD. I did try some <a href="http://code4lib.org/2007/summers">experiments</a> along these lines for my <a href="http://code4lib.org/2007/summers">presentation</a> back in February...but they never got as well defined as SWORD.<br /></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>158</wp:post_id>
		<wp:post_date>2007-07-16 06:32:49</wp:post_date>
		<wp:post_date_gmt>2007-07-16 13:32:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>app-and-repositories</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>38549</wp:comment_id>
			<wp:comment_author><![CDATA[Dorothea Salo]]></wp:comment_author>
			<wp:comment_author_email>dorothea@textartisan.com</wp:comment_author_email>
			<wp:comment_author_url>http://cavlec.yarinareth.net/</wp:comment_author_url>
			<wp:comment_author_IP>128.104.60.80</wp:comment_author_IP>
			<wp:comment_date>2007-07-16 09:42:06</wp:comment_date>
			<wp:comment_date_gmt>2007-07-16 16:42:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Speaking from the trenches... institutional-repository deposits are OFTEN done on behalf of other people. In fact, that's the substantial majority of 'em to date, at least in the States. So, yes, mediated deposit is a big win.

SWORD is amazing. I want it yesterday!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>38755</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan Rochkind]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>128.220.205.186</wp:comment_author_IP>
			<wp:comment_date>2007-07-17 12:31:10</wp:comment_date>
			<wp:comment_date_gmt>2007-07-17 19:31:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA["Perhaps there are cases (as the doc suggests) where a deposit is done for another user?"

Where a mediating application is doing the deposit, and the mediating application does not have access to the user's credentials (and should not for security purposes)?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>39125</wp:comment_id>
			<wp:comment_author><![CDATA[Julie Allinson]]></wp:comment_author>
			<wp:comment_author_email>j.allinson@ukoln.ac.uk</wp:comment_author_email>
			<wp:comment_author_url>http://www.ukoln.ac.uk/repositories/digirep/index/SWORD</wp:comment_author_url>
			<wp:comment_author_IP>138.38.192.65</wp:comment_author_IP>
			<wp:comment_date>2007-07-20 04:46:08</wp:comment_date>
			<wp:comment_date_gmt>2007-07-20 11:46:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed, thanks for the positive comments about our project - good to see growing interest in APP, and SWORD, from the repositories community.  To comment on your questions - there was a pretty strong use case for including mediated deposit - it does happen in practice.  This could support either an authenticated 'person' depositing on behalf of someone else (say a repository manager on behalf of an author) or an authenticated or non-authenticated machine doing the same.  Regarding collection lists, we don't want to mandate against repositories doing this (or, indeed, against them supporting update and delete if they wish to) we just don't have time to implement this in the context of our very small project.  Your comments makes me wonder whether we should find time, though. If you get chance to do any implementation of the profile, I'd be really interested to hear how you got on.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>39129</wp:comment_id>
			<wp:comment_author><![CDATA[Martin Morrey]]></wp:comment_author>
			<wp:comment_author_email>m.morrey@intrallect.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.intrallect.com</wp:comment_author_url>
			<wp:comment_author_IP>12.104.90.162</wp:comment_author_IP>
			<wp:comment_date>2007-07-20 06:43:56</wp:comment_date>
			<wp:comment_date_gmt>2007-07-20 13:43:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yes, the main use-case for mediated deposit, is when a user is working in another environment, such as a Learning Management System (LMS), and chooses to make their materials available to others through a shared repository.  The LMS system would  make the deposit into the repository on behalf of the user, but it is important that the actual owner of the material is identified at the repository end.  The user would either already have an account in the repository system itself, or it would be able to pull their details out of a shared directory of users, such as an LDAP server.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>39138</wp:comment_id>
			<wp:comment_author><![CDATA[Jim Downing]]></wp:comment_author>
			<wp:comment_author_email>ojd20@cam.ac.uk</wp:comment_author_email>
			<wp:comment_author_url>http://wwmm.ch.cam.ac.uk/blogs/downing/</wp:comment_author_url>
			<wp:comment_author_IP>131.111.121.66</wp:comment_author_IP>
			<wp:comment_date>2007-07-20 09:23:01</wp:comment_date>
			<wp:comment_date_gmt>2007-07-20 16:23:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Jonathan; that's exactly the case we're thinking of. 

Ed; the lack of collection listing basically comes down to the fact that there is no consensus between repo softwares on what a "collection" is. APP defines it in a way which is subtly different to each of the current repo platform implementations.

Personally I'm hoping SWORD will encourage a move to more RESTful, web oriented interfaces in repository softwares that will mean a future SWORD spec can be closer to APP.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>39200</wp:comment_id>
			<wp:comment_author><![CDATA[Repository deposits via APP and SWORD &laquo; Humanitarian Relief Deployment Coordination System]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://ryansdissertation.wordpress.com/2007/07/21/repository-deposits-via-app-and-sword/</wp:comment_author_url>
			<wp:comment_author_IP>66.135.48.143</wp:comment_author_IP>
			<wp:comment_date>2007-07-20 19:42:12</wp:comment_date>
			<wp:comment_date_gmt>2007-07-21 02:42:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] APP and Repositories, Inkdroid, July 16, 2007. (Thanks to Charles Bailey.) Excerpt: Pete Johnston blogged recently about a very nice use of the Atom Publishing Protocol (APP) to provide digital library repository functionality. The project is supported by UKOLN at the University of Bath and is called Simple Web-service Offering Repository Deposit (SWORD). [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>pymarc, marc8 and nothingness</title>
		<link>http://inkdroid.org/2007/07/20/pymarc-marc8/</link>
		<pubDate>Fri, 20 Jul 2007 12:51:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/07/20/pymarc-marc8/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.python.org/pypi/pymarc/" title="pymarc">pymarc 1.0</a> went out day before yesterday with a new function: <em>marc8_to_unicode()</em>. When trying to leverage MARC bibliographic data in today's networked world it is inevitable that the MARC8 character encoding will at some point rear its ugly head and make your brain hurt. The problem is that the standard character set tools for various programming languages do not support it. So you need to know to use a specialized tool like marc4j, yaz, MARC::Charset for converting from MARC8 into something useful like Unicode. The MARC8 support in pymarc is the brainchild of <a href="http://www.panix.com/~asl2">Aaron Lav</a> and <a href="http://thesecretmirror.com/">Mark Matienzo</a>. Aaron gave permission for us to package up some of is code from <a href="http://www.panix.com/~asl2/software/PyZ3950/">PyZ3950</a> into pymarc. In testing with equivalent MARC-8 and UTF-8 record batches from the Library of Congress we were able to find and fix a few glitches. The exercise was instructive to me because of my previous experience working with the <a href="http://cpan.org/MARC-Charset">MARC::Charset</a> Perl module. When I wrote MARC::Charset I was overly concerned with <em>not</em> storing the mapping table in memory, I used an on disk Berkeley-DB originally. Aaron's code simply stored the mapping in memory. Since python stores bytecode on disk after compiling there were some performance gains to be had over Perl--since Perl would compile the big mapping hash every time. But the main thing is that Aaron seemed to choose the simplest solution first-- whereas I was busy performing a premature optimization. I also went through some pains to enable mapping not only MARC-8 to Unicode but Unicode back to MARC-8. In hindsight this was a mistake because going back to MARC-8 is increasingly more insane as each day passes. Aaron's code as a result is much cleaner and easier to understand because, well, there's less of it. I'm reading <a href="http://www.oreilly.com/catalog/9780596510046/">Beautiful Code</a> at the moment and was just reading Jon Bentley's chapter "The Most Beautiful Code I Never Wrote" -- which really crystallized things. Definitely check out Beautiful Code if you have a chance. Maybe the quiet <a href="http://web.archive.org/web/20070617063746/http://books.code4lib.org/">books4code</a> could revive to read it as a group?</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>160</wp:post_id>
		<wp:post_date>2007-07-20 05:51:23</wp:post_date>
		<wp:post_date_gmt>2007-07-20 12:51:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pymarc-marc8</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>59652</wp:comment_id>
			<wp:comment_author><![CDATA[Nodalities &raquo; Blog Archive &raquo; This Week&#8217;s Semantic Web]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blogs.talis.com/nodalities/2007/07/this_weeks_semantic_web_3.php</wp:comment_author_url>
			<wp:comment_author_IP>80.86.35.200</wp:comment_author_IP>
			<wp:comment_date>2008-04-17 10:05:39</wp:comment_date>
			<wp:comment_date_gmt>2008-04-17 17:05:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] pymarc, marc8 and nothingness - new function, marc8_to_unicode() [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>39150</wp:comment_id>
			<wp:comment_author><![CDATA[Gabriel Farrell]]></wp:comment_author>
			<wp:comment_author_email>gsf@perfectlygood.net</wp:comment_author_email>
			<wp:comment_author_url>http://perfectlygood.net</wp:comment_author_url>
			<wp:comment_author_IP>129.25.131.205</wp:comment_author_IP>
			<wp:comment_date>2007-07-20 10:58:16</wp:comment_date>
			<wp:comment_date_gmt>2007-07-20 17:58:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great things, Ed (and thanks to Mark and Aaron, too).  I svn'd and tried it out on a dump I had lying around of 105,000 of our records.  Following the pymarc.__doc__, I did:

&gt;&gt;&gt; reader = MARCReader('bibs105000.out')
&gt;&gt;&gt; for record in reader:
...     print record['245']['a']
... 
Traceback (most recent call last):
  File "", line 1, in ?
  File "build/bdist.linux-x86_64/egg/pymarc/reader.py", line 51, in next
ValueError: invalid literal for int(): bibs1
&gt;&gt;&gt; 

But I noticed "self.reader = pymarc.MARCReader(file('test/test.dat'))" in test/reader.py, so I:

&gt;&gt;&gt; marc8_file = file('bibs105000.out')
&gt;&gt;&gt; reader = MARCReader(marc8_file)
&gt;&gt;&gt; for record in reader:
...     print record['245']['a']
... 
Microeconomics :
The multilateral development banks :
The Accountants digest.
Achievement.
ALA bulletin.
Acta arithmetica.
Acta crystallographica.
ASLE transactions.
Acta mathematica.
Acta mechanica.
Acta physica Polonica,
Acta physica Austriaca /
Acta physica Polonica.
Acta polytechnica Scandinavica.
[etc. -- big old list of titles, streaming by, very fast]

So MARCReader expects a file object, not a filename.  Does that doc string need updating or did I misread it?

I then tried to test marc8_to_unicode:

&gt;&gt;&gt; utf8_file = marc8_to_unicode(marc8_file)
Traceback (most recent call last):
  File "", line 1, in ?
  File "build/bdist.linux-x86_64/egg/pymarc/marc8.py", line 8, in marc8_to_unicode
  File "build/bdist.linux-x86_64/egg/pymarc/marc8.py", line 43, in translate
TypeError: len() of unsized object
&gt;&gt;&gt; 

Looking at test/marc8.py I saw marc8_to_unicode expects a string, so I:

&gt;&gt;&gt; marc8_file = file('bibs105000.out')
&gt;&gt;&gt; marc8_file_str = marc8_file.read()
&gt;&gt;&gt; utf8_file_str = marc8_to_unicode(marc8_file_str)
couldn't find 66 69 221 221
couldn't find 66 69 220 220
couldn't find 66 69 221 221
couldn't find 66 69 220 220
couldn't find 66 69 221 221
couldn't find 66 69 221 221
couldn't find 66 69 220 220
couldn't find 66 69 220 220
couldn't find 66 69 221 221
&gt;&gt;&gt; 

This took a long time, running at about 98% of my cpu and 20% of my ~4GB of  memory.  So I timed it (only 3 times because, you know, I'm impatient):

&gt;&gt;&gt; t = Timer(stmt='utf8_file_str = marc8_to_unicode(marc8_file_str)', setup='marc8_file = file("bibs105000.out"); marc8_file_str = marc8_file.read(); from pymarc import marc8_to_unicode')
&gt;&gt;&gt; t.timeit(3)
[a bunch of the "couldn't find 66 69 22[01] 22[01]", three times over]
786.2618350982666

So it took a little while.  How big is the file?

&gt;&gt;&gt; len(marc8_file_str)
93747869
&gt;&gt;&gt; len(utf8_file_str)
87474412

Hmm.  Those should match, right?  Well, let's see if we can read the new file:

&gt;&gt;&gt; pymarc_utf8_file = file('bibs105000_pymarc_utf8.dat', 'w')
&gt;&gt;&gt; pymarc_utf8_file.write(utf8_file_str.encode('utf8'))
&gt;&gt;&gt; reader = MARCReader(file('bibs105000_pymarc_utf8.dat'))
&gt;&gt;&gt; for record in reader:
...     print record['245']['a']
... 
None
Traceback (most recent call last):
  File "", line 1, in ?
  File "build/bdist.linux-x86_64/egg/pymarc/reader.py", line 54, in next
  File "build/bdist.linux-x86_64/egg/pymarc/record.py", line 46, in __init__
  File "build/bdist.linux-x86_64/egg/pymarc/record.py", line 123, in decodeMARC
pymarc.exceptions.BaseAddressInvalid: Base address exceeds size of record
&gt;&gt;&gt; 

Have I done something screwy, or is it something in the file?  I can get the file to you if you want to test on your system.  I also have a utf8 file to compare the output against, produced by `yaz-marcdump -f MARC-8 -t UTF-8 -I bibs105000.out &gt; bibs105000_utf8.out`, and of course the diff is a mile long.  If I get time later I'll test line by line, as done in test/marc8.py.  

Apologies for the long long comment -- should be on some mailing list or something.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>39168</wp:comment_id>
			<wp:comment_author><![CDATA[thesecretmirror.com &raquo; Blog Archive &raquo; When Life Hands You MARC, make pymarc]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://thesecretmirror.com/cataloging/when-life-hands-you-marc-make-pymarc</wp:comment_author_url>
			<wp:comment_author_IP>208.113.155.14</wp:comment_author_IP>
			<wp:comment_date>2007-07-20 15:13:45</wp:comment_date>
			<wp:comment_date_gmt>2007-07-20 22:13:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] I&#8217;m no expert, but I&#8217;m glad that I could help bring pymarc up to version 1.0 and that I&#8217;ve had a chance to begin enjoy programming again. I&#8217;m also glad to see that Catalogablog has spread the word. Download a copy and start hacking; maybe you&#8217;ll be rewarded with rediscovering the joy of code like I was. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>53141</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.49.2.162</wp:comment_author_IP>
			<wp:comment_date>2008-01-30 20:13:32</wp:comment_date>
			<wp:comment_date_gmt>2008-01-31 03:13:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Wow, sorry for the delay gabe, this slipped my radar -- <a href="http://pypi.python.org/pypi/pymarc" rel="nofollow">1.7</a> was just released with some fixes you just recently sent to me in a patch. 

In addition I documented the marc8_to_unicode function more so that it hopefully makes clear you aren't supposed to pass in a serialized marc record, but just a chunk of text extracted from the record that you'd like to translate.

Thanks for the info and the patch!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>rockin&#039; the plastic</title>
		<link>http://inkdroid.org/2007/08/14/rockin-the-plastic/</link>
		<pubDate>Tue, 14 Aug 2007 16:01:33 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/index.php/2007/08/14/rockin-the-plastic/</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>High 5, more dead than alive<br />
Rockin' the plastic like a man from a casket<br />
</blockquote>
<br /><br />Yeah, the blog is back after getting routed by the LinuXploit Crew. The whole episode was really rewarding actually. I learned what projects I work on that need to be hosted elsewhere at more stable locations--that are likely to outlive my pathetic musings. I (re)learned how important good friends are (ksclarke, dchud, wtd, jaf, gabe, rsinger) in a pinch. And I watched in awe as the Wordpress 2.2 upgrade actually worked on my pathologically old  instance (which I suspect to have been the front door). Oh, and it was a good excuse to ditch gentoo for ubuntu.<br /><br />Spring cleaning came a bit late this year I guess. Thanks LinuXploit Crew!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>161</wp:post_id>
		<wp:post_date>2007-08-14 09:01:33</wp:post_date>
		<wp:post_date_gmt>2007-08-14 16:01:33</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>rockin-the-plastic</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>linking open data</title>
		<link>http://inkdroid.org/2007/08/27/linking-open-data/</link>
		<pubDate>Mon, 27 Aug 2007 16:01:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/08/27/linking-open-data/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData"><img src="http://www.inkdroid.org/wp-content/uploads/2007/08/lod.png" border="0" /></a><br />If it isn't already, put the <a href="http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData">Linking Open Data</a> project on your radar. It's a grassroots effort to make large data sets available on the web. These aren't just tarballs sitting in an FTP directory either--they're URL addressable information resources available in machine readable format. A few weeks ago Joshua Tauberer announced the availability of the US Census as close to 1 billion triples. If you like data and the web the <a href="http://simile.mit.edu/mailman/listinfo/linking-open-data">discussion list</a> is a wonderful place to watch these data sets getting released and linked together.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>164</wp:post_id>
		<wp:post_date>2007-08-27 09:01:23</wp:post_date>
		<wp:post_date_gmt>2007-08-27 16:01:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>linking-open-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>42211</wp:comment_id>
			<wp:comment_author><![CDATA[darcusblog &raquo; Blog Archive &raquo; Linked Library Data]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20071013225110/http://netapps.muohio.edu:80/blogs/darcusb/darcusb/archives/2007/09/26/linked-library-data</wp:comment_author_url>
			<wp:comment_author_IP>134.53.7.35</wp:comment_author_IP>
			<wp:comment_date>2007-09-26 10:42:18</wp:comment_date>
			<wp:comment_date_gmt>2007-09-26 17:42:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] data providers like OCLC and the Library of Congress—getting on board the semantic web train. The first is a more high-level goal of the open data movement, complete with nice diagram. The second is a [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>59651</wp:comment_id>
			<wp:comment_author><![CDATA[Nodalities &raquo; Blog Archive &raquo; This Week&#8217;s Semantic Web]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20110806220623/http://blogs.talis.com:80/nodalities/2007/08/this_weeks_semantic_web_7.php</wp:comment_author_url>
			<wp:comment_author_IP>80.86.35.200</wp:comment_author_IP>
			<wp:comment_date>2008-04-17 10:02:39</wp:comment_date>
			<wp:comment_date_gmt>2008-04-17 17:02:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] Inkdroid: &#8220;If it isn’t already, put the Linking Open Data project on your radar. It’s a grassroots effort to make large data sets available on the web. These aren’t just tarballs sitting in an FTP directory either–they’re URL addressable information resources available in machine readable format&#8230;&#8221; [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>marcdb</title>
		<link>http://inkdroid.org/2007/10/01/marcdb/</link>
		<pubDate>Mon, 01 Oct 2007 16:35:28 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/10/01/marcdb/</guid>
		<description></description>
		<content:encoded><![CDATA[If you are a library data wrangler at some point you've probably wanted to stuff MARC data into a relational database so you can do queries across it.  Perhaps your $vendor supports querying like this, but perhaps not. At any rate for some work I've been doing I've really needed to be able to get a feel for a batch of MARC authority data, in particular the data that Simon Spero has kindly made <a href="http://www.ibiblio.org/fred2.0/authorities/">available</a>.  

So I created a little tool I'm calling marcdb which slurps in MARCXML or MARC and stuffs it into a relational database schema. The source for marcdb is <a href="http://inkdroid.org/svn/marcdb/trunk/">available</a> and you can install via the <a href="http://cheeseshop.python.org/pypi/marcdb">python cheeseshop</a> with easy_install if you have it. As you can see from the <a href="http://inkdroid.org/svn/marcdb/trunk/README">README</a> it lets <a href="http://www.sqlalchemy.org/">SQLAlchemy</a> and <a href="http://elixir.ematia.de/">Elixir</a> do the database talkin'. This results in a nice little python <a href="http://inkdroid.org/svn/marcdb/trunk/marcdb/models.py">file</a> that defines the schema in terms of Python classes. You ought to be able to use marcdb with any backend database (mysql, sqlite, postgres) that is supported by SQLAlchemy.

At any rate, the point of all this is to enable querying. So for example after I loaded Simon's authority data I can do a query to see what the lay of the land is in terms of number of tags.

<pre>
SELECT tag, COUNT(*) AS tag_count <br />
FROM data_fields <br />
GROUP BY tag <br />
ORDER BY tag_count DESC;<br /><br /> 

tag | tag_count <br />
-----+-----------<br /> 
035 |    558727<br />
670 |    496600<br />
040 |    379999<br />
010 |    379999<br />
953 |    369625<br />
906 |    272196<br />
550 |    232544<br />
150 |    217556<br />
450 |    211067<br /> 
952 |    185012<br /> 
151 |    158900<br /> 
451 |    143538<br /> 
781 |    122490<br /> 
043 |     92656<br /> 
053 |     92404<br /> 
675 |     42496<br /> 
551 |     24797<br /> 
667 |     14434<br /> 
985 |     13725<br /> 
680 |     10342<br /> 
681 |      8873<br /> 
410 |      7103<br /> 
360 |      4126<br /> 
073 |      3540<br /> 
180 |      3000<br /> 
019 |      1832<br /> 
678 |      1311<br /> 
580 |       857<br /> 
480 |       808<br /> 
260 |       753<br /> 
185 |       501<br /> 
510 |       369<br /> 
485 |       262<br /> 
042 |       260<br /> 
500 |       259<br /> 
016 |       243<br /> 
585 |       192<br /> 
400 |       147<br /> 
682 |       134<br /> 
710 |       132<br /> 
979 |       107<br /> 
530 |        93<br /> 
430 |        82<br /> 
665 |        44<br /> 
182 |        36<br /> 
482 |         8<br /> 
969 |         4<br /> 
181 |         4<br /> 
555 |         4<br /> 
581 |         4<br /> 
455 |         4<br /> 
582 |         3<br /> 
481 |         3<br /> 
052 |         3<br /> 
411 |         2<br /> 
155 |         2<br /> 
751 |         2<br /> 
014 |         2<br /> 
050 |         2<br /> 
856 |         1<br />
</pre>

Or, here's a more complex query for determining the types of <a href="http://www.loc.gov/marc/authority/ecadtref.html#mrcasimp">relationships</a> found in <a href="http://www.loc.gov/marc/authority/ecadalso.html">See Also From Tracing</a> fields.

<pre>
SELECT subfields.value, count(*) AS value_count
FROM data_fields, subfields
WHERE data_fields.tag in ('500', '510', '511', '530', '548', '550', '551',
  '555', '580', '581', '582', '585')
AND data_fields.id = subfields.id
AND subfields.code = 'w'
GROUP BY subfields.value
ORDER BY value_count

 value | value_count 
-------+-------------
 g     |        8438
 nne   |        1243
 nnaa  |        1083
 a     |         146
 b     |         140
 nna   |           8
 bnna  |           4
 anna  |           3
 n     |           2
 nnnd  |           2
 nnnb  |           1
(11 rows)
</pre>

So most of the relations are 'g' which is for broader relations. I know MARC is kind of passé these days, but there's a lot of it around in libraries, and it's important to be able to make decisions about it--especially when converting it to more web-viable formats. I'd be interested in feedback if you get a chance to try it out.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>166</wp:post_id>
		<wp:post_date>2007-10-01 09:35:28</wp:post_date>
		<wp:post_date_gmt>2007-10-01 16:35:28</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>marcdb</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>43317</wp:comment_id>
			<wp:comment_author><![CDATA[Xiaoming Liu]]></wp:comment_author>
			<wp:comment_author_email>xiaoming.liu@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>67.83.29.25</wp:comment_author_IP>
			<wp:comment_date>2007-10-11 10:49:34</wp:comment_date>
			<wp:comment_date_gmt>2007-10-11 17:49:34</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks, this is nice and handy.

Since the goal is to run easy SQL query, perhaps it make sense to bend SQL rule a bit to use less tables and joins? right now the schema using four tables, if it's reduced to one table:

(recordid, tag, position, indicator1,indicator2, subfield, value)

 fol05731351, leader, ..... , 00755cam  22002414a 4500 
 fol05731351,008, ....  000107s2000    nyua          001 0 eng
 fol05731351,245,1,0,1,a, Perl (Computer program language) 
....]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>44619</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &raquo; Blog Archive &raquo; more marcdb]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org/2007/11/05/more-marcdb/</wp:comment_author_url>
			<wp:comment_author_IP>208.68.173.106</wp:comment_author_IP>
			<wp:comment_date>2007-11-05 10:14:27</wp:comment_date>
			<wp:comment_date_gmt>2007-11-05 17:14:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] headings in the LC authority file. You know how it is. Anyhow, I remembered that I&#8217;d used marcdb to import all of Simon Spiro&#8217;s authority data&#8211;so I fired psql and wrote a [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>43437</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.241.132</wp:comment_author_IP>
			<wp:comment_date>2007-10-15 06:08:40</wp:comment_date>
			<wp:comment_date_gmt>2007-10-15 13:08:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I like this idea. Had you thought about what the queries would look like? Flattening like this is typical in data warehousing applications where the emphasis is more on reporting. There would be a lot of duplication of data, but I don't see that as a particular problem. But it might make queries that want to treat the record as a unit a bit problematic. Thanks for the feedback.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>OCLC deserves some REST</title>
		<link>http://inkdroid.org/2007/09/26/oclc-deserves-some-rest/</link>
		<pubDate>Wed, 26 Sep 2007 12:40:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/09/26/oclc-deserves-some-rest/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Hey Worldcat Identities you are doing awesome work--you deserve some <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">REST</a>. Why not use content-negotiation to serve up your HTML and XML representations? So:</p>

<pre>
  curl --header "Accept: text/html" <a href="http://orlabs.oclc.org/Identities/key/lccn-no99-10609">http://orlabs.oclc.org/Identities/key/lccn-no99-10609</a>
</pre>

<p>would return HTML and</p>

<pre>
  curl --header "Accept: application/xml" <a href="http://orlabs.oclc.org/Identities/key/lccn-no99-10609">http://orlabs.oclc.org/Identities/key/lccn-no99-10609</a>
</pre>

<p>would return XML. This would allow you to:</p>

<ul>
  <li>not be limited to XSLT driven user views (doesn't that get tedious?)</li>
  <li>allow you to scale to other sorts of output (application/rdf+xml, etc)</li>
</ul>

<p>At least from the outside I'd have to disagree w/ <a href="http://www.libraryjournal.com/blog/1090000309/post/1630014963.html?nid=3565">Roy</a> -- it appears that <a href="http://web.archive.org/web/20091114013754/http://www.oclc.org:80/research/researchworks">institutions</a> can and do innovate. But I won't say it is easy ...</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>167</wp:post_id>
		<wp:post_date>2007-09-26 05:40:19</wp:post_date>
		<wp:post_date_gmt>2007-09-26 12:40:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oclc-deserves-some-rest</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="worldcat"><![CDATA[worldcat]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>42194</wp:comment_id>
			<wp:comment_author><![CDATA[Jason Ronallo]]></wp:comment_author>
			<wp:comment_author_email>jronallo@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://ronallo.wordpress.com</wp:comment_author_url>
			<wp:comment_author_IP>149.166.220.116</wp:comment_author_IP>
			<wp:comment_date>2007-09-26 07:23:38</wp:comment_date>
			<wp:comment_date_gmt>2007-09-26 14:23:38</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah, I wonder if something like this is in the works since I noticed they did just <a href="http://ronallo.wordpress.com/2007/09/25/worldcat-identities-change/#comment-9" rel="nofollow">recently change to more RESTful URIs</a> for their identity records. At least I hope they go at it full on.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>42195</wp:comment_id>
			<wp:comment_author><![CDATA[Ralph LeVan]]></wp:comment_author>
			<wp:comment_author_email>levan@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.17</wp:comment_author_IP>
			<wp:comment_date>2007-09-26 07:35:21</wp:comment_date>
			<wp:comment_date_gmt>2007-09-26 14:35:21</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The XSLT hasn't been all that tedious, so I'm not particularly worried about delivering HTML.  But, come the day that we want to deliver XML in other schemas, that's exactly the mechanism we'll use.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>groupthink</title>
		<link>http://inkdroid.org/2007/09/26/168/</link>
		<pubDate>Wed, 26 Sep 2007 17:00:44 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/09/26/168/</guid>
		<description></description>
		<content:encoded><![CDATA[This little hack came up in <a href="irc://chat.freenode.net/code4lib">channel</a> after Bruce <a href="http://groups.google.com/group/bibliographic-ontology-specification-group/browse_frm/thread/80937796903dd62b">posted</a> some <a href="http://inkdroid.org/data/identity-foaf.xsl ">XSLT</a> to transform OCLC Identities XML into FOAF.

<pre>
xsltproc \
  http://inkdroid.org/data/identity-foaf.xsl \
  http://orlabs.oclc.org/Identities/lccn-no99-10609 \
  | xmllint --format -
</pre>

!!! 

XSLT has its place to be sure.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>168</wp:post_id>
		<wp:post_date>2007-09-26 10:00:44</wp:post_date>
		<wp:post_date_gmt>2007-09-26 17:00:44</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>168</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"75642";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>42212</wp:comment_id>
			<wp:comment_author><![CDATA[darcusblog &raquo; Blog Archive &raquo; Linked Library Data]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20071013225110/http://netapps.muohio.edu:80/blogs/darcusb/darcusb/archives/2007/09/26/linked-library-data</wp:comment_author_url>
			<wp:comment_author_IP>134.53.7.35</wp:comment_author_IP>
			<wp:comment_date>2007-09-26 10:42:40</wp:comment_date>
			<wp:comment_date_gmt>2007-09-26 17:42:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] The first is a more high-level goal of the open data movement, complete with nice diagram. The second is a much more grounded example of the kind of thing that can make it happen that he and I put [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>75653</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.195</wp:comment_author_IP>
			<wp:comment_date>2008-10-03 07:45:50</wp:comment_date>
			<wp:comment_date_gmt>2008-10-03 14:45:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[thanks danbri! HTTP 301 considered helpful eh?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>75642</wp:comment_id>
			<wp:comment_author><![CDATA[Dan Brickley]]></wp:comment_author>
			<wp:comment_author_email>danbri@danbri.org</wp:comment_author_email>
			<wp:comment_author_url>http://danbri.org/</wp:comment_author_url>
			<wp:comment_author_IP>86.95.110.219</wp:comment_author_IP>
			<wp:comment_date>2008-10-03 03:21:14</wp:comment_date>
			<wp:comment_date_gmt>2008-10-03 10:21:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Update - the OCLC pages have moved now to URLs like http://orlabs.oclc.org/Identities/lccn-no99-10609]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>tools</title>
		<link>http://inkdroid.org/2007/10/18/tools-2/</link>
		<pubDate>Thu, 18 Oct 2007 15:50:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/10/18/tools-2/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://memory.loc.gov/cgi-bin/query/r?ammem/fawbib:@field(DOCID+@lit(apc0042))"><img src="/images/tools.gif" title="Hammer and wood-working tools attributed to Thomas Lincoln" style="margin-right: 10px; float: left; border: none" /></a>
At $work recently many late nights were spent hackety-hacking on a prototype that got <a href="http://www.nytimes.com/2007/10/18/technology/18world.html">written up</a> in the New York Times today. Apart from some <a href="http://www.worlddigitallibrary.org">promotional</a> materials, not much is available to the public just yet. I just got pulled in near the end to do some search stuff. Over the past few months I've seen <a href="http://onebiglibrary.net">dchud</a> in top form managing complicated data/organizational workflows while making technical decisions. A nice outgrowth of working with smarties is ending up with a fun and productive technology stack: <a href="http://python.org">python</a>, <a href="http://www.djangoproject.com/">django</a>, <a href="http://www.postgresql.org/">postgres</a>, <a href="http://jquery.com/">jquery</a>, <a href="http://lucene.apache.org/solr">solr</a>, <a href="http://tilecache.org/">tilecache</a>, <a href="http://www.ubuntu.com/">ubuntu</a>, <a href="http://trac.edgewall.org/">trac</a>, <a href="http://subversion.tigris.org/">subversion</a>, <a href="http://www.vmware.com/">vmware</a>. Given the press and the commitment to UNESCO I think the code is going to start being a bit more than a prototype pretty soon :-)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>170</wp:post_id>
		<wp:post_date>2007-10-18 08:50:41</wp:post_date>
		<wp:post_date_gmt>2007-10-18 15:50:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>tools-2</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>43593</wp:comment_id>
			<wp:comment_author><![CDATA[&#964;&#949;&#967;&#957;&#959;&#963;&#959;&#966;&#953;&#945; &raquo; Blog Archive &raquo; My Left Arm for a Teleporter]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20100106001739/http://lackoftalent.org:80/michael/blog/2007/10/19/my-left-arm-for-a-teleporter/</wp:comment_author_url>
			<wp:comment_author_IP>208.113.188.14</wp:comment_author_IP>
			<wp:comment_date>2007-10-18 23:09:30</wp:comment_date>
			<wp:comment_date_gmt>2007-10-19 06:09:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] one of the big projects my team has been working on got a write-up in the NY Times today (hat tip: Ed). I can&#8217;t wait to jump on board and start wrapping my mind around these projects. I fully [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>44287</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &raquo; Blog Archive &raquo; wdl peeps]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org/2007/10/31/wdl-peeps/</wp:comment_author_url>
			<wp:comment_author_IP>208.68.173.106</wp:comment_author_IP>
			<wp:comment_date>2007-10-31 09:58:05</wp:comment_date>
			<wp:comment_date_gmt>2007-10-31 16:58:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid beep beep      &laquo; tools [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>wdl peeps</title>
		<link>http://inkdroid.org/2007/10/31/wdl-peeps/</link>
		<pubDate>Wed, 31 Oct 2007 16:57:22 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/10/31/wdl-peeps/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://flickr.com/photos/inkdroid/1808726129/"><img src="/images/wdl_team.jpg" style="margin-bottom: 10px; border: none; width:450px;" /></a>Speaking of <a href="http://www.inkdroid.org/2007/10/18/tools-2/">smarties</a> here's a picture of some of the folks I was fortunate to work with on the recent WDL effort. From left to right: Dan Chudnov, Andy Boyko, Babak Hamidzadeh, Dave Hafken, myself, and Chris Thatcher. I feel really fortunate to be working with all of them. The best part is that these are just the folks that were involved with the WDL project--and there are a bunch more equally fun/talented people in our group that are working on other things. I can safely say that I haven't worked with a group before that is as simultaneously top-notch and fun to work with.

<i>...thanks to Michael Neubert for the snapshot taken outside the Adams building at the Library of Congress</i>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>171</wp:post_id>
		<wp:post_date>2007-10-31 09:57:22</wp:post_date>
		<wp:post_date_gmt>2007-10-31 16:57:22</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wdl-peeps</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="people"><![CDATA[people]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>44305</wp:comment_id>
			<wp:comment_author><![CDATA[dchud]]></wp:comment_author>
			<wp:comment_author_email>dchud@umich.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>140.147.241.132</wp:comment_author_IP>
			<wp:comment_date>2007-10-31 14:33:44</wp:comment_date>
			<wp:comment_date_gmt>2007-10-31 21:33:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[erk... talk about yer Mötley Crües...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>good ore</title>
		<link>http://inkdroid.org/2007/11/02/good-ore/</link>
		<pubDate>Fri, 02 Nov 2007 17:45:40 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/11/02/good-ore/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.openarchives.org/pipermail/oai-general/2007-November/000480.html"><img src="/images/ore.png" style="margin-right: 10px; border: none; float: left;"/></a>In case you missed <a href="http://www.openarchives.org/pipermail/oai-general/2007-November/000480.html">it</a> the <a href="http://www.openarchives.org/ore/">Object-Reuse-and-Exchange (ORE) folks</a> are having a get together at Johns Hopkins University (Baltimore, MD) on March 3, 2008. It's free to register, but space is limited. The <a href="http://www.openarchives.org/ore/documents/CompoundObjects-200705.html">Compound information objects whitepaper</a>, <a href="http://www.openarchives.org/ore/documents/OAI-ORE%20TC%20Meeting%20200705_public.pdf">May 2007 Technical Committee notes</a> and the more recent <a href="http://www.ctwatch.org/quarterly/articles/2007/08/interoperability-for-the-discovery-use-and-re-use-of-units-of-scholarly-communication"> Interoperability for the Discovery, Use, and Re-Use of Units of Scholarly Communication</a> provide a good taste of what the beta ORE specs are likely to look like.</p>

<p>The ORE <a href="http://www.openarchives.org/ore/ORE_Community.php">group</a> isn't small, and includes individuals from quite different organizations. So any consensus that can be garnered I think will be quite powerful. Personally I've been really pleased to see how much the ORE work is leaning on <a href="http://www.w3.org/TR/webarch/">web architecture</a>: notably resolvable HTTP URIs, <a href="http://en.wikipedia.org/wiki/Content_negotiation">content-negotiation</a>, <a href="http://www.w3.org/DesignIssues/LinkedData.html">linked-data</a> and <a href="http://www.w3.org/2004/03/trix/">named graphs</a>. Also interesting in the recent announcement is that the initial specs will use <a href="http://www.ietf.org/rfc/rfc4287.txt">RFC 4287</a> for encoding the data model. Who knows, perhaps the spec will rely on <a href="http://www.ietf.org/rfc/rfc5005.txt">archive feeds</a> as <a href="http://www.mail-archive.com/code4lib%40listserv.nd.edu/msg02060.html">discussed </a> recently on the code4lib discussion list.</p>

<p>I'm particularly interested to see what flavor of URIs are used to identify the compound objects:</p>

<blockquote>
The protocol-based URI of the Resource Map identifies an aggregation of resources (components of a compound object) and their boundary-type inter-relationships. While this URI is clearly not the identifier of the compound object itself, it does provide an access point to the Resource Map and its representations that list all the resources of the compound object. For many practical purposes, this protocol-based URI may be a handy mechanism to reference the compound object because of the tight dependency of the visibility of the compound object in web space on the Resource Map (i.e., in ORE terms, a compound object exists in web space if and only if there exists a Resource Map describing it).

We note, however, two subtle points regarding the use of the URI of the Resource Map to reference the compound object. First, doing so is inconsistent with the web architecture and URI guidelines that are explicit in their suggestion that a URI should identify a single resource. Strictly interpreted, then, the use of the URI of the Resource Map to identify both the Resource Map and the compound object that it describes is incorrect. Second, some existing information systems already use dedicated URIs for the identification of compound information objects “as a whole.” For example, many scholarly publishers use DOIs whereas the Fedora and aDORe repositories have adopted identifiers of the info URI scheme. These identifiers are explicitly distinct from the URI of the Resource Map. <i>from: <a href="http://www.ctwatch.org/quarterly/articles/2007/08/interoperability-for-the-discovery-use-and-re-use-of-units-of-scholarly-communication/6/"> Interoperability for the Discovery, Use, and Re-Use of Units of Scholarly Communication</a></i>
</blockquote>

<p>I understand the ORE group is intentionally not aligning themselves too closely with the semantic web community. However I think they need to consider whether <i>compound information objects</i> are WWW <i>information resources</i> or not:</p>

<blockquote>
By design a URI identifies one resource. We do not limit the scope of what might be a resource. The term "resource" is used in a general sense for whatever might be identified by a URI. It is conventional on the hypertext Web to describe Web pages, images, product catalogs, etc. as “resources”. The distinguishing characteristic of these resources is that all of their essential characteristics can be conveyed in a message. We identify this set as “information resources.” <i>(<a href="http://www.w3.org/TR/webarch/#id-resources">from Architecture of the World Wide Web vol. 1</a>).
</i></blockquote>

<p>I'm not totally convinced that the resource map can't serve as a suitable representation for the compound information object--however for the sake of argument lets say I am. It seems to me that the URI for the compound information object identifies the <i>concept</i> of a particular compound information object, which lies in various pieces on the network. However this doesn't preclude the use of HTTP URLs to identify the compound objects.  Indeed the <a href="http://www.w3.org/DesignIssues/HTTP-URI2">What HTTP URIs identify</a> and <a href="http://web.archive.org/web/20100531061447/http://www.dfki.uni-kl.de:80/~sauermann/2006/11/cooluris/">Cool URIs for the Semantic Web</a> provide specific guidance on how to serve up these non-information resources. Of course philosophical arguments around httpRange-14 have raged for a while. But the <a href="http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData">Linking Open Data</a> project is using the hash URI and 303 redirect very effectively. There has even been some work on a <a href="http://docs.google.com/View?docid=ajch7tkjqjwz_23g8dfzc&pli=1">sitemap extension</a> to enable crawling. As a practical matter using URLs to identify compound information objects will encourage their use because they will naturally find their ways into publications, blogs, other compound objects. Using non-resolvable or quasi-resolvable info-uris or dois will mean people just won't create the links--and when they do they will create links that can't be easily verified and evolved over time with standard web tools. The OAI-ORE effort represents a giant leap forward for the digital library community into the web. Here's to hoping they land safely--we need this stuff.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>172</wp:post_id>
		<wp:post_date>2007-11-02 10:45:40</wp:post_date>
		<wp:post_date_gmt>2007-11-02 17:45:40</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>good-ore</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oai-ore"><![CDATA[oai-ore]]></category>
		<category domain="post_tag" nicename="oai-pmh"><![CDATA[oai-pmh]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="post_tag" nicename="w3c"><![CDATA[w3c]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>more marcdb</title>
		<link>http://inkdroid.org/2007/11/05/more-marcdb/</link>
		<pubDate>Mon, 05 Nov 2007 17:13:44 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/11/05/more-marcdb/</guid>
		<description></description>
		<content:encoded><![CDATA[This morning Clay and I were chatting about <a href="http://en.wikipedia.org/wiki/Library_of_Congress_Subject_Headings">Library of Congress Subject Heading</a>s and <a href="http://www.w3.org/2004/02/skos/">SKOS</a> a bit. At one point we found ourselves musing about how much reuse there is of topical subdivisions in topical headings in the LC  authority file. You know how it is. Anyhow, I remembered that I'd used <a href="http://www.inkdroid.org/2007/10/01/marcdb/">marcdb</a> to import all of Simon Spiro's <a href="http://www.ibiblio.org/fred2.0/authorities/">authority data</a>--so I fired up <a href="http://www.postgresql.org/docs/8.1/static/app-psql.html">psql</a> and wrote a query:

<pre>
SELECT subfields.value AS subdivision, count(*) AS total
FROM subfields, data_fields
WHERE subfields.code = 'x'
  AND subfields.data_field_id = data_fields.id
  AND data_fields.tag = '150'
GROUP BY subfields.value
ORDER BY total DESC;
</pre>

And a few seconds later...

<pre>
 subdivision                          | total  
--------------------------------------+-------
 Law and legislation                  |  3342
 Religious aspects                    |  2500
 Buddhism, [Christianity, etc.]       |   898
 History                              |   847
 Equipment and supplies               |   571
 Taxation                             |   566
 Baptists, [Catholic Church, etc.]    |   476
 Diseases                             |   450
 Research                             |   422
 Campaigns                            |   378
 Awards                               |   342
 Finance                              |   284
 Study and teaching                   |   284
 Surgery                              |   275
 Employees                            |   269
 Spectra                              |   261
 Computer programs                    |   259
 Labor unions                         |   218
 Testing                              |   207
 Diagnosis                            |   194
 Isotopes                             |   190
 Complications                        |   183
 Physiological effect                 |   172
 Programming                          |   163
</pre>

There's nothin' like the smell of strong set theory in the morning. Although something seems a bit fishy about <em>[Christianity, etc.]</em> and <em>[Catholic Church, etc.]</em>... If you want to try similar stuff and don't want to wait hours for marcdb to import all the data and you use postgres, here's the full database <a href="http://inkdroid.org/data/authorities.sql.bz2">dump</a> which you ought to be able to import:

<pre>
  % createdb authorities
  % wget http://inkdroid.org/data/authorities.sql.bz2
  % bunzip2 authorities.sql.bz2
  % psql authorities &lt; authorities.sql
</pre>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>173</wp:post_id>
		<wp:post_date>2007-11-05 10:13:44</wp:post_date>
		<wp:post_date_gmt>2007-11-05 17:13:44</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>more-marcdb</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="post_tag" nicename="marc-metadata-postgres-lcsh"><![CDATA[marc metadata postgres lcsh]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>and now</title>
		<link>http://inkdroid.org/2007/11/06/and-now/</link>
		<pubDate>Wed, 07 Nov 2007 05:19:08 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/11/06/and-now/</guid>
		<description></description>
		<content:encoded><![CDATA[for something completely different ....

<object width="425" height="355"><param name="movie" value="http://www.youtube.com/v/QW2F-_1lYvM"></param><param name="wmode" value="transparent"></param><embed src="http://www.youtube.com/v/QW2F-_1lYvM" type="application/x-shockwave-flash" wmode="transparent" width="425" height="355"></embed></object>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>175</wp:post_id>
		<wp:post_date>2007-11-06 22:19:08</wp:post_date>
		<wp:post_date_gmt>2007-11-07 05:19:08</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>and-now</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<category domain="post_tag" nicename="music-video-life-art"><![CDATA[music video life art]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>44839</wp:comment_id>
			<wp:comment_author><![CDATA[Claudia]]></wp:comment_author>
			<wp:comment_author_email>clmccowan@mac.com</wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20080613200844/http://web.mac.com:80/jeffmee/McMeeans/Home.html</wp:comment_author_url>
			<wp:comment_author_IP>76.170.70.50</wp:comment_author_IP>
			<wp:comment_date>2007-11-09 20:41:43</wp:comment_date>
			<wp:comment_date_gmt>2007-11-10 03:41:43</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Delightful. Depressing as hell, but delightful. Which, I suppose, is the point.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>w00t!</title>
		<link>http://inkdroid.org/2007/12/12/w00t/</link>
		<pubDate>Wed, 12 Dec 2007 18:14:39 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/12/12/w00t/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.reuters.com/article/internetNews/idUSN1155159520071212">w00t!</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>176</wp:post_id>
		<wp:post_date>2007-12-12 11:14:39</wp:post_date>
		<wp:post_date_gmt>2007-12-12 18:14:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>w00t</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>47352</wp:comment_id>
			<wp:comment_author><![CDATA[Leo Klein]]></wp:comment_author>
			<wp:comment_author_email>leo@leoklein.com</wp:comment_author_email>
			<wp:comment_author_url>http://ChicagoLibrarian.com</wp:comment_author_url>
			<wp:comment_author_IP>76.197.214.222</wp:comment_author_IP>
			<wp:comment_date>2007-12-12 19:30:36</wp:comment_date>
			<wp:comment_date_gmt>2007-12-13 02:30:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Personally, when I find someone using that frightening cliche in a sentence, I tend to break off communication and bail out.  It's a health issue.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>47317</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Murray]]></wp:comment_author>
			<wp:comment_author_email>peter@OhioLINK.edu</wp:comment_author_email>
			<wp:comment_author_url>http://dltj.org/</wp:comment_author_url>
			<wp:comment_author_IP>192.153.30.22</wp:comment_author_IP>
			<wp:comment_date>2007-12-12 11:34:31</wp:comment_date>
			<wp:comment_date_gmt>2007-12-12 18:34:31</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Please.

Actually, come to think of it, there is a lot of entertainment value to be had from the anticipated Gorman-esque responses.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>47528</wp:comment_id>
			<wp:comment_author><![CDATA[Edward Vielmetti]]></wp:comment_author>
			<wp:comment_author_email>edward.vielmetti@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://vielmetti.typepad.com</wp:comment_author_url>
			<wp:comment_author_IP>68.84.161.65</wp:comment_author_IP>
			<wp:comment_date>2007-12-15 18:46:10</wp:comment_date>
			<wp:comment_date_gmt>2007-12-16 01:46:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[i saw a version of the lord's prayer in l33t speak, let me dig it up]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>permalinks reloaded</title>
		<link>http://inkdroid.org/2007/12/17/permalinks-reloaded/</link>
		<pubDate>Mon, 17 Dec 2007 21:17:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/12/17/permalinks-reloaded/</guid>
		<description></description>
		<content:encoded><![CDATA[The recently <a href="http://www.dancohen.org/2007/12/12/zotero-and-the-internet-archive-join-forces/">announced</a> <a href="http://zotero.org">Zotero</a> / <a href="http://archive.org">InternetArchive</a> partnership is exciting on a bunch of levels. The one that immediately struck me was the use of the Internet Archive URI. As you may have noticed before all the content in Internet Archive <a href="http://web.archive.org">Wayback Machine</a> can be referenced with a URL that looks something like:

<ul>
<li>http://web.archive.org/web/{yyyymmddhhmmss}/{url}</li>
</ul>

Where url is the document URL you want to look up in the archive at the given time. So for example:

<ul>
<li><a href="http://web.archive.org/web/19981202230410/http://www.google.com/">http://web.archive.org/web/19981202230410/http://www.google.com/</a></li>
</ul>

is a URL for what http://google.com looked like on December 02, 1998 at 23:04:10. Perhaps this is documented somewhere prominent or is common knowledge, but it looks like you can play with the timestamp, and archive.org will adjust as needed, redirecting you to the closest snapshot it can find:

<ul>
<li><a href="http://web.archive.org/web/19981202/http://www.google.com/">http://web.archive.org/web/19981202/http://www.google.com/</a></li>
<li><a href="http://web.archive.org/web/199812/http://www.google.com/">http://web.archive.org/web/199812/http://www.google.com/</a></li>
<li><a href="http://web.archive.org/web/1998/http://www.google.com/">http://web.archive.org/web/1998/http://www.google.com/</a></li>
</ul>

and even:

<ul>
<li><a href="http://web.archive.org/web/http://www.google.com/">http://web.archive.org/web/http://www.google.com/</a></li>
</ul>

which redirects to the most recent content for a given URL. It's just a good old 302 at work:

<pre>
ed@curry:~$ curl -I http://web.archive.org/web/199812/http://www.google.com/
HTTP/1.1 302 Found
Date: Mon, 17 Dec 2007 21:11:12 GMT
Server: Apache/2.0.54 (Ubuntu) PHP/5.0.5-2ubuntu1.2 mod_ssl/2.0.54 OpenSSL/0.9.7g mod_perl/2.0.1 Perl/v5.8.7
Location: http://web.archive.org/web/19981202230410/www.google.com/
Content-Type: text/html; charset=iso-8859-1
</pre>

So anyhow, pretty cool use of URIs and HTTP right? The addition of zotero to the mix will mean that scholars can cite the web as it appeared at a particular point in time:

<blockquote>
... as scholars begin to use not only traditional primary sources that have been digitized but also “born digital” materials on the web (blogs, online essays, documents transcribed into HTML), the possibility arises for Zotero users to leverage the resources of IA to ensure a more reliable form of scholarly communication. One of the Internet Archive’s great strengths is that it has not only archived the web but also given each page a permanent URI that includes a time and date stamp in addition to the URL.

Currently when a scholar using Zotero wishes to save a web page for their research they simply store a local copy. For some, perhaps many, purposes this is fine. But for web documents that a scholar believes will be important to share, cite, or collaboratively annotate (e.g., among a group of coauthors of an article or book) we will provide a second option in the Zotero web save function to grab a permanent copy and URI from IA’s web archive. A scholar who shares this item in their library can then be sure that all others who choose to use it will be referring to the exact same document.
</blockquote>

This is pretty fundamental to scholarship on the web. Of course when generating a time anchored permalink with zotero one can well expect that archive.org will on occasion not have a snapshot of said content, resulting in a 404. It would be great if archive.org could leverage these requests for snapshots as requests to go out and archive the page. One could imagine a blocking and nonblocking request: the former which would spawn a request to fetch a particular URI, stash content away, and return the permalink; and the latter which would just quickly return the best match its already got (which may be a 404).

Anyhow, it's really good to see these two outfits working together. Nice work! 

<i>ps. dear lazyweb is there a documented archive.org api available?</i>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>177</wp:post_id>
		<wp:post_date>2007-12-17 14:17:07</wp:post_date>
		<wp:post_date_gmt>2007-12-17 21:17:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>permalinks-reloaded</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>59655</wp:comment_id>
			<wp:comment_author><![CDATA[Nodalities &raquo; Blog Archive &raquo; This Week&#8217;s Semantic Web]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20100416052007/http://blogs.talis.com:80/nodalities/2008/01/this_weeks_semantic_web_25.php</wp:comment_author_url>
			<wp:comment_author_IP>80.86.35.200</wp:comment_author_IP>
			<wp:comment_date>2008-04-17 10:09:12</wp:comment_date>
			<wp:comment_date_gmt>2008-04-17 17:09:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] permalinks reloaded [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>47731</wp:comment_id>
			<wp:comment_author><![CDATA[Andy Boyko]]></wp:comment_author>
			<wp:comment_author_email>andy@boyko.net</wp:comment_author_email>
			<wp:comment_author_url>http://andy.boyko.net/</wp:comment_author_url>
			<wp:comment_author_IP>216.164.33.8</wp:comment_author_IP>
			<wp:comment_date>2007-12-17 20:02:15</wp:comment_date>
			<wp:comment_date_gmt>2007-12-18 03:02:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[My understanding from discussing this with our Zoteronian mutual <a>neighbor</a> is that not only will this allow a Zotero user to point at an already-archived instance of a site @ IA, but also will support Zotero's capture of a site being sent up to IA, in a sort of micro-harvesting for shared use.  See also the wise Britons at <a href="http://hanzoweb.com/" rel="nofollow">HanzoWeb</a> who offer a similar sort of social web-capture service (although it's not answering the phone at this moment?)

API-wise, there's not much to the Wayback Machine (the software underneath the IA web archive)  because there isn't much else that it does -- it just retrieves items by URL and date.   You might see <a href="http://archive-access.sourceforge.net/projects/wayback/administrator_manual.html#Setting%20up%20the%20Replay%20User%20Interface%20within%20an%20AccessPoint" rel="nofollow">the docs</a> for the new <a href="http://archive-access.sourceforge.net/projects/wayback/" rel="nofollow">open-source Java wayback</a> (which is, in theory, a drop-in replacement for the older Perl-based Wayback that I think still runs the main archive today).  You hit on most of the fuzzy-date-matching thing, but there's also wildcards in the date and the URL, as in:

http://web.archive.org/web/*/http://www.google.com/

which shows a 'calendar view' of all known captures for the URL; then, there's wildcarding the URL suffix, as in:

http://web.archive.org/web/*/inkdroid.org/*

which shows all the pages from the site at all.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>metadata hackers</title>
		<link>http://inkdroid.org/2007/12/31/metadata-hackers/</link>
		<pubDate>Mon, 31 Dec 2007 14:42:45 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2007/12/31/metadata-hackers/</guid>
		<description></description>
		<content:encoded><![CDATA[I opened the paper this morning to read a <a href="http://www.washingtonpost.com/wp-dyn/content/article/2007/12/30/AR2007123002435.html">story </a> of another person involved in the creation of MARC who has just died. I hadn't realized before reading <a href="http://www.washingtonpost.com/wp-dyn/content/article/2006/04/27/AR2006042702105.html">Henrietta Avram</a> and <a href="http://www.washingtonpost.com/wp-dyn/content/article/2007/12/30/AR2007123002435.html">Samuel Snyder's</a> obituaries that there was a bit of an <a href="http://nsa.gov">NSA</a> <a href="http://loc.gov">LC</a> connection when MARC was being created.


<blockquote>
From 1964 to 1966, [Samuel Snyder] was coordinator of the Library of Congress's information systems office. He was among the creators of the library's Machine Readable Cataloging system that replaced the handwritten card with an electronic searchable database system that became the standard worldwide.
</blockquote>

I imagine NSA folks had a lot to do with early automation efforts in the federal government...but it's still an interesting connection. One of my <a href="http://onebiglibrary.net">coworkers</a> is reading up on this early history of MARC so this is for him in the unlikely event that he missed it...email would probably have worked better I guess, but I also wanted to pay tribute. Libraries wouldn't be what they are today without this influential early work.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>179</wp:post_id>
		<wp:post_date>2007-12-31 07:42:45</wp:post_date>
		<wp:post_date_gmt>2007-12-31 14:42:45</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>metadata-hackers</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="lc"><![CDATA[lc]]></category>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="nsa"><![CDATA[nsa]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="standards"><![CDATA[standards]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"49644";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>49644</wp:comment_id>
			<wp:comment_author><![CDATA[Dan Chudnov]]></wp:comment_author>
			<wp:comment_author_email>dchud@umich.edu</wp:comment_author_email>
			<wp:comment_author_url>http://claimid.com/dchud</wp:comment_author_url>
			<wp:comment_author_IP>71.127.61.132</wp:comment_author_IP>
			<wp:comment_date>2007-12-31 09:42:16</wp:comment_date>
			<wp:comment_date_gmt>2007-12-31 16:42:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[We saw that in the paper today, too.  Just imagine how many times the thought "this changes everything" came into the minds of librarians in the 1960s... keeps you humble, eh.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>following your nose to the web of data</title>
		<link>http://inkdroid.org/2008/01/04/following-your-nose-to-the-web-of-data/</link>
		<pubDate>Fri, 04 Jan 2008 15:57:33 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2008/01/04/following-your-nose-to-the-web-of-data/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>This is a draft of a column that's slated to be published some time in <a href="http://www.niso.org/standards/std_resources.html">Information Standards Quarterly</a>. <a href="http://www.bookism.org/open/">Jay</a> was kind enough to let me post it here in this form before it goes to press. It seems <a href="http://onebiglibrary.net/story/will-i-need-to-understand-the-semantic-web-in-2008">timely</a> to put it out there. Please feel free to leave comments to point out inaccuracies, errors, tips, suggestions, etc.</em></p>

<hr />

<p><a href="http://en.wikipedia.org/wiki/Image:WWWlogo.png"><img src="http://upload.wikimedia.org/wikipedia/commons/2/25/WWWlogo.png" style="float: left; border: none; margin-right: 10px;"/></a>
</p>

<p>It&#8217;s hard to imagine today that in 1991 the entire World Wide Web existed on a single server at CERN in Switzerland. By the end of that year the first web server outside of Europe was <a href="http://www.w3.org/History.html">set up</a> at Stanford. The <a href="http://ksi.cpsc.ucalgary.ca/archives/WWW-TALK/www-talk-1991.index.html">archives</a> of the www-talk discussion list bear witness to the grassroots community effort that grew the early web&#8211;one document and one server at a time.</p>

<p>Fast forward to 2007 when 24.7 billion web pages are <a href="http://www.worldwidewebsize.com/">estimated</a> to exist. The rapid and continued growth of the Web of Documents can partly be attributed to the elegant simplicity of the hypertext link enabled by two of Tim Berners-Lee's creations: the HyperText Markup Language (HTML) and the Uniform Resource Locator (URL). There is a similar movement afoot today to build a new kind of web using this same linking technology, the so called <a href="http://en.wikipedia.org/wiki/Linked_Data">Web of Data</a>.

</p>

<p>The Web of Data has its beginnings in the vision of a Semantic Web <a href="http://www.sciam.com/article.cfm?articleID=00048144-10D2-1C70-84A9809EC588EF21">articulated</a> by Tim Berners-Lee in 2001. The basic idea of the Semantic Web is to enable intelligent machine agents by augmenting the web of HTML documents with a web of machine processable information. A recent follow up <a href="http://www.sciam.com/article.cfm?id=the-semantic-web-in-action">article</a> covers the "layer cake" of standards that have been created since, and how they are being successfully used today to enable data integration in research, government, and business. However the repositories of data associated with these success stories are largely found behind closed doors. As a result there is little large scale integration happening across organizational boundries on the World Wide Web.</p>

<p>The Web of Data represents a distillation and simplification of the Semantic Web vision. It de-emphasizes the automated reasoning aspects of Semantic Web research and focuses instead on the actual linking of data across organizational boundaries. To make things even simpler the linking mechanism relies on already deployed web technologies: the HyperText Transfer Protocol (HTTP), Uniform Resource Identifiers (URI), and Resource Description Framework (RDF).  Tim Berners-Lee has called this technique Linked Data, and <a href="http://www.w3.org/DesignIssues/LinkedData.html">summarized</a> it as a short set of guidelines for publishing data on the web:</p>

<ol>
<li>Use URIs as names for things.</li>
<li>Use HTTP URIs so that people can look up those things.</li>
<li>When someone looks up a URI, provide useful information.</li>
<li>Include links to other URIs, so that they can discover more things.</li>
</ol>

<p>
The <a href="http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData">Linking Open Data</a> community project of the <a href="http://www.w3.org/2001/sw/sweo/">W3C Semantic Web Education and Outreach Group</a> has published two additional documents <a href="http://web.archive.org/web/20100531061447/http://www.dfki.uni-kl.de:80/~sauermann/2006/11/cooluris/">Cool URIs for the Semantic Web</a> and <a href="http://sites.wiwiss.fu-berlin.de/suhl/bizer/pub/LinkedDataTutorial/">How to Publish Linked Data on the Web</a> that help IT professionals understand what it means to publish their assets as linked data.  The goal of the Linking Open Data Project is to</p>

<blockquote>
extend the Web with a data commons by publishing various open datasets as RDF on the Web and by setting RDF links between data items from different sources.
</blockquote>

<p>Central to the Linked Data concept is the publication of RDF on the World Wide Web. The essence of RDF is the "triple" which is a statement about a resource in three parts: a subject, predicate and object. The RDF triple provides a way of modeling statements about resources and it can have multiple serialization formats including XML and some more human readable formats such as <a href="http://www.w3.org/DesignIssues/Notation3">notation3</a>. For example to represent a statement that the website at http://niso.org has the title "NISO - National Information Standards Organization" one can create the following triple:</p>

<pre>
<code>
&lt;http://niso.org&gt; &lt;http://purl.org/dc/elements/1.1/title&gt; "NISO - National Information Standards Organization" .
</code>
</pre>

<p>
The subject is the URL for the website, the predicate is "has title" represented as a URI from the Dublin Core vocabulary, and the object is the literal "NISO - National Information Standards Organization". The Linked Data movement encourages the extensive interlinking of your data with other people's data: so for example by creating another triple such as:
</p>

<pre>
<code>
&lt;http://niso.org&gt; &lt;http://purl.org/dc/elements/1.1/creator&gt; &lt;http://dbpedia.org/resource/National_Information_Standards_Organization&gt; .
</code>
</pre>

<p>
This indicates that the website was created by NISO which is identified using URI from the dbpedia (a Linked Data version of the Wikipedia). One of the benefits of linking data in this way is the "follow your nose" effect.  When a person in their browser or an automated agent runs across the creator in the above triple they are able to dereference the URL and retrieve  more information about this creator. For example when a software agent dereferences a URL for NISO
</p>

<pre>
<code>
http://dbpedia.org/resource/National_Information_Standards_Organization
</code>
</pre>

<p>
24 additional RDF triples are returned including one like:
</p>

<pre>
<code>
&lt;http://dbpedia.org/resource/National_Information_Standards_Organization&gt; &lt;http://www.w3.org/2004/02/skos/core#subject&gt; &lt;http://dbpedia.org/resource/Category:Standards_organizations&gt; .
</code>
</pre>

<p>
This triple says that NISO belongs to a class of resources that are standards organizations. A human or agent can follow their nose to the dbpedia URL for standards organizations:
</p>

<pre>
<code>
http://dbpedia.org/resource/Category:Standards_organizations
</code>
</pre>

<p>
and retrieve 156 triples describing other standards organizations are returned such as:
</p>

<pre>
<code>
&lt;http://dbpedia.org/resource/World_Wide_Web_Consortium&gt; &lt;http://www.w4.org/2004/02/skos/core#subject&gt; &lt;http://dbpedia.org/resource/Category:Standards_organizations&gt; .
</code>
</pre>

<p>
And so on. This ability for humans and automated crawlers to follow their noses in this way makes for a powerfully simple data discovery heuristic. The philosophy is quite different from other data discovery methods, such as the typical web2.0 APIs of Flickr, Amazon, YouTube, Facebook, Google, etc., which all differ in their implementation details and require you to digest their API documentation before you can do anything useful. Contrast this with the Web of Data which uses the ubiquitous technologies of URIs and HTTP plus the secret sauce of the RDF triple.
</p>

<p>
As with the initial growth of the web over 10 years ago the creation of the Web of Data is happening at a grassroots level by individuals around the world. Much of the work takes place on an open <a href="http://simile.mit.edu/mailman/listinfo/linking-open-data">discussion list</a> at MIT where people share their experiences of making data sets available,  discuss technical problems/solutions, and announce the availability of resources. At this time some 27 different data sets have been published including Wikipedia, the US Census, the CIA World Fact Book, Geonames, MusicBrainz, WordNet, OpenCyc. The data and relationships between the data are by definition distributed around the web and harvestable by anyone by anyone with a web browser or HTTP client. Contrast this openness with the relationships that Google extracts from the Web of Documents and locks up on their own private network.
</p>

<p>
Various services aggregate Linked Data and provide services on top of it such as <a href="http://dbpedia.org">dbpedia</a> which has an estimated 3 million RDF links, and over 2 billion RDF triples. It's quite possible that the emerging set of Linked Data will serve as a data test bed for intiatives like the <a href="http://web.archive.org/web/20090630042349/http://www.mindswap.org:80/blog/2007/12/05/announcing-the-open-web-billion-triple-challenge-iswc-08">Billion Triple Challenge</a> which aims to foster creative approaches to data mining and Semantic Web research by making large sets of real data available. In much the same way that Tim Berners-Lee could not have predicted the impact of Google's PageRank algorithm, or the improbable success of Wikipedia's collaborative editing while creating the Web of Documents, it may be that simply building links between data sets on the Web of Data will bootstrap a new class of technologies we cannot begin to imagine today.
</p>

<p><a href="http://richard.cyganiak.de/2007/10/lod/lod-datasets_2007-11-10.png"><img src="http://richard.cyganiak.de/2007/10/lod/lod-datasets_2007-11-10.png" border="0" /></a></p>

<p>
So if you are in the business of making data available on the web and have a bit more time to spare, have a look at Tim Berners-Lee's <a href="http://www.w3.org/DesignIssues/LinkedData.html">Linked Data</a> document and familiarize yourself with the simple web publishing techniques behind the Web of Data: HTTP, URI and RDF. If you catch the Linked Data bug join the <a href="http://simile.mit.edu/mailman/listinfo/linking-open-data">discussion list</a> and the conversation, and try publishing some of your data as a pilot project using the tutorials. Who knows what might happen--you might just help build a new kind of web, and rest assured you'll definitely have some fun.
</p>

<p>Thanks to <a href="http://f00die.com/">Jay Luker</a>, <a href="http://paulmiller.typepad.com/">Paul Miller</a>, <a href="http://dannyayers.com">Danny Ayers</a> and <a href="http://onebiglibrary.net">Dan Chudnov</a> for their contributions and suggestions.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>180</wp:post_id>
		<wp:post_date>2008-01-04 08:57:33</wp:post_date>
		<wp:post_date_gmt>2008-01-04 15:57:33</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>following-your-nose-to-the-web-of-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="uri"><![CDATA[uri]]></category>
		<category domain="post_tag" nicename="url"><![CDATA[url]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_753bcc8a69771324bca68aa4b54c9662</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_2aaf3d6ea6eb446fe5541ab999799fab</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_2d033bbcf8df45efa6520b8e3e183368</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_c46937825a9927743bffffa17ca635d5</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>64893</wp:comment_id>
			<wp:comment_author><![CDATA[Lost in Knowledge]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://lostinknowledge.org/post/interesting-overview-about-how-it-works/</wp:comment_author_url>
			<wp:comment_author_IP>69.57.168.194</wp:comment_author_IP>
			<wp:comment_date>2008-06-12 10:53:09</wp:comment_date>
			<wp:comment_date_gmt>2008-06-12 17:53:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<strong>Interesting overview about how it works&#8230;...</strong>

Semantic Web, Linked data, RDF, Web 3.0 and the like are now buzz. A lot is being written about them but sometimes it&#8217;s hard for the newbie to fully understand how this data is to be related and how any software could navigate through it.
At inkd...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>trackback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>76263</wp:comment_id>
			<wp:comment_author><![CDATA[Interesting Semantic Web links &laquo; Derivadow.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://derivadow.com/2008/10/15/interesting-semantic-web-links/</wp:comment_author_url>
			<wp:comment_author_IP>72.233.2.75</wp:comment_author_IP>
			<wp:comment_date>2008-10-15 14:40:19</wp:comment_date>
			<wp:comment_date_gmt>2008-10-15 21:40:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Following your nose to the web of data [inkdroid] The philosophy is quite different from other data discovery methods, such as the typical web2.0 APIs of Flickr, Amazon, YouTube, Facebook, Google, etc., which all differ in their implementation details and require you to digest their API documentation before you can do anything useful. Contrast this with the Web of Data which uses the ubiquitous technologies of URIs and HTTP plus the secret sauce of the RDF triple. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>76414</wp:comment_id>
			<wp:comment_author><![CDATA[Media companies should embrace the generative nature of the web &laquo; Derivadow.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://derivadow.com/2008/10/18/media-companies-should-embrace-the-generative-nature-of-the-web/</wp:comment_author_url>
			<wp:comment_author_IP>72.233.104.8</wp:comment_author_IP>
			<wp:comment_date>2008-10-18 11:54:13</wp:comment_date>
			<wp:comment_date_gmt>2008-10-18 18:54:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] the Internet is a generative system it means it has a different philosophy from most other data discovery systems and APIs (including some that are built with Internet [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86843</wp:comment_id>
			<wp:comment_author><![CDATA[OCLC Works | inkdroid]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2014/02/26/oclc-works/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-02-26 10:44:29</wp:comment_date>
			<wp:comment_date_gmt>2014-02-26 17:44:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This Article was mentioned on <a href="http://inkdroid.org/2014/02/26/oclc-works/" rel="nofollow">inkdroid.org</a>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>webmention</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393494295.59212589263916015625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393436669.3852579593658447265625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86839</wp:comment_id>
			<wp:comment_author><![CDATA[FYN v FT$ | AI3:::Adaptive Information]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.mkbergman.com/1713/fyn-v-ft/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-02-24 10:56:50</wp:comment_date>
			<wp:comment_date_gmt>2014-02-24 17:56:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] references of interest. FYN is a specific  pattern of linked data. Ed Summers provided one of the  better overviews of the use of FYN in the context of linked data and the Web of Data.[2] See the  MusicBrainz blog [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393282109.354361057281494140625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393264610.6184570789337158203125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86980</wp:comment_id>
			<wp:comment_author><![CDATA[FYN v FT$ - Future Wave Web Development Information]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://futurewavewebdevelopment.com/wp/semantic-web/fyn-v-ft/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-06-18 00:27:49</wp:comment_date>
			<wp:comment_date_gmt>2014-06-18 07:27:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] references of interest. FYN is a specific  pattern of linked data. Ed Summers provided one of the  better overviews of the use of FYN in the context of linked data and the Web of Data. [2] See the  MusicBrainz blog [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1403078936.5408060550689697265625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1403076469.9631290435791015625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title> WoGroFuBiCo wc</title>
		<link>http://inkdroid.org/2008/01/10/wogrofubico-wc/</link>
		<pubDate>Fri, 11 Jan 2008 04:21:50 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2008/01/10/wugrofubico-wc/</guid>
		<description></description>
		<content:encoded><![CDATA[<br />
<table cellspacing="0" cellpadding="3" width="30%">
<tr><th width="25%">word</th><th>count</th></tr>
<tr style="background: #FFFFCC;"> <td>library</td><td>263</td></tr>
<td>bibliographic</td><td>236</td>
<tr style="background: #FFFFCC;"> <td>data</td><td>170</td></tr>
<td>libraries</td><td>144</td>
<tr style="background: #FFFFCC;"> <td>lc</td><td>127</td></tr>
<td>control</td><td>109</td>
<tr style="background: #FFFFCC;"> <td>information</td><td>98</td></tr>
<td>cataloging</td><td>91</td>
<tr style="background: #FFFFCC;"> <td>records</td><td>88</td></tr>
<td>subject</td><td>82</td>
<tr style="background: #FFFFCC;"> <td>materials</td><td>81</td></tr>
<td>standards</td><td>81</td>
<tr style="background: #FFFFCC;"> <td>use</td><td>80</td></tr>
<td>congress</td><td>79</td>
<tr style="background: #FFFFCC;"> <td>work</td><td>76</td></tr>
<td>record</td><td>73</td>
<tr style="background: #FFFFCC;"> <td>community</td><td>67</td></tr>
<td>users</td><td>61</td>
<tr style="background: #FFFFCC;"> <td>working</td><td>59</td></tr>
<td>group</td><td>58</td>
<tr style="background: #FFFFCC;"> <td>access</td><td>57</td></tr>
<td>recommendations</td><td>56</td>
<tr style="background: #FFFFCC;"> <td>resources</td><td>53</td></tr>
<td>authority</td><td>52</td>
<tr style="background: #FFFFCC;"> <td>metadata</td><td>47</td></tr>
<td>future</td><td>46</td>
<tr style="background: #FFFFCC;"> <td>new</td><td>40</td></tr>
<td>environment</td><td>37</td>
<tr style="background: #FFFFCC;"> <td>development</td><td>37</td></tr>
<td>web</td><td>36</td>
<tr style="background: #FFFFCC;"> <td>collections</td><td>35</td></tr>
<td>systems</td><td>35</td>
<tr style="background: #FFFFCC;"> <td>available</td><td>35</td></tr>
<td>creation</td><td>35</td>
<tr style="background: #FFFFCC;"> <td>services</td><td>34</td></tr>
<td>headings</td><td>32</td>
<tr style="background: #FFFFCC;"> <td>national</td><td>31</td></tr>
<td>findings</td><td>30</td>
<tr style="background: #FFFFCC;"> <td>research</td><td>30</td></tr>
<td>unique</td><td>29</td>
<tr style="background: #FFFFCC;"> <td>sharing</td><td>29</td></tr>
<td>oclc</td><td>28</td>
<tr style="background: #FFFFCC;"> <td>model</td><td>28</td></tr>
<td>catalog</td><td>28</td>
<tr style="background: #FFFFCC;"> <td>international</td><td>27</td></tr>
<td>develop</td><td>27</td>
<tr style="background: #FFFFCC;"> <td>value</td><td>27</td></tr>
<td>lcsh</td><td>26</td>
<tr style="background: #FFFFCC;"> <td>pcc</td><td>26</td></tr>
<td>user</td><td>26</td>
<tr style="background: #FFFFCC;"> <td>need</td><td>26</td></tr>
<td>report</td><td>25</td>
<tr style="background: #FFFFCC;"> <td>make</td><td>25</td></tr>
<td>practices</td><td>25</td>
<tr style="background: #FFFFCC;"> <td>rda</td><td>25</td></tr>
<td>used</td><td>25</td>
<tr style="background: #FFFFCC;"> <td>time</td><td>24</td></tr>
<td>needs</td><td>24</td>
<tr style="background: #FFFFCC;"> <td>rare</td><td>24</td></tr>
<td>including</td><td>24</td>
<tr style="background: #FFFFCC;"> <td>provide</td><td>23</td></tr>
<td>discovery</td><td>23</td>
<tr style="background: #FFFFCC;"> <td>communities</td><td>23</td></tr>
<td>special</td><td>23</td>
<tr style="background: #FFFFCC;"> <td>frbr</td><td>23</td></tr>
<td>current</td><td>22</td>
<tr style="background: #FFFFCC;"> <td>resource</td><td>22</td></tr>
<td>rules</td><td>22</td>
<tr style="background: #FFFFCC;"> <td>digital</td><td>21</td></tr>
<td>cooperative</td><td>21</td>
<tr style="background: #FFFFCC;"> <td>program</td><td>21</td></tr>
<td>participants</td><td>21</td>
<tr style="background: #FFFFCC;"> <td>management</td><td>21</td></tr>
<td>service</td><td>20</td>
<tr style="background: #FFFFCC;"> <td>dc</td><td>20</td></tr>
<td>programs</td><td>20</td>
<tr style="background: #FFFFCC;"> <td>online</td><td>20</td></tr>
<td>costs</td><td>20</td>
<tr style="background: #FFFFCC;"> <td>washington</td><td>20</td></tr>
<td>standard</td><td>19</td>
<tr style="background: #FFFFCC;"> <td>support</td><td>19</td></tr>
<td>knowledge</td><td>19</td>
<tr style="background: #FFFFCC;"> <td>different</td><td>19</td></tr>
<td>appropriate</td><td>19</td>
<tr style="background: #FFFFCC;"> <td>effort</td><td>18</td></tr>
<td>applications</td><td>18</td>
<tr style="background: #FFFFCC;"> <td>marc</td><td>18</td></tr>
<td>shared</td><td>18</td>
<tr style="background: #FFFFCC;"> <td>exchange</td><td>18</td></tr>
<td>process</td><td>18</td>
<tr style="background: #FFFFCC;"> <td>changes</td><td>17</td></tr>
<td>lcs</td><td>17</td>
<tr style="background: #FFFFCC;"> <td>increase</td><td>16</td></tr>
<td>public</td><td>16</td>
<tr style="background: #FFFFCC;"> <td>search</td><td>16</td></tr>
<td>creating</td><td>16</td>
<tr style="background: #FFFFCC;"> <td>broader</td><td>16</td></tr>
<td>catalogs</td><td>16</td>
<tr style="background: #FFFFCC;"> <td>controlled</td><td>16</td></tr>

</table>

I converted the <a href="http://www.loc.gov/bibliographic-future/news/lcwg-ontherecord-jan08-final.pdf">pdf</a> to text file called 'lc' with <a href="http://www.foolabs.com/xpdf/">xpdf</a> and then wrote a little python:

<pre lang="python">
#!/usr/bin/env python

from urllib import urlopen
from re import sub

stop_words = urlopen('http://www.dcs.gla.ac.uk/idom/ir_resources/linguistic_utils/stop_words').read().split()
text = file('lc').read()

counts = {}
for word in text.split():
    word = word.lower()
    word = sub(r'\W', '', word)
    word = sub(r'\d+', '', word)
    if word == ''  or word in stop_words: continue
    counts[word] = counts.get(word,0) + 1

words = counts.keys()
words.sort(lambda a,b: cmp(counts[b], counts[a]))
for word in words[0:100]:
    print "%20s %i" % (word, counts[word])
</pre>

Does me writing code to read the report count as reading the report? ... ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>181</wp:post_id>
		<wp:post_date>2008-01-10 21:21:50</wp:post_date>
		<wp:post_date_gmt>2008-01-11 04:21:50</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wogrofubico-wc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bibliography"><![CDATA[bibliography]]></category>
		<category domain="post_tag" nicename="cataloging"><![CDATA[cataloging]]></category>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="lc"><![CDATA[lc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="word"><![CDATA[word]]></category>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[wugrofubico-wc]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>50722</wp:comment_id>
			<wp:comment_author><![CDATA[pbinkley]]></wp:comment_author>
			<wp:comment_author_email>peter.binkley@ualberta.ca</wp:comment_author_email>
			<wp:comment_author_url>http://www.wallandbinkley.com/quaedam/</wp:comment_author_url>
			<wp:comment_author_IP>75.153.231.138</wp:comment_author_IP>
			<wp:comment_date>2008-01-10 22:54:08</wp:comment_date>
			<wp:comment_date_gmt>2008-01-11 05:54:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Heck, I'd say you <b>wrote</b> the report.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>50767</wp:comment_id>
			<wp:comment_author><![CDATA[Jodi Schneider]]></wp:comment_author>
			<wp:comment_author_email>jschneider@amherst.edu</wp:comment_author_email>
			<wp:comment_author_url>http://jodischneider.com/</wp:comment_author_url>
			<wp:comment_author_IP>148.85.213.30</wp:comment_author_IP>
			<wp:comment_date>2008-01-11 07:20:11</wp:comment_date>
			<wp:comment_date_gmt>2008-01-11 14:20:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[If you're going to the trouble of converting the report to text, I'd rather have a diff from the draft, Ed.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>50814</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan Rochkind]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com</wp:comment_author_url>
			<wp:comment_author_IP>128.220.205.186</wp:comment_author_IP>
			<wp:comment_date>2008-01-11 15:39:27</wp:comment_date>
			<wp:comment_date_gmt>2008-01-11 22:39:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Oh yeah, what Jodi said. I actually would really like to see a diff with the draft. I have a sense of what was changed and what wasn't, but I could be wrong, and it's too much work to actually try to compare it by hand.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>WoGroFuBiCo cloud</title>
		<link>http://inkdroid.org/2008/01/11/wogrofubico-cloud/</link>
		<pubDate>Fri, 11 Jan 2008 08:13:24 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2008/01/11/wogrofubico-cloud/</guid>
		<description></description>
		<content:encoded><![CDATA[<style>
<!--
span.tagcloud0 { font-size: 1.0em; padding: 0em; color: #ACC1F3; z-index: 10;
position: relative}
span.tagcloud0 a {text-decoration: none;  color: #ACC1F3;}
span.tagcloud1 { font-size: 1.4em; padding: 0em; color: #ACC1F3; z-index: 9;
position: relative}
span.tagcloud1 a {text-decoration: none; color: #ACC1F3;}
span.tagcloud2 { font-size: 1.8em; padding: 0em; color: #86A0DC; z-index: 8;
position: relative}
span.tagcloud2 a {text-decoration: none; color: #86A0DC;}
span.tagcloud3 { font-size: 2.2em; padding: 0em; color: #86A0DC; z-index: 7;
position: relative}
span.tagcloud3 a {text-decoration: none; color: #86A0DC;}
span.tagcloud4 { font-size: 2.6em; padding: 0em; color: #607EC5; z-index: 6;
position: relative}
span.tagcloud4 a {text-decoration: none; color: #607EC5;}
span.tagcloud5 { font-size: 3.0em; padding: 0em; color: #607EC5; z-index: 5;
position: relative}
span.tagcloud5 a {text-decoration: none; color: #607EC5;}
span.tagcloud6 { font-size: 3.3em; padding: 0em; color: #4C6DB9; z-index: 4;
position: relative}
span.tagcloud6 a {text-decoration: none; color: #4C6DB9;}
span.tagcloud7 { font-size: 3.6em; padding: 0em; color: #395CAE; z-index: 3;
position: relative}
span.tagcloud7 a {text-decoration: none; color: #395CAE;}
span.tagcloud8 { font-size: 3.9em; padding: 0em; color: #264CA2; z-index: 2;
position: relative}
span.tagcloud8 a {text-decoration: none; color: #264CA2;}
span.tagcloud9 { font-size: 4.2em; padding: 0em; color: #133B97; z-index: 1;
position: relative}
span.tagcloud9 a {text-decoration: none; color: #133B97;}
span.tagcloud10 { font-size: 4.5em; padding: 0em; color: #002A8B; z-index: 0;
position: relative}
span.tagcloud10 a {text-decoration: none; color: #002A8B;}
//-->
</style>

<span class="tagcloud1">access</span> <span class="tagcloud0">accessible</span> <span class="tagcloud0">addition</span> <span class="tagcloud0">al</span> <span class="tagcloud0">american</span> <span class="tagcloud0">analysis</span> <span class="tagcloud0">application</span> <span class="tagcloud0">applications</span> <span class="tagcloud0">appropriate</span> <span class="tagcloud0">archives</span> <span class="tagcloud0">areas</span> <span class="tagcloud0">association</span> <span class="tagcloud1">authority</span> <span class="tagcloud1">available</span> <span class="tagcloud0">based</span> <span class="tagcloud0">benefit</span> <span class="tagcloud0">benefits</span> <span class="tagcloud6">bibliographic</span> <span class="tagcloud0">broad</span> <span class="tagcloud0">broader</span> <span class="tagcloud1">catalog</span> <span class="tagcloud0">catalogers</span> <span class="tagcloud2">cataloging</span> <span class="tagcloud0">catalogs</span> <span class="tagcloud0">cataloguing</span> <span class="tagcloud0">chain</span> <span class="tagcloud0">change</span> <span class="tagcloud0">changes</span> <span class="tagcloud0">classification</span> <span class="tagcloud0">code</span> <span class="tagcloud0">collaboration</span> <span class="tagcloud1">collections</span> <span class="tagcloud0">committee</span> <span class="tagcloud1">communities</span> <span class="tagcloud2">community</span> <span class="tagcloud2">congress</span> <span class="tagcloud0">consequences</span> <span class="tagcloud0">consider</span> <span class="tagcloud0">considered</span> <span class="tagcloud0">content</span> <span class="tagcloud0">continue</span> <span class="tagcloud3">control</span> <span class="tagcloud0">controlled</span> <span class="tagcloud1">cooperative</span> <span class="tagcloud0">cost</span> <span class="tagcloud0">costs</span> <span class="tagcloud0">create</span> <span class="tagcloud0">created</span> <span class="tagcloud0">creating</span> <span class="tagcloud1">creation</span> <span class="tagcloud1">current</span> <span class="tagcloud4">data</span> <span class="tagcloud0">databases</span> <span class="tagcloud0">dc</span> <span class="tagcloud0">description</span> <span class="tagcloud0">descriptive</span> <span class="tagcloud0">desired</span> <span class="tagcloud1">develop</span> <span class="tagcloud0">developed</span> <span class="tagcloud1">development</span> <span class="tagcloud0">different</span> <span class="tagcloud1">digital</span> <span class="tagcloud1">discovery</span> <span class="tagcloud0">distribution</span> <span class="tagcloud0">dublin</span> <span class="tagcloud0">ed</span> <span class="tagcloud0">education</span> <span class="tagcloud0">effort</span> <span class="tagcloud0">encourage</span> <span class="tagcloud0">enhance</span> <span class="tagcloud1">environment</span> <span class="tagcloud0">et</span> <span class="tagcloud0">evidence</span> <span class="tagcloud0">exchange</span> <span class="tagcloud0">exist</span> <span class="tagcloud1">findings</span> <span class="tagcloud0">focus</span> <span class="tagcloud0">format</span> <span class="tagcloud0">formats</span> <span class="tagcloud0">frameworks</span> <span class="tagcloud1">frbr</span> <span class="tagcloud1">future</span> <span class="tagcloud0">greater</span> <span class="tagcloud1">group</span> <span class="tagcloud1">headings</span> <span class="tagcloud0">hidden</span> <span class="tagcloud0">identifiers</span> <span class="tagcloud0">identify</span> <span class="tagcloud0">ifla</span> <span class="tagcloud0">impact</span> <span class="tagcloud0">include</span> <span class="tagcloud1">including</span> <span class="tagcloud0">increase</span> <span class="tagcloud0">increasingly</span> <span class="tagcloud2">information</span> <span class="tagcloud0">institution</span> <span class="tagcloud0">institutions</span> <span class="tagcloud1">international</span> <span class="tagcloud0">knowledge</span> <span class="tagcloud0">language</span> <span class="tagcloud3">lc</span> <span class="tagcloud0">lcs</span> <span class="tagcloud1">lcsh</span> <span class="tagcloud10">libraries</span> <span class="tagcloud0">limited</span> <span class="tagcloud0">lis</span> <span class="tagcloud0">maintaining</span> <span class="tagcloud1">make</span> <span class="tagcloud1">management</span> <span class="tagcloud0">marc</span> <span class="tagcloud2">materials</span> <span class="tagcloud1">metadata</span> <span class="tagcloud1">model</span> <span class="tagcloud1">national</span> <span class="tagcloud1">need</span> <span class="tagcloud1">needs</span> <span class="tagcloud0">networks</span> <span class="tagcloud1">new</span> <span class="tagcloud0">number</span> <span class="tagcloud1">oclc</span> <span class="tagcloud0">online</span> <span class="tagcloud0">organization</span> <span class="tagcloud0">organizations</span> <span class="tagcloud0">outcomes</span> <span class="tagcloud0">outside</span> <span class="tagcloud1">participants</span> <span class="tagcloud0">particular</span> <span class="tagcloud1">pcc</span> <span class="tagcloud0">possible</span> <span class="tagcloud0">potential</span> <span class="tagcloud0">practice</span> <span class="tagcloud1">practices</span> <span class="tagcloud0">primary</span> <span class="tagcloud0">principles</span> <span class="tagcloud0">process</span> <span class="tagcloud0">processes</span> <span class="tagcloud0">production</span> <span class="tagcloud1">program</span> <span class="tagcloud0">programs</span> <span class="tagcloud1">provide</span> <span class="tagcloud0">public</span> <span class="tagcloud0">publishers</span> <span class="tagcloud0">quo</span> <span class="tagcloud0">range</span> <span class="tagcloud1">rare</span> <span class="tagcloud1">rda</span> <span class="tagcloud1">recommendations</span> <span class="tagcloud4">records</span> <span class="tagcloud0">reference</span> <span class="tagcloud0">relationships</span> <span class="tagcloud1">report</span> <span class="tagcloud0">require</span> <span class="tagcloud0">requirements</span> <span class="tagcloud1">research</span> <span class="tagcloud1">resource</span> <span class="tagcloud1">resources</span> <span class="tagcloud0">responsibility</span> <span class="tagcloud0">results</span> <span class="tagcloud0">role</span> <span class="tagcloud1">rules</span> <span class="tagcloud0">search</span> <span class="tagcloud0">serve</span> <span class="tagcloud0">service</span> <span class="tagcloud1">services</span> <span class="tagcloud0">share</span> <span class="tagcloud0">shared</span> <span class="tagcloud1">sharing</span> <span class="tagcloud0">sources</span> <span class="tagcloud1">special</span> <span class="tagcloud0">specific</span> <span class="tagcloud2">standards</span> <span class="tagcloud0">states</span> <span class="tagcloud0">status</span> <span class="tagcloud2">subject</span> <span class="tagcloud0">supply</span> <span class="tagcloud0">support</span> <span class="tagcloud1">systems</span> <span class="tagcloud0">technology</span> <span class="tagcloud0">terms</span> <span class="tagcloud1">time</span> <span class="tagcloud0">today</span> <span class="tagcloud0">tools</span> <span class="tagcloud0">types</span> <span class="tagcloud1">unique</span> <span class="tagcloud0">united</span> <span class="tagcloud0">university</span> <span class="tagcloud2">use</span> <span class="tagcloud1">used</span> <span class="tagcloud2">users</span> <span class="tagcloud0">using</span> <span class="tagcloud1">value</span> <span class="tagcloud0">variety</span> <span class="tagcloud0">various</span> <span class="tagcloud0">vendors</span> <span class="tagcloud0">vocabularies</span> <span class="tagcloud0">washington</span> <span class="tagcloud0">ways</span> <span class="tagcloud1">web</span> <span class="tagcloud1">working</span> <span class="tagcloud2">works</span>

same stats as <a href="/journal/2008/01/10/wogrofubico-wc/">before</a>, but the top 200 this time, and as a cloud. It's crying out for some kind of stemming to collapse some terms together I suppose...but it's also 3:17AM.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>182</wp:post_id>
		<wp:post_date>2008-01-11 01:13:24</wp:post_date>
		<wp:post_date_gmt>2008-01-11 08:13:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wogrofubico-cloud</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bibliography"><![CDATA[bibliography]]></category>
		<category domain="post_tag" nicename="cataloging"><![CDATA[cataloging]]></category>
		<category domain="post_tag" nicename="cloud"><![CDATA[cloud]]></category>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="lc"><![CDATA[lc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"50826";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>50826</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Murray]]></wp:comment_author>
			<wp:comment_author_email>peter@OhioLINK.edu</wp:comment_author_email>
			<wp:comment_author_url>http://dltj.org/</wp:comment_author_url>
			<wp:comment_author_IP>12.150.3.226</wp:comment_author_IP>
			<wp:comment_date>2008-01-11 18:55:23</wp:comment_date>
			<wp:comment_date_gmt>2008-01-12 01:55:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<a href="http://dltj.org/2007/12/descriptive-enrichment/" rel="nofollow">I'm</a> with <a href="http://www.libraryjournal.com/blog/1090000309/post/1920018592.html" rel="nofollow">Roy</a> -- the word "control" (particularly when combined with "controlled"...do you have that stemming algorithm working yet?) leaps off the page.  Also leaping off, though is "community" and the fact that "data" is more used than "information".

Neat, cheap analysis.  I'll have to bookmark this post just for the value of the Python script.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>tripleshot</title>
		<link>http://inkdroid.org/2008/01/11/tripleshot/</link>
		<pubDate>Fri, 11 Jan 2008 16:39:35 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://www.inkdroid.org/2008/01/11/tripleshot/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://en.wikipedia.org/wiki/Image:Linea_doubleespresso.jpg"><img src="/images/espresso.jpg" style="width: 150px; margin-left: 10px; float: right; border: none;" /></a>

Recently there was a bit of interesting <a href="http://listserv.loc.gov/cgi-bin/wa?A2=ind0801&L=marc&T=0&P=1470">news</a> around a MARBI Discussion Paper 2008-DP04 regarding semweb technologies at <a href="http://loc.gov">LC</a>. 

<blockquote>
Related to this work are RDF/OWL representations and models for MODS and MARC, which we are also developing.  Several representations of MODS in RDF/OWL, such as the one from the SIMILE project, have been made available as part of various projects and we have found they useful for our analysis and to inform our design process.  We want to bring them together into one easily downloaded and maintained RDF/OWL file for use in community experimentation with RDF applications.  Our time line is to have the MODS RDF ready for community comment by June.
</blockquote>


]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>183</wp:post_id>
		<wp:post_date>2008-01-11 09:39:35</wp:post_date>
		<wp:post_date_gmt>2008-01-11 16:39:35</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>tripleshot</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="lc"><![CDATA[lc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="marc"><![CDATA[marc]]></category>
		<category domain="post_tag" nicename="mod"><![CDATA[mod]]></category>
		<category domain="post_tag" nicename="owl"><![CDATA[owl]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="simile"><![CDATA[simile]]></category>
		<category domain="post_tag" nicename="skos"><![CDATA[skos]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>52215</wp:comment_id>
			<wp:comment_author><![CDATA[Tom Kim]]></wp:comment_author>
			<wp:comment_author_email>terry.flanagan@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>64.61.109.118</wp:comment_author_IP>
			<wp:comment_date>2008-01-23 07:10:59</wp:comment_date>
			<wp:comment_date_gmt>2008-01-23 14:10:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey Ed want to go get a gyro from Sunrise?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>52243</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.241.132</wp:comment_author_IP>
			<wp:comment_date>2008-01-23 11:24:19</wp:comment_date>
			<wp:comment_date_gmt>2008-01-23 18:24:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[hahaha, hey terry!

Suzy Creempuff ; nyah, nyah!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>lcsh, thesauri and skos</title>
		<link>http://inkdroid.org/2008/01/23/lcsh-thesauri-and-skos/</link>
		<pubDate>Wed, 23 Jan 2008 14:46:29 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/01/23/lcsh-thesauri-and-skos/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.ibiblio.org/fred2.0/wordpress/?p=20">Simon Spero</a> has an interesting post on why LCSH cannot be considered a thesaurus. At $work I've been working on mapping <a href="http://www.loc.gov/marc/authority/">LCSH/MARC</a> to <a href="http://www.w3.org/2004/02/skos/">SKOS</a>, so Simon's efforts in both collecting and analyzing LCSH authority data have been extremely valuable. In particular <a href="http://lists.w3.org/Archives/Public/public-swd-wg/2007Dec/0076.html">Simon</a> and <a href="http://lists.w3.org/Archives/Public/public-swd-wg/2007Dec/0081.html">Leonard Willpower's</a> involvement with SKOS alerted me relatively early on to some of the problems that lie in store when thinking of LCSH in terms of a thesaurus.</p>

<p>The problem stems from very specific (standardized) notions of what thesauri are. <a href="http://www.niso.org/standards/resources/Z39-19-2005.pdf">Z39-19-2005</a> defines <em>broader</em> relationships in thesauri as being transitive. So if <em>a</em> has the broader term <em>b</em>, and <em>b</em> has the broader term <em>c,</em> then you can infer <em>a</em> has the broader term <em>c</em>.</p>

<p>Now consider the broader relationships (BT for those of you w/ the red books handy, or care to browse <a href="http://authorities.loc.gov">authorities.loc.gov</a> from the comfort of your chair) from the heading "Non-alcoholic cocktails":</p>

<p><img src="/images/non-alcoholic-cocktails.png" /></p>

<p>If broader relationships are to be considered transitive one is obliged to treat <em>Alcoholic beverages</em> as a broader term for <em>Non-alcoholic cocktails</em>. But clearly it's nonsense to consider a non-alcoholic cocktail a specialization of an alcoholic beverage. As Simon <a href="http://lists.w3.org/Archives/Public/public-swd-wg/2007Dec/0078.html">pointed out</a> the problem was recognized by Mary Dykstra soon after LCSH adopted terminology from the thesauri world (BT, NT, RT) in 1986. Her article, <em><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Focoins.info%3Agenerator&amp;rft.genre=article&amp;rft.atitle=LC+Subject+Headings+Disguised+as+a+Thesaurus&amp;rft.title=Library+Journal&amp;rft.stitle=Library+Journal&amp;rft.issn=0363-0277&amp;rft.date=1988&amp;rft.volume=113&amp;rft.issue=4&amp;rft.spage=42&amp;rft.epage=46&amp;rft.aulast=Dykstra&amp;rft.aufirst=Mary&amp;rft.au=Mary+Dykstra">LC Subject Headings Disguised as a Thesaurus</span></em> describes the many difficulties of treating LCSH as a thesaurus. In the example above from LCSH the broader (BT) relationship is used for both hierarchical (IS-A) relationships, as well as part/whole (HAS-A) relationships. According to thesauri folks this is a no-no.</p>

<p>LCSH aside, the semantics of broader/narrower have been an <a href="http://www.w3.org/2006/07/SWD/track/issues/44">issue</a> for SKOS for a fair amount of time. <a href="http://www.cs.vu.nl/~guus/">Guus Schreiber</a> proposed a <a href="http://lists.w3.org/Archives/Public/public-swd-wg/2008Jan/0090.html">resolution</a>, which was just accepted at yesterday's SWD telecon. SKOS is trying to straddle several different worlds, enabling the representation of a range of knowledge organization systems from thesauri and taxonomies to subject heading lists, folksonomy and other controlled vocabularies. To remain flexible in this way, while still appealing to the thesaurus world a compromise was reached where the skos:broader and skos:narrower <a href="http://www.w3.org/2006/07/SWD/SKOS/reference/20080118#L1930">semantic relations</a> were declared to be sub-properties of two new properties: skos:broaderTransitive and skos:narrowerTransitive (respectively). Since transitivity is not inherited, SKOS can still be used by people who want to represent loose broader relationships (LCSH, and others). At the same time SKOS will allow vocabulary owners to infer transitive broader/narrower relationships across concepts. <em>Incidentally the <a href="http://www.w3.org/2006/07/SWD/SKOS/reference">SKOS Reference</a> was just approved yesterday as a W3C Working Draft, which is its first step along the way to hopefully becoming a Recommendation.</em></p>

<p>My pottering about with LCSH and SKOS has also illustrated the value in making links between concepts explicit. Modeling LCSH as a graph data structure (SKOS), where each concept has a unique identifier has been a simple and yet powerful step in working with the data. For example to generate the image above, I simply wrote a <a href="http://web.archive.org/web/20081011194200/http://inkdroid.org/svn/lcsh-skos/trunk/skos2dot">script</a> that transformed the <a href="http://inkdroid.org/data/non-alcoholic-cocktails.n3">subgraph</a> related to "Non-alcoholic cocktails" to a <a href="http://www.graphviz.org/">graphviz</a> dot file:</p>

<pre><code>
digraph G {
  rankdir = "BT"
  "Non-alcoholic cocktails" -> "Cocktails";
  "Alcoholic beverages" -> "Beverages";
  "Non-alcoholic beverages" -> "Beverages";
  "Cocktails" -> "Alcoholic beverages";
  "Non-alcoholic cocktails" -> "Non-alcoholic beverages";
  "Non-alcoholic beer" -> "Non-alcoholic beverages";
}
</code>
</pre>

<p>And then ran that through the graphviz dot utility:</p>

<pre>
<code>
% dot -T png non-alcoholic-cocktails.dot > non-alcoholic-cocktails.png
</code>
</pre>

<p>to generate the PNG file you see. It's my hope that making a richly linked graph like LCSH/SKOS available will enable not only enhanced use of the vocabulary, but also aid in creative, collaborative refactoring of the graph. I know that these issues are not new to LC, however tools that enable refactoring along the lines of what <a href="http://lists.w3.org/Archives/Public/public-swd-wg/2007Dec/0085.html">Margherita Sini</a> proposed for the cocktail problem above will only be possible in a world where the graph can easily be manipulated and, downstream applications (library catalogs, etc) can easily adapt to the changing concept scheme.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>184</wp:post_id>
		<wp:post_date>2008-01-23 07:46:29</wp:post_date>
		<wp:post_date_gmt>2008-01-23 14:46:29</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lcsh-thesauri-and-skos</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="skos-semweb-rdf-lcsh-metadata-libraries-thesarui"><![CDATA[skos semweb rdf lcsh metadata libraries thesarui]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>59654</wp:comment_id>
			<wp:comment_author><![CDATA[Nodalities &raquo; Blog Archive &raquo; This Week&#8217;s Semantic Web]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20100807213341/http://blogs.talis.com:80/nodalities/2008/02/this_weeks_semantic_web_29.php</wp:comment_author_url>
			<wp:comment_author_IP>80.86.35.200</wp:comment_author_IP>
			<wp:comment_date>2008-04-17 10:08:27</wp:comment_date>
			<wp:comment_date_gmt>2008-04-17 17:08:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] lcsh, thesauri and skos [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>52420</wp:comment_id>
			<wp:comment_author><![CDATA[Graham Fawcett]]></wp:comment_author>
			<wp:comment_author_email>graham.fawcett@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://fawcett.blogspot.com/</wp:comment_author_url>
			<wp:comment_author_IP>206.248.164.2</wp:comment_author_IP>
			<wp:comment_date>2008-01-24 20:55:49</wp:comment_date>
			<wp:comment_date_gmt>2008-01-25 03:55:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the link to Miles' paper, Ed, it looks interesting.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>52231</wp:comment_id>
			<wp:comment_author><![CDATA[John Cowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url>http://www.ccil.org/~cowan</wp:comment_author_url>
			<wp:comment_author_IP>74.68.112.78</wp:comment_author_IP>
			<wp:comment_date>2008-01-23 08:56:05</wp:comment_date>
			<wp:comment_date_gmt>2008-01-23 15:56:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I believe that transitivity is not really essential here: the fundamental issue is class inclusion vs. prototypes.  For example, is "stone lion" a hyponym of "lion"?  If we say no, we are hard put to it to understand what the relationship is; if we say yes, we have to abandon such obvious facts about lions as that they are made out of meat (hat tip to Terry Bisson here) and that they have parents that are also lions.   Similar issues arise over "teddy bear" and "bear", "ostrich" and "bird", and "T-girl" and "girl" :-).

If on the other hand we treat "lion" as a prototype category, then it's easy to see that stone lions are lions that simply lack some of the prototypical lion properties while preserving others such as the mane, the tail, the jaws, and the pugnacious expression.

The OO version of this problem has people deriving a ColoredPoint or 3DPoint class directly from a (2D, colorless) Point class, because it's easy to add just one instance variable, though it should be obvious that a 3D point is not a 2D point.  Instead, both should be derived from AbstractPoint, a class that is uncommitted to issues of dimensionality and color.  Similarly we could have "abstract lion" as the hypernym of both "meat lion" and "stone lion".  But then how much do we factor out?  It's impossible to say a priori.  By having a lion prototype object, we can clone it several times to create real or stone lions as appropriate by overriding prototype properties.

And yet.  Class inclusion is so handy when it does work, so powerful and expressive, it's hard to think of abandoning it altogether.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>52366</wp:comment_id>
			<wp:comment_author><![CDATA[Jakob]]></wp:comment_author>
			<wp:comment_author_email>jakob.voss@gbv.de</wp:comment_author_email>
			<wp:comment_author_url>http://jakoblog.de</wp:comment_author_url>
			<wp:comment_author_IP>195.37.139.208</wp:comment_author_IP>
			<wp:comment_date>2008-01-24 11:15:52</wp:comment_date>
			<wp:comment_date_gmt>2008-01-24 18:15:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[You stubled upon the classical <a href="http://en.wikipedia.org/wiki/Diamond_problem" rel="nofollow">diamond problem</a> of object oriented knowledge modelling. Just forget about transivity and secondary differences between IS-A and HAS-A. In a general thesaurus there is only a broader/narrower relationship, everything else depends on your specific use case and can be discussed.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>52345</wp:comment_id>
			<wp:comment_author><![CDATA[Graham Fawcett]]></wp:comment_author>
			<wp:comment_author_email>graham.fawcett@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://fawcett.blogspot.com</wp:comment_author_url>
			<wp:comment_author_IP>137.207.200.153</wp:comment_author_IP>
			<wp:comment_date>2008-01-24 08:15:39</wp:comment_date>
			<wp:comment_date_gmt>2008-01-24 15:15:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Spot-on, John.

Cocktails aside, "alcoholic beverages" and "non-alcoholic beverages" are clearly disjoint sets, can this be exploited somehow?

The fact that "non-alcoholic cocktails" has a path (indirectly) to both might be used to derive a score for edges in the graph, hinting at how strict the parental relationships are. We can derive from your graph that "cocktails share many properties of alcoholic drinks, but not all of them", and this could be presented as an annotation on the edge between cocktails and alcoholic beverages.

A simple score model might be to count the number of edges in a node's subtree which link to a peer of the current node, and apply this value to the edge connecting the current node to its parent. The distance of the subnode from the current node should probably be factored into the score. So, cocktails-to-alcoholic beverages might get a weight of 0.5, since it has a child that refers to Non-alcoholic beverages. 

I'm not a graph expert by any means, and this weighting approach may be naive, but at least it is something that can be derived programatically, and can be presented visually (e.g. the thickness of the edges could depend on their scores), and that might help others in studying relationships in the graph, especially when viewing subgraphs like the one in your post. (For example, "Beer" isn't on the graph, but its influence could be implied by edge annotation.)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>52380</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan Rochkind]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com</wp:comment_author_url>
			<wp:comment_author_IP>128.220.205.186</wp:comment_author_IP>
			<wp:comment_date>2008-01-24 13:44:31</wp:comment_date>
			<wp:comment_date_gmt>2008-01-24 20:44:31</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[And, damn. 

Dykstra says "We librarians have lived with LCSH as a liability for a long time. The matter now, however, must no longer be lived with, for it has become a professional disgrace."

OUCH. She said that in 1988. Read her essay. Pretty much everything she complains about is still with us 20 years later. 20 YEARS.  That's an awful long time to still be living with what Dykstra was not afraid to call a professional disgrace. Ouch ouch ouch.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>52379</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan Rochkind]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com</wp:comment_author_url>
			<wp:comment_author_IP>128.220.205.186</wp:comment_author_IP>
			<wp:comment_date>2008-01-24 13:40:28</wp:comment_date>
			<wp:comment_date_gmt>2008-01-24 20:40:28</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice embedded COinS! One click, and I'm reading Dykstra's article. SO COOL.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>52372</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.241.133</wp:comment_author_IP>
			<wp:comment_date>2008-01-24 12:24:47</wp:comment_date>
			<wp:comment_date_gmt>2008-01-24 19:24:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the helpful comments John, Graham and Jakob. I agree, I kind of muddied the waters focusing on transitivity in this post. The distinction between class inclusion vs prototypes is what I was after, and I appreciate the clarification.

The good news is there is nothing preventing SKOS from being extended in a way to capture these two specializations of skos:broader...the bad news is that, well you have to extend SKOS, and multiple communities might do it totally differently. This is the double-edged sword of trying to serve multiple communities.

Graham, if memory serves Alistair Miles' <a href="http://isegserv.itd.rl.ac.uk/retrieval/" rel="nofollow">thesis</a> contains some details about the weighting of links between concepts along similar lines to what you suggested. I'm not a graph expert either, so these suggestions are most welcome.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>sicp reading</title>
		<link>http://inkdroid.org/2008/01/23/sicp-reading/</link>
		<pubDate>Thu, 24 Jan 2008 02:42:12 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/01/23/sicp-reading/</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://mitpress.mit.edu/sicp/"><img src="/images/sicp.jpg" style="float: left; width: 175px; margin-right: 15px; border: none;" /></a> If you've ever harbored any interest in reading (or re-reading) <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rfr_id=info%3Asid%2Focoins.info%3Agenerator&amp;rft.genre=book&amp;rft.btitle=Structure+and+Interpretation+of+Computer+Programs&amp;rft.title=Structure+and+Interpretation+of+Computer+Programs+-+2nd+Edition+%28MIT+Electrical+Engineering+and+Computer+Science%29&amp;rft.isbn=0262011530&amp;rft.aulast=Abelson&amp;rft.aufirst=Harold&amp;rft.au=Harold+Abelson&amp;rft.au=Gerald+Jay+Sussman&amp;rft.date=1996-07&amp;rft.pub=The+MIT+Press&amp;rft.place=Cambridge%2C+Mass.&amp;rft.tpages=683&amp;rft.id=http%3A%2F%2Fwww.amazon.com%2Fgp%2Fproduct%2F0262011530%253ftag%3Dlinkbaton%2526link_code%3Dxm2%2526camp%3D2025%2526dev-t%3DD2WMXA685PFEEC"<a href="http://mitpress.mit.edu/sicp/">The Structure and Interpretation of Computer Programs</span> please consider <a href="http://groups.google.com/group/books4code/web/sicp---200801">joining</a> some of the <a href="http://groups.google.com/group/books4code">books4code</a> folks as we work through the <a href="http://ocw.mit.edu/OcwWeb/Electrical-Engineering-and-Computer-Science/6-001Spring-2005/CourseHome/index.htm">SICP MIT OpenCourseWare</a> (free) course. <a href="http://weblog.lonelylion.com/">Chris McAvoy</a> has set up a <a href="http://groups.google.com/group/books4code/web/sicp---200801">wiki-page</a> with details, and a <a href=http://www.google.com/calendar/embed?src=neatlo1k66fqopnodb39cm2vjk%40group.calendar.google.com">calendar </a> to subscribe to, to keep us honest. The book is available for free, and so are video lectures, notes, exercise answers, etc ... Thanks <a href="http://multiply.org/notebook">Jason</a> for getting us to take this up again :-)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>185</wp:post_id>
		<wp:post_date>2008-01-23 19:42:12</wp:post_date>
		<wp:post_date_gmt>2008-01-24 02:42:12</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>sicp-reading</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="lisp-scheme-programming-mit-courses"><![CDATA[lisp scheme programming mit courses]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85340</wp:comment_id>
			<wp:comment_author><![CDATA[&gt;Saturday Afternoon in Berlin: A snapshot of my brain at the moment &laquo; flyingzumwalt]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://flyingzumwalt.com/2008/02/23/saturday-afternoon-in-berlin-a-snapshot-of-my-brain-at-the-moment/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-02-08 16:44:40</wp:comment_date>
			<wp:comment_date_gmt>2012-02-08 23:44:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] a few months of stop-and-start, an email cropped up on the list pointing to another SICP study group with a similar schedule. The link referenced the inkdroid blog, which I recognized but [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1328744680.8566";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1328928496.5522";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>calais and ocr newspaper data</title>
		<link>http://inkdroid.org/2008/02/13/calais-and-ocr-newspaper-data/</link>
		<pubDate>Wed, 13 Feb 2008 20:30:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/02/13/calais-and-ocr-newspaper-data/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://opencalais.com/"><img src="/images/calais.gif" style="margin-right: 10px; float: left;"/></a> Like you I've been <a href="http://web.archive.org/web/20080211070303/http://radar.oreilly.com:80/archives/2008/02/reuters_semantic_web_moneytech.html">reading</a> <a href="http://ebiquity.umbc.edu/blogger/2008/02/02/reuters-calais-offers-free-text-extraction-services-producing-rdf/">about</a> the new <a href="http://opencalais.com/">Reuters Calais Web Service</a>. The basic gist is you can send the service text and get back machine readable data about recognized entities (personal names, state/province names, city names, etc). The response format is kind of interesting because it's RDF that uses a bunch of homespun vocabularies.</p>

<p>At work <a href="http://eikeon.com">Dan</a>, <a href="http://ardvaark.net">Brian</a> and I have been working on ways to map document centric XML formats to intellectual models represented as OWL. At our last meeting one of our colleagues passed out the Calais documentation, and suggested we might want to take a look at it in the context of this work. It's a very different approach in that Calais is doing <a href="http://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and we instead are looking for patterns in the structure of XML. But the end result is the same--an RDF graph. We essentially have large amounts of XML metadata for newspapers, but we also have <a href="http://loc.gov/chroniclingamerica">large amounts</a> of OCR for the newspaper pages themselves. Perfect fodder for nlp and calais...</p>

<p>To aid in the process I wrote a helper utility (<a href="http://web.archive.org/web/20101216230820/http://inkdroid.org/bzr/calais/calais.py">calais.py</a>) that bundles up the Calais web service into a function call that returns a rdf graph, courtesy of Dan's <a href="http://rdflib.net">rdflib</a>:</p>

<pre lang="python">
  import calais
  graph = calais_graph(content)
</pre>

<p>This is dependent on you getting a calais <a href="http://web.archive.org/web/20080507000713/http://developer.opencalais.com:80/member/register">license key</a> and stashing it away in ~/.calais. I wrote a couple sample scripts that use calais.py to do stuff like output all the personal names found in the text. For example here's the <a href="http://web.archive.org/web/20101216230559/http://inkdroid.org/bzr/calais/people">people</a> script. <em>note, the angly brackets are missing from the sparql prefixes intentionally, since they don't render properly (yet) in wordpress</em>.</p>

<pre lang="python">
  from calais import calais_graph
  from sys import argv

  filename = argv[1]
  content = file(filename).read()
  g = calais_graph(content)

  sparql = """
          PREFIX rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#
          PREFIX ct: http://s.opencalais.com/1/type/em/e/
          PREFIX cp: http://s.opencalais.com/1/pred/
          SELECT ?name
          WHERE {
            ?subject rdf:type ct:People .
            ?subject cp:name ?name .
          }
          """

  for row in g.query(sparql):
      print row[0] 

</pre>

<p>Notice the content is sent to calais, the graph comes back, and then a SPARQL query is executed on it? Here's what we get when I run <a href="http://web.archive.org/web/20101216230637/http://inkdroid.org/bzr/calais/data/ndnp:774348">this</a> OCR data through (take a <a href="http://web.archive.org/web/20101216230637/http://inkdroid.org/bzr/calais/data/ndnp:774348">look</a> at the linked OCR to see just how irregular this data is).</p>

<pre>
  ed@curry:~/bzr/calais$ ./people data/ndnp\:774348 
  McKmley
  Edwin W. Joy
  A. Musto
  JOHN D. SPRECKELS
  George Dlxoh
  Le Roy
  Bryan
  Charles P. Braslan
  Siegerfs Angostura Bitters
  James Stafford
  Herbert Putnam
  H. G. Pond
  Charles F. Joy
  Santa Rosa
  Allen S. Qlmsted
  Pptter Palmer
</pre>

<p>Clearly there are some errors, but you could imagine ranked list of these as they occurred across a million pages, where the anomalies would fall off on the long tail somewhere. It could be really useful in faceted browse applications. And here's the output of <a href="http://web.archive.org/web/20101216230758/http://inkdroid.org/bzr/calais/cities">cities</a>.</p>

<pre>
  ed@curry:~/bzr/calais$ ./cities data/ndnp:774348 
  Valencia
  San Jose
  Seattle
  Newport
  Santa Clara
  St. Louis
  New York
  Haifa
  Venice
  Rochester
  Fremont
  San Francisco
  San Francisco
  Chicago
  Oakland
  Los Angeles
  Fresno
  Watsonville
  Philadelphia
  Washington
  CHICAGO
</pre>

<p>Not too shabby. If you want to try this out, install <a href="http://rdflib.net">rdflib</a>, and you can grab calais.py and the sample scripts and OCR samples from my bzr repo:</p>

<pre>
  bzr branch http://web.archive.org/web/20101217003936/http://inkdroid.org/bzr/calais/
</pre>

<p>If you do dive into calais.py you'll notice that currently the REST interface is returning the RDF escaped in an XML envelope of some kind. I think this is a bug, but calais.py extracts and unescapes the RDF.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>186</wp:post_id>
		<wp:post_date>2008-02-13 13:30:16</wp:post_date>
		<wp:post_date_gmt>2008-02-13 20:30:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>calais-and-ocr-newspaper-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="datamining"><![CDATA[datamining]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="newspapers"><![CDATA[newspapers]]></category>
		<category domain="post_tag" nicename="ocr"><![CDATA[ocr]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="sparql"><![CDATA[sparql]]></category>
		<category domain="post_tag" nicename="webservices"><![CDATA[webservices]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>74718</wp:comment_id>
			<wp:comment_author><![CDATA[Open Libraries &#8211; Mining for Meaning]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.bookism.org/open/2008/07/28/mining-for-meaning/</wp:comment_author_url>
			<wp:comment_author_IP>66.33.213.17</wp:comment_author_IP>
			<wp:comment_date>2008-09-20 14:46:39</wp:comment_date>
			<wp:comment_date_gmt>2008-09-20 21:46:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] text, but it also works for extracting meaning from the most recent weblog posting to historic newspapers newly scanned into text via Optical Character Recognition (OCR). Since human-created metadata and [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>55224</wp:comment_id>
			<wp:comment_author><![CDATA[Pete]]></wp:comment_author>
			<wp:comment_author_email>peter.skomoroch@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>68.49.46.66</wp:comment_author_IP>
			<wp:comment_date>2008-02-22 11:18:52</wp:comment_date>
			<wp:comment_date_gmt>2008-02-22 18:18:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Tried the example, but seem to be having a problem with rdflib... any ideas?

<pre lang="text">
$ ./calais.py data/ndnp:1396148

Traceback (most recent call last):
  File "./calais.py", line 127, in ?
    print g.serialize(format='n3')
  File "/opt/local/lib/python2.4/site-packages/rdflib/Graph.py", line 414, in serialize
    return serializer.serialize(destination, base=base, encoding=encoding)
  File "/opt/local/lib/python2.4/site-packages/rdflib/syntax/serializer.py", line 28, in serialize
    self.serializer.serialize(stream, base=base, encoding=encoding)
  File "/opt/local/lib/python2.4/site-packages/rdflib/syntax/serializers/N3Serializer.py", line 17, in serialize
    self._ser(self.store, stream)
  File "/opt/local/lib/python2.4/site-packages/rdflib/syntax/serializers/N3Serializer.py", line 22, in _ser
    for s, p, o in store:
ValueError: need more than 2 values to unpack
</pre>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>55225</wp:comment_id>
			<wp:comment_author><![CDATA[Pete]]></wp:comment_author>
			<wp:comment_author_email>peter.skomoroch@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>68.49.46.66</wp:comment_author_IP>
			<wp:comment_date>2008-02-22 11:27:08</wp:comment_date>
			<wp:comment_date_gmt>2008-02-22 18:27:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Another one:

<pre lang="text">
$ python people.py ndnp:774348        

Traceback (most recent call last):
  File "people.py", line 22, in ?
    for row in g.query(sparql):
AttributeError: 'ConjunctiveGraph' object has no attribute 'query'
</pre>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>55227</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.181</wp:comment_author_IP>
			<wp:comment_date>2008-02-22 13:55:05</wp:comment_date>
			<wp:comment_date_gmt>2008-02-22 20:55:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[What version of rdflib are you running?

<pre lang="text">
uqbar:~/bzr/calais ed$ python
Python 2.5.1 (r251:54869, Apr 18 2007, 22:08:04) 
[GCC 4.0.1 (Apple Computer, Inc. build 5367)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import rdflib
>>> print rdflib.__version__
2.4.0
</pre>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>55230</wp:comment_id>
			<wp:comment_author><![CDATA[Pete]]></wp:comment_author>
			<wp:comment_author_email>peter.skomoroch@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>68.49.46.66</wp:comment_author_IP>
			<wp:comment_date>2008-02-22 14:09:25</wp:comment_date>
			<wp:comment_date_gmt>2008-02-22 21:09:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks, looks like I just have some python path cleaning to do...

&gt;&gt;&gt; import rdflib
&gt;&gt;&gt; print rdflib.__version__
2.3.1]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>55231</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.181</wp:comment_author_IP>
			<wp:comment_date>2008-02-22 14:17:01</wp:comment_date>
			<wp:comment_date_gmt>2008-02-22 21:17:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah, you'll need 2.4.0 (eikeon is here sitting next to me telling me). 

<pre lang="text">
easy_install -U rdflib==2.4.0
</pre>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>54590</wp:comment_id>
			<wp:comment_author><![CDATA[Dave]]></wp:comment_author>
			<wp:comment_author_email>david.e.woodward@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>98.204.232.47</wp:comment_author_IP>
			<wp:comment_date>2008-02-13 19:38:44</wp:comment_date>
			<wp:comment_date_gmt>2008-02-14 02:38:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is real cool Ed. Mmmm, Angostura Bitters - I think I'm going to make a Manhattan.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>oai-ore and the shadow web</title>
		<link>http://inkdroid.org/2008/02/22/oai-ore-and-the-shadow-web/</link>
		<pubDate>Fri, 22 Feb 2008 20:09:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/02/22/oai-ore-and-the-shadow-web/</guid>
		<description></description>
		<content:encoded><![CDATA[The OAI-ORE <a href="http://www.openarchives.org/ore/documents/ore-hopkins-press-release.pdf">meeting</a> is coming up, and in general I've been really impressed with the alpha <a href="http://www.openarchives.org/ore/0.1/toc">specs</a> that have come out. It's not clear that there's an established vocabulary for talking about aggregated resources on the web, so the <a href="http://www.openarchives.org/ore/0.1/datamodel">Data Model</a> and <a href="http://www.openarchives.org/ore/0.1/vocabulary">Vocabulary</a> documents were of particular interest to me.

One thing I didn't quite understand, and which I think may have some significance for implementors, is some language in the <a href="http://www.openarchives.org/ore/0.1/discovery#URIConflation">Discovery</a> document on the subject of URI conflation:

<blockquote>The Data Model document [ORE Model] explicitly prohibits a URI of a ReM (URI-R) ever returning anything other than a ReM. This allows multiple representations to be associated with URI-R, such as using content negotiation to return ReMs in different languages, character sets, or compression encodings. But it does not allow URI-R to return a human readable "splash page", either by HTTP content negotiation or redirection. For example, clients MUST NOT merge with content negotiation the following URI pair that would correspond to a ReM and a "splash page" for an object:</blockquote>

If I'm understanding right this would prohibit using technologies like <a href="http://microformats.org">microformats</a>, <a href="http://research.talis.com/2005/erdf/wiki/Main/RdfInHtml">eRDF</a>, <a href="http://rdfa.info/">RDFa</a> and <a href="http://www.w3.org/2001/sw/grddl-wg/">GRDDL</a> in a "splash page" to represent the resource map. It seems odd to me that you can represent a resource map in Atom, but not in HTML. 

To illustrate what this might look like I took a splash page off of <a href="http://arxiv.org/abs/0711.1533v1">arXiv</a> (hope that was ok!) and marked it up with oai-ore RDFa. 

<a href="http://inkdroid.org/data/0711.1533"><img src="/images/arxiv-screenshot.png" border="0" /></a>

Take a <a href="http://inkdroid.org/data/0711.1533">look</a>. So all I did is modify the existing XHTML at arxiv.org, and I've been able to represent an ORE Resource Map. This seems like a relatively simple, and powerful way for existing repositories to make their aggregated resources available. 

RDFa just entered <a href="http://www.w3.org/News/2008#item26">Last Call</a>, but there are already multiple implementations. Try out the <a href="http://www.w3.org/2006/07/SWD/RDFa/impl/js/">GetN3</a> bookmarklet on the splash page, and you should see some triples come back. I ran them through the validator at w3c and got the following <a href="/images/oai_ore_graph.png">graph</a> (kinda too big to include here inline).

This kind of issue seem to be at the heart of what Ian Davis refers to when he asks "<a href="http://iandavis.com/blog/2007/11/is-the-semantic-web-destined-to-be-a-shadow">Is the Semantic Web Destined to be a Shadow?</a>". <a href="http://efoundations.typepad.com/efoundations/2008/02/repositories-th.html">Andy Powell</a> and <a href="http://efoundations.typepad.com/efoundations/2008/02/linked-data-and.html">Pete Johnston</a> have also been strong voices for integrating digital library repositories and the web--and they are also involved with the oai-ore effort. It feels like some of the oia-ore language could be loosened a bit to allow machine readable and human readable information to commingle a bit more. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>187</wp:post_id>
		<wp:post_date>2008-02-22 13:09:41</wp:post_date>
		<wp:post_date_gmt>2008-02-22 20:09:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oai-ore-and-the-shadow-web</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="erdf"><![CDATA[erdf]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oai-ore"><![CDATA[oai-ore]]></category>
		<category domain="post_tag" nicename="oai-pmh"><![CDATA[oai-pmh]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="xhtml"><![CDATA[xhtml]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>59653</wp:comment_id>
			<wp:comment_author><![CDATA[Nodalities &raquo; Blog Archive &raquo; This Week&#8217;s Semantic Web]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20101127151441/http://blogs.talis.com:80/nodalities/2008/02/this_weeks_semantic_web_32.php</wp:comment_author_url>
			<wp:comment_author_IP>80.86.35.200</wp:comment_author_IP>
			<wp:comment_date>2008-04-17 10:06:30</wp:comment_date>
			<wp:comment_date_gmt>2008-04-17 17:06:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] oai-ore and the shadow web [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>55456</wp:comment_id>
			<wp:comment_author><![CDATA[Pete Johnston]]></wp:comment_author>
			<wp:comment_author_email>pete.johnston@eduserv.org.uk</wp:comment_author_email>
			<wp:comment_author_url>http://efoundations.typepad.com/efoundations/</wp:comment_author_url>
			<wp:comment_author_IP>195.188.238.252</wp:comment_author_IP>
			<wp:comment_date>2008-02-25 05:10:36</wp:comment_date>
			<wp:comment_date_gmt>2008-02-25 12:10:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed,

Good point.

Speaking only for myself here(!),  I completely agree that an XHTML/RDFa document would make a good representation of an ORE Resource Map; and that such a document could/would probably look very much like what we think of as a "splash page".

Of course if you want to talk about both 

(i) the Resource Map (with an XHTML/RDFa representation and possibly other representations via conneg) and 
(ii) a "splash page" 

as two distinct resources, which I guess we might want to do in some circumstances, then we need two distinct URIs for those two distinct resources.

But that's still perfectly do-able. One could probably even serve the _same_ XHTML/RDFa doc as a representation of _both_ of those _distinct_ resources (pace taking care with base URIs etc).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>55236</wp:comment_id>
			<wp:comment_author><![CDATA[Mark Birbeck]]></wp:comment_author>
			<wp:comment_author_email>mark.birbeck@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://internet-apps.blogspot.com</wp:comment_author_url>
			<wp:comment_author_IP>89.240.39.75</wp:comment_author_IP>
			<wp:comment_date>2008-02-22 16:57:11</wp:comment_date>
			<wp:comment_date_gmt>2008-02-22 23:57:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Hi,</p>

<p>I agree with you that things could be loosened. The Discovery document position tends to arise when it is assumed that it must be possible to tell the difference between a resource and an information resource, perhaps by performing an HTTP request. (This is something I know that Ian has argued in the past.)</p>

<p>In my view this is an over-literal reading of the situation, something various people have tried to tackle. My own comments are in <a href="http://internet-apps.blogspot.com/2007/11/once-more-on-information-resources-and.html" rel="nofollow">Once more on information resources and RDFa</a>. I also have an older post from a couple of years ago, which was originally intended to be a critical look at the whole discussion from the same standpoint as the Discovery document that you quote, but in the course of working it through I discovered that my own view was wrong. It may be of interest to others who are as confused as I was, and it's called <a href="http://internet-apps.blogspot.com/2006/05/information-resource-debate-and-rdfa.html" rel="nofollow">The Information Resource Debate, and RDFa</a>.</p>

<p>Interesting work, though. And a great use of RDFa.</p>

<p>All the best,</p>

<p>Mark Birbeck
http://web.archive.org/web/20111121115551/http://webbackplane.com/mark-birbeck/</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>59664</wp:comment_id>
			<wp:comment_author><![CDATA[delicious mark hubery]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://del.icio.us/markhuberyd1</wp:comment_author_url>
			<wp:comment_author_IP>195.24.77.135</wp:comment_author_IP>
			<wp:comment_date>2008-04-17 13:50:53</wp:comment_date>
			<wp:comment_date_gmt>2008-04-17 20:50:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<strong>Blog Hopper...</strong>

Hi There. I'm blog hopping....]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>trackback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>55809</wp:comment_id>
			<wp:comment_author><![CDATA[Mark Diggory]]></wp:comment_author>
			<wp:comment_author_email>mdiggory@mit.edu</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>72.192.139.209</wp:comment_author_IP>
			<wp:comment_date>2008-02-28 12:10:17</wp:comment_date>
			<wp:comment_date_gmt>2008-02-28 19:10:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I would like to see this be the case, most of the new DSpace UI (Manakin specifically) technology will support including RDFa and microformats into the Item page generation and this would be an ideal use-case for it.

Cheers,
Mark Diggory
DSpace Systems Manager 
MIT Libraries]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>pymarc PEP-8 cleanup</title>
		<link>http://inkdroid.org/2008/02/28/pymarc-pep-8-cleanup/</link>
		<pubDate>Thu, 28 Feb 2008 09:21:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/02/28/pymarc-pep-8-cleanup/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://memory.loc.gov/cgi-bin/query/r?ammem/cdn:@field(NUMBER+@band(ichicdn+s066678)) "><img title="[Pugilist, Gene Tunney tipping his hat in front of a passenger train car in a railroad station]."src="http://inkdroid.org/images/hattip.gif" style="margin-right: 10px; float: left" /></a><a href="http://pypi.python.org/pypi/pymarc/">pymarc v2.0</a> was released yesterday afternoon. I'm mentioning it here to give a big tip of the hat to <a href="http://web.archive.org/web/20080409175507/http://fruct.us:80/bio">Gabriel Farrell</a> (gsf on <a href="irc://chat.freenode.net/code4lib">#code4lib</a>) who spent a significant amount of time cleaning up the code to be <a href="http://www.python.org/dev/peps/pep-0008/">PEP-8</a> compliant.</p>

<p>If you are a current user of pymarc your code will most likely break, since methods like: addField() will now look like add_field(). This is a small price to pay for pythonistas who typically prefer clean, consistent and more coherent code (how's that for alliteration?). It had to be done and I'm very grateful to gsf for taking the time to do it.</p>

<p><a href="http://bazaar-vcs.org/"><img src="http://inkdroid.org/images/bzr.png" style="float: right; margin-left: 10px; border: none;"/></a>Another big thing is that we've switched from using <a href="http://subversion.tigris.org/">subversion</a> to <a href="http://bazaar-vcs.org/">bzr</a> for revision control. Initially it seemed like a lightweight way for gsf and I to collaborate without monkeying with svn authentication (again)...and to learn the zen of <a href="http://www.youtube.com/watch?v=4XpnKHJAok8">distributed revision control</a>. We both liked it so much that we moved the repository to <a href="https://launchpad.net/pymarc">LaunchPad</a>.</p>

<p>So if you like the latest/greatest/shiniest, and/or want to contribute some of your own changes to pymarc:</p>

<pre lang="text">
  % bzr branch lp:pymarc
  % # hack, hack, hack, hackety, hack
  % bzr commit
  % bzr send --mail-to gsf@fruct.us --message "Gabe, I added a jammies method to the record object!"
  % # or publish your own repo and point us at it :-)
</pre>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>188</wp:post_id>
		<wp:post_date>2008-02-28 02:21:19</wp:post_date>
		<wp:post_date_gmt>2008-02-28 09:21:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>pymarc-pep-8-cleanup</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bazaar"><![CDATA[bazaar]]></category>
		<category domain="post_tag" nicename="code4lib"><![CDATA[code4lib]]></category>
		<category domain="post_tag" nicename="hats"><![CDATA[hats]]></category>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="post_tag" nicename="marc"><![CDATA[marc]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="subversion"><![CDATA[subversion]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"56238";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>56238</wp:comment_id>
			<wp:comment_author><![CDATA[Gabriel Sean Farrell]]></wp:comment_author>
			<wp:comment_author_email>gsf@fruct.us</wp:comment_author_email>
			<wp:comment_author_url>http://fruct.us/</wp:comment_author_url>
			<wp:comment_author_IP>129.25.131.205</wp:comment_author_IP>
			<wp:comment_date>2008-03-04 09:35:22</wp:comment_date>
			<wp:comment_date_gmt>2008-03-04 16:35:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Glad I could help, Ed, and I second the call for anyone who might have even a passing fancy to grab the code and dig into it.  Also, thanks to <a href="http://www.logilab.org/857" rel="nofollow">pylint</a> for some pointers on the way to PEP 8 goodness.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>oai-ore post baltimore thoughts</title>
		<link>http://inkdroid.org/2008/03/13/oai-ore-post-baltimore-thoughts/</link>
		<pubDate>Thu, 13 Mar 2008 16:34:29 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/03/13/oai-ore-post-baltimore-thoughts/</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://inkdroid.org/images/constellation.png" style="float: left; margin-right: 5px;" title="Herbert's (or was it Carl's) analogy of oai-ore resource maps as constellations in the night sky was perfect" />The recent <a href="http://www.openarchives.org/ore/meetings/hopkins/agenda.htm">OAI-ORE meeting</a> was just up the road in Baltimore, so it was easy for a bunch of us from the Library of Congress to attend. I work on a team at LC that is specifically looking at the role that repositories play at the library; I've implemented OAI-PMH data providers and harvesters, and in the past couple of years I've gotten increasingly interested in semantic web technologies -- so OAI-ORE is of particular interest to me. I've commented a bit about OAI-ORE on <a href="http://inkdroid.org/2008/02/22/oai-ore-and-the-shadow-web/">here</a> <a href="http://inkdroid.org/journal/2007/11/02/good-ore/">before</a>, but I figure it can't hurt to follow in my <a href="http://onebiglibrary.net/story/the-cranky-librarians-guide-to-oai-ore">coworker's footsteps</a> and summarize my thoughts after the meeting.</p>

<p><em>(BTW, above is an image of some constellations I nabbed off of <a href="http://commons.wikimedia.org/wiki/Image:Tri%C3%A1ngulo_de_verano.png">wikipedia</a>. I included it here because the repeated analogy (during the meeting) of OAI-ORE resource maps as constellations was really compelling -- and quite poetic.)</em></p>

<h3>The Vocabulary</h3>

<p>It seems to me that the real innovation of the OAI-ORE effort is that it provides a lightweight RDF vocabulary for talking about aggregated resources on the web. Unfortunately I think that this kernel gets a little bit lost in the <a href="http://www.openarchives.org/ore/0.1/">6 specification documents</a> that were released en masse a few months ago.</p>

<p>The ORE vocabulary essentially consists of three new resource types: <code>ore:ResourceMap</code>, <code>ore:Aggregation</code>, <code>ore:AggregatedResource</code> ; and 5 new properties to use with those types: <code>ore:describes</code>, <code>ore:isDescribedBy</code>, <code>ore:aggregates</code>, <code>ore:isAggregatedBy</code>, <code>ore:analogousTo</code>. In addition, the <a href="http://www.openarchives.org/ore/0.1/vocabulary">Vocabulary</a> document
provides guidance on how to use a few terms from the DublinCore vocabulary: <code>dc:creator</code>, <code>dc:rights</code>, <code>dcterms:modified</code>, <code>dcterms:created</code>.</p>

<p>The vocabulary is small, so if I were them I would publish the vocabulary elements using hash URIs, instead of slash URIs. The reason for this is that you don't have to jigger the web server to do a <a href="http://www.w3.org/2001/tag/issues.html#httpRange-14">httpRange-14</a> style 303 correctly:</p>

<ul>
<li>http://www.openarchives.org/ore/0.2/terms#Aggregation</li>
<li>http://www.openarchives.org/ore/0.2/terms#AggregatedResource</li>
<li>http://www.openarchives.org/ore/0.2/terms#ResourceMap</li>
<li>http://www.openarchives.org/ore/0.2/terms#describes</li>
<li>http://www.openarchives.org/ore/0.2/terms#isDescribedBy</li>
<li>http://www.openarchives.org/ore/0.2/terms#aggregates</li>
<li>http://www.openarchives.org/ore/0.2/terms#isAggregatedBy</li>
<li>http://www.openarchives.org/ore/0.2/terms#analogousTo</li>
</ul>

<p>Also, I think <code>ore:AggregatedResource</code> is currently missing from the <a href="http://www.openarchives.org/ore/0.2/terms">rdf/xml vocabulary</a>, so it should be added. Also <code>ore:isDescribedBy</code> seems to be commented out.</p>

<p>There is a lot of redundancy between the <a href="http://www.openarchives.org/ore/0.1/datamodel">Abstract Data Model</a> and the <a href="http://www.openarchives.org/ore/0.1/vocabulary">Vocabulary</a> documents--so I would recommend collapsing them down into a single, succinct document. This is in keeping with the <a href="http://en.wikipedia.org/wiki/Don't_repeat_yourself">DRY principle</a> and will have the added benefit of making it easier for newbies to hit the ground  running (not having to wade through multiple docs and mentally reconcile them). I could understand having a separate Abstract Data Model document if it were totally divorced from the web and semantic web technologies like RDF, but it's not.</p>

<h3>The Graph</h3>

<p>The OAI-ORE effort seemed to be mostly driven by a desire to take harvesting agents the last mile to the actual repository resources themselves--enabling  digital library objects (in addition to their metadata) to be harvested from  repositories (using HTTP) ; and to be referenced from other contexts (say objects in other repositories). This desire was born out of <a href="http://arxiv.org/abs/cs.DL/0601125">real, hard won experience</a> with harvesting metadata records, and marked a shift from metadata-centric harvesting to resource-centric harvesting.</p>

<p>In addition OAI-ORE marks a departure from predictable and mind-numbing arguments about SIP formats (<a href="http://www.loc.gov/standards/mets/">METS</a>, DIDL, <a href="http://www.fedora.info/download/2.0/userdocs/digitalobjects/introFOXML.html">FOXML</a>, <a href="http://ltsc.ieee.org/wg12/">IEEE LOM</a>, <a href="http://sindbad.gsfc.nasa.gov/xfdu">XFDU</a>, etc). Yet as soon as we have our shiny new OAI-ORE vocabulary we have to learn yet-another-packaging-format, this time one built on top of Atom.</p>

<p>First, let me just say I'm a big fan of <a href="http://www.ietf.org/rfc/rfc4287.txt">RFC 4287</a>, in particular how it is used in the RESTful <a href="http://www.ietf.org/rfc/rfc5023.txt">Atom Publishing Protocol</a> (RFC 5023). I also think it makes sense to have an Atom serialization for OAI-ORE resource maps -- assuming there is a GRDDL transform for turning it into RDF. But the workshop in Baltimore seemed to stress that the Atom serialization was <em>the only way</em> to do OAI-ORE, and didn't emphasize that there are in fact <em>lots of ways</em> of representing RDF graphs on the web. For example <a href="http://www.w3.org/TR/grddl/">GRDDL</a> allows you to associate arbitrary XML with an XSLT transform to extract a RDF graph. And you could encode your RDF graph directly with RDFa, N3, Turtle, ntriples, or RDF/XML.</p>

<p>Perhaps there is a feeling that stressing the RDF graph too much will alienate some people who are more familiar with XML technologies.  Or perhaps all these graph serialization choices could be perceived as being too overwhelming. But I think the opposite extreme of making it look like you can only use an overloaded Atom document as a means to publishing ORE resource maps is misguided, and will ultimately slow adoption.  Why not encourage people to publish GRDDL transforms for METS, DIDL or mark up their "splash pages" with RDFa?  This would bring the true value of the OAI-ORE work home--it's not about  yet-another-packaging format, it's about what the various packaging formats have in common on the web.</p>

<h3>Release Early, Release Often</h3>

<p>In hindsight I think it would've been helpful for the OAI-ORE group to privately build consensus about the core OAI-ORE vocabulary (if necessary), then release that into the world wild web for discussion. Then once the kinks were worked out, and there was general understanding, moving on to issues such as discovery and serialization.  As it stands the various documents were all dumped at the same time, and seem somewhat fragmented, and in places redundant. Clearly a lot of conversations have gone on that aren't happening on the public <a href="http://groups.google.com/group/oai-ore">discussion list</a>.</p>

<p>I expressed interest in being part of the OAI-ORE and was politely turned down. I'm actually kind of glad really because I also don't want to be part of some cabal of digital library practitioners. Maybe I should've titled this post "Sour Grapes" :-) Seriously though, the digital library needs good practical solutions and communities of users that encourage widespread adoption and tool support. We don't need research-ware. Having secret discussions and occasional public events that feel more like lectures than meetings isn't a good way to encourage adoption.</p>

<p>Anyhow, I hope that this isn't all seen as being too harsh. Everyone's a critic eh? All in all there is a lot in OAI-ORE to be proud of. The effort to integrate Web Architecture into Digital Library practices is most welcome indeed. Keep up the good work y'all.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>190</wp:post_id>
		<wp:post_date>2008-03-13 09:34:29</wp:post_date>
		<wp:post_date_gmt>2008-03-13 16:34:29</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oai-ore-post-baltimore-thoughts</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="atom"><![CDATA[atom]]></category>
		<category domain="post_tag" nicename="digital-libraries"><![CDATA[digital libraries]]></category>
		<category domain="post_tag" nicename="grddl"><![CDATA[grddl]]></category>
		<category domain="post_tag" nicename="harvesting"><![CDATA[harvesting]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oai-ore"><![CDATA[oai-ore]]></category>
		<category domain="post_tag" nicename="oai-pmh"><![CDATA[oai-pmh]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81629</wp:comment_id>
			<wp:comment_author><![CDATA[Open Repositories 2008 &laquo; pintiniblog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://pintiniblog.wordpress.com/2008/03/31/open-repositories-2008/</wp:comment_author_url>
			<wp:comment_author_IP>72.233.104.9</wp:comment_author_IP>
			<wp:comment_date>2010-01-08 00:06:00</wp:comment_date>
			<wp:comment_date_gmt>2010-01-08 07:06:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Mind, 08/12/07) - OAI-ORE Open Meeting, Johns Hopkins University, March 3, 2008 (présentations) | Oai-Ore post Baltimore thoughts (à propos de la conférence précitée, sur inkdroid, [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Cyganiak on linked data, microformats and the semweb</title>
		<link>http://inkdroid.org/2008/03/14/cyganiak-on-linked-data-microformats-and-the-semweb/</link>
		<pubDate>Fri, 14 Mar 2008 17:07:32 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/03/14/cyganiak-on-linked-data-microformats-and-the-semweb/</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In case you missed it <a href="http://dannyayers.com/">Danny Ayers</a> has a fun <a href="http://web.archive.org/web/20100625173255/http://blogs.talis.com:80/nodalities/2008/03/a_chat_with_richard_cyganiak.php">interview</a> with <a href="http://richard.cyganiak.de/">Richard Cyganiak</a> who is one of the prime movers behind the <a href="http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData">Linking Open Data Project</a> of <a href="http://www.w3.org/2001/sw/sweo/">Semantic Web Education and Outreach Group</a> at the W3C, and authors of <a href="http://web.archive.org/web/20100531061447/http://www.dfki.uni-kl.de:80/~sauermann/2006/11/cooluris/">Cool URIs for the Semantic Web</a> and <a href="http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/">How to Publish Linked Data on the Web</a>.  Among other things you'll learn some details about <a href="http://sindice.com">sindice</a> (the semantic web search engine at <a href="http://www.deri.ie/">DERI</a>) which indexes (using  <a href="http://lucene.apache.org/solr">Solr</a>!) structured data like rdf/xml, <a href="http://microformats.org">microformats</a> (I never noticed last.fm had microformat content) and (soon) rdfa from the world wild web. More details about Sindice can be found in an earlier <a href="http://web.archive.org/web/20080920095924/http://talk.talis.com:80/archives/2008/01/eyal_oren_talks.html">podcast</a> <a href="http://paulmiller.typepad.com/">Paul Miller</a> did with <a href="http://web.archive.org/web/20130813171432/http://www.eyaloren.org/">Eyal Oren</a> (also at DERI).</p>

<p>Richard's perspective on the past and future of the semantic web is particularly refreshing. Rather than hard selling SPARQL or even RDF his attitude seems to be to try what works now, while recognizing that the technologies that make the semantic web work may very well be different in a few years. Also there's an interesting discussion of microformats and RDF, highlighting the strengths and weaknesses of both. Plus there is a fun side story to the <a href="http://richard.cyganiak.de/2007/10/lod">LOD diagram</a> that shows the links between various open data sets.</p>

<p>If you've ever wanted to hear more about linked-data from someone in the know now is your chance. Nice questions danja!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>191</wp:post_id>
		<wp:post_date>2008-03-14 10:07:32</wp:post_date>
		<wp:post_date_gmt>2008-03-14 17:07:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>cyganiak-on-linked-data-microformats-and-the-semweb</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="interviews"><![CDATA[interviews]]></category>
		<category domain="post_tag" nicename="microformats"><![CDATA[microformats]]></category>
		<category domain="post_tag" nicename="podcasts"><![CDATA[podcasts]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="post_tag" nicename="search"><![CDATA[search]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="solr"><![CDATA[solr]]></category>
		<category domain="post_tag" nicename="sweo"><![CDATA[sweo]]></category>
		<category domain="post_tag" nicename="w3c"><![CDATA[w3c]]></category>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>tabulator and google reader notifier oddness</title>
		<link>http://inkdroid.org/2008/03/17/tabulator-and-google-reader-notifier/</link>
		<pubDate>Mon, 17 Mar 2008 13:19:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/03/17/tabulator-and-google-reader-notifier/</guid>
		<description></description>
		<content:encoded><![CDATA[If you've ever tried installing the <a href="http://www.w3.org/2005/ajar/tab">Tabulator</a> (Tim Berners-Lee's experimental <a href="http://www.w3.org/DesignIssues/LinkedData.html">linked-data</a> browser) and not seen it work you may have run into the same problem as me. 

On a hunch I guessed that there might be some weird interaction with another Firefox plugin -- so I went through all 15 of them, disabling each one and restarting Firefox to see if Tabulator would start working. Sure enough, after I disabled <a href="http://www.markdbd.com/proyectos/google_reader_notifier/">Google Reader Notifier</a> the Tabulator worked fine. 

I dropped a message to <a href="http://lists.w3.org/Archives/Public/public-semweb-ui/2008Mar/0000.html">public-semweb-ui</a>, but figured it couldn't hurt to add this here for other linked-data nerds casting about in google with the same problem.

<pre>
Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.12) Gecko/20080207 Ubuntu/7.10 (gutsy) Firefox/2.0.0.12
Tabulator v0.8.2
Google Reader Notifier v0.4.5
</pre>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>192</wp:post_id>
		<wp:post_date>2008-03-17 06:19:41</wp:post_date>
		<wp:post_date_gmt>2008-03-17 13:19:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>tabulator-and-google-reader-notifier</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="firefox-tabulator-semweb-rdf-google-plugins"><![CDATA[firefox tabulator semweb rdf google plugins]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"57183";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>59649</wp:comment_id>
			<wp:comment_author><![CDATA[Nodalities &raquo; Blog Archive &raquo; This Week&#8217;s Semantic Web]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20120312083750/http://blogs.talis.com:80/nodalities/2008/03/this_weeks_semantic_web_36.php</wp:comment_author_url>
			<wp:comment_author_IP>80.86.35.200</wp:comment_author_IP>
			<wp:comment_date>2008-04-17 09:56:15</wp:comment_date>
			<wp:comment_date_gmt>2008-04-17 16:56:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] tabulator and google reader notifier oddness [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>57183</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Murray]]></wp:comment_author>
			<wp:comment_author_email>peter@OhioLINK.edu</wp:comment_author_email>
			<wp:comment_author_url>http://dltj.org/</wp:comment_author_url>
			<wp:comment_author_IP>75.42.216.110</wp:comment_author_IP>
			<wp:comment_date>2008-03-17 17:38:45</wp:comment_date>
			<wp:comment_date_gmt>2008-03-18 00:38:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[You mean it's not just me?  I've tried the extension a couple times but never got it to do anything recognizable, so I thought I just wasn't smart enough to use it.  I still might not be smart enough, but now I know that the blank display might be more that just my (mis-)use.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>literals and resources</title>
		<link>http://inkdroid.org/2008/03/26/literals-and-resources/</link>
		<pubDate>Wed, 26 Mar 2008 13:51:32 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/2008/03/26/literals-and-resources/</guid>
		<description></description>
		<content:encoded><![CDATA[<img src="http://inkdroid.org/images/rda.jpg" style="margin-left: 10px; float: left;"/>There's a fascinating modeling discussion going on over on the <a href="http://www.jiscmail.ac.uk/cgi-bin/webadmin?A2=ind0803&L=dc-rda&T=0&F=&S=&P=2694">DC-RDA</a> list about whether <a href="http://docs.google.com/View?docid=dhpg2gtj_54fgnz8rfs">RDA properties</a> should reference literals or resources in descriptions. For example when describing an author you could use a literal:

<pre>
Twain, Mark, 1835-1910
</pre>

or a resource:

<pre>
http://lccn.loc.gov/n79021164
</pre>

There are some shades of gray in between (using blank nodes, auto-generated URIs, typed literals) but that's the basic gist of it. The discussion basically concerns what the DC-RDA Application Profile should allow. There seems to be two competing interests:

<ol>
<li>perceived ease of migrating legacy data (MARC -> RDA)</li>
<li>perceived benefits to explicitly modeling the relationships found in bibliographic data</li>
</ol> 

More information can also be found in the blogs of <a href="http://kcoyle.blogspot.com/2008/01/more-on-rda-and-literals.html">Karen Coyle</a> and <a href="http://jonphipps.wordpress.com/2008/03/16/simple-dc-and-rda/">Jon Phipps</a>.

My personal opinion is that RDA should take the high road on this one and really drive home the <a href="http://en.wikipedia.org/wiki/Value_proposition">value proposition</a> for using resources wherever possible, modeling relationships in bibliographic data, and leveraging hundreds of years of work maintaining controlled vocabularies. This will have the positive side effect of pushing library controlled vocabularies (LCSH, name authority, language and geographic codes, etc.) into the open on the web. More importantly I think it will highlight what libraries (at their best) do best, for the larger semantic web and computing world. I think it's worth limping along a bit longer with MARC and waiting for RDA to actually "do the right thing".

How to do this effectively is another matter, and is really what the discussion is about. It's really nice to see people talking openly about these issues.

<em>(PS, using an author isn't a particularly good example because I don't see it in the current <a href="http://docs.google.com/View?docid=dhpg2gtj_54fgnz8rfs">list</a> of RDA properties...)</em>

<em>(PSS, no that lccn url doesn't currently resolve (it does for bibliographic records, but not authority) or return rdf (hopefully someday))</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>193</wp:post_id>
		<wp:post_date>2008-03-26 06:51:32</wp:post_date>
		<wp:post_date_gmt>2008-03-26 13:51:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>literals-and-resources</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="post_tag" nicename="marc"><![CDATA[marc]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="rda"><![CDATA[rda]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"57819";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_9b84bee3dda10886b3a5871d13f28cce</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_28165336900efdb29d8d55fb2b34e229</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_01c78d07753231ef65dc5f7b2e262059</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_b11707e32fc12c4705789074d27960d8</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_5ff3a348a66734f7f68a15f3d3f458bb</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_d65d9a5bffa1ad584f52c36795929c65</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_79b11290a5f467455aee4fcde66fef91</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>57819</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Murray]]></wp:comment_author>
			<wp:comment_author_email>peter@OhioLINK.edu</wp:comment_author_email>
			<wp:comment_author_url>http://dltj.org/</wp:comment_author_url>
			<wp:comment_author_IP>192.153.30.24</wp:comment_author_IP>
			<wp:comment_date>2008-03-26 07:15:46</wp:comment_date>
			<wp:comment_date_gmt>2008-03-26 14:15:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<blockquote>(PSS, no that lccn url doesn’t currently resolve (it does for bibliographic records, but not authority) or return rdf (hopefully someday))</blockquote>

Well, what are you waiting for?  Get to work on that, okay?  ;-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>57823</wp:comment_id>
			<wp:comment_author><![CDATA[Bruce D'Arcus]]></wp:comment_author>
			<wp:comment_author_email>bdarcus@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>24.210.249.184</wp:comment_author_IP>
			<wp:comment_date>2008-03-26 08:39:32</wp:comment_date>
			<wp:comment_date_gmt>2008-03-26 15:39:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hear, hear Ed.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>MIME types and library metadata</title>
		<link>http://inkdroid.org/2008/04/23/mime-types-and-library-metadata/</link>
		<pubDate>Wed, 23 Apr 2008 14:46:24 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=195</guid>
		<description></description>
		<content:encoded><![CDATA[<p>While thinking about library metadata and <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">RESTful web services</a> I got to wondering how many application/<em>+xml <a href="http://www.iana.org/assignments/media-types/application/">MIME types</a> have actually been registered. It <a href="http://web.archive.org/web/20101216203407/http://inkdroid.org/bzr/bin/xml_mimes">turns out</a> that 120 out of the 633 other application/</em> MIME types.</p>

<p>Does it seem like a generally useful thing to be able to identify metadata representations with MIME types? Rebecca Guenther registered <a href="http://www.rfc-editor.org/rfc/rfc2220.txt">application/marc</a> back in 1997. Maybe we could have application/marc+xml, application/mods+xml, application/dc+xml?</p>

<p>MIME types for established library metadata formats would be useful to use in applications like <a href="http://www.ietf.org/rfc/rfc5023.txt">AtomPub</a> implementations, or say <a href="http://www.openarchives.org/ore/">OAI-ORE</a> resource maps that want to identify the format of a particular resource. In general it would be useful to have in RESTful environments where content-negotiation for resources is encouraged.</p>

<p>If you are curious, here is a current (as of Apr 23, 2008) list of registered MIME types that are in the application/*+xml space.</p>

<pre>
application/atom+xml
application/atomcat+xml
application/atomsvc+xml
application/auth-policy+xml
application/beep+xml
application/ccxml+xml
application/cellml+xml
application/cnrp+xml
application/conference-info+xml
application/cpl+xml
application/csta+xml
application/CSTAdata+xml
application/davmount+xml
application/dialog-info+xml
application/epp+xml
application/im-iscomposing+xml
application/kpml-request+xml
application/kpml-response+xml
application/mbms-associated-procedure-description+xml
application/mbms-deregister+xml
application/mbms-envelope+xml
application/mbms-msk-response+xml
application/mbms-msk+xml
application/mbms-protection-description+xml
application/mbms-reception-report+xml
application/mbms-register-response+xml
application/mbms-register+xml
application/mbms-user-service-description+xml
application/media_control+xml
application/mediaservercontrol+xml
application/oebps-package+xml
application/pidf+xml
application/pls+xml
application/poc-settings+xml
application/rdf+xml
application/reginfo+xml
application/resource-lists+xml
application/rlmi+xml
application/rls-services+xml
application/samlassertion+xml
application/samlmetadata+xml
application/sbml+xml
application/shf+xml
application/simple-filter+xml
application/smil+xml
application/soap+xml
application/sparql-results+xml
application/spirits-event+xml
application/srgs+xml
application/ssml+xml
application/vnd.3gpp.bsf+xml
application/vnd.3gpp2.bcmcsinfo+xml
application/vnd.adobe.xdp+xml
application/vnd.apple.installer+xml
application/vnd.avistar+xml
application/vnd.chemdraw+xml
application/vnd.criticaltools.wbs+xml
application/vnd.ctct.ws+xml
application/vnd.eszigno3+xml
application/vnd.google-earth.kml+xml
application/vnd.HandHeld-Entertainment+xml
application/vnd.informedcontrol.rms+xml
application/vnd.irepository.package+xml
application/vnd.liberty-request+xml
application/vnd.llamagraphics.life-balance.exchange+xml
application/vnd.marlin.drm.actiontoken+xml
application/vnd.marlin.drm.conftoken+xml
application/vnd.mozilla.xul+xml
application/vnd.ms-playready.initiator+xml
application/vnd.nokia.conml+xml
application/vnd.nokia.iptv.config+xml
application/vnd.nokia.landmark+xml
application/vnd.nokia.landmarkcollection+xml
application/vnd.nokia.n-gage.ac+xml
application/vnd.nokia.pcd+xml
application/vnd.oma.bcast.associated-procedure-parameter+xml
application/vnd.oma.bcast.drm-trigger+xml
application/vnd.oma.bcast.imd+xml
application/vnd.oma.bcast.notification+xml
application/vnd.oma.bcast.sgdd+xml
application/vnd.oma.bcast.smartcard-trigger+xml
application/vnd.oma.bcast.sprov+xml
application/vnd.oma.dd2+xml
application/vnd.oma.drm.risd+xml
application/vnd.oma.group-usage-list+xml
application/vnd.oma.poc.detailed-progress-report+xml
application/vnd.oma.poc.final-report+xml
application/vnd.oma.poc.groups+xml
application/vnd.oma.poc.invocation-descriptor+xml
application/vnd.oma.poc.optimized-progress-report+xml
application/vnd.oma.xcap-directory+xml
application/vnd.omads-email+xml
application/vnd.omads-file+xml
application/vnd.omads-folder+xml
application/vnd.otps.ct-kip+xml
application/vnd.poc.group-advertisement+xml
application/vnd.pwg-xhtml-print+xml
application/vnd.recordare.musicxml+xml
application/vnd.solent.sdkm+xml
application/vnd.sun.wadl+xml
application/vnd.syncml.dm+xml
application/vnd.syncml+xml
application/vnd.uoml+xml
application/vnd.wv.csp+xml
application/vnd.wv.ssp+xml
application/vnd.zzazz.deck+xml
application/voicexml+xml
application/watcherinfo+xml
application/wsdl+xml
application/wspolicy+xml
application/xcap-att+xml
application/xcap-caps+xml
application/xcap-el+xml
application/xcap-error+xml
application/xcap-ns+xml
application/xenc+xml
application/xhtml+xml
application/xmpp+xml
application/xop+xml
application/xv+xml
</pre>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>195</wp:post_id>
		<wp:post_date>2008-04-23 07:46:24</wp:post_date>
		<wp:post_date_gmt>2008-04-23 14:46:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>mime-types-and-library-metadata</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="conneg"><![CDATA[conneg]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="marc"><![CDATA[marc]]></category>
		<category domain="post_tag" nicename="mime"><![CDATA[mime]]></category>
		<category domain="post_tag" nicename="mods"><![CDATA[mods]]></category>
		<category domain="post_tag" nicename="rest"><![CDATA[rest]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<category domain="post_tag" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>60223</wp:comment_id>
			<wp:comment_author><![CDATA[Kevin S. Clarke]]></wp:comment_author>
			<wp:comment_author_email>ksclarke@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>152.10.144.20</wp:comment_author_IP>
			<wp:comment_date>2008-04-23 07:54:12</wp:comment_date>
			<wp:comment_date_gmt>2008-04-23 14:54:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is a great idea, I think.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>SKOS in the Context of Semantic Web Deployment</title>
		<link>http://inkdroid.org/2008/04/30/skos-in-the-context-of-semantic-web-deployment/</link>
		<pubDate>Wed, 30 Apr 2008 14:33:46 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=196</guid>
		<description></description>
		<content:encoded><![CDATA[If you happen to be in the DC area on May 8th and are interested in <a href="http://www.w3.org/DesignIssues/LinkedData.html">linked data</a> and the practical application of <a href="http://en.wikipedia.org/wiki/Semantic_Web">semantic web</a> technologies like <a href="http://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> and <a href="http://en.wikipedia.org/wiki/Web_Ontology_Language">OWL</a> please join us at the <a href="http://loc.gov">Library of Congress</a> for a presentation by <a href="http://purl.org/net/aliman">Alistair Miles</a>, key developer of <a href="http://www.w3.org/2004/02/skos/">SKOS</a>, and semantic web practitioner at the <a href="http://www.ox.ac.uk/">University of Oxford</a>. 

Below is the announcement, I hope you can make it. Oh, and if you are really interested in this stuff we're having some brown bag sessions later in the afternoon that you are welcome to attend, just email me at <em>ehs [at] pobox [dot] com</em>. 

<div style="border: thin gray inset; padding: 10px; background-color: #ffffdd;" id="skos-lc" class="vevent"><span class="summary"><b>The Simple Knowledge Organization System (SKOS), in the Context of Semantic Web Deployment</b></span>, Alistair Miles, University of Oxford <abbr title="20080508T1000-0400" class="dtstart">May 8th  10am</abbr> – <abbr title="20080507T1130-0400" class="dtend">6th 11:30am, 2008</abbr>, <span class="location">Montepelier Room, Madison Building, Library of Congress (<a href="http://maps.google.com/maps?f=q&hl=en&q=101+Independence+Ave+SE,+Washington,+District+of+Columbia,+District+of+Columbia+20003,+United+States&sll=37.0625,-95.677068&sspn=41.003738,62.841797&ie=UTF8&cd=2&geocode=0,38.887371,-77.004703&ll=38.887793,-77.004697&spn=0.009871,0.015342&z=16&iwloc=addr">map</a>) </span>. <a href="http://suda.co.uk/projects/microformats/hcalendar/get-cal.php?uri=http://inkdroid.org/2008/04/30/skos-in-the-context-of-semantic-web-deployment/"><img src="http://inkdroid.org/images/hcalendar.png" style="border: none; vertical-align: bottom;" /></a>

<br />

<div class="description">
Links are valuable. Links between documents, between people, between ideas, between data. Data is now a first class Web citizen, and the Web is expanding as more of these valuable networks are deployed within its fabric. Well-established knowledge organization systems like the Library of Congress Subject Headings will play a major role within these networks, as hubs, connecting people with information and providing a firm foundation for network growth as many new routes to the discovery of information emerge through the collective action of individuals. Or will they?

This talk introduces the Simple Knowledge Organization System (SKOS), a soon-to-be-completed W3C standard for publishing thesauri, classification schemes and subject headings as linked data in the Web. This talk also presents SKOS in the context of the W3C's Semantic Web Activity, and in particular the work of the W3C's Semantic Web Deployment Working Group where other specifications are being developed for publishing linked data in the Web, for embedding linked data in Web pages, and for managing Semantic Web vocabularies. Finally, this talk takes a mildly inquisitive look at the value propositions for linked data in the Web, and how LCSH might be deployed in the Web for better information discovery.

Alistair's background is in the development of Web technologies for scientific applications. He was a research associate in the e-Science department of the Rutherford Appleton Laboratory, where he was introduced to Semantic Web technologies and first developed SKOS. He has recently moved to the University of Oxford to work on linking fruit fly genomics research data, and he hopes everything he knows about the Semantic Web will turn out to be useful after all.

</div>
</div>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>196</wp:post_id>
		<wp:post_date>2008-04-30 07:33:46</wp:post_date>
		<wp:post_date_gmt>2008-04-30 14:33:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>skos-in-the-context-of-semantic-web-deployment</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="cataloging"><![CDATA[cataloging]]></category>
		<category domain="post_tag" nicename="lcsh"><![CDATA[lcsh]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="links"><![CDATA[links]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="post_tag" nicename="owl"><![CDATA[owl]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="skos"><![CDATA[skos]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[skos-in-the-context-of-semantic-web-deployment-2008-05-08loc]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>62071</wp:comment_id>
			<wp:comment_author><![CDATA[Presentation at the Library of Congress: Simple Knowledge Organization System (SKOS) in the context of Semantic Web Deployment &laquo; Alistair Miles]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://alimanfoo.wordpress.com/2008/05/13/presentation-at-the-library-of-congress-simple-knowledge-organization-system-skos-in-the-context-of-semantic-web-deployment/</wp:comment_author_url>
			<wp:comment_author_IP>72.232.101.83</wp:comment_author_IP>
			<wp:comment_date>2008-05-13 09:47:59</wp:comment_date>
			<wp:comment_date_gmt>2008-05-13 16:47:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] presentation on SKOS and Semantic Web Deployment last week at the Library of Congress. Here&#8217;s the blurb.. Links are valuable. Links between documents, between people, between ideas, between data. Data is [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>61969</wp:comment_id>
			<wp:comment_author><![CDATA[OpenSource Connections]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.opensourceconnections.com/2008/05/12/the-simple-knowledge-organization-system-skos-in-the-context-of-semantic-web-deployment/</wp:comment_author_url>
			<wp:comment_author_IP>64.202.161.130</wp:comment_author_IP>
			<wp:comment_date>2008-05-12 13:27:15</wp:comment_date>
			<wp:comment_date_gmt>2008-05-12 20:27:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<strong>The Simple Knowledge Organization System (SKOS), in the Context of Semantic Web Deployment...</strong>

This past Thursday, May 8th I had the privilege to attend a presentation at the Library of Congress by Alistair Miles, key developer of SKOS, and semantic web practitioner at the University of Oxford.  The presentation was held at the Library of Congre...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>trackback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>baby steps at linking library data</title>
		<link>http://inkdroid.org/2008/05/05/baby-steps-at-linking-library-data/</link>
		<pubDate>Tue, 06 May 2008 03:53:10 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=197</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://purl.org/net/aliman">Alistair</a> wanted to have some data to demonstrate the potential of linked library data, so I quickly converted 10K MARC records (using a slightly <a href="http://web.archive.org/web/20101216220649/http://inkdroid.org/bzr/marc-rdf/MARC21slim2RDFDC.xsl">modified</a> version of <a href="http://www.loc.gov/standards/marcxml/xslt/MARC21slim2RDFDC.xsl">MARC21slim2RDFDC.xsl</a>  and rewrote the subjects as <a href="http://web.archive.org/web/20130812145007/http://lcsh.info/">lcsh.info</a> URIs using a few lines of <a href="http://web.archive.org/web/20101216220710/http://inkdroid.org/bzr/marc-rdf/rewrite.py">python</a>...all a bit hackish, but it got this particular job done quickly.</p>

<p>The rewriting of subjects is basically a transformation of:</p>

<pre>
&lt;http://lccn.loc.gov/00009010#manifestation&gt;
  dc:creator "Rollo, David.";
  dc:date "c2000." ;
  dc:description "Includes bibliographical references (p. 173-223) and index." ;
  dc:identifier 
     "URN:ISBN:0816635463 (alk. paper)", 
     "URN:ISBN:0816635471 (pbk. : alk. paper)", 
     "http://www.loc.gov/catdir/toc/fy032/00009010.html" ;
  dc:language "eng" ;
  dc:publisher "Minneapolis : University of Minnesota Press," ;
  dc:subject 
    "Anglo-Norman literature", 
    "Benoi?t, de Sainte-More, 12th cent.", 
    "Latin prose literature, Medieval and modern", 
    "Literacy", 
    "Literature and history", 
    "Magic in literature." ;
  dc:title "Glamorous sorcery : magic and literacy in the High Middle Ages /" ;
  dc:type "text" .
</pre>

<p>to:</p>

<pre>
&lt;http://lccn.loc.gov/00009010#manifestation&gt;
    dc:creator "Rollo, David." ;
    dc:date "c2000." ;
    dc:description "Includes bibliographical references (p. 173-223) and
index." ;
    dc:identifier "URN:ISBN:0816635463 (alk. paper)", "URN:ISBN:0816635471 (pbk. : alk. paper)", "http://www.loc.gov/catdir/toc/fy032/00009010.html" ;
    dc:language "eng" ;
    dc:publisher "Minneapolis : University of Minnesota Press," ;
    dc:subject &lt;http://lcsh.info/sh85005082#concept&gt;,
      &lt;http://lcsh.info/sh85077482#concept&gt;,
      &lt;http://lcsh.info/sh85077565#concept&gt;,
      &lt;http://lcsh.info/sh85079624#concept&gt;,
      &lt;http://lcsh.info/sh86008161#concept&gt;, 
      "Benoi?t, de Sainte-More, 12th cent." ;
    dc:title "Glamorous sorcery : magic and literacy in the High Middle Ages
/" ;
    dc:type "text" .
</pre>

<p>Clearly there are lots of ways to improve even this simplified description: URIs for entries in the Name Authority File, referencing identifiers as resources rather than string literals (an artifact of the XSLT transform), removing ISBD punctuation, unicode normalization (&amp;cough;), etc.</p>

<p>You may notice I kind of fudged the URI for the book itself using the LCCN service at LC: <a href="http://lccn.loc.gov/00009010#manifestation">http://lccn.loc.gov/00009010#manifestation</a> (which does resolve, but doesn't serve up RDF yet). I'm no FRBR expert so I'm not sure if the use of "manifestation" in this hash URI makes sense. I just wanted to distinguish between the URI for the description, and the URI for the thing being described. I think it's high time for me to understand FRBR a lot more.</p>

<p>If you prefer diagrams to turtle <a href="http://inkdroid.org/images/bib-lcsh.png">here</a> is a graph visualization from the w3c rdf validator for the record.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>197</wp:post_id>
		<wp:post_date>2008-05-05 20:53:10</wp:post_date>
		<wp:post_date_gmt>2008-05-06 03:53:10</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>baby-steps-at-linking-library-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="dublincore"><![CDATA[dublincore]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="skos"><![CDATA[skos]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>61330</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.123</wp:comment_author_IP>
			<wp:comment_date>2008-05-06 05:12:05</wp:comment_date>
			<wp:comment_date_gmt>2008-05-06 12:12:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[d'oh, I just updated the post to use a link out to the img, instead of including it inline (it scaled poorly anyway) ... thanks for the heads up about the Linked Data Planet!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>61335</wp:comment_id>
			<wp:comment_author><![CDATA[Bruce D'Arcus]]></wp:comment_author>
			<wp:comment_author_email>bdarcus@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>134.53.57.142</wp:comment_author_IP>
			<wp:comment_date>2008-05-06 06:37:50</wp:comment_date>
			<wp:comment_date_gmt>2008-05-06 13:37:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm wondering about the "#manifestation" bit myself, and if Ian Davis has any thoughts.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>61326</wp:comment_id>
			<wp:comment_author><![CDATA[Danny]]></wp:comment_author>
			<wp:comment_author_email>danny.ayers@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://dannyayers.com</wp:comment_author_url>
			<wp:comment_author_IP>79.9.5.186</wp:comment_author_IP>
			<wp:comment_date>2008-05-06 03:58:19</wp:comment_date>
			<wp:comment_date_gmt>2008-05-06 10:58:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I just did a slicehost upgrade, and thought something had broken because all I could see over <a href="http://hyperdata.org/raw/" rel="nofollow">here</a> was your first paragraph then blank...
...scroll...what a diagram :-)

btw, nice work!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>perl is my mood ring</title>
		<link>http://inkdroid.org/2008/05/21/perl-is-my-mood-ring/</link>
		<pubDate>Wed, 21 May 2008 12:26:50 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=200</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Every day for the past 8 years (give or take), cron has run a little <a href="http://web.archive.org/web/20101217005819/http://inkdroid.org/bzr/bin/apod">script</a> to change my Desktop background image to the <a href="http://antwrp.gsfc.nasa.gov/apod/astropix.html">astronomy picture of the day</a>.</p>

<p>I logged in today, and this is what I got:</p>

<p><a href='/images/desktop-20080521.jpg'><img src='/images/desktop-20080521.jpg' alt='desktop 2008-05-21' width='400' class='alignnone' /></a></p>

<p>I realize it's <a href="http://antwrp.gsfc.nasa.gov/apod/ap080521.html">Gliese 876d</a>, but I took it as a statement about the current state of my psyche. Some days are just like that...</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>200</wp:post_id>
		<wp:post_date>2008-05-21 05:26:50</wp:post_date>
		<wp:post_date_gmt>2008-05-21 12:26:50</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>perl-is-my-mood-ring</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="astronomy"><![CDATA[astronomy]]></category>
		<category domain="post_tag" nicename="hell"><![CDATA[hell]]></category>
		<category domain="post_tag" nicename="humor"><![CDATA[humor]]></category>
		<category domain="post_tag" nicename="perl"><![CDATA[perl]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>justify my links</title>
		<link>http://inkdroid.org/2008/05/29/justify-my-links/</link>
		<pubDate>Thu, 29 May 2008 12:58:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=201</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Thanks to a tip from <a href="http://iandavis.com/">Ian</a>, I'm looking forward to (hopefully) attending the <a href="http://www.linkeddataplanet.com/">Linked Data Planet</a> conference in New York City as a volunteer. The idea is that I just have to pay for my hotel, and the cost of admission is waived. It seems my travel money is a bit limited at the moment (sometimes it's there, sometimes it isn't), so I figured minimizing costs would be appreciated. But today I got a request to "justify" my attendance at the conference. It was actually kind of a good exercise to sit down and write why I think the conference and <a href="http://linkeddata.org/">Linked Data</a> in general is important to the <a href="http://loc.gov">Library of Congress</a>.</p>

<blockquote>One of the challenges of Digital Repository work is modeling the context for digital objects. The context for a digital object includes the set of relationships a particular digital object has with other objects in the repository. 30 years of relational database research and development have allowed us to do this modeling pretty effectively within the scope of a particular application.

Very often, particularly in institutions the size of the Library of Congress, the context for a digital object includes digital objects found elsewhere in the enterprise--in other applications, with their own databases. In addition some institutions (like LC) also need to make their digital resources available publicly for other organizations to reference. The challenge here is in making the objects found in silos or islands of application data (typically housed in databases) reference-able and resolvable, so that other applications inside and outside the enterprise can use them.

As a practical example, a  picture of Dizzie Gilliespie found in the America Memory collection 

<div style="text-align: center;">
<a href=" http://lcweb2.loc.gov/cgi-bin/query/i?ammem/van:@field(NUMBER+@band(van+5a52027)):displayType=1:m856sd=van:m856sf=5a52027 "><img src="http://memory.loc.gov/pnp/van/5a52000/5a52000/5a52027r.jpg" /></a>
</div>

is related to the book:

<em>
  To be, or not--to bop: memoirs / Dizzy Gillespie, with Al Fraser.
</em>

which we have described in our <a href="http://lccn.loc.gov/84029213">online catalog</a>. The person Dizzy Gillespie is also represented in LC's name authority file with the <a href="http://www.loc.gov/marc/lccn.html">Library of Congress Control Number</a> n50033872, and the <a href="http://web.archive.org/web/20080829115221/http://orlabs.oclc.org/viaf/LC|n50033872">Linked Authority File at OCLC</a>. And perhaps this picture of Dizzie Gillespie in American Memory will find it's way into the <a href="http://memory.loc.gov/pnp/van/5a52000/5a52000/5a52027r.jpg">World Digital Library</a> application that is currently being built. How can we practically and explicitly identify and then represent the relationships between these resources? Is it even possible?

The Linked Data Planet conference is a two day workshop describing how to use traditional web technologies in conjunction with semantic web technologies (RDF, OWL, SPARQL, RDFa and GRDDL) to enable this sort of linking of resources inside particular applications, within the enterprise and around the world. My hope is that the conference will provide guidance on simple things LC can do with web technologies that have been in use for 20 years, to model the relationships between digital resources at the Library of Congress. 
</blockquote>

<p>Hopefully that will convince them :-)</p>

<p><em>Apologies to <a href="http://en.wikipedia.org/wiki/Justify_My_Love">Madonna</a> for the blog post title...</em></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>201</wp:post_id>
		<wp:post_date>2008-05-29 05:58:07</wp:post_date>
		<wp:post_date_gmt>2008-05-29 12:58:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>justify-my-links</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="conferences"><![CDATA[conferences]]></category>
		<category domain="post_tag" nicename="lc"><![CDATA[lc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="nyc"><![CDATA[nyc]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>SKOS displays w/ SPARQL</title>
		<link>http://inkdroid.org/2008/06/04/skos-displays-w-sparql/</link>
		<pubDate>Wed, 04 Jun 2008 13:10:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=202</guid>
		<description></description>
		<content:encoded><![CDATA[I'm just in the process of getting my head around <a href="http://www.w3.org/TR/rdf-sparql-query/">SPARQL</a> a bit more. At $work, Clay and I ran up against a situation where we wanted a query that would return a subgraph from an entire SKOS concept scheme for any assertions involving a particular concept URI as the subject. Easy enough right?

<pre>
  DESCRIBE &lt;http://lcsh.info/sh96010624#concept&gt;
</pre>

The thing is, for human readable displays we don't want to display the URIs for related concepts (skos:broader, skos:narrower or skos:related) ... we want to display the nice skos:prefLabel for them. Something akin to:

  <img src="http://inkdroid.org/images/ethnoscience.png" />

So how can we get a subgraph for a concept as well as any concept that might be directly related to it, in a single query? We came up with the following but I'd be interested in more elegant solutions:

<pre>
PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;

CONSTRUCT {&lt;http://lcsh.info/sh96010624#concept&gt; ?p1 ?o1. ?s2 ?p2 ?o2}
WHERE
{
    {&lt;http://lcsh.info/sh96010624#concept&gt; ?p1 ?o1.}
    UNION 
    {
        {&lt;http://lcsh.info/sh96010624#concept&gt; skos:narrower ?s2.}
        {?s2 ?p2 ?o2.}
    }
    UNION
    { 
        {&lt;http://lcsh.info/sh96010624#concept&gt; skos:broader ?s2.}
        {?s2 ?p2 ?o2.}
    }
    UNION
    { 
        {&lt;http://lcsh.info/sh96010624#concept&gt; skos:related ?s2.}
        {?s2 ?p2 ?o2.}
    }           
}
</pre>

The above ran quite nicely in my <a href="http://arc.semsol.org/">Arc</a> playground. Any suggestions or ideas on how to boil this down would be appreciated. I also wanted to jot this query in the likely event that I forget how I did it.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>202</wp:post_id>
		<wp:post_date>2008-06-04 06:10:03</wp:post_date>
		<wp:post_date_gmt>2008-06-04 13:10:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>skos-displays-w-sparql</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="skos"><![CDATA[skos]]></category>
		<category domain="post_tag" nicename="sparql"><![CDATA[sparql]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>BagIt</title>
		<link>http://inkdroid.org/2008/06/06/bagit/</link>
		<pubDate>Fri, 06 Jun 2008 12:57:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=203</guid>
		<description></description>
		<content:encoded><![CDATA[One little bit of goodness that has percolated out from my group at $work in collaboration with the <a href="http://www.cdlib.org/">California Digital Library</a> is the <a href="http://tools.ietf.org/id/draft-kunze-bagit">BagIt</a> spec (<a href="http://www.cdlib.org/inside/diglib/bagit/bagitspec.html">more readable version</a>). BagIt is an IETF RFC for bundling up files for transfer over the network, or for shipping on physical media. Just yesterday a little <a href="http://www.digitalpreservation.gov/news/2008/20080602news_article_bagit.html">article</a> about BagIt surfaced on the LC digital preservation website, so I figure now is a good time to mention it.

The goodness of BagIt is in its simplicity and utility. A Bag is essentially: a set of files in a particular directory named <em>data</em>, a <em>manifest file</em> which states what files ought to be in the <em>data</em> directory, and a <em>bagit.txt</em> file that states the version of BagIt. For example here's a sample (abbreviated) directory structure for a bag of digitized newspapers via the <a href="http://www.loc.gov/ndnp/">National Digital Newspaper Program</a>:

<pre>
mybag
|-- bagit.txt
|-- data
|   `-- batch_lc_20070821_jamaica
|       |-- batch.xml
|       |-- batch_1.xml
|       `-- sn83030214
|           |-- 00175041217
|           |   |-- 00175041217.xml
|           |   |-- 1905010401
|           |   |   |-- 1905010401.xml
|           |   |   `-- 1905010401_1.xml
|           |   |-- 1905010601
|           |   |   |-- 1905010601.xml
|           |   |   `-- 1905010601_1.xml
</pre>

The manifest itself is just the relative file path, and a fixity value:

<pre>
ea9dee53c2c2dd4027984a2b59f58d1f  data/batch_lc_20070821_jamaica/batch.xml
72134329a82f32dd44d59b509928b6cd  data/batch_lc_20070821_jamaica/batch_1.xml
dc5740d295521fcc692bb58603ce8d1a  data/batch_lc_20070821_jamaica/sn83030214/00175041217/1905010601/1905010601_1.xml
e16e74988ca927afc10ee2544728bd14  data/batch_lc_20070821_jamaica/sn83030214/00175041217/1905010601/1905010601.xml
fd480b2c4bcb6537c3bc4c9e7c8d7c21  data/batch_lc_20070821_jamaica/sn83030214/00175041217/1905010401/1905010401.xml
e0e4a981ddefb574fa1df98a8a55b7a4  data/batch_lc_20070821_jamaica/sn83030214/00175041217/1905010401/1905010401_1.xml
c8dffa3cdb7c13383151e0cd8263d082  data/batch_lc_20070821_jamaica/sn83030214/00175041217/00175041217.xml
</pre>

The manifest format happens to be the same format understood and generated by the common unix (and windows) utility <a href="http://en.wikipedia.org/wiki/Md5deep">md5deep</a>. So it's pretty easy to generate and validate the manifests.

The context for this work has largely been <a href="http://www.digitalpreservation.gov/">NDIIPP</a> partners (like CDL) transferring data generated by funded projects back to LC. Although it's likely to get used in some other places as well internally. It's funny to see the spec in its current state, after Justin Littman rattled off the LC Manifest wiki page in a few minutes after a meeting where <a href="http://boyko.net/andy">Andy Boyko</a> initially brought up the issue. Andy has just left LC to work for a <a href="http://www.apple.com/itunes/">record company in Cupertino</a>. I don't think I fully understood simplicity in software development until I worked with Andy. He has a real talent for boiling down solutions to their most simple expression, often leveraging existing tools to the point where very little software actually needs to be written. I think Andy and John found a natural affinity for striving for simplicity, and it shows in BagIt. Andy will be sorely missed, but that record store is lucky to get him on their team.

There are some additional cool features to BagIt, including the ability to include a <em>fetch.txt</em> file which contains http and/or rsync URIs to fill in parts of the bag from the network. We've come to refer to bags with a fetch.txt as "holey bags" because they have holes in them that need to be filled in. This allows very large bags to be assembled quickly in parallel (using a 100 line python script Andy Boyko wrote, or whatever variant of wget, curl, rsync makes you happy). Also you can include a <em>package-info.txt</em> which includes some basic metadata as key/value pairs ... designed primarily for humans.

<a href="http://eikeon.com/">Dan Krech</a> and I are in the process of creating a prototype deposit web application that will essentially allow bags to be submitted via a <a href="http://www.ukoln.ac.uk/repositories/digirep/index/SWORD">SWORD</a> (profile of<a href="http://www.rfc-editor.org/rfc/rfc5023.txt"> AtomPub</a> for Repositories) service. The SWORD part should be pretty easy, but getting the retrieval of "holey bags" kicked off and monitored propertly will be the more challenging part. Hopefully I'll be able to report more here as things develop. 

Feedback on the BagIt RFC is most welcome.





  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>203</wp:post_id>
		<wp:post_date>2008-06-06 05:57:03</wp:post_date>
		<wp:post_date_gmt>2008-06-06 12:57:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bagit</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="atompub"><![CDATA[atompub]]></category>
		<category domain="post_tag" nicename="bagit"><![CDATA[bagit]]></category>
		<category domain="post_tag" nicename="cdl"><![CDATA[cdl]]></category>
		<category domain="post_tag" nicename="checksums"><![CDATA[checksums]]></category>
		<category domain="post_tag" nicename="ietf"><![CDATA[ietf]]></category>
		<category domain="post_tag" nicename="lc"><![CDATA[lc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="standards"><![CDATA[standards]]></category>
		<category domain="post_tag" nicename="sword"><![CDATA[sword]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"64642";i:1;s:5:"64645";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>64642</wp:comment_id>
			<wp:comment_author><![CDATA[Gabriel Sean Farrell]]></wp:comment_author>
			<wp:comment_author_email>gsf@fruct.us</wp:comment_author_email>
			<wp:comment_author_url>http://fruct.us/</wp:comment_author_url>
			<wp:comment_author_IP>129.25.131.205</wp:comment_author_IP>
			<wp:comment_date>2008-06-06 06:47:58</wp:comment_date>
			<wp:comment_date_gmt>2008-06-06 13:47:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[One thing I'm struck by right away is the similarity of of BagIt to the DSpace Simple Archive Format, which I'll call SAF even though the DSpace docs fail to employ such a lovely acronym.  The unfortunate URL for that section of the docs is http://www.dspace.org/index.php?option=com_content&amp;task=view&amp;id=144#itemimporter.  Anyway, as you can see there, SAF consists of an archive directory with item subdirectories.  Each item directory contains a dublin_core.xml describing the item and a "contents" file similar to the BagIt "manifest", but without the checksums.  

BagIt is obviously more flexible, and the fetch.txt is a banging idea (one question: why aren't URLs in the fetch.txt preceded by a checksum?).  It occurs to me now that it would be simple to send an SAF item or bunch of items within a Bag, though I'm not sure what the gain there would be.  In any case, that part of SWORD (i.e. converting from BagIt to something DSpace/Fedora/etc. can ingest) should be pretty simple, so you're probably right that the retrieval of "holey bags" will be the kicker.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>64650</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Binkley]]></wp:comment_author>
			<wp:comment_author_email>peter.binkley@ualberta.ca</wp:comment_author_email>
			<wp:comment_author_url>http://www.wallandbinkley.com/quaedam/</wp:comment_author_url>
			<wp:comment_author_IP>142.244.34.117</wp:comment_author_IP>
			<wp:comment_date>2008-06-06 14:32:25</wp:comment_date>
			<wp:comment_date_gmt>2008-06-06 21:32:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[We heard about this at the PASIG meeting in San Francisco last week - it looks very handy. I didn't pick up on the possibility of holey bags there, though. Given that this is intended for delivery rather than persistence, it might be helpful to allow a best-before date in fetch.txt (maybe per line). Since this will often be used to share ingestion-friendly files, which I may not want to leave lying around in web-accessible space, the date would let me say "get it by Friday cuz it may not be there next week".]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>65445</wp:comment_id>
			<wp:comment_author><![CDATA[Jim]]></wp:comment_author>
			<wp:comment_author_email>jim@braggtown.com</wp:comment_author_email>
			<wp:comment_author_url>http://braggtown.com</wp:comment_author_url>
			<wp:comment_author_IP>152.1.24.205</wp:comment_author_IP>
			<wp:comment_date>2008-06-24 10:41:29</wp:comment_date>
			<wp:comment_date_gmt>2008-06-24 17:41:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I was pretty pleased when LC suggested we use the spec to deliver NDIIPP content to them.  Super low cost and easy to validate.  

Addressing Peter's comment regarding 'best before' dates, it's my suspicion that LC isn't in a position to fast-track data acquisition presently.  That doesn't mean that the spec, as implemented by others, wouldn't benefit from a data associated with data.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>64645</wp:comment_id>
			<wp:comment_author><![CDATA[Michael J. Giarlo]]></wp:comment_author>
			<wp:comment_author_email>httppurl.orgnetleftwing</wp:comment_author_email>
			<wp:comment_author_url>http://lackoftalent.org/michael/</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.139</wp:comment_author_IP>
			<wp:comment_date>2008-06-06 09:34:06</wp:comment_date>
			<wp:comment_date_gmt>2008-06-06 16:34:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[One quick response to gsf's query about fetch.txt, "one question: why aren’t URLs in the fetch.txt preceded by a checksum?"

The checksums for files listed in fetch.txt are already listed in the manifest.  I'm not sure how useful the length param is since the checksum is already recorded elsewhere, but I suppose it's a good way to -quickly- check if the content from a URL is what you're looking for.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>provide and enable</title>
		<link>http://inkdroid.org/2008/06/18/provide-and-enable/</link>
		<pubDate>Wed, 18 Jun 2008 18:05:24 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=204</guid>
		<description></description>
		<content:encoded><![CDATA[<img src="http://inkdroid.org/images/natarch.gif" style="float: left; border: thin inset gray; margin-right: 15px;" />I got a chance to meet Jennifer Rigby of the <a href="http://www.nationalarchives.gov.uk/">National Archives UK</a> at the <a href="http://linkeddataplanet.com">LinkedDataPlanet</a> Conference in New York City (thanks <a href="http://iandavis.com">Ian</a>).  Jennifer is the Head of IT Strategy, and told me lots of interesting stuff related to a profound shift they've had in their online strategies to:

<blockquote>
<a href="http://www.nationalarchives.gov.uk/documents/provide-enable-summary.pdf">Provide and Enable</a>
</blockquote>

So rather than pouring all their energy into making applications to visualize archival resources, the National Archives have recognized that making machine readable resources available to the public (in formats like RDF and RDFa) is really important to their core mission. In addition to <em>providing</em> services and data, they are trying to <em>enable</em> an ecosystem of innovation around their assets--or in their words:

• We will allow others to harness the power of our information, leading to a far wider range of products and services than we could provide ourselves. 
• We will continue to work with commercial partners to provide online access to millions of records.

Jennifer said we can look forward to an announcement around <a href="http://www.ukuug.org/events/opentech2008/">OpenTech2008</a> (July 5th) about a set of important publications that are going to made available by the Archives as RDF and RDFa. In addition I heard about how they work with website data harvested by <a href="http://archive.org">Internet Archive</a> to create a resolver service for transient publications on the web.

Hearing how a big organization like the National Archives can come to this realization of "Provide and Enable", and then start to execute on it was really encouraging--and inspiring.  It is also refreshing to see people recognize, in writing the importance of semantic web technologies:

<blockquote>We have started exploring new ideas and technologies, including using RDFa for publishing the Gazettes. The way we now publish legislation has a key role to play in the further development of the semantic web.</blockquote>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>204</wp:post_id>
		<wp:post_date>2008-06-18 11:05:24</wp:post_date>
		<wp:post_date_gmt>2008-06-18 18:05:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>provide-and-enable</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="government"><![CDATA[government]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>65398</wp:comment_id>
			<wp:comment_author><![CDATA[RDFa &raquo; Blog Archive &raquo; UK National Archive Uses RDFa]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20111010043548/http://rdfa.info/2008/06/23/uk-national-archive-uses-rdfa/</wp:comment_author_url>
			<wp:comment_author_IP>66.135.32.165</wp:comment_author_IP>
			<wp:comment_date>2008-06-23 10:46:05</wp:comment_date>
			<wp:comment_date_gmt>2008-06-23 17:46:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] Gazette (published since 1665), which we have already reported on, the next UK Government site reported to use RDFa is the UK National [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Content-MD5 considered helpful</title>
		<link>http://inkdroid.org/2008/06/23/content-md5-considered-helpful/</link>
		<pubDate>Mon, 23 Jun 2008 19:10:15 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=205</guid>
		<description></description>
		<content:encoded><![CDATA[Kind of an interesting <a href="http://developer.amazonwebservices.com/connect/thread.jspa?threadID=22709">thread</a> going on the Amazon Web Services Forum, about data corruption on S3. It highlights how important it is for clients to send something like the <em>Content-MD5</em> HTTP header to checksum the HTTP payload, and for the server to check it before saying 200 OK back...at least for data storage REST applications:

<blockquote>	
When Amazon S3 receives a PUT request with the Content-MD5 header, Amazon S3 computes the MD5 of the object received and returns a 400 error if it doesn't match the MD5 sent in the header. Looking at our service logs from the period between 6/20 11:54pm PDT and 6/22 5:12am PDT, we do see a modest increase in the number of 400 errors.  This may indicate that there were elevated network transmission errors somewhere between the customer and Amazon S3.  
</blockquote>

Some customers are claiming that the md5 checksums coming back from s3 are different than the ones for the content that was originally sent there. Perhaps the clients ignored the 400? Or maybe there is data corruption elsewhere. It'll be interesting to follow the thread. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>205</wp:post_id>
		<wp:post_date>2008-06-23 12:10:15</wp:post_date>
		<wp:post_date_gmt>2008-06-23 19:10:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>content-md5-considered-helpful</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="http-rest-amazon-storage-checsums-md5"><![CDATA[http rest amazon storage checsums md5]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"65401";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>65401</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Murray]]></wp:comment_author>
			<wp:comment_author_email>peter@OhioLINK.edu</wp:comment_author_email>
			<wp:comment_author_url>http://dltj.org/</wp:comment_author_url>
			<wp:comment_author_IP>192.153.30.29</wp:comment_author_IP>
			<wp:comment_date>2008-06-23 13:26:32</wp:comment_date>
			<wp:comment_date_gmt>2008-06-23 20:26:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Very interesting, indeed!  I would have thought that the reliable transport of "TCP" was enough.  Page 207 (section 13.14 "TCP Checksum Computation") of my trusty third edition of Comer's Internetworking with TCP/IP says "TCP uses 16-bit arithmetic and takes the one's complement of the one's complement sum.  At the receiving site, TCP software performs the same computation to verify that the segment [headers plus playload] arrived intact."]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>65560</wp:comment_id>
			<wp:comment_author><![CDATA[Andy Boyko]]></wp:comment_author>
			<wp:comment_author_email>andy@boyko.net</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>24.151.97.65</wp:comment_author_IP>
			<wp:comment_date>2008-06-27 13:47:15</wp:comment_date>
			<wp:comment_date_gmt>2008-06-27 20:47:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Did you see where they traced the corruption to a faulty load balancer?  (Via <a href="http://gigaom.com/2008/06/27/storage-outages-can-todays-hardware-handle-the-cloud/" rel="nofollow">GigaOm</a>)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>65729</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.123</wp:comment_author_IP>
			<wp:comment_date>2008-06-30 06:22:53</wp:comment_date>
			<wp:comment_date_gmt>2008-06-30 13:22:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[No I hadn't seen that <a href="http://gigaom.com/2008/06/27/storage-outages-can-todays-hardware-handle-the-cloud/" rel="nofollow">article</a>, thanks Andy! So is the benefit of using the Content-MD5 HTTP header in addition to the checks built in to TCP? It allows you to check the integrity of the transfer explicitly rather than implicitly by just using TCP? The header does seem redundant somehow--it would be interesting to know the history of why it was added.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>lcsh.info SPARQL endpoint</title>
		<link>http://inkdroid.org/2008/07/07/lcshinfo-sparql-endpoint/</link>
		<pubDate>Mon, 07 Jul 2008 14:45:46 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=207</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>disclaimer: <a href="http://web.archive.org/web/20130812145007/http://lcsh.info/">lcsh.info</a> was a prototype, and is no longer available, see <a href="http://id.loc.gov">id.loc.gov</a> for the service from the <a href="http://id.loc.gov">Library of Congress</a></em></p>

<p><a href="http://arc.semsol.org/"><img src="http://inkdroid.org/images/arc.gif" style="float: left; margin-right: 10px; border: 0;" /></a> I've set up a <a href="http://en.wikipedia.org/wiki/SPARQL">SPARQL endpoint</a> for <a href="http://web.archive.org/web/20130812145007/http://lcsh.info/">lcsh.info</a> at <a href="http://web.archive.org/web/20090316095546/http://sparql.lcsh.info:80/?">sparql.lcsh.info</a>. If you are new to SPARQL endpoints, they are essentially REST web services that allow you to query a pool of RDF data using a query language that combines features of pattern matching, set logic and the web, and then get back results in a variety of formats. If you are a regular expression and/or SQL junkie, and like data, then SPARQL is definitely worth taking a look at.</p>

<p>If you are new to SPARQL and/or LCSH as SKOS you can try the default query and you'll <a href="http://sparql.lcsh.info/?query=SELECT+%3Fs+%3Fp+%3Fo+WHERE+{%0D%0A++%3Fs+%3Fp+%3Fo+.%0D%0A}%0D%0ALIMIT+10%0D%0A++++++++++++&output=htmltab&jsonp=&key=">get back</a> the first 10 triples in the triple store:</p>

<pre>
SELECT ?s ?p ?p 
WHERE {?s ?p ?o}
LIMIT 10
</pre>

<p>As a first tweak try increasing the limit to 100. If you are feeling more adventurous perhaps you'd like to look up all the triples for a concept like <a href="http://sparql.lcsh.info/?query=PREFIX+skos%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2004%2F02%2Fskos%2Fcore%23%3E%0D%0A%0D%0ASELECT+%3Fs+%3Fp+%3Fo+WHERE+{%0D%0A++%3Fs+%3Fp+%3Fo+.%0D%0A++%3Fs+skos%3AprefLabel+%22Buddhism%22%40en+.%0D%0A}%0D%0ALIMIT+3000%0D%0A%0D%0A++++++++++++&output=htmltab&jsonp=&key">Buddhism</a>:</p>

<pre>
PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;

SELECT ?s ?p ?o 
WHERE {
  ?s ?p ?o .
  ?s skos:prefLabel "Buddhism"@en .
}
</pre>

<p>Or, perhaps you are interested in seeing what <a href="http://sparql.lcsh.info/?query=PREFIX+skos%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2004%2F02%2Fskos%2Fcore%23%3E%0D%0A%0D%0ASELECT+%3Fnarrower+%3Flabel+WHERE+{%0D%0A++%3Chttp%3A%2F%2Flcsh.info%2Fsh85017454%23concept%3E+skos%3Anarrower+%3Fnarrower+.%0D%0A++%3Fnarrower+skos%3AprefLabel+%3Flabel%0D%0A}%0D%0A++++++++++++&output=htmltab&jsonp=&key=">narrower terms</a> there are for Buddhism:</p>

<pre>
PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;

SELECT ?uri ?label 
WHERE {
  &lt;http://lcsh.info/sh85017454#concept&gt; skos:narrower ?uri .
  ?uri skos:prefLabel ?label
}
</pre>

<p>Or maybe you don't know the skos:prefLabel (aka authorized heading), so look for all the lcsh headings that start with <a href="http://sparql.lcsh.info/?query=PREFIX+skos%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2004%2F02%2Fskos%2Fcore%23%3E%0D%0A%0D%0ASELECT+%3Fs+%3Flabel++WHERE+{%0D%0A++%3Fs+skos%3AprefLabel+%3Flabel.%0D%0A++FILTER+regex(%3Flabel%2C+%27^independence%27%2C+%27i%27)%0D%0A}%0D%0A%0D%0A++++++++++++&output=htmltab&jsonp=&key=">Independence</a></p>

<pre>
PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;

SELECT ?s ?label  
WHERE {
  ?s skos:prefLabel ?label.
  FILTER regex(?label, '^independence', 'i')
}

</pre>

<p>Feel free to use the service however you want. I'm interested in seeing what its limitations are.</p>

<p><a href="http://bnode.org/">Benjamin Nowack's</a> <a href="http://arc.semsol.org/">ARC</a> made it extremely easy to load up the 2,441,494 LCSH triples in a few hours with a script like:</p>

<pre lang="php">
include_once('arc/ARC2.php');

$config = array(
    'db_name'               => 'arc',
    'db_user'               => 'arc',
    'db_pwd'                => 'notapassword',
    'store_name'            => 'lcsh',
    'store_log_inserts'     => 1,
);

$store = ARC2::getStore($config);

if (!$store->isSetup()) {
    $store->setUp();
}

$store->reset();
$rs = $store->query('LOAD &lt;http://lcsh.info/static/lcsh.nt&gt;');

print_r($rs);
</pre>

<p>Then it's just a simple matter of putting up a php script like:</p>

<pre lang="php">
/* ARC2 static class inclusion */
include_once('arc/ARC2.php');

/* MySQL and endpoint configuration */
$config = array(
  /* db */
  'db_host' => 'localhost', /* optional, default is localhost */
  'db_name' => 'arc',
  'db_user' => 'arc',
  'db_pwd' => 'fakepassword',

  /* store name */
  'store_name' => 'lcsh',


  /* endpoint */
  'endpoint_features' => array(
    'select', 'construct', 'ask', 'describe'
  ),
  'endpoint_timeout' => 60, /* not implemented in ARC2 preview */
  'endpoint_read_key' => '', /* optional */
  'endpoint_write_key' => 'fakekey', /* optional */
  'endpoint_max_limit' => 1000, /* optional */
);

/* instantiation */
$ep = ARC2::getStoreEndpoint($config);

/* request handling */
$ep->go();
</pre>

<p>Ideally I would've been able to quickly bring up a SPARQL endpoint on top of the <a href="http://rdflib.net/">rdflib</a> Sleepycat triple store that is being used to serve up the linked data at <a href="http://web.archive.org/web/20130812145007/http://lcsh.info/">lcsh.info</a>. But rather that pursuing elegance (this is kinda side work after all), I wanted to quickly put the SPARQL service out there for experimentation, and this was the quickest way for me to do that. If the service proves useful I'll look more at what it takes to create an rdflib SPARQL service, or porting over the little python code I have to php (gasp).</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>207</wp:post_id>
		<wp:post_date>2008-07-07 07:45:46</wp:post_date>
		<wp:post_date_gmt>2008-07-07 14:45:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lcshinfo-sparql-endpoint</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="lcsh"><![CDATA[lcsh]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="post_tag" nicename="php"><![CDATA[php]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="sparql"><![CDATA[sparql]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>70943</wp:comment_id>
			<wp:comment_author><![CDATA[Alistair]]></wp:comment_author>
			<wp:comment_author_email>alistair.rutherford@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.netthreads.co.uk</wp:comment_author_url>
			<wp:comment_author_IP>217.204.65.2</wp:comment_author_IP>
			<wp:comment_date>2008-08-18 05:36:01</wp:comment_date>
			<wp:comment_date_gmt>2008-08-18 12:36:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed, As part of testing a Flex app I have developed which queries SPARQL endpoints I have pointed it at your endpoint. 

You can see a demo of it here:

http://ccgi.arutherford.plus.com/website/flex/dbPedia/fb3/sparqlQueryViewer/#

Anyway, I am stuck in the first simple query. The format I request back is XML. The XML comes back but the parser throws an error because there appears to be some strange characters tagged onto the result. Specifically the number 0 and a bunch of spaces.

You can test this your self by pulling sticking this proxy request into IE:

http://ccgi.arutherford.plus.com/cgi-bin/sparql-proxy-al.cgi?format=application%2Fsparql%2Dresults%2Bxml&amp;query=PREFIX%20skos%3A%20%3Chttp%3A%2F%2Fwww%2Ew3%2Eorg%2F2004%2F02%2Fskos%2Fcore%23%3E%20%0D%0ASELECT%20%3Fs%20%3Fp%20%3Fo%20%0DWHERE%20%7B%20%0D%20%20%3Fs%20%3Fp%20%3Fo%20%2E%20%0D%20%20%3Fs%20skos%3AprefLabel%20%27Buddhism%27%40en%20%2E%20%0D%7D%20%0D%20LIMIT%2040&amp;url=http%3A%2F%2Fsparql%2Elcsh%2Einfo%2F


Just paste it in as is.

All the best,
Al.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>70956</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.123</wp:comment_author_IP>
			<wp:comment_date>2008-08-18 08:51:50</wp:comment_date>
			<wp:comment_date_gmt>2008-08-18 15:51:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Alistair, I upgraded to the latest version of <a href="http://arc.semsol.org/download" rel="nofollow">Arc</a> and this bug went away. Thanks for letting me know about it!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>66332</wp:comment_id>
			<wp:comment_author><![CDATA[Bruce D'Arcus]]></wp:comment_author>
			<wp:comment_author_email>bdarcus@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>24.210.249.184</wp:comment_author_IP>
			<wp:comment_date>2008-07-07 16:26:13</wp:comment_date>
			<wp:comment_date_gmt>2008-07-07 23:26:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Would be nice if it was almost as easy to setup an endpoint with rdflib for those of us not exactly thrilled with using php. Will be curious see what you come up with.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>identi.ca and linked data</title>
		<link>http://inkdroid.org/2008/07/11/identica-and-linked-data/</link>
		<pubDate>Fri, 11 Jul 2008 13:26:55 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=208</guid>
		<description></description>
		<content:encoded><![CDATA[<p>If you've already caught the <a href="http://en.wikipedia.org/wiki/Micro-blogging">micro-blogging</a> bug <a href="http://identi.ca">identi.ca</a> is an interesting twitter clone for a <a href="http://times.usefulinc.com/2008/07/03-identica">variety</a> of <a href="http://danbri.org/words/2008/07/10/367">reasons</a>...not the least of which is that it's an <a href="http://web.archive.org/web/20080602015854/http://laconi.ca:80/">open source project</a>, and has been designed to run in a decentralized way. The thing I was pleasantly surprised to see was FOAF exports like <a href="http://web.archive.org/web/20130308050525/http://identi.ca/edsu/foaf">this</a> for user networks, and HTTP URIs for foaf:Person resources:</p>

<pre>
ed@hammer:~$ curl -I http://identi.ca/user/6104
HTTP/1.1 302 Found
Date: Fri, 11 Jul 2008 12:58:56 GMT
Server: Apache/2.2.8 (Ubuntu) PHP/5.2.4-2ubuntu5.1 with Suhosin-Patch
X-Powered-By: PHP/5.2.4-2ubuntu5.1
Status: 303 See Other
Location: http://identi.ca/edsu
Content-Type: text/html
</pre>

<p>It looks like there's a slight bug in the way the HTTP status is being returned, but clearly the intent was to do the right thing by <a href="http://norman.walsh.name/2005/06/19/httpRange-14">httpRange-14</a>. If I have time I'll get laconi.ca running locally so I can confirm the bug, and attempt a fix.</p>

<p>It's also cool to see that <a href="http://evan.prodromou.name/">Evan Prodromou</a> (the lead developer, and creator of identi.ca and laconi.ca) has opened a <a href="http://web.archive.org/web/20080731120253/http://laconi.ca:80/PITS/00047">couple</a> <a href="http://web.archive.org/web/20080731120258/http://laconi.ca:80/PITS/00048">tickets</a> for adding <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a> to various pages. If I have the time this would be a fun hack as well. I'd also like to take a stab at doing conneg at foaf:Person URIs to enable this sorta thing:</p>

<pre>
ed@hammer:~$ curl -I --header "Content-type: application/rdf+xml" http://identi.ca/user/6104
HTTP/1.1 303 See Other
Date: Fri, 11 Jul 2008 13:08:42 GMT
Server: Apache/2.2.8 (Ubuntu) PHP/5.2.4-2ubuntu5.1 with Suhosin-Patch
X-Powered-By: PHP/5.2.4-2ubuntu5.1
Location: http://web.archive.org/web/20130308050525/http://identi.ca/edsu/foaf
</pre>

<p>instead of what happens currently:</p>

<pre>
ed@hammer:~$ curl -I --header "Content-type: application/rdf+xml" http://identi.ca/user/6104
HTTP/1.1 302 Found
Date: Fri, 11 Jul 2008 13:08:42 GMT
Server: Apache/2.2.8 (Ubuntu) PHP/5.2.4-2ubuntu5.1 with Suhosin-Patch
X-Powered-By: PHP/5.2.4-2ubuntu5.1
Status: 303 See Other
Location: http://identi.ca/edsu
Content-Type: text/html
</pre>

<p>I guess this is also just a complicated way of saying I'm <a href="http://identi.ca/user/6104">edsu</a> on identi.ca--and that the opportunity to learn more about <a href="http://oauth.net">OAuth</a> and <a href="http://www.xmpp.org/">XMPP</a> is a compelling enough reason alone for me to make the switch.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>208</wp:post_id>
		<wp:post_date>2008-07-11 06:26:55</wp:post_date>
		<wp:post_date_gmt>2008-07-11 13:26:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>identica-and-linked-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="blogging"><![CDATA[blogging]]></category>
		<category domain="post_tag" nicename="foaf"><![CDATA[foaf]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="oauth"><![CDATA[oauth]]></category>
		<category domain="post_tag" nicename="php"><![CDATA[php]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="xmpp"><![CDATA[xmpp]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>78582</wp:comment_id>
			<wp:comment_author><![CDATA[Bookmarks about Foaf]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.remmrit.com/foaf</wp:comment_author_url>
			<wp:comment_author_IP>67.228.47.154</wp:comment_author_IP>
			<wp:comment_date>2008-12-03 16:45:11</wp:comment_date>
			<wp:comment_date_gmt>2008-12-03 23:45:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] - bookmarked by 5 members originally found by candid on 2008-10-26  identi.ca and linked data  http://inkdroid.org/2008/07/11/identica-and-linked-data/ - bookmarked by 6 members [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>premis v2.0 and schema munging</title>
		<link>http://inkdroid.org/2008/07/21/premis-v20-and-schema-munging/</link>
		<pubDate>Tue, 22 Jul 2008 02:14:17 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=209</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In an effort to get a better understanding of <a href="http://www.loc.gov/standards/premis/">PREMIS</a> after reading about the <a href="http://listserv.loc.gov/cgi-bin/wa?A2=ind0807&L=pig&T=0&P=152">v2.0 release</a>, I dug around for 5 minutes looking for a way to convert an XML Schema to RelaxNG. The theory being that the compact syntax of RelaxNG would be easier to read than the XSD.</p>

<p>I ended up with a little hack suggested <a href="http://web.archive.org/web/20120509194237/http://postneo.com/2007/01/16/all-i-want-to-do-is-convert-my-schema">here</a> to chain together the rngconv from the <a href="http://web.archive.org/web/20070630182011/https://msv.dev.java.net/">Multi-Schema Validator</a> and James Clarke's <a href="http://www.thaiopensource.com/relaxng/trang.html">Trang</a>, which oddly can't read an XSD as input.</p>

<pre lang="bash">
#!/bin/bash

for i in $*
do
  BN=$(basename $i .xsd)
  java -jar /opt/rngconv/rngconv.jar ${i} > /tmp/${BN}.rng
  java -jar /opt/trang/trang.jar -I rng -O rnc /tmp/${BN}.rng ${BN}.rnc
done
</pre>

<p>The resulting  RelaxNG can be seen <a href="http://inkdroid.org/data/premis.rnc">here</a>. As you can see I'm not sure it helps much...but it's a start I guess. I'm interested in looking at what it might take to sublimate an PREMIS RDF vocabulary (hopefully just RDFS?) out of the XSD, mainly because I <em>think</em> parts of the vocabulary could prove useful in <a herf="http://www.openarchives.org/ore/">OAI-ORE</a> resource maps.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>209</wp:post_id>
		<wp:post_date>2008-07-21 19:14:17</wp:post_date>
		<wp:post_date_gmt>2008-07-22 02:14:17</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>premis-v20-and-schema-munging</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bash"><![CDATA[bash]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="premis"><![CDATA[premis]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="relaxng"><![CDATA[relaxng]]></category>
		<category domain="post_tag" nicename="schemas"><![CDATA[schemas]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>68700</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &raquo; Blog Archive &raquo; RepoCamp recap]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2008/07/28/repocamp-recap/</wp:comment_author_url>
			<wp:comment_author_IP>208.83.140.70</wp:comment_author_IP>
			<wp:comment_date>2008-07-28 06:29:44</wp:comment_date>
			<wp:comment_date_gmt>2008-07-28 13:29:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid beep beep      &laquo; premis v2.0 and schema munging [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>67675</wp:comment_id>
			<wp:comment_author><![CDATA[John Cowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url>http://www.ccil.org/~cowan</wp:comment_author_url>
			<wp:comment_author_IP>74.68.115.13</wp:comment_author_IP>
			<wp:comment_date>2008-07-21 20:38:31</wp:comment_date>
			<wp:comment_date_gmt>2008-07-22 03:38:31</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Trang is intended to produce a conversion that's isomorphic to the original, whereas the MSV produces a conversion that's equivalent only in effect.  Nobody has written an isomorphism-preserving input module for XSD; I suspect that if James considered it too hard, nobody else has wanted to bother trying.  Jing uses Xerces directly for XSD validation, bypassing the RNG engine altogether.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>RepoCamp recap</title>
		<link>http://inkdroid.org/2008/07/28/repocamp-recap/</link>
		<pubDate>Mon, 28 Jul 2008 13:29:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=210</guid>
		<description></description>
		<content:encoded><![CDATA[<p>So <a href="http://barcamp.org/RepoCamp">RepoCamp</a> was a lot of fun. The goal was to discuss repository interoperability--and at the very least repository <em>practitioners</em> got to interoperate, and have a few beers afterwards. Hats off to <a href="http://dfflanders.wordpress.com/">David Flanders</a> who clearly has got running these events down to a fine art.</p>

<p>I finally got to meet <a href="http://oxfordrepo.blogspot.com/">Ben O'Steen</a> after bantering with him on <a href="irc://freenode.net/code4lib">#code4lib</a> and <a href="irc://freenode.net/talis">#talis</a> ... and also got to chat with <a href="/http://wwmm.ch.cam.ac.uk/blogs/downing/">Jim Downing</a> (Cambridge Univ) about <a href="http://www.ukoln.ac.uk/repositories/digirep/index/SWORD">SWORD</a> stuff, and Stephan Drescher (Los Alamos National Lab) about <a href="http://web.archive.org/web/20100408161955/http://african.lanl.gov/ovalnet/validate.jsp">validating OAI-ORE</a>.</p>

<p>Stephan and I had a varied and wide ranging discussion about the web in general, which was a lot of fun. I really dug his metaphor of the web as an aquatic ecosystem, with interdependent organisms and shared environments. It reminded me a bit of how shocked I was to <a href="http://twitter.pbwiki.com/Communities">discover</a> how rich and varied the ecosystem is around a "simple" service like <a href="http://twitter.com">twitter</a>. If I ever return to school it will be to study something along the lines of <a href="http://webscience.org/">web science</a>.</p>

<p>It was also interesting to hear that other people saw a parallel between <a href="http://www.openarchives.org/ore/0.9/primer.html#ResourceMap">OAI-ORE Resource Maps</a> and<a href="http://tools.ietf.org/html/draft-kunze-bagit"> BagIt's</a> fetch.txt. The parallel being that both resource maps and bags are aggregations of web resources. Of course bags can also just be files on disk, it's when the fetch.txt is present in the bag that the package is made up of web resources. It would be interesting to see what vocabularies are available for expressing fixity information (md5 checksums and the like), and if they could be layered into the resource map atom serialization. Perhaps <a href="http://inkdroid.org/2008/07/21/premis-v20-and-schema-munging/">PREMIS v2.0</a>? It might be fun to code up what a simple OAI-ORE resource map harvester would look like, that checked fixity values -- using LC's existing BagIt parallelretriever.py as a starting point. God I wish I could just hyperlink to that :-(</p>

<p>At any rate, I now need to investigate <a href="http://oauth.net/">OAuth</a> because Jim thinks it fits really nicely with <a href="http://bitworking.org/projects/atom/rfc5023.html">AtomPub</a> and <a href="http://www.ukoln.ac.uk/repositories/digirep/index/SWORD">SWORD</a> in particular. And if it's good enough for <a href="http://www.readwriteweb.com/archives/google_oauth.php">Google</a> it's probably worth checking out. Jim also said that there is a possibility that the SWORD 2.0 might take shape as an IETF RFC, which would be good to see.</p>

<p>Thanks to all that made it happen, and for all of you that traveled long distances to join us at the Library of Congress.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>210</wp:post_id>
		<wp:post_date>2008-07-28 06:29:19</wp:post_date>
		<wp:post_date_gmt>2008-07-28 13:29:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>repocamp-recap</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="atom"><![CDATA[atom]]></category>
		<category domain="post_tag" nicename="atompub"><![CDATA[atompub]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="loc"><![CDATA[loc]]></category>
		<category domain="post_tag" nicename="oai-ore"><![CDATA[oai-ore]]></category>
		<category domain="post_tag" nicename="oauth"><![CDATA[oauth]]></category>
		<category domain="post_tag" nicename="premis"><![CDATA[premis]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="sword"><![CDATA[sword]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>resource maps and site maps</title>
		<link>http://inkdroid.org/2008/08/01/resource-maps-and-site-maps/</link>
		<pubDate>Fri, 01 Aug 2008 13:00:02 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=241</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.openarchives.org/ore/"><img src="http://inkdroid.org/images/ore.png" style="margin-right: 10px; border: none; float: left;" /></a><a href="http://efoundations.typepad.com/efoundations/2008/08/seo-and-digital.html">Andy</a> reminds me that a relatively simple idea (I think it was <a href="http://dfflanders.wordpress.com/">David's</a> at <a href="http://barcamp.org/RepoCamp">RepoCamp</a>) for the <a href="http://groups.google.com/group/oai-ore/browse_thread/thread/3c743774cef00e23">OAI-ORE Challenge</a> would be to create a tool that transformed OAI-ORE resource maps expressed as <a href="http://www.openarchives.org/ore/0.9/atom-implementation.html">Atom</a> into <a href="https://www.google.com/webmasters/tools/docs/en/protocol.html">Google Site Maps</a>. This would allow "repositories" that exposed their "objects" as resource maps, to easily be crawled by Google and others. 

It would also be useful to demonstrate what value-add OAI-ORE resource maps give you: to answer the question of why not just generate the site map and be done with it. I think there definitely are advantages, such as being able to identify compound objects or aggregations of web resources, and then make assertions about them (a.k.a. attach metadata to them).]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>241</wp:post_id>
		<wp:post_date>2008-08-01 06:00:02</wp:post_date>
		<wp:post_date_gmt>2008-08-01 13:00:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>resource-maps-and-site-maps</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="atom"><![CDATA[atom]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="harvesting"><![CDATA[harvesting]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="oai-ore"><![CDATA[oai-ore]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"69317";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>69317</wp:comment_id>
			<wp:comment_author><![CDATA[Sean Gillies]]></wp:comment_author>
			<wp:comment_author_email>sgillies@frii.com</wp:comment_author_email>
			<wp:comment_author_url>http://sgillies.net/</wp:comment_author_url>
			<wp:comment_author_IP>216.17.140.139</wp:comment_author_IP>
			<wp:comment_date>2008-08-01 09:42:46</wp:comment_date>
			<wp:comment_date_gmt>2008-08-01 16:42:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Google, Yahoo, and Microsoft already support Atom as a sitemap format. Does OAI-ORE bring anything else to the table (in the way of more specific sitemapping instructions) that would warrant another serialization?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>69756</wp:comment_id>
			<wp:comment_author><![CDATA[Michael L. Nelson]]></wp:comment_author>
			<wp:comment_author_email>mln@cs.odu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://www.cs.odu.edu/~mln/</wp:comment_author_url>
			<wp:comment_author_IP>72.218.88.57</wp:comment_author_IP>
			<wp:comment_date>2008-08-04 14:30:09</wp:comment_date>
			<wp:comment_date_gmt>2008-08-04 21:30:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[We considered sitemaps as a serialization format early on, but their limitation of 
location limits their usefulness.  For example, /a/b/sitemap.xml could aggregate 
/a/b/foo.html, but not /a/c/bar.html or /x/y/z.html.  For some repos this might
not be a problem, but it would prevent things like arXiv.org aggregating resources
in citebase.org.

(see http://www.sitemaps.org/protocol.php#location).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>69377</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.230.151</wp:comment_author_IP>
			<wp:comment_date>2008-08-01 18:16:38</wp:comment_date>
			<wp:comment_date_gmt>2008-08-02 01:16:38</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[GoogleSiteMaps support things like changefreq and priority that don't have an analog in the Atom world. But the main problem with repositories is discovery, so perhaps simply making OAI-ORE resource maps available as Atom will be enough eh? Thanks for the comment.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>We&#039;ve got five years, my brain hurts a lot</title>
		<link>http://inkdroid.org/2008/08/07/weve-got-five-years-my-brain-hurts-a-lot/</link>
		<pubDate>Thu, 07 Aug 2008 13:47:21 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=251</guid>
		<description></description>
		<content:encoded><![CDATA[Recently there's been a few discussions about persistent identifiers on the web: in particular one about the <a href="http://lists.w3.org/Archives/Public/www-tag/2008Aug/0005.html">persistence of XRIs</a>, and another about the <a href="http://www.crossref.org/CrossTech/2008/07/five_years.html">use of</a> HTTP URIs in semantic web applications like <a href="http://dbpedia.org">dbpedia</a>. 

As you probably know already, the w3c <a href="http://lists.w3.org/Archives/Public/www-tag/2008May/0078">publicly recommended</a> against the use of Extensible Resource Identifiers (<a href="http://www.oasis-open.org/committees/xri/faq.php">XRI</a>). The net effect of this was to <a href="http://www.equalsdrummond.name/?p=130">derail</a> the standardization of XRIs within <a href="http://www.oasis-open.org">OAISIS</a> itself. Part of the process that Ray Denenberg (my colleague at the Library of Congress) helped kick off was a further discussion between XRI people and the w3c-tag about what XRI specifically provides that HTTP URIs do not. Recently that discussion hit a key point by Stuart Williams:

<blockquote>
... the point that I'm trying to make is that the issue is with the social and administrative policies associated with the DNS system - and the solution is to establish a separate namespace outside the DNS system that has different social/adminsitrative policies (particularly wrt persistent name segments) that better suits the requirements of the XRI community. There is the question as to whether that alternate social/administrative system will endure into the long term such the the persistence intended guarantees endure... or not - however that will largely be determined by market forces (adoption) and 'crudely' the funding regime that enables the administrative structure of XRI to persist - and probably includes the use of IPRs to prevent duplicate/alternate root <a href="http://en.wikipedia.org/wiki/Alternative_DNS_root">problems</a> which we have seen  in the DNS world.
</blockquote>

It'll be interesting to see the response. I basically have the same issue with <a href="http://en.wikipedia.org/wiki/Digital_object_identifier">DOIs</a> and the <a href="http://www.handle.net/">Handle System</a> that they depend on. Over at <a href="http://www.crossref.org/CrossTech/2008/07/five_years.html">CrossTech</a> Tony Hammond suggests that the Handle System would make RDF assertions such as those that involve <a href="http://dbpedia.org">DBPedia</a> more persistent. But just how isn't entirely clear to me. It seems that Handles like URLs are only persistent to the degree that they are maintained. 

I'd love to see a use case from Tony that describes just how DOIs and the Handle System would provide more persistence than HTTP URLs in the context of RDF assertions involving dbpedia. As Stuart said eloquently in his email:

<blockquote>
Again just seeking to understand - not to take a particular position
</blockquote>

<em>PS. Sorry if the blog post title is too cryptic, it's Bowie's <a href="http://www.lyricsfreak.com/d/david+bowie/five+years_20036908.html">"Five Years"</a> which Tony's post (perhaps intentionally) reminded me of :-)</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>251</wp:post_id>
		<wp:post_date>2008-08-07 06:47:21</wp:post_date>
		<wp:post_date_gmt>2008-08-07 13:47:21</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>weve-got-five-years-my-brain-hurts-a-lot</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="identifiers"><![CDATA[identifiers]]></category>
		<category domain="post_tag" nicename="standards"><![CDATA[standards]]></category>
		<category domain="post_tag" nicename="uri"><![CDATA[uri]]></category>
		<category domain="post_tag" nicename="url"><![CDATA[url]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="xri"><![CDATA[xri]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>70064</wp:comment_id>
			<wp:comment_author><![CDATA[Tony Hammond]]></wp:comment_author>
			<wp:comment_author_email>t.hammond@nature.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>195.138.194.1</wp:comment_author_IP>
			<wp:comment_date>2008-08-07 09:04:08</wp:comment_date>
			<wp:comment_date_gmt>2008-08-07 16:04:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed:

You're right - and then you're wrong!

Of course it was Bowie's "Five Years" that I was riffing in the post title - "Five years, that's all we've got".  (And yes I know it's really 10 years that IANA caps DNS registations at, but Dan Brickley actually mentioned a figure of 5 years in his post and that was too good to turn down.)

But I do believe you misrepresent my post and my follow up comments where I was not advocating the use of handles or DOIs. In fact I never referred to either in the original post and only fleetingly in my comments, e.g.

"My point was not that HTTP URIs are bad or that DOI is any way better but rather that there is a fairly general belief that URLs are pretty stable things if managed carefully and that once somebody procures (er, leases) a DNS auth name they can churn out semantic terms till the cows come home."

The purpose of the post was to point out the fact that HTTP URIs are commonly based on DNS (leastways the W3C TAG seems to think so) and as such have an inherent time limitation associated with them which was the reason behind Dan's original post. So, there was obviously some cause for concern which motivated his post. And yet, this time limitation is nowhere (that I know of) talked about in the semantic web canon.

Please do me the kindness to re-read what I said because I think you do me a mischief.

Cheers,

Tony]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>70095</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.230.151</wp:comment_author_IP>
			<wp:comment_date>2008-08-07 20:33:22</wp:comment_date>
			<wp:comment_date_gmt>2008-08-08 03:33:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Point taken. Sometimes it's difficult to take one thing that someone says out of the context of things that they have said in the past :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>lingvoj</title>
		<link>http://inkdroid.org/2008/08/13/lingvoj/</link>
		<pubDate>Wed, 13 Aug 2008 13:12:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=272</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData"><img src="http://inkdroid.org/images/lod.jpg" style="margin-right: 20px; margin-bottom: 20px; float: left;"/></a>

I'm just now running across <a href="http://www.lingvoj.org/">lingvoj.org</a>, a linked-data application for languages created by <a href="http://universimmedia.blogspot.com/">Bernard Vatant</a>. lingvoj basically mints URIs for languages (using the ISO-639-1 code) and when resolved (yay HTTP) nice human and machine readable descriptions about the language are returned. So for example the URI for Chinese is:

<blockquote>
<a href="http://www.lingvoj.org/lang/zh">http://www.lingvoj.org/lang/zh</a>
</blockquote>

If you click on that link, your browser will display some HTML that describes the Chinese language, and if a client wants "application/rdf+xml" it'll get back a nice chunk of rdf -- all via a 303 redirect <a href="http://www.w3.org/TR/cooluris/">as it should be</a>.

lingvoj is interesting for a few reasons:

<ul>

<li>I work at the Library of Congress, who are the maintainers of <a href="http://www.loc.gov/standards/iso639-2/">iso639-2</a>, and I know someone experimenting with a linked-data application for delivering it.</li>

<li>I know software developers at LC and elsewhere who need access to this data in a predictable and explicit machine readable format, which lends itself to being updated (re-harvesting language URIs).</li>

<li>lingvoj follows the <a href="http://www.w3.org/TR/cooluris/#r303gendocument">303 URIs forwarding to One Generic Document</a> pattern, which is nice to see in practice. I also learned about the use of <code>rdfs:isDefinedBy</code> to assert (in this case) that a language is defined by the HTML representation for the language. Not sure how I missed that in the Cool URIs document before.</li>

<li>There are <code>owl:sameAs</code> links between lingvoj and <a href="http://dbpedia.org/resource/Chinese_language">dbpedia</a> and <a href="http://sw.cyc.com/2006/07/27/cyc/ChineseLanguageSet">opencyc</a>, which in turn are linked data, and allow an agent to walk outwards and discover more about a language. Maybe one day lingvoj could link to our ISO693-2 codelist at LC?
</li>

<li>lingvoj defines a <a href="http://www.lingvoj.org/ontology">vocabulary</a> which includes a new OWL class <code>Lingvo</code> for languages, that happens to extend <a href="http://purl.org/dc/terms/LinguisticSystem">dcterms:LinguisticSystem</a>.</li>

</ul>

It's a lot o' fun discovering this emerging, rich data-universe on the web. If you are the least bit curious take a look for yourself:

<pre>
  <a href="http://curl.haxx.se/">curl</a> --location --header "Accept: application/rdf+xml" http://www.lingvoj.org/lang/zh
</pre>

Or better yet:

<pre>
  <a href="http://librdf.org/raptor/rapper.html">rapper</a> -o turtle http://lingvoj.org/lang/zh
</pre>

Or if you are really adventurous grab the <a href="http://www.lingvoj.org/lingvoj.rdf">whole data set</a> and put it into your triple-store-du-jour.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>272</wp:post_id>
		<wp:post_date>2008-08-13 06:12:19</wp:post_date>
		<wp:post_date_gmt>2008-08-13 13:12:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lingvoj</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="dublincore"><![CDATA[dublincore]]></category>
		<category domain="post_tag" nicename="languages"><![CDATA[languages]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="uris"><![CDATA[uris]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[lingvoj-linked-data-for-languages]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"71213";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>70581</wp:comment_id>
			<wp:comment_author><![CDATA[Kingsley Idehen]]></wp:comment_author>
			<wp:comment_author_email>kidehen@openlinksw.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.openlinksw.com/blog/~kidehen</wp:comment_author_url>
			<wp:comment_author_IP>24.91.210.172</wp:comment_author_IP>
			<wp:comment_date>2008-08-13 09:02:24</wp:comment_date>
			<wp:comment_date_gmt>2008-08-13 16:02:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed,

People can also explore via:
http://demo.openlinksw.com/rdfbrowser2/?uri=http%3A%2F%2Fwww.lingvoj.org%2Flang%2Fzh

and other RDF Browsers / Linked Data Explorers :-)

BTW - OpenID base authentication isn't working re. this comments interface.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>71213</wp:comment_id>
			<wp:comment_author><![CDATA[Ed Summers]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/</wp:comment_author_url>
			<wp:comment_author_IP>68.48.230.151</wp:comment_author_IP>
			<wp:comment_date>2008-08-21 03:31:19</wp:comment_date>
			<wp:comment_date_gmt>2008-08-21 10:31:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the links to the openlink rdfbrowser. Sometimes I think the linked data browsers actually hide the simplicity of what's going on ... which is something I'm trying to communicate. But still I imagine the browsers are good for some folks who aren't interested in the HTTP interactions with URIs.

I'm trying to submit this comment with my openid. So if you see this it worked for me :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80851</wp:comment_id>
			<wp:comment_author><![CDATA[User links about "rdfs" on iLinkShare]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.ilinkshare.com/tagged/rdfs</wp:comment_author_url>
			<wp:comment_author_IP>67.228.47.154</wp:comment_author_IP>
			<wp:comment_date>2009-02-08 15:03:31</wp:comment_date>
			<wp:comment_date_gmt>2009-02-08 22:03:31</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Towards an Implementation Model for Library Catalogs ...&gt;&gt; saved by Mattychan 39 days ago1 voteslingvoj&gt;&gt; saved by maphon 40 days ago6 votesBack from ESWC 2008&gt;&gt; saved by SasukeXluversXunite 41 days ago5 [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>mmalmsten++</title>
		<link>http://inkdroid.org/2008/08/21/mmalmsten/</link>
		<pubDate>Thu, 21 Aug 2008 13:29:13 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=309</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://permalink.gmane.org/gmane.culture.libraries.ngc4lib/4617">Holy crap</a> ... now I need to listen to <a href="http://blogs.talis.com/panlibus/archives/2008/08/semantic-future-for-libraries-martin-marlmsten-talks-with-talis.php">this</a>. It's so nice to know you're not alone, and off on another planet.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>309</wp:post_id>
		<wp:post_date>2008-08-21 06:29:13</wp:post_date>
		<wp:post_date_gmt>2008-08-21 13:29:13</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>mmalmsten</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Martin Malmsten and linked library data</title>
		<link>http://inkdroid.org/2008/09/02/martin-malmsten-and-linked-library-data/</link>
		<pubDate>Tue, 02 Sep 2008 14:13:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=315</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I'm currently listening to Richard Wallis' <a href="http://blogs.talis.com/panlibus/archives/2008/08/semantic-future-for-libraries-martin-marlmsten-talks-with-talis.php">interview</a> w/ Martin Malmsten of the Royal Library of Sweden. It's a really fascinating view inside a library, and the mind of a developer that are <a href="http://article.gmane.org/gmane.culture.libraries.ngc4lib/4617">publishing bibliographic resources as linked data</a>.</p>

<p>Partly as a dare from Roy Tennant to do something useful with linked-data, I spent 30 minutes w/ <a href="http://rdflib.net">rdflib</a> creating a very simplistic (42 lines of code) <a href="http://web.archive.org/web/20101216213250/http://inkdroid.org/bzr/linked-data-crawler/crawl.py">crawler</a> that can walk the links in the Royal Library's linked data, and store the bibliographic resources encountered. I ran it over the weekend (it had a 3 second sleep between requests, so as not to arouse the ire of the Royal Library of Sweden), and it ended up pulling down <a href="http://web.archive.org/web/20090106021214/http://inkdroid.org/bzr/linked-data-crawler/libris.kb.se.n3">919,190 triples</a> describing a variety of resources (kind of a fun unix hack here to get the types of resources in a ntriples rdf dump):</p>

<pre>
ed@hammer:~/bzr/linked-data-crawler$ grep 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type' libris.kb.se.n3 \
  | cut -f 3 -d " " \
  | sort \ 
  | uniq -c \
  | sort -rn
  18445 &lt;http://purl.org/ontology/bibo/Book&gt;.
   1686 &lt;http://purl.org/ontology/bibo/Article&gt;.
    258 &lt;http://www.w3.org/2004/02/skos/core#Concept&gt;.
    245 &lt;http://purl.org/ontology/bibo/Film&gt;.
    237 &lt;http://xmlns.com/foaf/0.1/Organization&gt;.
    219 &lt;http://xmlns.com/foaf/0.1/Person&gt;.
     58 &lt;http://purl.org/ontology/bibo/Periodical&gt;.
      4 &lt;http://purl.org/ontology/bibo/Map&gt;.
      4 &lt;http://purl.org/ontology/bibo/Manuscript&gt;.
      1 &lt;http://purl.org/ontology/bibo/Collection&gt;.
</pre>

<p>As I pointed out on <a href="http://article.gmane.org/gmane.culture.libraries.ngc4lib/4647">ngc4lib</a>, the purpose of this wasn't to display any technical prowess--much to the contrary, it was to share how the nature of linked-data being on the web we know and love makes it natural to work with.</p>

<p>One of the many gems in the interview, was Martin's response to Richard's question about whether the "semantic web" that we talk about today is subtly different than the semantic web that was introduced in <a href="http://www.sciam.com/article.cfm?id=the-semantic-web">2001</a>.</p>

<blockquote>
People saw the words "semantic web" and then they sort of forgot the <em>web</em> part, and started to work on the <em>semantic</em> part (vocabularies)--and that can become arbitrarily complex. If you forget the <em>web</em> part then it is just metadata, and then people can ask "ok, you have this semantics thing and we have marc21, it's not really that different" and they'd be right. But now linked data is starting to feed the semantic <em>web</em>, and it's the <em>web</em> part that makes it special. <em>(about 34:00 into the interview)</em>.
</blockquote>

<p>I'm not an expert on the history of the web and libraries, but this seems to be spot on to me. The notion that traditional library assets (bibliographic resources like catalog records, name/subject authority records, holdings records, etc.) can be made available directly on the web as machine readable data is the real promise of linked-data for libraries. It feels like we're at an inflexion point like the one where libraries realized their catalogs could be made available on the web. The web-opac allowed there to be links between say bibliographic records and subject headings, which could be expressed in HTML for people to traverse. But now we can express these links explicitly in a machine readable way, for automated agents to traverse. If you (like Roy Tennant) are skeptical of the value in this ask yourself how companies like Google were able to build up their most valuable asset, their index of the web. They used the open architecture of the web, to walk the links between resources. Imagine if we could allow people to do the same with our data? To gather say a union catalog of Sweden by crawling it's member libraries catalogs, and periodically updating them with HTTP GET for that resource?</p>

<p>Martin's main point is that a lot of valuable effort has gone into vocabulary development like <a href="http://dublincore.org/">DublinCore</a>, <a href="http://www.loc.gov/standards/mods/">MODS</a> etc, and even some on the distribution of descriptions using these vocabularies using <a href="http://www.openarchives.org/">OAI-PMH</a>. But the real exciting part IMHO is giving these resources URLs, and linking them together...much as the web of documents is linked together. I agree with Martin, this is new territory, that really combines what librarians and web-technologists do best. I'm looking forward to meeting Martin at <a href="http://web.archive.org/web/20130722011956/http://dc2008.de/">DC2008</a>, where hopefully we can do a linked-data BOF or something.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>315</wp:post_id>
		<wp:post_date>2008-09-02 07:13:49</wp:post_date>
		<wp:post_date_gmt>2008-09-02 14:13:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>martin-malmsten-and-linked-library-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="podcasts"><![CDATA[podcasts]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>73084</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.230.151</wp:comment_author_IP>
			<wp:comment_date>2008-09-04 20:42:36</wp:comment_date>
			<wp:comment_date_gmt>2008-09-05 03:42:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@royt ok, I honestly didn't think you'd think I had met your challenge :-) I'd need another blog post to talk about why I think aggregating resources in this way (web harvesting) is superior to mag tapes in a VW bus. I would've thought it was self-evident, but there you go... A better comparison, I think would be with OAI-PMH. Do you think OAI-PMH is useful?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>72844</wp:comment_id>
			<wp:comment_author><![CDATA[Roy Tennant]]></wp:comment_author>
			<wp:comment_author_email>roytennant@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://roytennant.com/</wp:comment_author_url>
			<wp:comment_author_IP>132.174.183.7</wp:comment_author_IP>
			<wp:comment_date>2008-09-03 12:22:21</wp:comment_date>
			<wp:comment_date_gmt>2008-09-03 19:22:21</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Not to be a monkey on your back (he says, as he clambers on for another ride), but I don't think you've yet met my challenge. That is, what problem does this solve and how does it work better than anything previous? As far as I can tell you've solved a problem (aggregating data) that was solved back when you could pack a VW bus with mag tapes. What is so useful about this that will have people sit up and saying "Yeah! I WANT me one of those?"]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>72943</wp:comment_id>
			<wp:comment_author><![CDATA[Internet Alchemy &raquo; Bookmarks for May 7th through September 2nd]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://iandavis.com/blog/2008/09/bookmarks-for-may-7th-through-september-2nd</wp:comment_author_url>
			<wp:comment_author_IP>67.19.173.4</wp:comment_author_IP>
			<wp:comment_date>2008-09-04 02:08:47</wp:comment_date>
			<wp:comment_date_gmt>2008-09-04 09:08:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Martin Malmsten and linked library data - [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>73517</wp:comment_id>
			<wp:comment_author><![CDATA[Roy Tennant]]></wp:comment_author>
			<wp:comment_author_email>roytennant@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://roytennant.com/</wp:comment_author_url>
			<wp:comment_author_IP>205.160.169.41</wp:comment_author_IP>
			<wp:comment_date>2008-09-08 10:54:43</wp:comment_date>
			<wp:comment_date_gmt>2008-09-08 17:54:43</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The point I'm trying to make about your specific demonstration can be equally made with OAI-PMH. Both that protocol and your technique have the same problem: they are a good way to aggregate _unique_ records, but they will always be flawed when aggregating records for commonly held items. This is because variations in those records are much easier to deal with in a batch mode over time than dynamically. Ask anyone who has ever had to create and maintain a union catalog.  This is why I don't find this demonstration a compelling of linked data, as fun as it may have been to be asleep while the catalog was being sucked down -- which as you point out could have been done via OAI-PMH as well. Solve a problem that many of us have in a more effective way than before and you may have something. Without that, why should we care?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85389</wp:comment_id>
			<wp:comment_author><![CDATA[Bookmarks for May 7th through September 2nd &laquo; Internet Alchemy]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.iandavis.com/2008/09/04/bookmarks-for-may-7th-through-september-2nd/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-08 12:12:50</wp:comment_date>
			<wp:comment_date_gmt>2012-05-08 19:12:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Martin Malmsten and linked library data &#8211; [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1336504370.809";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336526654.0305";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>w3c semweb use cases and lcsh</title>
		<link>http://inkdroid.org/2008/09/05/w3c-semweb-use-cases-and-lcsh/</link>
		<pubDate>Fri, 05 Sep 2008 20:12:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=341</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Via <a href="http://www.w3.org/People/Ivan/">Ivan Herman</a> I learned that the <a href="http://www.w3.org/2001/sw/sweo/public/UseCases/">Semantic Web Use Cases</a> use concepts from <a href="http://web.archive.org/web/20130812145007/http://lcsh.info/">lcsh.info</a>. For example look at the RDFa in <a href="http://www.w3.org/2001/sw/sweo/public/UseCases/NRK/">this</a> case study for the Digital Music Archive for the Norwegian National Broadcaster. You can also look at the Document metadata in a linked data browser like <a href="http://demo.openlinksw.com/rdfbrowser/?uri=http%3A//www.w3.org/2001/sw/sweo/public/UseCases/NRK/">OpenLink</a>. Click on the "Document" and then on the various subject "concepts" and you'll see the linked data browser go out and fetch the triples from lcsh.info for "Semantic Web" and "Broadcasting".</p>

<p>One of the downsides to linked-data browsers (for me) is that they hide a bit of what's going on. Of course this is by-design. For a more rdf centric view on the data take a look at this output of <a href="http://librdf.org/raptor/rapper.html">rapper</a>.</p>

<pre>
ed@curry:~$ rapper -o turtle http://www.w3.org/2001/sw/sweo/public/UseCases/NRK/
rapper: Serializing with serializer turtle
@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix bibo: &lt;http://purl.org/ontology/bibo/&gt; .
@prefix dc: &lt;http://purl.org/dc/terms/&gt; .
@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .
@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .
@prefix xhv: &lt;http://www.w3.org/1999/xhtml/vocab#&gt; .
@prefix xml: &lt;http://www.w3.org/XML/1998/namespace&gt; .
@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .

&lt;http://www.w3.org/2001/sw/sweo/public/UseCases/NRK/&gt;
    a bibo:Document;
    dc:rights "\u00A9 Copyright 2007, ESIS, NRK."@en-us;
    dc:subject &lt;http://lcsh.info/sh85017004#concept&gt;, &lt;http://lcsh.info/sh2002000569#concept&gt;;
    dc:date "2007-09"^^xsd:dateTime;
    dc:title "Case Study: A Digital Music Archive (DMA) for the Norwegian National Broadcaster (NRK) using Semantic Web techniques"@en-us;
    bibo:distributor &lt;http://www.w3.org/&gt;;
    bibo:authorList (
        [
            a foaf:Person;
            foaf:workplaceHomepage &lt;http://www.esis.no&gt;;
            foaf:name "Dr. Robert H.P. Engels"@nl
        ]
        [
            a foaf:Person;
            foaf:workplaceHomepage &lt;http://www.nrk.no&gt;;
            foaf:name "Jon Roar T\u00F8nnesen"@no
        ]
    ) .

&lt;http://www.w3.org/2001/sw/sweo/public/UseCases/NRK/Overview.html&gt;
    xhv:stylesheet &lt;http://www.w3.org/2001/sw/sweo/public/UseCases/style/ucstyle.css&gt; .
</pre>

<p>You can see Ivan's using the dc, foaf, skos and bibo vocabularies, and the links out lcsh Concepts. Fun stuff.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>341</wp:post_id>
		<wp:post_date>2008-09-05 13:12:16</wp:post_date>
		<wp:post_date_gmt>2008-09-05 20:12:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>w3c-semweb-use-cases-and-lcsh</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bibo"><![CDATA[bibo]]></category>
		<category domain="post_tag" nicename="dublincore"><![CDATA[dublincore]]></category>
		<category domain="post_tag" nicename="foaf"><![CDATA[foaf]]></category>
		<category domain="post_tag" nicename="lcsh"><![CDATA[lcsh]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="post_tag" nicename="rds"><![CDATA[rds]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>nkos/cendi</title>
		<link>http://inkdroid.org/2008/09/11/nkoscendi/</link>
		<pubDate>Thu, 11 Sep 2008 10:49:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=347</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Jon Phipps and I are speaking about SKOS at the World Bank today for a <a href="http://cendievents.infointl.com/kos_workshop_091108/">joint meeting</a> of the <a href="http://www.cendi.gov/">CENDI</a> and <a href="http://nkos.slis.kent.edu/">NKOS</a> groups. The talk is entitled "SKOS: New Directions in Interoperability" ... which is kind of ironic since SKOS has been a long running <a href="http://web.archive.org/web/20110106173746/http://esw.w3.org/mt/esw/archives/000087.html">topic</a> at NKOS meetings. The idea is to describe SKOS (for those who don't know it), cover the <a href="http://web.archive.org/web/20101217002227/http://inkdroid.org/bzr/vdiff/example/skos-changes-summary.txt">recent changes</a> to SKOS (for those that do), and describe an implementation of SKOS (<a href="http://web.archive.org/web/20110106173746/http://esw.w3.org/mt/esw/archives/000087.html">lcsh.info</a>). A tall order for 30 minutes!</p>

<p>One new direction that I hope I'll be able to get to is the notion of <a href="http://linkeddata.org/">linked-data</a>. I created some simple graph visualizations of the Royal Library of Sweden's linked bibliographic data implementation. I really wanted to emphasize how linked data can model data across enterprise boundaries. By the way this example really exists, it's not library-science-fiction.</p>

<p><a href="http://inkdroid.org/images/sweden-lod.png"><img src="http://inkdroid.org/images/sweden-lod.png" /></a></p>

<p>Wish us luck! There are going to be some other interesting talks during the day, on OCLC'S Terminology Services, Semantic Media Wiki for vocabulary development at the Mayo Clinic, mapping agriculture vocabularies, the intersection of folksonomy and taxonomy, and more.</p>

<p>PS. Roy I haven't forgotten your follow-up comment :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>347</wp:post_id>
		<wp:post_date>2008-09-11 03:49:41</wp:post_date>
		<wp:post_date_gmt>2008-09-11 10:49:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>nkoscendi</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="skos-nkos-cendi-talks-linkeddata-semweb-vocabularies-thesauri"><![CDATA[skos nkos cendi talks linkeddata semweb vocabularies thesauri]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>terminology services sneak peak</title>
		<link>http://inkdroid.org/2008/09/11/terminology-services-sneak-peak/</link>
		<pubDate>Thu, 11 Sep 2008 19:31:20 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=353</guid>
		<description></description>
		<content:encoded><![CDATA[I just saw Diane Vizine-Goetz demo <a href="http://tspilot.oclc.org/resources">OCLC's Terminology Services</a> at the CENDI/SKOS meeting and was excited to see various things out on the public web. For example, the LCSH concept "World Wide Web" is over here:

<blockquote>
  <a href="http://tspilot.oclc.org/lcsh/sh2008114004">http://tspilot.oclc.org/lcsh/sh2008114004</a>
</blockquote>

At the moment it's not the most friendly human readable display, but that's just a XSLT stylesheet away (assuming TS follows the patterns of other OCLC Services). I'm not quite sure what the default namespace urn:uuid:D30A7E67-31BF-40A3-9956-9668674FCD84 is. But the response looks like it indicates what resources are related to a given conceptual resource.

<ol>
<li><a href="http://tspilot.oclc.org/lcsh/sh2008114004.html">http://tspilot.oclc.org/lcsh/sh2008114004.html</a></li>
<li><a href="http://tspilot.oclc.org/lcsh/sh2008114004.json">http://tspilot.oclc.org/lcsh/sh2008114004.json</a></li>
<li><a href="http://tspilot.oclc.org/lcsh/sh2008114004.marcxml">http://tspilot.oclc.org/lcsh/sh2008114004.marcxml</a></li>
<li><a href="http://tspilot.oclc.org/lcsh/sh2008114004.meta">http://tspilot.oclc.org/lcsh/sh2008114004.meta</a></li>
<li><a href="http://tspilot.oclc.org/lcsh/sh2008114004.skos">http://tspilot.oclc.org/lcsh/sh2008114004.skos</a></li>
<li><a href="http://tspilot.oclc.org/lcsh/sh2008114004.stats">http://tspilot.oclc.org/lcsh/sh2008114004.stats</a></li>
<li><a href="http://tspilot.oclc.org/lcsh/sh2008114004.zthes">http://tspilot.oclc.org/lcsh/sh2008114004.zthes</a></li>
</ol>

And LCSH is just one of the vocabularies available through the pilot service, if you examine the <a href="http://tspilot.oclc.org/ ">XML</a> you'll see references to FAST, TGM and MESH + SRU services for each. 

I think this is way cool, and a step in the right direction...particulary because they are going to make vocabularies available for free as long as the original publisher has no problem with it. My only complaint is that the URIs for the concepts don't appear to do content-negotiation for application/rdf+xml. It looks like text/html and application/javascript (isn't it application/json?) work just fine though. Try them out:

<pre>
curl --header "Accept: application/javascript" http://tspilot.oclc.org/lcsh/sh2008114004
curl --header "Accept: text/html" http://tspilot.oclc.org/lcsh/sh2008114004
</pre>

But not application/rdf+xml:

<pre>
curl --header "Accept: application/rdf+xml" http://tspilot.oclc.org/lcsh/sh2008114004
</pre>

It seems like it would be a pretty easy fix, and pretty important for being able to follow your nose on the semantic web.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>353</wp:post_id>
		<wp:post_date>2008-09-11 12:31:20</wp:post_date>
		<wp:post_date_gmt>2008-09-11 19:31:20</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>terminology-services-sneak-peak</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="json"><![CDATA[json]]></category>
		<category domain="post_tag" nicename="lcsh"><![CDATA[lcsh]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="mesh"><![CDATA[mesh]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="tgm"><![CDATA[tgm]]></category>
		<category domain="post_tag" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>74175</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.230.151</wp:comment_author_IP>
			<wp:comment_date>2008-09-14 18:01:24</wp:comment_date>
			<wp:comment_date_gmt>2008-09-15 01:01:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Aye, maybe that's why they opted for application/javascript as the MIME type?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74324</wp:comment_id>
			<wp:comment_author><![CDATA[Andrew Houghton]]></wp:comment_author>
			<wp:comment_author_email>houghton@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.5</wp:comment_author_IP>
			<wp:comment_date>2008-09-16 06:43:18</wp:comment_date>
			<wp:comment_date_gmt>2008-09-16 13:43:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[A concept in the terminology service may be represented by multiple RDF application profiles.  SKOS is just one type of RDF application profile.  Actually SKOS currently is two RDF application profiles, one for the SKOS 2005 draft and one for the SKOS 2008 draft.  These two SKOS drafts have different semantics and namespaces.  The terminology service that Diane described could support both SKOS drafts which would result in an ambigious condition when a user agent sent an HTTP Accept header with the value application/rdf+xml.

Prior to SKOS there was the Thesaurus Interchange Format (TIF) which is another RDF application profile specifically for describing thesauri.  Further the OWL Web Ontologoy language is another RDF application profile for describing ontologies.  In addition, your blog post shows the URI for the meta format, e.g., http://tspilot.oclc.org/lcsh/sh2008114004.meta.  A closer examination of the meta format reveals that it's a stripped RDF/XML representation of the service's internal information about the concept.  Yet another RDF application profile used to describe a representation of the concept.

I touched on a few RDF application profiles available to describe controlled vocabularies.  The terminology service does not want to assume that SKOS is the only RDF application profile that is available in the present, near or distant future.  This is why when a user agent sends an HTTP Accept header with a value of application/rdf+xml, the terminology service treats it as an ambigious condition.

In regard to the ambigious condition for application/xml, the terminology service could register with IANA a specific MIME type for XML application profiles such as MARC-XML, MADS, Zthes, etc.  However, registration really needs to be done by the markup's authorized agent and not OCLC.  There are issues with registration that OCLC isn't prepared to answer on behalf of the markup's authorized agent.  For example, should MARC-XML have one media type and/or a media type for MARC authorities, bibliographic, classification, holdings, community information, where the generic media type would be used to describe mixed MARC-XML collections?  Instead the terminology service used the documented IANA media type extension mechanism and created local service specific media types that begin with application/x-oclc-tspilot to distinguish specific representations of XML and RDF application profiles so content negotiation was possible by user agents.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74327</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.123</wp:comment_author_IP>
			<wp:comment_date>2008-09-16 07:38:21</wp:comment_date>
			<wp:comment_date_gmt>2008-09-16 14:38:21</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm not sure I understand what RDF application profile means. There's no reason whatsoever that you can't mash (mesh?) up all the descriptions of the resource into one response. It's the power of RDF afterall. You can evolve the triples over time, as namespaces change.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74341</wp:comment_id>
			<wp:comment_author><![CDATA[Andrew Houghton]]></wp:comment_author>
			<wp:comment_author_email>houghton@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.5</wp:comment_author_IP>
			<wp:comment_date>2008-09-16 09:52:48</wp:comment_date>
			<wp:comment_date_gmt>2008-09-16 16:52:48</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ahh... I finally understand your position now.  The terminology service is based on the notion that a controlled vocabulary and each concept in that controlled vocabulary have a generic URI.  That generic URI, when dereferenced, responds with an agent-driven content negotiation response entity that details the dimensions that a user agent can access resource representations either through content negotiation or direct URI dereference.  The service is built around representations.

Since RDF application profiles describe the collections, resources, classes, properties and literals (along with their semantics in a defined namespace) that can be used in RDF, you can as you suggest, throw all the triples into one big pot and send that back to the user agent when the user agent sends the service an HTTP Accept header with the value application/rdf+xml.  If the service is supporting many RDF application profiles, then the service has the computational cost of generating the RDF for each RDF application profile and could send back tens of megabytes of RDF triples that the user agent is unable to digest due to bandwidth, storage or processing issues.

From a service and user agent perspective I don't see that strategy as beneficial since it doesn't seem to be a balanced approach for a service or its user agents.  You might be tempted to discount the bandwidth issue, everyone in the world has broadband right?  Err... no I'm living in the U.S. and I am unable to get broadband where I live which is only 13 miles outside a major city in my state.  Also, there is a whole new world out there called mobile devices that do have limited bandwidth, storage, and processing capabilities and we would like the terminology service to be used in mobile device mashups.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74373</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.230.151</wp:comment_author_IP>
			<wp:comment_date>2008-09-16 20:19:23</wp:comment_date>
			<wp:comment_date_gmt>2008-09-17 03:19:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I don't discount the bandwidth issue. But I also think it's a bit of a red-herring. The issue for me is that something meaningful should come back when an agent resolves your Concept URIs like http://tspilot.oclc.org/lcsh/sh2008114004 asking for application/rdf+xml. At the moment it seems to do just fine when asking for text/html and application/json. 

Perhaps I've missed the reason in the previous comments (I'm sorry if I have) -- but why don't you return the RDF description of the Concept and it's constituent representations when someone requests the application/rdf+xml from http://tspilot.oclc.org/lcsh/sh2008114004 ?

Or put differently, when someone asks for application/rdf+xml why not return what you currently do for */* ?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74242</wp:comment_id>
			<wp:comment_author><![CDATA[Andrew Houghton]]></wp:comment_author>
			<wp:comment_author_email>houghton@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.5</wp:comment_author_IP>
			<wp:comment_date>2008-09-15 09:36:42</wp:comment_date>
			<wp:comment_date_gmt>2008-09-15 16:36:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Note application/rdf+xml is not accepted as a content type, it returns an HTTP 406 Not Acceptable status code, since there could be multiple RDF representations for a concept.  Similarly asking for application/xml is not acceptable since the service cannot determine whether you want MARC-XML, Zthes, or some other XML representation.  This is a problem with content types that are too generic like RDF and XML.

When a user agent requests the generic URI from the service, e.g., http://tspilot.oclc.org/lcsh/sh2008114004 it retrieves a striped RDF/XML response entity that describes an agent-driven content negotiation scheme, see http://tools.ietf.org/html/rfc2616#section-12.2.  The striped RDF/XML response entity describes the dimensions that a user agent can request.

To do content negotiation with an HTTP Accept header the user agent needs to look at the first dc:format property that has an RDF datatype of http://purl.org/dc/terms/IMT.  The secondary dc:format properties that have an RDF datatype of http://purl.org/dc/terms/IMT describe conflicting MIME types for a representation.

So if your user agent was looking for application/rdf+xml resources, it would search all dc:format properties that had an RDF datatype of http://purl.org/dc/terms/IMT and a value of application/rdf+xml, then use the first dc:format that had an RDF datatype of http://purl.org/dc/terms/IMT for each representation that it wanted to request from the service.

Using your example URI http://tspilot.oclc.org/lcsh/sh2008114004 the search would return only one representation, at this point in time, for representation describing SKOS.  The user agent could request the resource with the specific URI based on the value of the rdf:about attribute (implicitly based on the xml:base attribute in the stripped RDF/XML) on the Representation collection member or it could request content negotiation using the first dc:format property with the RDF datatype http://purl.org/dc/terms/IMT as the value for the HTTP Accept header, which in this case is the local service application/x-oclc-tspilot.skos MIME type.

What the service is currently missing is when a user agent sends in a content type such as application/rdf+xml or application/xml, the service will automatically select the representations that have that content type as the value of a dc:format property with the RDF datatype http://purl.org/dc/terms/IMT and return an HTTP 300 Multiple Choices status code with an agent-driven content negotiation entity for only those resources.  Basically, the user agent would receive a subset of the generic URI agent-driven content negotiation response entity.  Currently the service just returns an HTTP 406 Not Acceptable response code to user agents.

The service needs to document the agent-driven content negotiation scheme that it is using, but we have been busy trying to add some additional features such as hierarchy for controlled vocabularies that have a coherent hierarchal reference structure...

As far as the JSON issues are concerned, the first dc:format with an RDF datatype of http://purl.org/dc/terms/IMT is the application/json MIME type.  Doing content negotiation with an HTTP Accept header that has a value of application/json, does return the JSON representation for me and I believe that the JSON being returned in valid JSON as it has been tested in both Firefox and Internet Explorer without issue.

There are issues with using browsers as your user agent to access the service.  The service currently tries to accomodate oddites with Internet Explorer.  Sending back application/rdf+xml or application/json to Internet Explorer will cause the browser to pop-up a download dialog box since by default Internet Explorer doesn't understand those MIME types.  When the service detects that Internet Explorer is the user agent then application/rdf+xml is changed to application/xml and application/json is changed to application/javascript or text/plain.  Firefox is also not without issue since requests to the generic URI will always return the XHTML representation.  This is because Firefox automatically adds an HTTP Accept header that says it prefers application/xhtml+xml over */*.  The service may need to sniff for Firefox and override its behaviour too.

Feel free to contact the project with comments or questions at http://www.oclc.org/programsandresearch/feedback/form.asp?project=Terminology%20Services

]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74246</wp:comment_id>
			<wp:comment_author><![CDATA[Andrew Houghton]]></wp:comment_author>
			<wp:comment_author_email>houghton@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.5</wp:comment_author_IP>
			<wp:comment_date>2008-09-15 10:36:17</wp:comment_date>
			<wp:comment_date_gmt>2008-09-15 17:36:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[When a user agent requests the generic URI without an HTTP Accept header or an HTTP Accept header with the value */*, then the agent-driven content negotiation response entity, the stripped RDF/XML, contains all the RDF triples necessary to describe all the dimensions that can be used for content negotiation that can be used to request a representation of the generic URI.

As I indicated with MARC-XML vs. Zthes example it's ambiguous when a user agent sends an HTTP Accept header with application/xml.  Neither MARC-XML or Zthes is described by a specific IANA MIME type, however both are application profiles of the application/xml content type.  The only reasonable thing the service can do is to return an HTTP 300 Multiple Choices status along with an entity that the user agent can use to disambiguate which representation it really wants.  This part of the service, as I indicated, has not been implemented and instead the service returns an HTTP 406 Not Acceptable status.

PS. Ed I mistakenly in my last post used angle brackets around URI's in the text so the blog made them disappear.  Would it be possible to edit my last post so it shows the URI's. Thx.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74250</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.123</wp:comment_author_IP>
			<wp:comment_date>2008-09-15 12:11:50</wp:comment_date>
			<wp:comment_date_gmt>2008-09-15 19:11:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm still confused why there are multiple RDF representations for a concept. I can understand about the XML I suppose, although you could always register the ones you want: say application/zthes+xml or some such.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74251</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.123</wp:comment_author_IP>
			<wp:comment_date>2008-09-15 12:17:24</wp:comment_date>
			<wp:comment_date_gmt>2008-09-15 19:17:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Re: the links, sorry they're not in the entry I can't seem to recover them. Which is a shame because they look interesting, particularly what you have for "agent-driven content negotiation scheme".]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74243</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.123</wp:comment_author_IP>
			<wp:comment_date>2008-09-15 09:53:49</wp:comment_date>
			<wp:comment_date_gmt>2008-09-15 16:53:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah, but why not return all the RDF triples at once? They are all about the same subject no? As for the xml responses...you can't identify the response with a MIME type or extension?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74441</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.123</wp:comment_author_IP>
			<wp:comment_date>2008-09-17 12:25:57</wp:comment_date>
			<wp:comment_date_gmt>2008-09-17 19:25:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Cool I look forward to seeing it. Thanks for the conversation!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>74440</wp:comment_id>
			<wp:comment_author><![CDATA[Andrew Houghton]]></wp:comment_author>
			<wp:comment_author_email>houghton@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.5</wp:comment_author_IP>
			<wp:comment_date>2008-09-17 12:00:56</wp:comment_date>
			<wp:comment_date_gmt>2008-09-17 19:00:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed, I think we just went around in a circle when you said: "... why not return what you currently do for */*" :)  One of the missing pieces to the terminology service, mention above, is that when a user agent asks for a overloaded media type, such as application/rdf+xml, application/rdf+n3, application/xml, text/xml, etc., the service is suppose to respond with an entity based on */* that has been filtered for the media type that the user agent requested.

So when a user agent sends an HTTP Accept header with the value application/rdf+xml to the generic URI, then the terminology service would respond with the representation metadata for http://tspilot.oclc.org/lcsh/sh2008114004.meta and http://tspilot.oclc.org/lcsh/sh2008114004.skos since both are RDF application profiles.

I hacked together an XSL transform and the configuration rules to do this last night.  The changes have been committed to our SVN repository and should be pushed to the public server in the next couple of days or by the latest next Monday evening, 2008-09-22T23:59:00.0-05:00, which is our  regularly scheduled integration cycle.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>73933</wp:comment_id>
			<wp:comment_author><![CDATA[Ian Davis]]></wp:comment_author>
			<wp:comment_author_email>nospam@iandavis.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>90.217.110.216</wp:comment_author_IP>
			<wp:comment_date>2008-09-12 05:35:33</wp:comment_date>
			<wp:comment_date_gmt>2008-09-12 12:35:33</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[There's no way that JSON is valid - it has functions in it :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>public.resource.org to liberate Code of Federal Regulations</title>
		<link>http://inkdroid.org/2008/09/17/publicresourceorg-to-liberate-code-of-federal-regulations/</link>
		<pubDate>Wed, 17 Sep 2008 21:51:15 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=365</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://public.resource.org/gpo.gov"><img src="http://inkdroid.org/images/public-resource-org.png" style="border: none; float: right;" /></a>

good news via the <a href="http://groups.yahoo.com/group/govtrack/message/629">govtrack mailing list</a>

<blockquote>
Carl Malamud of public.resource.org, with funding from a bunch of places including a small bit from GovTrack's ad profits, announced his intention to purchase from the Government Printing Office documents they produce in the course of their statutory obligations and then have the nerve to sell back to the public at prohibitive prices. The document to be purchased is the Code of Federal Regulations, the component of federal law created by executive branch agencies, in electronic form. Once obtained, it will be posted openly/freely online.

More here: <a href="http://public.resource.org/gpo.gov/index.html">http://public.resource.org/gpo.gov/index.html</a>

And Carl's letter to the GPO:
<a href="http://public.resource.org/gpo.gov/the_honorable.html">http://public.resource.org/gpo.gov/the_honorable.html</a>
</blockquote>

It's pretty sad that it has to come to this...but it's also pretty awesome that it's happening.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>365</wp:post_id>
		<wp:post_date>2008-09-17 14:51:15</wp:post_date>
		<wp:post_date_gmt>2008-09-17 21:51:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>publicresourceorg-to-liberate-code-of-federal-regulations</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="gpo"><![CDATA[gpo]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="post_tag" nicename="politics"><![CDATA[politics]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"74457";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>74457</wp:comment_id>
			<wp:comment_author><![CDATA[Gabriel Farrell]]></wp:comment_author>
			<wp:comment_author_email>gsf@breaksalot.org</wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20090106134754/http://breaksalot.org/</wp:comment_author_url>
			<wp:comment_author_IP>129.25.131.72</wp:comment_author_IP>
			<wp:comment_date>2008-09-17 15:22:54</wp:comment_date>
			<wp:comment_date_gmt>2008-09-17 22:22:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Man, Malamud rocks. The GPO should not be a revenue-generating, or even self-sustaining, arm of the goverment.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>iswc2009, DC and vocamp</title>
		<link>http://inkdroid.org/2008/09/22/iswc2009-dc-and-vocamp/</link>
		<pubDate>Mon, 22 Sep 2008 15:03:46 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=378</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I just learned from <a href="http://tomheath.com">Tom Heath</a> that <a href="http://web.archive.org/web/20111213180703/http://iswc.semanticweb.org:80/">The International Semantic Web Conference</a> is coming to Washington DC <a href="http://iswc2009.semanticweb.org/">next year.</a> This is pretty cool news to me, since traveling to conferences isn't always the easiest thing to navigate. Also, Tom suggested that it might be fun to organize a <a href="http://vocamp.org/wiki/Main_Page">VoCamp</a> around the conference, to provide an informal collaboration space for vocabulary demos, development, q/a, etc. If you want to help out please join the <a href="http://vocamp.org/wiki/Main_Page#VoCamp_Mailing_List">mailing list</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>378</wp:post_id>
		<wp:post_date>2008-09-22 08:03:46</wp:post_date>
		<wp:post_date_gmt>2008-09-22 15:03:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>iswc2009-dc-and-vocamp</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="barcamp"><![CDATA[barcamp]]></category>
		<category domain="post_tag" nicename="conferences"><![CDATA[conferences]]></category>
		<category domain="post_tag" nicename="dc"><![CDATA[dc]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="vocamp"><![CDATA[vocamp]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>lcsh.info logs</title>
		<link>http://inkdroid.org/2008/09/24/lcshinfo-logs/</link>
		<pubDate>Wed, 24 Sep 2008 07:50:55 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=384</guid>
		<description></description>
		<content:encoded><![CDATA[If you are curious how lcsh.info is being used I've made the <a href="http://logs.lcsh.info">apache server logs</a> available, including the ones for the sparql service. I've been meaning to do some analysis of the logs but haven't got the time yet. You'll notice that among the data that's collected is the Accept header sent by agents, since it's so important to what representation is served up. Thanks to <a href="http://danbri.org">danbri</a> for the idea to simply make them available.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>384</wp:post_id>
		<wp:post_date>2008-09-24 00:50:55</wp:post_date>
		<wp:post_date_gmt>2008-09-24 07:50:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lcshinfo-logs</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>80888</wp:comment_id>
			<wp:comment_author><![CDATA[epugh]]></wp:comment_author>
			<wp:comment_author_email>epugh@opensourceconnections.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>76.104.26.41</wp:comment_author_IP>
			<wp:comment_date>2009-02-23 14:35:39</wp:comment_date>
			<wp:comment_date_gmt>2009-02-23 21:35:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Ed,  I just went to lcsh.info looking to leverage it tie tags together by proximity on my project http://web.archive.org/web/20100325223647/http://www.hightechcville.com:80/ and much to my dismay, it's gone!  I wanted to comment but the comments are closed.  At any rate, do you have any suggestions on alternatives?  I want to connect people together based on how close their tags are.  So if I am tagged with "Java" and you are tagged with "Visual Basic", we share with one degree of seperation "Programming Languages" and therefore should chat.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>82</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81188</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.195</wp:comment_author_IP>
			<wp:comment_date>2009-03-06 06:58:32</wp:comment_date>
			<wp:comment_date_gmt>2009-03-06 13:58:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Sounds like a perfect use case. Definitely check out dbpedia for this. Hopefully before too long id.loc.gov will be coming online.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>json vs pickle</title>
		<link>http://inkdroid.org/2008/10/24/json-vs-pickle/</link>
		<pubDate>Fri, 24 Oct 2008 07:09:02 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=401</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>in python JSON is faster, smaller and more portable than pickle ... </em></p>

<p>At work, I'm working on a project where we're modeling newspaper content in a relational database. We've got newspaper titles, issues, pages, institutions, places and some other fun stuff. It's a django app, and the db schema currently looks something like:</p>

<p><a href="http://inkdroid.org/images/ndnp-schema.png">
<img width="500" src="http://inkdroid.org/images/ndnp-schema.png" title="Click for Big Picture" border="none" />
</a></p>

<p>Anyhow, if you look at the schema you'll notice that we have a <code>Page</code> model, and that attached to that is an <code>OCR</code> model. If you haven't heard of it before OCR is an acronym for <a href="http://en.wikipedia.org/wiki/Optical_character_recognition">optical character recognition</a>. For each newspaper page we have, we have a TIF image for the original page, and we have rectangle coordinates for the position of every word on the page. Basically it's xml that looks something like <a href="http://inkdroid.org/data/ndnp-ocr.xml">this</a> (warning your browser may choke on this, you might want to right-click-download).</p>

<p>So there are roughly around 2500 words on a page of newspaper text, and there can sometimes be 350 occurrences of a particular word on a page...and we're looking to model 1,000,000 pages soon ... so if we got really prissy with normalization we could soon be looking at (worst case) 875,000,000,000 rows in a table. While I am interested in getting a handle on how to manage large databases like this, we just don't need the fine grained queries into the word coordinates. But we do need to be able to look up the coordinates for a particular word on a particular page to do hit highlighting in search results.</p>

<p>So let me get to the interesting part already. To avoid having to think about databases with billions of rows, I radically denormalized the data and stored the word coordinates as a blob of <a href="http://www.json.org/">JSON</a> in the database. So we just have a <code>word_coordinates_json</code> column in the OCR table, and when we need to look up the coordinates for a page we just load up the JSON dictionary and we're good to go. JSON is nice with django, since django's ORM doesn't seem to support storing blobs in the database, and JSON is just text. This worked just fine on single page views, but we also do hit highlighting on pages where there are 10 pages being viewed at the same time. So we started noticing large lags on these page views -- because it was taking a while to load the JSON (sometimes 327K * 10 of JSON).</p>

<p>As I mentioned we're using Django, so it was easy to use django.utils.simplejson for the parsing. When we noticed slowdowns I decided to compare django.utils.simplejson to the latest <a href="http://www.undefined.org/python/">simplejson</a> and <a href="http://pypi.python.org/pypi/python-cjson">python-cjson</a>. And just for grins I figured it couldn't hurt to see if using pickle or cPickle (protocols 0, 1 and 2) would prove to be faster than using JSON. So I wrote a little benchmark script that timed the loading of a 327K JSON and a 507K pickle file 100 times using each technique. Here are the results:</p>

<table padding="10px" style="border: thin solid gray; padding: 15px;">
<tr><th>method</th><th>total seconds</th><th>avg seconds</th></tr>
<tr><td>django-simplejson</td><td>140.606723</td><td>1.406067</td></tr>
<tr><td>simplejson</td><td>2.260988</td><td>0.022610</td></tr>
<tr><td>pickle</td><td>45.032428</td><td>0.450324</td></tr>
<tr><td>cPickle</td><td>4.569351</td><td>0.45694</td></tr>
<tr><td>cPickle1</td><td>2.829307</td><td>0.028293</td></tr>
<tr><td>cPickle2</td><td>3.042940</td><td>0.030429</td></tr>
<tr><td>python-cjson</td><td>1.852755</td><td>0.018528</td></tr>
</table>

<p><img src="http://chart.apis.google.com/chart?cht=bhs&chd=t:139.92,2.24,44.03,4.4,3.03,3.11,1.85&chdl=django-simplejson|simplejson|pickle|cpickle|cpickle1|cpickle2|python-cjson&chco=33FF33|9900FF|FF0033|3366FF|2211EE|99AAFF|00FFCC&chs=450x225&chg=20&chxt=x&chx0=0,100" /></p>

<p>Yeah, that's right. The real simplejson is 62 times faster than django.utils.simplejson! Even more surprising simplejson seems to be faster than even cPickle (even using binary protocols 1 and 2) python-cjson seems to have a slight edge on simplejson. This is good news for our search results page that has 10 newspaper pages to highlight on it, since it'll take 10 * 0.033183 = .3 seconds to parse all the JSON instead of the totally unacceptable 10 * 0.976193 = 9.7 seconds. I guess in some circles 0.3 seconds might be unacceptable, we'll have to see how it pans out. We may be able to remove the JSON deserialization from the page load time by pushing some of the logic into the browser w/ AJAX. If you want, please try out <a href="http://web.archive.org/web/20101216134637/http://inkdroid.org/bzr/jsonickle/">my benchmarks</a> yourself on your own platform. I'd be curious if you see the same ranking.</p>

<p>Here are the versions for various bits I used:</p>

<ul>
<li>python v2.5.2</li>
<li>django trunk: r9231 2008-10-13 15:38:18 -0400</li>
<li>simplejson 2.0.3</li>
</ul>

<p>So in summary for pythoneers: JSON is faster, smaller and more portable than pickle. Of course there are caveats in that you can only store simple datatypes that JSON allows you to, not the full fledged Python objects. But in my use case JSON's data types were just fine. Makes me that much happier that simplesjson aka <code>json</code> is now cooked into the <a href="http://docs.python.org/whatsnew/2.6.html">Python 2.6</a> standard library.</p>

<p><em>Note: if you aren't seeing simplejson performing better than cPickle you may need to have python development libraries installed:</p>

<pre>
  aptitude install python-dev # or the equivalent for your system
</pre>

<p>You can verify if the optimizations are available in simplejson by:</p>

<pre>
ed@hammer:~/bzr/jsonickle$ python
Python 2.5.2 (r252:60911, Jul 31 2008, 17:28:52) 
[GCC 4.2.3 (Ubuntu 4.2.3-2ubuntu7)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&lt;&lt;&lt; import simplejson
&lt;&lt;&lt; simplejson._speedups
&lt;module 'simplejson._speedups' from '/home/ed/.python-eggs/simplejson-2.0.3-py2.5-linux-i686.egg-tmp/simplejson/_speedups.so'&gt;
</pre>

<p>Thanks <a href="http://blog.ryaneby.com/">eby</a>, <a href="http://lackoftalent.org/michael/blog/">mjgiarlo</a>, <a href="http://oxfordrepo.blogspot.com/">BenO</a> and Kapil for their pointers and ideas.
</em></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>401</wp:post_id>
		<wp:post_date>2008-10-24 00:09:02</wp:post_date>
		<wp:post_date_gmt>2008-10-24 07:09:02</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>json-vs-pickle</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="databases"><![CDATA[databases]]></category>
		<category domain="post_tag" nicename="newspapers"><![CDATA[newspapers]]></category>
		<category domain="post_tag" nicename="ocr"><![CDATA[ocr]]></category>
		<category domain="post_tag" nicename="performance"><![CDATA[performance]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>SemanticProxy</title>
		<link>http://inkdroid.org/2008/10/27/semanticproxy/</link>
		<pubDate>Mon, 27 Oct 2008 18:54:31 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=472</guid>
		<description></description>
		<content:encoded><![CDATA[I spent a 1/2 an hour goofing around with with the new (to me) <a href="http://semanticproxy.opencalais.com/">SemanticProxy</a> service from Calais. You  give the service a URL along with your API key, and it'll go pull down the content and then give you back some HTML or RDF/XML. The call is pretty simple, it's just a GET:

<pre>
GET http://service.semanticproxy.com/processurl/{key}/rdf/{url}
</pre>

Here's an example of some <a href="http://inkdroid.org/data/obl.txt">turtle</a> you can get for my friend Dan's <a href="http://onebiglibrary.net">blog</a>. Obviously there's a lot of data there, but I wanted to see exactly what entities are being recognized, and their labels. It doesn't take long to notice that most of the resource types are in the namespace: <code>http://s.opencalais.com/1/type/em/e/</code>

For example:

<ul>
<li><code>http://s.opencalais.com/1/type/em/e/Person</code></li>
<li><code>http://s.opencalais.com/1/type/em/e/Country</code></li>
<li><code>http://s.opencalais.com/1/type/em/e/Company</code></li>
</ul>

And most of these resources have a property which seems to assign a literal string label to the resource:

  <code>http://s.opencalais.com/1/pred/name</code> 

It's kind of a bummer that these vocabulary terms don't resolve, because it would be sweet to get a bigger picture look at their vocabulary.

At any rate, with these two little facts gleaned from looking at the RDF for a few moments I wrote a little <a href="http://inkdroid.org/bzr/calais/entities.py">script</a> (using <a href="http://rdflib.net">rdflib</a>) which you feed a URL and it'll munge through the RDF and print out the recognized entities:

<pre>
ed@curry:~/bzr/calais$ ./entities.py http://onebiglibrary.net
a Company named Lehman Bros.
a Company named Southwest Airlines
a Company named Costco
a Company named Everbank
a Holiday named New Year's Day
a ProvinceOrState named Illinois
a ProvinceOrState named Arizona
a ProvinceOrState named Michigan
a IndustryTerm named media ownership rules
a IndustryTerm named unreliable technologies
a IndustryTerm named bank
a IndustryTerm named health care insurance
a IndustryTerm named bank panics
a IndustryTerm named free software
a City named Lansing
a Facility named Big Library
a Person named Ralph Nader
a Person named Dan Chudnov
a Person named Shouldn't Bob Barr
a Person named John Mayer
a Person named Daniel Chudnov
a Person named Cynthia McKinney
a Person named Bob Barr
a Person named John Legend
a Country named Iraq
a Country named United States
a Country named Afghanistan
a Organization named FDIC
a Organization named senate
a Currency named USD
</pre>

Quite easy and impressive IMHO. One thing that is missing from this output are the URIs that identify the various resources that are recognized like Dan's:

<code>
http://d.opencalais.com/pershash-1/f7383d60-c27b-309c-889a-4e34d0938a0f
</code>

Like the vocabulary URIs it doesn't resolve (at least outside the Reuters media empire). Sure would be nice if it did. It's got the fact that it's a person cooked into it (pershash)...but otherwise seems to be just a simple hashing algorithm applied to the string "Dan Chudnov".

I didn't actually spend any time looking at the licensing issues around using the service. I've heard they are somewhat stultifying and vague, which is to be expected I guess. The news about <a href="http://www.nature.com/nature/journal/v455/n7214/full/455708a.html">Reuters and Zotero</a> isn't exactly encouraging ... but it is interesting to see how good some of the NLP analysis is getting at institutions like Reuters. It would be lovely to get a backend look at how this technology is actually being used internally at Reuters.

If you want to take this entities.py for a spin and can't be bothered to download it, just drop into <a href="irc://chat.freenode.net/code4lib">#code4lib</a> and ask #zoia for entities:

<pre>
14:45 < edsu> @entities http://www.frbr.org/2008/10/21/xid-updates-at-oclc
14:45 < zoia> edsu: 'ok I found: a Facility Library of Congress, a Company FRBR 
              Review Group, a City York, a EmailAddress wtd@pobox.com, a Person 
              Jenn Riley, a Person Robert Maxwell, a Person Arlene Taylor, a 
              Person William Denton, a Person Barbara Tillett, a Organization 
              Congress, a Organization Open Content Alliance, a Organization 
              York \nUniversity'
</pre>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>472</wp:post_id>
		<wp:post_date>2008-10-27 11:54:31</wp:post_date>
		<wp:post_date_gmt>2008-10-27 18:54:31</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>semanticproxy</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="calais"><![CDATA[calais]]></category>
		<category domain="category" nicename="linguistics"><![CDATA[linguistics]]></category>
		<category domain="post_tag" nicename="nlp"><![CDATA[nlp]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="reuters"><![CDATA[reuters]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_cd4f3105f11ea57ba8bcb64e4aab9558</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_6a1562cacc5594121dc466765df13e9f</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_b162d6dc7b9acc799857e971e7a82245</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_cdf0ee78083dd52f466017288db24473</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_a9f15a5bb437b2c9e052406500675fe5</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_8c90d12db26a039f4385d0f3b3f990e9</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_968eb29a9e0295ccc06bee8bdc06ecd6</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>77052</wp:comment_id>
			<wp:comment_author><![CDATA[Ttague]]></wp:comment_author>
			<wp:comment_author_email>tlt@intivo.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>24.91.108.158</wp:comment_author_IP>
			<wp:comment_date>2008-10-29 07:36:06</wp:comment_date>
			<wp:comment_date_gmt>2008-10-29 14:36:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed:

Tom Tague from Calais here.

Just a short note on a couple of things. 

First - yes the URIs will be resolvable at the end of the year. We're going to focus at populating interesting endpoint information and links to other linked data assets for company, geography and a few other types for that release - we'll continue to expand the endpoints over time.

Second - yes, we really are going to publish the ontology at the end of the year. This required a little more work than we expected around naming standardization and other boring stuff - but it's coming. We have to figure out the exact language - but we plan to make it open and usable by all. 

Last - Terms of Service. We'll be revisiting these in the next few weeks with goals of 1) improving privacy safeguards, and 2) removing ambiguity where possible. Ambiguity in TOS's is scary - and we want to eradicate it wherever possible. 

Thanks for creating and making the sample code available - we appreciate anything that can jump start people's usage of Calais.

 I'd also encourage you to jump into events and facts. While entity extraction is cool - the real power of Calais starts to become more apparent when you begin playing with the relationships contained in the source material.

Regards,]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>41</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>77020</wp:comment_id>
			<wp:comment_author><![CDATA[Bookmarks for October 22nd through October 28th &#171; context:forge]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://contextforge.com/2008/10/bookmarks-for-october-22nd-through-october-28th/</wp:comment_author_url>
			<wp:comment_author_IP>64.13.192.16</wp:comment_author_IP>
			<wp:comment_date>2008-10-28 19:40:56</wp:comment_date>
			<wp:comment_date_gmt>2008-10-29 02:40:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid &raquo; Blog Archive &raquo; SemanticProxy - [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>77048</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.195</wp:comment_author_IP>
			<wp:comment_date>2008-10-29 05:15:14</wp:comment_date>
			<wp:comment_date_gmt>2008-10-29 12:15:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[From Alf Eaton via email (sorry I had registration turned off Alf, it's back on)

<blockquote>
Hi Ed, ... [just] wanted to say that the OpenCalais ontology is supposed to be published 'in late Q4' - hopefully it'll be under a completely open license, but we'll see...http://opencalais.com/node/489
</blockquote>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80920</wp:comment_id>
			<wp:comment_author><![CDATA[Entity Extraction of URL&#8217;s made easy&#8230;. Partly. &laquo; HighTechCville]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://hightechcville.wordpress.com/2009/02/24/entity-extraction-of-urls-made-easy-partly/</wp:comment_author_url>
			<wp:comment_author_IP>72.233.104.10</wp:comment_author_IP>
			<wp:comment_date>2009-02-24 16:38:20</wp:comment_date>
			<wp:comment_date_gmt>2009-02-24 23:38:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] a comment &raquo;  Thanks to Ed Summers at the Library of Congress for his post on SemanticProxy. Semantic proxy offers a dead simple API for feeding URL&#8217;s to the OpenCalais entity [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>freebase and linked-data</title>
		<link>http://inkdroid.org/2008/10/29/freebase-and-linked-data/</link>
		<pubDate>Wed, 29 Oct 2008 14:32:08 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=479</guid>
		<description></description>
		<content:encoded><![CDATA[Ok, this is pretty big <a href="http://lists.w3.org/Archives/Public/public-lod/2008Oct/0047.html">news</a> for linked data folks, and for semweb-heads in general. <a href="http://freebase.com">Freebase</a> is <a href="http://blog.freebase.com/2008/10/30/introducing_the_rdf_service/">now</a> a <a href="http://rdf.freebase.com">linked-data target</a>. This is important news because Freebase is an active community of content creators, creating rich data-centric descriptions with a wiki style interface, fancy data loaders, and useful machine APIs. 

The web2.0-meets-semweb space is also being explored by folks like <a href="http://www.talis.com/platform/">Talis</a>. It'll be interesting to see how this plays out--particularly in light of SPARQL adoption, which I remain kind of neutral about for some undefined, wary, spooky reason. I get the idea of web resources having data views. It seems like a logical, "one small step for an web agent, one giant leap for the web". But queryability with SPARQL sounds like something to push off, particularly if you've already got a <a href="http://freebase.com/opensearch.xml">search api</a> that could be hooked up to the data views.

At any rate, what this announcement means is that you can get machine readable data back from freebase using a URI. The descriptions then use more URIs, which you can then follow-your-nose to, and get more machine readable data. So if you are on a page like:

<blockquote>
<a href="http://www.freebase.com/view/en/tim_berners-lee">http://www.freebase.com/view/en/tim_berners-lee</a> 
</blockquote>

you can construct a URL for Tim Berners-Lee like this:

<blockquote>
  <a href="http://rdf.freebase.com/ns/en.tim_berners-lee">http://rdf.freebase.com/ns/en.tim_berners-lee</a>
</blockquote>

Then you resolve that URL asking for <code>application/turtle</code> (you could ask for <code>application/rdf+xml</code> but I find the turtle more readable).

<pre>
curl --location --header "Accept: application/turtle" http://rdf.freebase.com/ns/en.tim_berners-lee
</pre>

And you'll get back a description like <a href="http://inkdroid.org/data/tbl-freebase.txt">this</a>. There's a lot of useful data there, but the interesting part for me is the follow-your-nose effect where you can see an assertion like:

<pre>
 &lt;http://rdf.freebase.com/ns/en.tim_berners-lee&gt;   
     &lt;http://rdf.freebase.com/ns/influence.influence_node.influenced_by&gt;
     &lt;http://rdf.freebase.com/ns/en.ted_nelson&gt; .
</pre>

And you can then go look up Ted Nelson using that URI:

<pre>
  curl --location --header "Accept: application/turtle" http://rdf.freebase.com/ns/en.ted_nelson
</pre>

And get another chunk of <a href="http://inkdroid.org/data/tednelson-freebase.txt">data</a> which includes this assertion: 

<pre>
 &lt;http://rdf.freebase.com/ns/en.ted_nelson&gt;
     &lt;http://rdf.freebase.com/ns/influence.influence_node.influenced_by&gt;
     &lt;http://rdf.freebase.com/ns/en.vannevar_bush&gt; .
</pre>

And you can then continue following your nose to:

<blockquote>
<a href="http://rdf.freebase.com/ns/en.vannevar_bush">http://rdf.freebase.com/ns/en.vannevar_bush</a>
</blockquote>

Lather, rinse, repeat.

So why is this important? Because following your nose in HTML is what enabled companies like Lycos, AltaVista, Yahoo and Google to be born. It allowed for agents to be able to crawl the web of documents and build indexes of the data to allow people to find what they want (hopefully). Being able to link data in this way allows us to harvest data assets across organizational boundaries and merge them together. It's early days still, but seeing an organization like Freebase get it is pretty exciting.

Oh, there are a few little <a href="http://lists.freebase.com/pipermail/developers/2008-October/002210.html">rough spots</a> which probably should be ironed out ... but when is that ever not the case eh? Inspiring stuff.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>479</wp:post_id>
		<wp:post_date>2008-10-29 07:32:08</wp:post_date>
		<wp:post_date_gmt>2008-10-29 14:32:08</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>freebase-and-linked-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="freebase"><![CDATA[freebase]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="turtle"><![CDATA[turtle]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>77088</wp:comment_id>
			<wp:comment_author><![CDATA[The Semantic Puzzle | The Day after Freebase went RDF]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.semantic-web.at/2008/10/30/the-day-after-freebase-went-rdf/</wp:comment_author_url>
			<wp:comment_author_IP>78.142.133.42</wp:comment_author_IP>
			<wp:comment_date>2008-10-30 02:44:37</wp:comment_date>
			<wp:comment_date_gmt>2008-10-30 09:44:37</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] and useful machine APIs.&#8221; This is followed up by quick and handy tutorial how you can get machine readable data back from freebase using a URI with Freebase. Conclusion: So why is this important? Because following your nose in HTML is what [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>bibliovirus</title>
		<link>http://inkdroid.org/2008/11/03/bibliovirus/</link>
		<pubDate>Mon, 03 Nov 2008 12:51:33 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=499</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://en.wikipedia.org/wiki/Image:Influenza_virus.jpg"><img src="http://inkdroid.org/images/flu.jpg" style="float: left; margin-right: 10px; border: none;" /></a>

Terry's <a href="http://oregonstate.edu/~reeset/blog/archives/574">analysis</a> of the proposed changes to OCLC's record policy is essential reading. I'm really concerned that these 996 fields will slip somewhat unnoticed into data that I use.

<blockquote>
996 $aOCLCWCRUP $iUse and transfer of this record is governed by the OCLC® Policy for Use and Transfer of WorldCat® Records. $u<a href="http://purl.org/oclc/wcrup">http://purl.org/oclc/wcrup</a>
</blockquote>

This appears to be an engineered, legal virus for our bibliographic ecosystems. I'm not a lawyer, so I can't fully determine the significance of these legal terms...mostly because there isn't a policy at the end of that PURL right now. There's a FAQ full of ominous references to "the Policy", and a glossy, feel-good overview, but the policy itself is empty at the moment. So the precise nature of the virus is so far unknown...or am I wrong?

At any rate, I think libraries need to be careful about letting these 996 fields creep into their data--especially data that they create. I wonder are there other examples of legalese that have slipped into MARC data over the years?

<em>Update 2008-11-03: it <a href="http://coffeecode.net/archives/174-Archive-of-OCLC-WorldCat-Policy-as-posted-2008-11-02.html%3E">appears</a> that "the Policy" was removed sometime Sunday evening? Perhaps its best not to jump to conclusions eh? But that image of the virus is too cool, and I needed an excuse to post it on my blog.</em>

<em>Update 2008-11-07: check out Terry's <a href="http://oregonstate.edu/~reeset/blog/archives/582">re-analysis</a> of "the Policy" when a new version was brought back online by OCLC.</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>499</wp:post_id>
		<wp:post_date>2008-11-03 05:51:33</wp:post_date>
		<wp:post_date_gmt>2008-11-03 12:51:33</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bibliovirus</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="cataloging"><![CDATA[cataloging]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="licenses"><![CDATA[licenses]]></category>
		<category domain="category" nicename="marc"><![CDATA[marc]]></category>
		<category domain="post_tag" nicename="marc"><![CDATA[marc]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>77322</wp:comment_id>
			<wp:comment_author><![CDATA[johnwcowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>72.14.228.89</wp:comment_author_IP>
			<wp:comment_date>2008-11-03 17:24:16</wp:comment_date>
			<wp:comment_date_gmt>2008-11-04 00:24:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The summary makes it look like the equivalent CC-BY-NC license.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>44</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>77345</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.55.17.90</wp:comment_author_IP>
			<wp:comment_date>2008-11-04 05:31:22</wp:comment_date>
			<wp:comment_date_gmt>2008-11-04 12:31:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for drawing that parallel John, yes it is very similar. 

The dimensions of CC-BY-NC are well known, apply to other works, and are clearly articulated. CC-BY-NC also <a href="http://creativecommons.org/licenses/by-nc/1.0/" rel="nofollow">has</a> <a href="http://creativecommons.org/licenses/by-nc/2.0/" rel="nofollow">versioned</a> <a href="http://creativecommons.org/licenses/by-nc/3.0/" rel="nofollow">URLs</a> for the terms. What if the license at the end of the OCLC URL changed at any time -- would it retroactively apply to people's data?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>77347</wp:comment_id>
			<wp:comment_author><![CDATA[Panlibus &raquo; Blog Archive &raquo; What are OCLC playing at?]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blogs.talis.com/panlibus/archives/2008/11/what-are-oclc-playing-at.php</wp:comment_author_url>
			<wp:comment_author_IP>80.86.35.222</wp:comment_author_IP>
			<wp:comment_date>2008-11-04 06:26:39</wp:comment_date>
			<wp:comment_date_gmt>2008-11-04 13:26:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] libraries alike”&#160;&#160; Whereas there are many who are questioning their motives and the negative viral effects of the policy as it is proposed to be [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>bagit and .deb</title>
		<link>http://inkdroid.org/2008/11/05/bagit-deb/</link>
		<pubDate>Wed, 05 Nov 2008 15:20:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=524</guid>
		<description></description>
		<content:encoded><![CDATA[I'm just now (OK I'm slow) marveling at how similar <a href="http://purl.oclc.org/net/bagit">BagIt</a> turned out to be to the <a href="http://en.wikipedia.org/wiki/Debian_package">Debian Package Format</a>.  Given <a href="http://andy.boyko.net/">some</a> of the <a href="http://dot.ucop.edu/home/jak/biography.html">folks</a> involved, this synchronicity isn't too surprising.

Both .deb and BagIt use a directory 'data' for bundling the files in the package (well .deb has it as a compressed file data.tar.gz). Both have md5sum-style checksum files for stating the fixity values of said files. Both have simple <a href="ftp://ftp.isi.edu/in-notes/rfc2822.txt">rfc2822</a>-style text files for expressing metadata. Both have files that contain the version number of the packaging format. One nice thing that deb has which BagIt intentionally eschewed was a serialization format. But no matter.

At LC we (a.k.a. coding machine <a href="http://www.dlib.org/dlib/may06/authors/05authors.html#LITTMAN">Justin Littman</a>) are working on a software library for creating and validating bags, as well as a shiny GUI that'll sit on top of it to assist in bag creation for people who like shiny things.

It's an interesting counterpoint to this process of creating BagIt tools to look how a .deb can be downloaded and inspected. Here's a sampling of a shell session where I downloaded and extracted the parts of the .deb for <a href="http://rdflib.net">python-rdflib</a>.

<pre>
ed@curry:~/tmp$ aptitude download python-rdflib
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Reading extended state information       
Initializing package states... Done
Building tag database... Done      
Get:1 http://us.archive.ubuntu.com hardy/universe python-rdflib 2.4.0-4 [276kB]
Fetched 276kB in 0s (346kB/s) 

ed@curry:~/tmp$ ar -xv python-rdflib_2.4.0-4_i386.deb 
x - debian-binary
x - control.tar.gz
x - data.tar.gz

ed@curry:~/tmp$ tar xvfz control.tar.gz 
./
./postinst
./prerm
./md5sums
./control

ed@curry:~/tmp$ cat control
Package: python-rdflib
Source: rdflib
Version: 2.4.0-4
Architecture: i386
Maintainer: Ubuntu MOTU Developers <ubuntu -motu@lists.ubuntu.com>
Original-Maintainer: Nacho Barrientos Arias <nacho @debian.org>
Installed-Size: 1608
Depends: libc6 (>= 2.5-5), python-support (>= 0.3.4), python (< < 2.6), python (>= 2.4), python-setuptools
Provides: python2.4-rdflib, python2.5-rdflib
Section: python
Priority: optional
Description: RDF library containing an RDF triple store and RDF/XML parser/serializer
 RDFLib is a Python library for working with RDF, a simple yet
 powerful language for representing information. The library
 contains an RDF/XML parser/serializer that conforms to the
 RDF/XML Syntax Specification and both in-memory and persistent
 Graph backend.
 .
 This package also provides a serialization format converter
 called rdfpipe in order to deal with the different formats
 RDFLib works with.
 .
  Homepage: http://rdflib.net/

ed@curry:~/tmp$ cat md5sums 
75af966e839159902537614e5815c415  usr/lib/python-support/python-rdflib/python2.5/rdflib/sparql/bison/SPARQLParserc.so
a33eb3985c6de5589cb723d03d2caeb1  usr/lib/python-support/python-rdflib/python2.4/rdflib/sparql/bison/SPARQLParserc.so
d1b5578dd1d64432684d86bbb816fafc  usr/bin/rdfpipe
0191b561e3efe1ceea7992e2c865949b  usr/share/doc/python-rdflib/changelog.gz
98a861211f3effe1e69d6148c1e31ab2  usr/share/doc/python-rdflib/copyright
d75c2ab05f3a4239963d8765c0e9e7c5  usr/share/doc/python-rdflib/examples/example.py
17b61c23d0600e6ce17471dc7216d3fa  usr/share/doc/python-rdflib/examples/swap_primer.py
3894fa16d075cf0eee1c36e6bcc043d8  usr/share/doc/python-rdflib/changelog.Debian.gz
15653f75f35120b16b1d8115e6b5a179  usr/share/man/man1/rdfpipe.1.gz
405cb531a83fd90356ef5c7113ecd774  usr/share/python-support/python-rdflib/rdflib/sparql/bison/CompositionalEvaluation.py
41e28217ddd2eb394017cd8f12b1dfd5  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Util.py
ec9ae5147463ed551d70947c2824bc82  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Resource.py
6e018a69ca242acb613effe420c2cdc7  usr/share/python-support/python-rdflib/rdflib/sparql/bison/SolutionModifier.py
7e72a08f29abc91faddb85e91f17e87c  usr/share/python-support/python-rdflib/rdflib/sparql/bison/FunctionLibrary.py
648384e5980ef39278466be38572523a  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Expression.py
494386730a6edf5c6caf7972ed0bf4ba  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Bindings.py
4513b2fdc116dc9ff02895222a81421d  usr/share/python-support/python-rdflib/rdflib/sparql/bison/IRIRef.py
a800bdac023ae0c02767ab623dffe67b  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Triples.py
6c31647f2b3be724bdfcc35f631162b1  usr/share/python-support/python-rdflib/rdflib/sparql/bison/SPARQLEvaluate.py
c158b3fb8fd66858f598180084f481c4  usr/share/python-support/python-rdflib/rdflib/sparql/bison/GraphPattern.py
bff095caa2db064cc2b1827c4b90a9e7  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Processor.py
2db0c4925d17b49f5bb355d7860150c2  usr/share/python-support/python-rdflib/rdflib/sparql/bison/QName.py
10e02ecf896d07c0546b791a450da633  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Query.py
eee29bb22b05b16da2a5e6552044bf22  usr/share/python-support/python-rdflib/rdflib/sparql/bison/__init__.py
a29a508631228f6674e11bb077c24afc  usr/share/python-support/python-rdflib/rdflib/sparql/bison/PreProcessor.py
479a4702ebee35f464055a554ebf5324  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Filter.py
d2fe75aa4394ec7d9106a1e02bb3015a  usr/share/python-support/python-rdflib/rdflib/sparql/bison/Operators.py
da186350e65c8e062887724b1758ef80  usr/share/python-support/python-rdflib/rdflib/sparql/Query.py
0130de0f5d28087d7c841e36d89714c4  usr/share/python-support/python-rdflib/rdflib/sparql/graphPattern.py
826ffe4c6b3f59a9635524f0746299fe  usr/share/python-support/python-rdflib/rdflib/sparql/sparqlOperators.py
...

ed@curry:~/tmp$ tar xvfz data.tar.gz 
./
./usr/
./usr/lib/
./usr/lib/python-support/
./usr/lib/python-support/python-rdflib/
./usr/lib/python-support/python-rdflib/python2.5/
./usr/lib/python-support/python-rdflib/python2.5/rdflib/
./usr/lib/python-support/python-rdflib/python2.5/rdflib/sparql/
./usr/lib/python-support/python-rdflib/python2.5/rdflib/sparql/bison/
./usr/lib/python-support/python-rdflib/python2.5/rdflib/sparql/bison/SPARQLParserc.so
./usr/lib/python-support/python-rdflib/python2.4/
./usr/lib/python-support/python-rdflib/python2.4/rdflib/
./usr/lib/python-support/python-rdflib/python2.4/rdflib/sparql/
./usr/lib/python-support/python-rdflib/python2.4/rdflib/sparql/bison/
./usr/lib/python-support/python-rdflib/python2.4/rdflib/sparql/bison/SPARQLParserc.so
./usr/bin/
./usr/bin/rdfpipe
./usr/share/
./usr/share/doc/
./usr/share/doc/python-rdflib/
./usr/share/doc/python-rdflib/changelog.gz
./usr/share/doc/python-rdflib/copyright
./usr/share/doc/python-rdflib/examples/
./usr/share/doc/python-rdflib/examples/example.py
./usr/share/doc/python-rdflib/examples/swap_primer.py
./usr/share/doc/python-rdflib/changelog.Debian.gz
./usr/share/man/
./usr/share/man/man1/
./usr/share/man/man1/rdfpipe.1.gz
./usr/share/python-support/
./usr/share/python-support/python-rdflib/
./usr/share/python-support/python-rdflib/rdflib/
./usr/share/python-support/python-rdflib/rdflib/sparql/
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/CompositionalEvaluation.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Util.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Resource.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/SolutionModifier.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/FunctionLibrary.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Expression.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Bindings.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/IRIRef.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Triples.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/SPARQLEvaluate.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/GraphPattern.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Processor.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/QName.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Query.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/__init__.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/PreProcessor.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Filter.py
./usr/share/python-support/python-rdflib/rdflib/sparql/bison/Operators.py
./usr/share/python-support/python-rdflib/rdflib/sparql/Query.py
./usr/share/python-support/python-rdflib/rdflib/sparql/graphPattern.py
./usr/share/python-support/python-rdflib/rdflib/sparql/sparqlOperators.py
...
</nacho></ubuntu></pre>

Here are some more useful <a href="http://thedarkmaster.wordpress.com/2008/05/24/how-to-create-manipulate-a-deb-file-of-a-compiled-application/">notes</a> on the structure of .deb files and how to create them. If you are interested in trying out the nascent-alpha BagIt tools give me a holler (ehs at pobox dot com) or just add a comment here...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>524</wp:post_id>
		<wp:post_date>2008-11-05 08:20:06</wp:post_date>
		<wp:post_date_gmt>2008-11-05 15:20:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bagit-deb</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bagit"><![CDATA[bagit]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linux"><![CDATA[linux]]></category>
		<category domain="post_tag" nicename="packages"><![CDATA[packages]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>simplicity</title>
		<link>http://inkdroid.org/2008/11/18/simplicity/</link>
		<pubDate>Tue, 18 Nov 2008 15:58:39 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=549</guid>
		<description></description>
		<content:encoded><![CDATA[So we have a few bookshelves in our house--one of which is in our kitchen. Only one or two of the shelves in this bookshelf actually house books, most of which are food-stained cookbooks. The rest of the 4 or 5 shelves are given over to photographs, albums, pamphlets from schools, framed pictures, compact discs, pencils, letters, screwdrivers, coins, candles, bills, artwork, crayons--basically the knickknacks and detritus of daily living. We spend a lot of time in the kitchen, so it's convenient and handy to just stash stuff there.

The only problem is IT DRIVES ME INSANE!

The randomness, and perceived messiness of the bookshelf drives me crazy.  I look at it and I see chaos, complexity and disorder. I know I have a <a href="http://en.wikipedia.org/wiki/Obsessive-compulsive_disorder">problem</a>, but that knowledge doesn't seem to help. I am constantly shuffling things around, grouping things, moving things, throwing things out while more and and more things are quietly added. I'd almost prefer the bookshelf to be somewhere out of sight, but then we'd probably use something else in the kitchen. 

This morning, on my way to work, I got a call from Kesa asking where two flower petals were that needed to be ironed on to Chloe's Girl Scouts <a href="http://www.girlscouts.org/program/gs_central/insignia/where_to_place/daisy/">uniform</a>. They were in the bookshelf at one point. Did I throw them away? I can't remember it's all a blur. I admit that I probably did. I can hear Chloe crying in the background. I feel bad...and resentful about having to keep this bookshelf organized.

Why am I writing here about this? Well mostly it wouldn't fit within a <a href="http://twitter.com/edsu">140 byte limit</a>. But srsly -- I guess I just feel like this bookshelf is a living emblem of my professional life as a software developer at a library.  I strive to create software that is simple in its expression, <a href="http://en.wikipedia.org/wiki/Unix_philosophy">that does one thing and does it well</a>, and which is hopefully easy to maintain by more people than just me. I relish working at an institution that values the preservation of objects and knowledge. 

But I threw away the flower decal ...

It's important to remember that real life is complicated, and that the messiness is something to be relished as well. The useful bookshelf, or bag of bits, chunk of json, or half-remembered perl script in someones homedir are valuable for their organic resilience. Or as Einstein famously said:

<blockquote>Things should be made as simple as possible, but not simpler.</blockquote>

I'm sorry Chloe.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>549</wp:post_id>
		<wp:post_date>2008-11-18 08:58:39</wp:post_date>
		<wp:post_date_gmt>2008-11-18 15:58:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>simplicity</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="family"><![CDATA[family]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="life"><![CDATA[life]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>digital-curation</title>
		<link>http://inkdroid.org/2008/11/26/digital-curation/</link>
		<pubDate>Wed, 26 Nov 2008 15:22:59 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=564</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://groups.google.com/group/digital-curation"><img src="http://inkdroid.org/images/digital-curation.png" style="margin-right: 10px; float: left; border: thick groove gray;" /></a>

Some folks at <a href="http://loc.gov">LC</a> and <a href="http://cdlib.org">CDL</a> are trying to kick-start a new <a href="http://groups.google.com/group/digital-curation">public discussion list</a> for talking about digital curation in its many guises: repositories, tools, standards, techniques, practices, etc. The intuition being that there is a social component to the problems of digital preservation and repository interoperability. 

Of course <a href="http://digitalpreservation.gov">NDIIPP</a> (the arena for the CDL/LC collaboration) has always been about <a href="http://www.digitalpreservation.gov/library/program_back.html">building and strengthening a network of partners</a>. But as Priscilla Caplan points out in her survey of the digital preservation landscape <a href="http://dx.doi.org/10.1108/07378830710840419">Ten Years After</a>, organizations in Europe like the <a href="http://www.jisc.ac.uk/">JISC</a> and <a href="http://www.langzeitarchivierung.de/">NESTOR</a> seem to have understood that there is an educational component to digital preservation as well. Yet even the JISC and NESTOR have tended to focus more on the preservation of scholarly output, whereas digital preservation really extends beyond that realm of materials.

The continual need to share good ideas and hard-won-knowledge about digital curation, and to build a network of colleagues and experts that extends out past the normal project/institution specific boundaries is just as important as building the collections and the technologies themselves. 

So I guess this is a rather highfalutin goal ... here's some text stolen from the <a href="http://groups.google.com/group/digital-curation">digital-curation</a> home page to give you more of a flavor:

<blockquote>
The digital preservation and repositories domain is fortunate to have a diverse set of institutional and consortial efforts, software projects, and standardization initiatives.  Many discussion lists have been created for these individual efforts. The digital-curation discussion list is intended to be a public forum that encourages cross-pollination across these project and institutional boundaries in order to foster wider awareness of project- and institution-specific work and encourage further collaboration.

Topic of conversation can include (but is not limited to)
<ul>
<li>digital repository software (Fedora, DSpace, EPrints, etc.)</li>
<li>management of digital formats (JHOVE, djatoka, etc.)</li>
<li>use and development of standards (OAIS, OAI-PMH/ORE, MPEG21, METS, BagIt, etc.)</li>
<li>issues related to identifiers, packaging, and data transfer</li>
<li>best practices and case studies around curation and preservation of digital content</li>
<li>repository interoperability</li>
<li>conference, workshop, tutorial announcements</li>
<li>recent papers</li>
<li>job announcements</li>
<li>general chit chat about problems, solutions, itches to be scratched</li>
<li>humor and fun</li>
</ul>
</blockquote>

We'll see how it goes. If you are at all interested please sign up.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>564</wp:post_id>
		<wp:post_date>2008-11-26 08:22:59</wp:post_date>
		<wp:post_date_gmt>2008-11-26 15:22:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>digital-curation</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bagit"><![CDATA[bagit]]></category>
		<category domain="post_tag" nicename="cdl"><![CDATA[cdl]]></category>
		<category domain="post_tag" nicename="jisc"><![CDATA[jisc]]></category>
		<category domain="post_tag" nicename="lc"><![CDATA[lc]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="ndiipp"><![CDATA[ndiipp]]></category>
		<category domain="category" nicename="people"><![CDATA[people]]></category>
		<category domain="post_tag" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="repositories"><![CDATA[repositories]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>100,000 Books and FRBR</title>
		<link>http://inkdroid.org/2009/01/05/100000-books-and-frbr/</link>
		<pubDate>Mon, 05 Jan 2009 23:35:08 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=590</guid>
		<description></description>
		<content:encoded><![CDATA[The news about <a href="http://blog.freebase.com/2009/01/05/100000-books/">100,000</a> books on Freebase got me poking around with curl. I was pleased to see that Freebase actually distinguishes between a book as a work, and a particular edition of that book. To <a href="http://en.wikipedia.org/wiki/FRBR">FRBR</a> aficionados this will be familiar as the difference between a Work and a Manifestation:

For example here is a URI for James Joyce's Dubliners as a work:

<blockquote>
<a href="http://rdf.freebase.com/ns/en.dubliners">http://rdf.freebase.com/ns/en.dubliners</a>
</blockquote>

and here is a URI for a 1991 edition of Dubliners:

<blockquote>
<a href="http://rdf.freebase.com/ns/guid.9202a8c04000641f80000000048ea5b4">http://rdf.freebase.com/ns/guid.9202a8c04000641f80000000048ea5b4</a>
</blockquote>

If you follow those links in your browser you'll most likely be redirected to the human readable html view. But machine agents can use the same URL to discover say an RDF <a href="http://inkdroid.org/data/dubliners.txt">representation</a> of this edition of Dubliners, for example with curl: 

<pre>curl --location --header "Accept: application/turtle" http://rdf.freebase.com/ns/guid.9202a8c04000641f80000000048ea5b4

@prefix fb: http://rdf.freebase.com/ns/.
@prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#.
@prefix rdfs: http://www.w3.org/2000/01/rdf-schema#.
@prefix xml: http://www.w3.org/XML/1998/namespace.

 <span style="color: red;">&lt;http://rdf.freebase.com/ns/guid.9202a8c04000641f80000000048ea5b4&gt; a 
         &lt;http://rdf.freebase.com/ns/book.book_edition&gt;</span>,
         &lt;http://rdf.freebase.com/ns/common.topic&gt;,
         &lt;http://rdf.freebase.com/ns/media_common.creative_work&gt;;
     &lt;http://rdf.freebase.com/ns/book.book_edition.ISBN&gt; "0486268705";
     <span style="color: blue">&lt;http://rdf.freebase.com/ns/book.book_edition.LCCN&gt; "91008517"</span>;
     &lt;http://rdf.freebase.com/ns/book.book_edition.author_editor&gt; &lt;http://rdf.freebase.com/ns/en.james_joyce&gt;;
     <span style="color: green;">&lt;http://rdf.freebase.com/ns/book.book_edition.book&gt; &lt;http://rdf.freebase.com/ns/en.dubliners&gt;</span>;
     &lt;http://rdf.freebase.com/ns/book.book_edition.dewey_decimal_number&gt; "823";
     &lt;http://rdf.freebase.com/ns/book.book_edition.number_of_pages&gt; &lt;http://rdf.freebase.com/ns/guid.9202a8c04000641f8000000009a3be60&gt;;
     &lt;http://rdf.freebase.com/ns/book.book_edition.publication_date&gt; "1991";
     &lt;http://rdf.freebase.com/ns/type.object.name&gt; "Dubliners";
     &lt;http://rdf.freebase.com/ns/type.object.permission&gt; &lt;http://rdf.freebase.com/ns/boot.all_permission&gt;. 

 &lt;http://rdf.freebase.com/ns/guid.9202a8c04000641f8000000009a3be60&gt; a &lt;http://rdf.freebase.com/ns/book.pagination&gt;;
     &lt;http://rdf.freebase.com/ns/book.pagination.numbered_pages&gt; "152"^^&lt;http://www.w3.org/2001/XMLSchema#int&gt;;
     &lt;http://rdf.freebase.com/ns/type.object.permission&gt; &lt;http://rdf.freebase.com/ns/boot.all_permission&gt;. 
</pre>

There are a few assertions that struck me as interesting:

<ul>
<li>the statement in <span style="color:red;">red</span> that states that the resource is in fact an edition (of type http://rdf.freebase.com/ns/book.book_edition)</li>
<li>the statement in <span style="color: green;">green</span> which links the edition with the work (http://rdf.freebase.com/ns/en.dubliners).
</li><li>and the assertion in <span style="color: blue;">blue</span> which states the Library of Congress Control Number (LCCN) for the book</li>
</ul>

I was mostly surprised to see the library-centric metadata being collected such as LCCN, OCLC Number, Dewey Decimal Classification, LC Classification. There are even human readable <a href="http://www.freebase.com/view/en/entering_data_for_a_book">instructions</a> for how to enter the data (take that AACR2!).

Anyhow it got me wondering what it would be like to stuff all the Freebase book <a href="http://download.freebase.com/datadumps/">data</a> into a triple store, assert:


<pre>
&lt;http://rdf.freebase.com/ns/book.book&gt; &lt;owl:sameAs&gt; &lt;http://purl.org/vocab/frbr/core#Work&gt; .
&lt;http://rdf.freebase.com/ns/book.book_edition&gt; &lt;owl:sameAs&gt; &lt;http://purl.org/vocab/frbr/core#Manifestation&gt; .
</pre>

and then run some basic inferencing and get some FRBR data. I know, <em>crazy-talk</em> ... but it's interesting in theory (to me at least).
  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>590</wp:post_id>
		<wp:post_date>2009-01-05 16:35:08</wp:post_date>
		<wp:post_date_gmt>2009-01-05 23:35:08</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>100000-books-and-frbr</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="frbr"><![CDATA[frbr]]></category>
		<category domain="post_tag" nicename="freebase"><![CDATA[freebase]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"80105";i:1;s:5:"80303";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>80105</wp:comment_id>
			<wp:comment_author><![CDATA[mhermans]]></wp:comment_author>
			<wp:comment_author_email>maarten.hermans@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://mhermans.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>193.190.253.149</wp:comment_author_IP>
			<wp:comment_date>2009-01-05 20:00:36</wp:comment_date>
			<wp:comment_date_gmt>2009-01-06 03:00:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[RE those assertions: maybe a mapping <a href="http://ontologies.freebase.com/" rel="nofollow">here</a>? That way the mappings can grow through a community-effort and will be dereferencable, instead of adding assertions locally...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>63</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80303</wp:comment_id>
			<wp:comment_author><![CDATA[me.yahoo.com/florentjoc]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/florentjoc#f7b43</wp:comment_author_url>
			<wp:comment_author_IP>132.180.195.30</wp:comment_author_IP>
			<wp:comment_date>2009-01-14 04:50:00</wp:comment_date>
			<wp:comment_date_gmt>2009-01-14 11:50:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi,

Just a detail: why owl:sameAs ? It seems to me that it should be rdfs:subClassOf as frbr Work can also be music/movies/whatever and you don't want to infer that they are book...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>68</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80424</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.124</wp:comment_author_IP>
			<wp:comment_date>2009-01-21 13:12:36</wp:comment_date>
			<wp:comment_date_gmt>2009-01-21 20:12:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@florentjoc good point, that does make more sense ...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>q &amp; a</title>
		<link>http://inkdroid.org/2009/01/07/q-a/</link>
		<pubDate>Wed, 07 Jan 2009 21:12:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=610</guid>
		<description></description>
		<content:encoded><![CDATA[<em><strong>Q:</strong> What do 100 year old knitting patterns and a lost Robert Louis-Stevenson story have in common?</em>

<strong>A:</strong> A digitally preserved <a href="http://www.loc.gov/chroniclingamerica/lccn/sn83030193/1904-02-19/ed-1/seq-15">newspaper page</a>.

<em><strong>Q:</strong> What about if you add:</em>

<ul>
	<li>URIs for<a href="https://www.ravelry.com"> knitting materials</a></li>
	<li><a href="http://en.wikipedia.org/wiki/William_Blake">William Blake</a>'s Engravings</li>
	<li>The similarities/differences between <a href="http://en.wikipedia.org/wiki/Xmpp">XMPP</a>, <a href="http://en.wikipedia.org/wiki/Http">HTTP</a> and <a href="http://en.wikipedia.org/wiki/Nntp">NNTP</a></li>
	<li>Web crawling as data integration</li>
        <li>Project coordination with <a href="http://friendfeed.com/rooms/semantic-web">rooms</a> on FriendFeed</li>
	<li>brewing <a href="http://en.wikipedia.org/wiki/Kombucha">Kombucha</a></li>
</ul>

<strong>A: </strong>Just a typical lunch time conversation at <a href="http://www.yelp.com/biz/petes-diner-washington">Pete's</a> with a <a href="http://davidbrunton.com">couple</a> <a href="http://eikeon.com">people</a> I work with. The cool thing (for me) is that this is normal, involves a host of smart/interesting characters, and is routinely encouraged. I love my job.









]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>610</wp:post_id>
		<wp:post_date>2009-01-07 14:12:57</wp:post_date>
		<wp:post_date_gmt>2009-01-07 21:12:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>q-a</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="computers"><![CDATA[computers]]></category>
		<category domain="post_tag" nicename="knitting"><![CDATA[knitting]]></category>
		<category domain="post_tag" nicename="lc"><![CDATA[lc]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="life"><![CDATA[life]]></category>
		<category domain="post_tag" nicename="love"><![CDATA[love]]></category>
		<category domain="post_tag" nicename="networks"><![CDATA[networks]]></category>
		<category domain="post_tag" nicename="newspapers"><![CDATA[newspapers]]></category>
		<category domain="post_tag" nicename="work"><![CDATA[work]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:4:{i:0;s:5:"80147";i:1;s:5:"80151";i:2;s:5:"80153";i:3;s:5:"80171";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>80147</wp:comment_id>
			<wp:comment_author><![CDATA[Andy Boyko]]></wp:comment_author>
			<wp:comment_author_email>andy@boyko.net</wp:comment_author_email>
			<wp:comment_author_url>http://openid.andy.boyko.net/</wp:comment_author_url>
			<wp:comment_author_IP>17.228.15.140</wp:comment_author_IP>
			<wp:comment_date>2009-01-07 14:23:23</wp:comment_date>
			<wp:comment_date_gmt>2009-01-07 21:23:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[/me sniffles. I miss you brainiac polymath bastards. And Pete's. Pete's, a lot.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>64</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80150</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.195</wp:comment_author_IP>
			<wp:comment_date>2009-01-07 15:00:52</wp:comment_date>
			<wp:comment_date_gmt>2009-01-07 22:00:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I forgot to add that we talked about 2G zip files too. So you can think of yourself as still here in spirit if that eases the pain. Assuming it's still possible to feel pain in Northern California :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80151</wp:comment_id>
			<wp:comment_author><![CDATA[dchud]]></wp:comment_author>
			<wp:comment_author_email>dchud@umich.edu</wp:comment_author_email>
			<wp:comment_author_url>http://onebiglibrary.net/</wp:comment_author_url>
			<wp:comment_author_IP>71.255.251.36</wp:comment_author_IP>
			<wp:comment_date>2009-01-07 15:43:52</wp:comment_date>
			<wp:comment_date_gmt>2009-01-07 22:43:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Bah, I hate missing lunches like that.  On the other hand the hot and sour soup was just right today in chinatown.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>66</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80152</wp:comment_id>
			<wp:comment_author><![CDATA[johnwcowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>72.14.228.89</wp:comment_author_IP>
			<wp:comment_date>2009-01-07 16:07:41</wp:comment_date>
			<wp:comment_date_gmt>2009-01-07 23:07:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nothing "lost" about it.  It's Chapter VIII (excluding the first two paragraphs) of "The Pavilion On The Links", an 1880 short story which forms part of his 1882 collection <i>New Arabian Nights</i>.  The 1900 date given in the copy presumably refers to an American edition.

I'll add to your collection of oddities that Finger, Gopher, and Whois are all basically the same protocol: write one line to the server, then read back an arbitrary amount of text until EOF.  You used to be able to talk to Finger servers using URLs of the form gopher://example.com:79/0&lt;username&gt; .]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>44</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80153</wp:comment_id>
			<wp:comment_author><![CDATA[Michael J. Giarlo]]></wp:comment_author>
			<wp:comment_author_email>michael.giarlo@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://lackoftalent.org/michael/</wp:comment_author_url>
			<wp:comment_author_IP>68.49.47.222</wp:comment_author_IP>
			<wp:comment_date>2009-01-07 18:09:30</wp:comment_date>
			<wp:comment_date_gmt>2009-01-08 01:09:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I sat at my desk and ate cold, three-day-old bratwurst.

Nope, doesn't sound like I missed much of anything at all...

And I wouldn't listen to a word Boyko says; he's got overexposure to that, uh, what do you call it, ah yes, "sun" thing.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>67</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80161</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.195</wp:comment_author_IP>
			<wp:comment_date>2009-01-08 10:13:25</wp:comment_date>
			<wp:comment_date_gmt>2009-01-08 17:13:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@dchud aren't they all like that?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80171</wp:comment_id>
			<wp:comment_author><![CDATA[dchud]]></wp:comment_author>
			<wp:comment_author_email>dchud@umich.edu</wp:comment_author_email>
			<wp:comment_author_url>http://onebiglibrary.net/</wp:comment_author_url>
			<wp:comment_author_IP>71.255.251.36</wp:comment_author_IP>
			<wp:comment_date>2009-01-08 16:12:03</wp:comment_date>
			<wp:comment_date_gmt>2009-01-08 23:12:03</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Maybe it's just the ones I'm not at.  On the other hand, again, the pastrami at the deli at 3rd and E is pretty good.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>66</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80172</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.55.17.90</wp:comment_author_IP>
			<wp:comment_date>2009-01-08 19:16:42</wp:comment_date>
			<wp:comment_date_gmt>2009-01-09 02:16:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@john thanks for details re: the "lost" story ... will relay on to <a href="http://www.davidbrunton.com/2009/01/for-love-of-woman-by-robert-louis.html" rel="nofollow">dbrun</a>.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>work identifiers and the web</title>
		<link>http://inkdroid.org/2009/01/21/work-identifiers-and-the-web/</link>
		<pubDate>Wed, 21 Jan 2009 21:06:43 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=626</guid>
		<description></description>
		<content:encoded><![CDATA[Michael Smethurst's <a href="http://www.bbc.co.uk/blogs/radiolabs/2009/01/in_search_of_cultural_identifi.shtml">In Search of Cultural Identifiers</a> post over at the BBC Radio Labs got me thinking about web identifiers for works, about LibraryThing and OCLC as linked library data providers, and finally about the <a href="http://www.istc-international.org">International Standard Text Code</a>. Admittedly it's kind of a hodge-podge of topics, and I'm going to taking some liberties with what 'linked data' and 'works' mean, so bear with me.

Both <a href="http://worldcat.org">OCLC Worldcat</a> and <a href="http://librarything.com">LibraryThing</a> mint URIs for bibliographic works, like these for Wide Sargasso Sea:

<ul>
<li><a href="http://www.librarything.com/work/27239">http://www.librarything.com/work/27239</a></li>
<li><a href="http://www.worldcat.org/oclc/24630204">http://www.worldcat.org/oclc/24630204</a>
</li></ul>

So the library community really <em>does</em> have web identifiers for works--or more precisely web identifiers for human readable <em>records</em> about works. What's missing (IMHO) is the ability to use that identifier to get back something meaningful for a machine. Tools like <a href="http://zotero.org">Zotero</a> need to scrape the screen to pull out the data points of interest to citation management. Sure, if you want you can implement <a href="http://ocoins.info">COinS</a> or <a href="http://unapi.info">unAPI</a> to allow the metadata to be extracted, but could there be a more web-friendly way of doing this?

Consider how blog syndication works on the web. You visit a blog (like this one) and your browser is able to magically figure out the location of an RSS or Atom feed for the blog, and give you an option to subscribe to it.

<img src="http://inkdroid.org/images/inkdroid-screenshot.png" width="450" />

 Well it's not really magic it's just a bit of markup in the HTML:

<pre lang="xml">
<link rel="alternate" 
         type="application/rss+xml" 
         title="inkdroid RSS Feed" 
         href="http://inkdroid.org/feed/" />
</pre>

Simple right? 

Now back to work identifiers.  Consider that both Worldcat and LibraryThing have <a href=http://www.oclc.org/productworks/worldcatapi.htm">web2.0</a> <a href="http://www.librarything.com/services/rest/documentation/1.0/">apis</a> for retrieving machine readable data for a work:

<pre>
http://www.librarything.com/services/rest/1.0/?method=librarything.ck.getwork&id={work_id}&apikey={your_key}
</pre>

or:

<pre>
http://www.worldcat.org/webservices/catalog/content/{oclc_number}?wskey={key}
</pre>

What if the web pages for these resources at OCLC and LibraryThing linked directly to these machine readable versions? For example if the page for <a href="http://www.librarything.com/work/27239">Wide Sargasso Sea</a> at LibraryThing contained this in its &lt;head&gt; element:

<pre lang="html">
<link rel="alternate" 
         type="application/xml" 
         title="XML for Wide Sargasso Sea" 
         href="http://www.librarything.com/services/rest/1.0/?method=librarything.ck.getwork&id=27239&apikey=d231aa37c9b4f5d304a60a3d0ad1dad4" />
</pre>

This would allow browsers, plugin tools like Zotero and web crawlers to follow the natural grain of the web and discover the machine readable representation. Admittedly this is something that <a href="http://ocoins.info">COinS</a> and <a href="http://unapi.info">unapi</a> are designed to do. But the COinS and unAPI protocols are really optimized for making citation data, and non web identifiers available and routable via a resolver of some kind. Maybe I'm just over reaching a bit, but this approach of using the &lt;link&gt; header seems to embrace the notion that there are resources within the Worldcat and Librarything websites, and there can be alternate representations of those resources that can be discovered in a <a href="http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven">hypertext-driven</a> way.

Of course there is the issue of the API key. In the example above I used the demo key in LibraryThing's docs. More important in the context of web identifiers for works is the need to distinguish between the identifier for the record, and the identifier for the concept of the work, which is most elegantly solved (IMHO) by following a pattern from the <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a> doc. But I think it's important that people realize that it's not necessary to jump headlong into RDF to start leveraging some of the principles behind the <a href="http://www.w3.org/TR/webarch/">Architecture of the World Wide Web</a>. Henry Thompson has a nice web-centric discussion of this issue in his <a href="http://www.ltg.ed.ac.uk/~ht/WhatAreURIs/">What's a URI and Why Does it Matter?</a>

While writing this blog post I noticed a thread over on <a href="http://thread.gmane.org/gmane.education.libraries.autocat/18832">Autocat</a> that Bowker has been named the US Registrar for the <a href="http://www.istc-international.org/index.php?ci_id=1828.">International Standard Text Code</a>. The gist is that the ISTC will be a "global identification system for textual works", and that registrars (like Bowker) will mint identifiers for works, such as:

<blockquote>
  ISTC 0A9 2002 12B4A105 7
</blockquote>

Where the <a href="http://www.istc-international.org/index.php?ci_id=1817#structure">structure</a> of the identifier is roughly:

<blockquote>
ISTC {registration agency} {year element} {work element} {check digit}
</blockquote>

It's interesting that the meat of the ISTC is the <em>work element</em> that is:

<blockquote>
... assigned automatically by the central ISTC registration system after a metadata record has been submitted for registration and the system has verified that the record is unique;
</blockquote>

The metadata record in question is actually a chunk of ONIX, which presumably Bowker will send to the ISTC central registrar, and get back a work id.

This work that the ISTC is taking on is really important--and one would imagine quite costly. One thing I would suggest to them is that they may want to make the ISTC codes have a URI equivalent like:

<blockquote>
  http://istc-international/0A9/2002/12B4A1057
</blockquote>

They also should encourage Bowker and other registrars to publish their work identifiers on the web:

<blockquote>
  http://bowker.com/istc/0A9/2002/12B4A1057
</blockquote>

It seems to me that we might (in the long term) be better served by a system that embraces the distributed nature of the web. A web in which organizations like Bowker, ISTC, OCLC, LibraryThing, Library of Congress and national libraries publish their work identifiers using URIs,  and return meaningful metadata for them. Rather than waiting for other people to solve our problems, why don't we start solving them ourselves bottom-up instead of waiting for someone else to solve it top-down? 

Anyhow I feel like I'm kind of being messy in suggesting this linked-data-lite idea. Is it heresy? My alibi/excuse is that I've been sitting in the same room as <a href="http://onebiglibrary.net/story/caching-and-proxying-linked-data">dchud</a> for extended periods of time.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>626</wp:post_id>
		<wp:post_date>2009-01-21 14:06:43</wp:post_date>
		<wp:post_date_gmt>2009-01-21 21:06:43</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>work-identifiers-and-the-web</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="worldcat"><![CDATA[worldcat]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{i:0;s:5:"80433";i:1;s:5:"80439";i:2;s:5:"80447";i:3;s:5:"80548";i:4;s:5:"80549";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[linked-data-lite-work-identifiers-and-the-web]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_45ec1499ef9d187af6dc00ce642245e7</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_9206037bed9db9a7edc88dc76d31b15f</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_da680560e1d73a8be88ec96b310eba3f</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_e5420b04b11671366feca4822608f88a</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_79cc8b3511b519a4e948e90e41f3b5f1</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_cd98f37c4b078f949d1080729c5bf3fd</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_887c6dad9194ea2797013de9c9bfa2af</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_4e1d532a0070eecb6ce2acb5b136ac7b</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_34e72d081d29afb318fc762d86bb4755</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_31334ee86e037c9150a32ae5752e7b2e</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_e364b3e34f3be25dfef606fee4c950d7</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_c88744e393a4b54cb7e1aa2d4d0debdb</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_5b08da7974e97623662b6b5d5970ebe0</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_d0a280e700103e88ff8decc5b69cf1f8</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_1903712ebd42ee2be419fa8671496828</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_da9deeb02e75bef8c3e292bff7b74758</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_77bdf4f3fea882735336a4587e7fde5c</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_f16acbcf4c171dad4972b184e1e08c6b</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_605922533eaed1e3468fb80234c74041</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_c7a01a2f19875c0687ad8fbafa980dfa</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_10d64bffb7cefd9c983b8e35336e29fb</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_3a5e3723393748cf09c51af98693dcf3</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_971e593205a3fde296050df083d2b76b</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_21d2793e8fef13bcbfef102e68aa938d</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_c499c790d99eafbf85b013342e1552a4</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_adf810485ec7ecc5a924c4390950cfcf</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_e8c1aac2029d93430edb364183c67fd9</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_cfad677e26422b0dbdae849abeb55d9e</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>80548</wp:comment_id>
			<wp:comment_author><![CDATA[dchud]]></wp:comment_author>
			<wp:comment_author_email>dchud@umich.edu</wp:comment_author_email>
			<wp:comment_author_url>http://onebiglibrary.net/</wp:comment_author_url>
			<wp:comment_author_IP>71.251.52.215</wp:comment_author_IP>
			<wp:comment_date>2009-01-25 15:05:04</wp:comment_date>
			<wp:comment_date_gmt>2009-01-25 22:05:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I really agree with this that you said, Ed: "treat web applications, and the resources within them, as instances of the vocabularies we aspire to use."

Just some quick nits, not to distract from your main point, though:  COinS isn't a protocol (OpenURL is), and unAPI isn't optimized for anything in particular.

In any case, the only main gap I see with using the link header for more is how much leverage it provides to be specific about relationships between distinct URIs. @type is limited to MIME, and @rel/@rev are limited to the dozen or so specified in HTML, unless you get into defining profiles.  And I can't say I've much experience with profiles, and I don't know if many other people do.

With unAPI we just left the format[@name] open to address this issue, to allow more human-readable specificity next to the @type value.  It'd be a stretch to say there has been enough unAPI usage to draw any useful conclusions about how the @name values tend to be used, though.

So that's one of the biggest open questions for me, but I'd like to help work on an answer.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>66</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80594</wp:comment_id>
			<wp:comment_author><![CDATA[hvdsomp]]></wp:comment_author>
			<wp:comment_author_email>hvdsomp@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>192.12.184.2</wp:comment_author_IP>
			<wp:comment_date>2009-01-26 15:43:28</wp:comment_date>
			<wp:comment_date_gmt>2009-01-26 22:43:28</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[With my team here in Los Alamos, I have done something that comes very close to what Ed describes in an ORE experiment. If I remember correctly, I presented the experiment at the ORE Open Meeting at John Hopkins University, March 2008. A QuickTime recording of the experiment is at http://public.lanl.gov/herbertv/images/cite_no_manager.mov . I am afraid this movie (90 Mb) has no voice over, and the experiment is a tad more elaborate than what Ed describes. Actually what Ed describes is, in this experiment, an enabler for a Web-based scholarly authoring tool that automatically inserts references to cited articles as follows:

1. The author links some text of his new article to (the HTTP URI of) the splash page of a to-be-cited article
2. The to-be-cited article splash page has the HTML LINK mechanism suggested by Ed that points at an ORE Resource Map for the article. One of the aggregated resources in the Resource Map is a bibliographic record, appropriately typed so that it can be recognized as being a biblio record.
3. Once the author is confident with the text, the Save button is clicked and a background process goes out to all links in the authors' text in search of biblio info, i.e. in search of HTML LINKs that point at ReMs that contain biblio info. That's the crawling biblio info part in Ed's post.
4. If biblio info is found, it is inserted in the reference list and tada ...

Basically, this experiment is about using the Web as the database for collecting article citations on a per need basis, instead of using a desktop tool with a local database.

Anyhow, the experiment was done in the days that an ORE Resource Map was still expressed as an Atom Feed. In ORE 1.0, a Resource Map is expressed as an entry, and the HTML LINK element that one would use now to point at such an entry is:

<pre lang="html">
<link href="http://example.net/hw.atom"
         type="application/atom+xml;type=entry" 
         rel="resourcemap" />
</pre>

(see http://www.openarchives.org/ore/1.0/discovery#HTMLLinkElement)

That is, of course, if one were to use ORE to achieve this all, as I did in the experiment.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>77</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80433</wp:comment_id>
			<wp:comment_author><![CDATA[Lorcan Dempsey]]></wp:comment_author>
			<wp:comment_author_email>dempseyl@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://claimid.com/lisld</wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.35</wp:comment_author_IP>
			<wp:comment_date>2009-01-21 15:43:24</wp:comment_date>
			<wp:comment_date_gmt>2009-01-21 22:43:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I am interested that you do not include the Library of Congress or national libraries in your list of organizations ....]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>72</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80436</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.55.17.90</wp:comment_author_IP>
			<wp:comment_date>2009-01-21 19:12:37</wp:comment_date>
			<wp:comment_date_gmt>2009-01-22 02:12:37</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@lorcan you are absolutely right. It wasn't intentional. I work at LC so it's a bit of a blind spot for me. The lccn.loc.gov service actually does implement the linked-data-lite pattern outlined in this post. For example if you look at http://lccn.loc.gov/2002405946 you'll see  a variety of link elements pointing at other xml documents.

Of course the lccn.loc.gov only provides access to traditional bibliographic records, aka manifestations...so they aren't the Works that you all have toiled so hard to create in your FRBRizations. ]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80439</wp:comment_id>
			<wp:comment_author><![CDATA[Andy Powell]]></wp:comment_author>
			<wp:comment_author_email>andy.powell@eduserv.org.uk</wp:comment_author_email>
			<wp:comment_author_url>http://andypowe11.net/</wp:comment_author_url>
			<wp:comment_author_IP>195.188.238.252</wp:comment_author_IP>
			<wp:comment_date>2009-01-22 05:50:51</wp:comment_date>
			<wp:comment_date_gmt>2009-01-22 12:50:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Re: "One thing I would suggest to them is that they may want to make the ISTC codes have a URI equivalent ".  I would go further... why create a new ITSC as a string of characters and then create a URI equivalent?  Why not simply mint new ITSCs directly as 'http' URIs?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>74</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80442</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.55.17.90</wp:comment_author_IP>
			<wp:comment_date>2009-01-22 08:34:13</wp:comment_date>
			<wp:comment_date_gmt>2009-01-22 15:34:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@andypowe11 absolute agreement. It seems like that particular leap of faith is hard for some people ; and for them perhaps its better to say they can have their cake and eat it too? /me shrugs]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80447</wp:comment_id>
			<wp:comment_author><![CDATA[yann nicolas]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://variable.pip.verisignlabs.com/</wp:comment_author_url>
			<wp:comment_author_IP>92.143.11.128</wp:comment_author_IP>
			<wp:comment_date>2009-01-22 13:14:01</wp:comment_date>
			<wp:comment_date_gmt>2009-01-22 20:14:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Just a doubt about what http://www.worldcat.org/oclc/24630204 actually refers to ?
Does it refer to the Work or one privileged manifestation among the manifestations of that work ?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>75</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80468</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.124</wp:comment_author_IP>
			<wp:comment_date>2009-01-23 06:43:30</wp:comment_date>
			<wp:comment_date_gmt>2009-01-23 13:43:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@yann yes, a good amount of skepticism here is definitely warranted. I think calling the resource at that URI a Work (FRBR) may be wrong. I was told the same thing by Jonathan Rochkind in #code4lib. The links out to various editions is what made me think perhaps it was a Work, or perhaps an Expression. Perhaps <a href="http://www.worldcat.org/oclc/24630204/editions" rel="nofollow">http://www.worldcat.org/oclc/24630204/editions</a> works better? Maybe the story with LibraryThing is clearer? 

I guess my point is that the library community could be attempting to treat web applications, and the resources within them, as instances of the vocabularies we aspire to use. Lets use the web as a medium for using our descriptive languages like FRBR, RDA, etc. When we create a web applications (OPACS, etc), what are the resources in it that we are making available? 

Perhaps I'm just saying what some people have been saying for years in forums like <a href="http://dir.gmane.org/gmane.culture.libraries.ngc4lib" rel="nofollow">ngc4lib</a>. But I feel like the <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer" rel="nofollow">REST</a> architectural style and notions of <a href="http://linkeddata.org" rel="nofollow">linked data</a> can help the library community grapple what it means to distribute its cataloging data on the web.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80549</wp:comment_id>
			<wp:comment_author><![CDATA[dchud]]></wp:comment_author>
			<wp:comment_author_email>dchud@umich.edu</wp:comment_author_email>
			<wp:comment_author_url>http://onebiglibrary.net/</wp:comment_author_url>
			<wp:comment_author_IP>71.251.52.215</wp:comment_author_IP>
			<wp:comment_date>2009-01-25 15:11:06</wp:comment_date>
			<wp:comment_date_gmt>2009-01-25 22:11:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hmm, just a quick extra thought about that last comment... in the interest of "using what's already there", we could come up with a convention for using the link[@rel="contents"] att/value pair to point to a human-readable HTML contents page which doubles as a "resource map"... with its own link[@rel="alternate", @type="application/xml+ore"] or whatever behind it.

I wonder if anybody is already doing this?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>66</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>crawling bibliographic data</title>
		<link>http://inkdroid.org/2009/01/22/crawling-bibliographic-data/</link>
		<pubDate>Thu, 22 Jan 2009 12:37:12 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=663</guid>
		<description></description>
		<content:encoded><![CDATA[Today's Guardian article <a href="http://www.guardian.co.uk/technology/2009/jan/22/library-search-engines-books">Why you can't find a library book in your search engine</a> prompted me to look at <a href="http://www.worldcat.org/robots.txt">Worldcat's robots.txt file</a> for the first time. Part of the beauty of the web is that it's an open information space where anyone (people and robots) can start with a single URL and <a href="http://efoundations.typepad.com/efoundations/2008/01/following-your.html">follow their nose</a> to other URLs. This seemingly simple principle is what has allowed a advertising^w search company like Google (that we all use every day) to grow and prosper. 

The <a href="http://www.robotstxt.org/">robots.txt</a> file is a simple mechanism that allows web publishers to tell web crawlers what they are allowed to look at on a website. Predictably, the files are always found at the root of a website in a file named <em>robots.txt</em>. You don't have to have one, but many publishers like to control what gets indexed on their website, sometimes to hide content, and other times to shield what may be costly server side operations. Anyway, here's what you see today for worldcat.org:

<pre>
User-agent: *
Disallow: /search

Sitemap: http://worldcat.org/identities/sitemap_index.xml
</pre>

So this instructs a web crawler to not follow any links that match /search in the path, such as:

<blockquote>
<a href="http://www.worldcat.org/search?qt=worldcat_org_all&q=everything+is+miscellaneous">http://www.worldcat.org/search?qt=worldcat_org_all&q=everything+is+miscellaneous</a>
</blockquote>

Now if you look on the homepage for Worldcat there are very few links into the dense bibliographic information space that is worldcat. But you'll notice a few in the lower left box "Create lists".  So a crawler could for example discover a link to:

<blockquote>
<a href="http://www.worldcat.org/oclc/77271226">http://www.worldcat.org/oclc/77271226</a>
</blockquote>

This URL is allowed by the robots.txt so the harvester could go on to that page.  Once at that item page there are lots of links to other bibliographic records: but notice the ones to other record displays all seem to match the /search pattern disallowed by the robots.txt, such as:

<blockquote>
<a href="http://www.worldcat.org/search?q=au%3AC++S+Harris&qt=hot_author">http://www.worldcat.org/search?q=au%3AC++S+Harris&qt=hot_author</a>
</blockquote>

or

<blockquote>
<a href="http://www.worldcat.org/search?q=su%3ALondon+%28England%29+Fiction.&qt=hot_subject">http://www.worldcat.org/search?q=su%3ALondon+%28England%29+Fiction.&qt=hot_subject</a>
</blockquote>

So a web crawler will not be able to wander into the rich syndetic structure of Worldcat and start indexing.

However, all is not lost. Notice above that OCLC does reference a <a href="http://worldcat.org/identities/sitemap_index.xml">Worldcat sitemap</a> in their robots.txt. <a href="http://www.sitemaps.org/">Sitemaps</a> are a lightweight mechanism that Yahoo, Google and Microsoft developed for instructing a web harvester on how to walk through a site. 

So if we look at OCLC's sitemap <a href="http://worldcat.org/identities/sitemap_index.xml">sitemap</a> we'll see this:

<pre lang="xml">
< ?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9
        http://www.sitemaps.org/schemas/sitemap/0.9/siteindex.xsd">
    <sitemap>
      <loc>http://worldcat.org/identities/lccn-no99-80690.sitemap.xml</loc>
      <lastmod>2008-05-19</lastmod>
    </sitemap>
    <sitemap>
      <loc>http://worldcat.org/identities/lccn-sh95-8559.sitemap.xml</loc>
      <lastmod>2008-05-19</lastmod>
    </sitemap>
  </sitemapindex>
</pre>

This essentially defers to two other sitemaps. The first 30 lines of the <a href="http://worldcat.org/identities/lccn-no99-80690.sitemap.xml">first one (careful in clicking it's big!)</a> looks like:

<pre lang="xml">
< ?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9
        http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd">
  <url>
    <loc>http://worldcat.org/identities/lccn-no99-80690</loc>
    <lastmod>2008-05-19</lastmod>
    <changefreq>monthly</changefreq>
    <priority>1.0000</priority>
  </url>
  <url>
    <loc>http://worldcat.org/identities/lccn-n78-95332</loc>
    <lastmod>2008-05-19</lastmod>
    <changefreq>monthly</changefreq>
    <priority>1.0000</priority>
  </url>
  <url>
    <loc>http://worldcat.org/identities/lccn-n79-41716</loc>
    <lastmod>2008-05-19</lastmod>
    <changefreq>monthly</changefreq>
    <priority>1.0000</priority>
  </url>
  <url>
    <loc>http://worldcat.org/identities/lccn-n80-92173</loc>
    <lastmod>2008-05-19</lastmod>
    <changefreq>monthly</changefreq>
    <priority>1.0000</priority>
  </url>
  ...
</urlset>
</pre>

Now we can see the beauty of sitemaps. They are basically just an XML representation for sets of web resources, much like syndicated feeds. There are actually 40,000 links listed in the first sitemap file, and 12,496 in the second. Now URLs like  

<blockquote>
<a href="http://worldcat.org/identities/lccn-no99-80690">http://worldcat.org/identities/lccn-no99-80690</a>
</blockquote>

are clearly allowed by the robots.txt file. So indexers can wander around and index the lovely identities portion of Worldcat.  It's interesting though, that the content served up by the identities portion of Worldcat is not HTML--it's XML that's transformed client side to HTML w/ XSLT.  So it's unclear how much a stock web crawler would be able to discover from the XML. If google/yahoo/microsoft's crawlers are able to apply the XSLT transform, they will get some HTML to chew on. But notice in the HTML view that all the links into Worldcat proper (that aren't other identities) are disallowed because they start with <em>/search</em>.

And a quick grep and perl pipeline confirm that all 52496 urls in the sitemap are to the identies portion of the site...

So this is a long way of asking: I wonder if web crawlers are crawling the books views on Worldcat at all?  I imagine someone else has written about this already, and there is a known answer, but I felt like writing about the web and library data anyhow.

Since OCLC has gone through the effort of providing a web presentation for millions of books, and even links out to the libraries that hold them, they seem uniquely positioned to provide a global gateway for web crawlers to the library catalogs around the world. The links from worldcat out to the rest of the world's catalogs would turn OCLC into a bibliographic <a href="http://www.worldcat.org/oclc/52315903">super node</a> in the graph of the web, much like Amazon and Google Books. But perhaps this is perceived as giving up the family jewels? Or maybe it would put to much stress on the system? Of course it would also be great to see machine readable data served up in a <a href="http://inkdroid.org/2009/01/21/work-identifiers-and-the-web/">similar</a> linked way

So in conclusion, it to would be awesome to see either (or maybe both):

<ul>
<li>the /search exclusion removed from the robots.txt file</li>
<li>sitemaps added for the web resources that look like http://www.worldcat.org/oclc/77271226</li>
</ul>

Of course one of the big projects I work on at LC is <a href="http://loc.gov/chroniclingamerica">Chronicling America</a> which is currently excluded by LC's <a href="http://loc.gov/robots.txt">robots.txt</a>...so I know that there can be real reasons for restricting crawling access (in our case performance problems we are trying to fix).  

<em>
Oh gosh, I just noticed when re-reading the Guardian article that my lcsh.info experiment was mentioned. Hopefully there will be good news to report from LC on this front shortly.
</em>

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>663</wp:post_id>
		<wp:post_date>2009-01-22 05:37:12</wp:post_date>
		<wp:post_date_gmt>2009-01-22 12:37:12</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>crawling-bibliographic-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="crawling"><![CDATA[crawling]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="indexing"><![CDATA[indexing]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="post_tag" nicename="robots"><![CDATA[robots]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="category" nicename="worldcat"><![CDATA[worldcat]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>80852</wp:comment_id>
			<wp:comment_author><![CDATA[Thom]]></wp:comment_author>
			<wp:comment_author_email>hickey@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.14</wp:comment_author_IP>
			<wp:comment_date>2009-02-13 14:54:03</wp:comment_date>
			<wp:comment_date_gmt>2009-02-13 21:54:03</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Actually, we notice the major crawlers and send them HTML rather than the raw XML.  We do this if we think you are on a mobile device too, since most the the current browsers on phones don't handle the XML+XSLT very well.

We've also turned off the 'do not follow' header to make harvesting of Identities work better.

--Th]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>80</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>80868</wp:comment_id>
			<wp:comment_author><![CDATA[teetsm]]></wp:comment_author>
			<wp:comment_author_email>teetsm@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>132.174.23.21</wp:comment_author_IP>
			<wp:comment_date>2009-02-20 12:41:26</wp:comment_date>
			<wp:comment_date_gmt>2009-02-20 19:41:26</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed,

I wanted to jump in and clarify something...  You are absolutely right in your description of robots.txt and potential implications.  What is not obvious from looking at our robots.txt is that we have alternative methods in place with the major engines.  When our relationships with the engines started many years ago, it was necessary to put special feeds to accommodate the size and structure of Worldcat.  We worked directly with them to provide the data in a way that worked best for their services.  There are aspects such as our robots.txt and no-follows that are remnants of their requests in previous years and prevented multiple feeds from conflicting within their environments.  With advances and changes within the engines and our services, much of what we did then can now be accomplished more standard methods.  You will see some changes such as what Th suggests above, as well as others over the next while as we transition away from the older models with the search engine partners.

Mike Teets, OCLC]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>83</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85444</wp:comment_id>
			<wp:comment_author><![CDATA[What to do with Linked Data? &raquo; Overdue Ideas]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.meanboyfriend.com/overdue_ideas/2012/08/what-to-do-with-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-08-01 10:55:50</wp:comment_date>
			<wp:comment_date_gmt>2012-08-01 17:55:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] her specific use case - 271,429,346 bibliographic records in WorldCat this is no small feat. Also Ed Summer&#8217;s post about crawling WorldCat points at some issues &#8211; although things have moved on since 2009, and now the sitemap files [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1343846714.3132";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>the importance of being crawled</title>
		<link>http://inkdroid.org/2009/01/29/the-importance-of-being-crawled/</link>
		<pubDate>Thu, 29 Jan 2009 10:33:56 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=734</guid>
		<description></description>
		<content:encoded><![CDATA[While lcsh.info was up and running harvesters actively crawled it. At its core all lcsh.info did was mint a URI for every Library of Congress Subject Heading. This is similar in spirit to <a href="http://en.wikipedia.org/wiki/Brewster_Kahle">Brewster Kahle's</a> more ambitious <a href="http://openlibrary.org">OpenLibrary</a> project to mint a URI for every book, or in his words:

<blockquote>
One web page for every book
</blockquote>

<em>Aside: It's also similar in spirit to RESTful web development, and to the linked data, semantic web effort generally.</em>

Minting a URI for every Library of Congress Subject Heading meant that there were lots of densely interlinked pages. Some researchers at Stanford did a data visualization of LCSH two years ago, which illustrates just how deeply linked LCSH is:

<div align="middle">
<img src="http://inkdroid.org/images/lcsh-viz.png"  />
</div>

I wanted lcsh.info to get crawled so I intentionally put some high level, well connected concepts (Humanities, Science, etc) on the home page to provide a doorway for web crawlers to walk through into the site and begin discovering all the broader, narrower, related links between concepts--without having to perform a search.

So lcsh.info is <a href="http://lcsh.info/comments1.html">down</a> now, but it turns out you can still see its shadow living on in quite a usable form in web search engines. For example type this into any of the big three search engines:

<blockquote>
site:lcsh.info mathematics
</blockquote>

And you'll see:

<h2>Google</h2>
<a href="http://www.google.com/search?q=site:lcsh.info+mathematics"><img src="http://inkdroid.org/images/lcsh_google.png" width="450" /></a>

<h2>Yahoo</h2>

<a href="http://search.yahoo.com/search?p=site:lcsh.info+mathematics">
<img src="http://inkdroid.org/images/lcsh_yahoo.png" width="450" />
</a>

<h2>Microsoft</h2>
<a href="http://search.live.com/results.aspx?q=site:lcsh.info+mathematics">
<img src="http://inkdroid.org/images/lcsh_microsoft.png" width="450" />
</a>

It's interesting that (unlike Google and Yahoo) Microsoft's relevancy ranking actually puts the heading for "Mathematics" at the top. Also note that simple things like giving the page a good title, and descriptive text make the heading show up in usable form in each search engine.

It's not too surprising that trying the same for <a href="http://www.google.com/search?q=site%3Aauthorities.loc.gov+mathematics">authorities.loc.gov</a> doesn't work out so well. Umm, yeah <a href="http://authorities.loc.gov/robots.txt">http://authorities.loc.gov/robots.txt</a>...

On the one hand, I'm just being nostalgic looking at the content that once was there &sigh;. But on the other there seems to be a powerful message here, that putting data out onto the open web, and making it crawlable means your content is viewable via lots of different lenses. Maybe you don't have to get search exactly right on your website, let other people do it for you.

Two other things come to mind: <a href="http://en.wikipedia.org/wiki/LOCKSS">LOCKSS</a> and Brewster's even more ambitious <a href="http://archive.org">project</a>. I've been sort <a href="http://www.flickr.com/photos/ari/2238959127/">hoping</a> that somehow or another the Internet Archive and the Open Library would find there way into being publicly funded projects. What if? I can daydream right?
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>734</wp:post_id>
		<wp:post_date>2009-01-29 03:33:56</wp:post_date>
		<wp:post_date_gmt>2009-01-29 10:33:56</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-importance-of-being-crawled</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="crawling"><![CDATA[crawling]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="lcsh"><![CDATA[lcsh]]></category>
		<category domain="post_tag" nicename="microsoft"><![CDATA[microsoft]]></category>
		<category domain="post_tag" nicename="search"><![CDATA[search]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="yahoo"><![CDATA[yahoo]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>c4l09</title>
		<link>http://inkdroid.org/2009/03/03/c4l09/</link>
		<pubDate>Tue, 03 Mar 2009 21:22:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=768</guid>
		<description></description>
		<content:encoded><![CDATA[<p>So <a href="http://code4lib.org/2009">code4lib2009</a> was a whole lot of fun. The amazing thing about the conference isn't really reflected in the <a href="http://code4lib.org/conference/2009/schedule">program</a> of talks. I feel like I can say that since I was <a href="http://inkdroid.org/talks/sword.pdf">one</a> of them.</p>

<p>The real value is the social space and the time to talk to people you've seen online, throw around ideas, get background/contextual information on projects, etc. Hats off to <a href="http://www.linkedin.com/pub/dir/jean/rainwater">Jean Rainwater</a> and <a href="http://bspace.us/notes/tags/code4lib/">Birkin Diana</a> for picking an beautifully casual and intimate <a href="http://www.marriott.com/hotels/travel/pvdbr-renaissance-providence-hotel/">hotel</a> to hold the conference in.</p>

<p>It's taken me a few days to get some perspective on all that happened. In the meantime I've read a few accounts that capture important aspects of the event from: <a href="http://oregonstate.edu/~reeset/blog/archives/623">Terry Reese</a>, <a href="http://managemetadata.org/blog/2009/02/25/embrace-the-chaos/">Jon Phipps</a>, <a href="http://blog.reallywow.com/archives/45">Jay Luker</a>, <a href="http://www.declan.net/2009/03/01/code4lib-2009-in-providence-ri/">Declan Fleming</a>, Richard Wallis (<a href="http://blogs.talis.com/panlibus/archives/2009/02/code4lib-2009-day-1.php">1</a>,<a href="http://blogs.talis.com/panlibus/archives/2009/02/fight-the-good-fight-code4lib-day-2.php">2</a>,<a href="http://blogs.talis.com/panlibus/archives/2009/02/code4lib-final-day-in-providence-looking-forward-to-asheville.php">3</a>), <a href="http://onebiglibrary.net/story/late-night-with-library-geeks">Dan Chudnov</a>, <a href="http://rc98.net/c4l2009">Gabe Farrell</a>.</p>

<p>The <a href="http://wiki.code4lib.org/index.php/LinkedData">Linked Data Pre-conference</a> was quite valuable. For one it gave attendees some experience in what it means to publish data in a distributed way, and to write code to aggregate it using a <a href="http://web.archive.org/web/20090302095749/http://inkdroid.org:80/c4l2009/attendees">attendees/FOAF</a> experiment. Mike Giarlo aptly <a href="http://twitter.com/mjgiarlo/statuses/1232273998">surmised</a> from this that the key points for teaching beginners about linked data are that:</p>

<ol>
    <li>Validators are essential</li>
    <li>You are not your FOAF</li>
</ol>

<p>In other words:</p>

<ol>
    <li>Am I doing this rdf/xml, turtle, rdfa right?</li>
        <li>ZOMG, httpRange-14!</li>
</ol>

<p>Ian Davis presented the <a href="http://www.slideshare.net/iandavis/30-minute-guide-to-rdf-and-linked-data">basics of RDF</a> for people who are already familiar with traditional data management. Apparently Ian's slides hit #1 for the day on SlideShare, which highlights the interest in linked data that is percolating through the Web. The pre-conf was very well attended as well.</p>

<p><a href="http://flickr.com/photos/inkdroid/3323422330/"><img src="http://inkdroid.org/images/lod_preconf.jpg" width="450"  border="0"/></a></p>

<p>Some folks like <a href="http://xplus3.net/">Jonathan Brinley</a> and Michael Klein were able to hack on a <a href="http://svn.breaksalot.org/supybot-plugins/plugins/FOAF/">Supybot Plugin</a> to work with the FOAF data generated by the <a href="http://web.archive.org/web/20101216222329/http://inkdroid.org/bzr/c4libbers/crawl.py">crawler</a>.  I also got chatting with <a href="http://www.miskatonic.org/">William Denton</a> about the potential of linked data for FRBR/RDA efforts. Unfortunately I didn't hear about <a href="http://alimanfoo.wordpress.com/">Alistair Miles</a>' new <a href="http://code.google.com/p/code4rda/wiki/HomePage">project</a> on google-code for exploring the translation of traditional MARC/MODS into RDA/FRBR until after the event. Most of the other slides from presenters at the pre-conf are available from the <a href="http://wiki.code4lib.org/index.php/LinkedData">wiki page</a>.</p>

<p>I was really struck by some of the issues that Dan Chudnov raised in his talk about <a href="http://onebiglibrary.net/story/code4lib-2009-talk-on-caching-and-proxying-linked-data">Caching and Proxying Linked Data</a> right before lunch.  In particular his comparison of the <a href="http://www4.wiwiss.fu-berlin.de/bizer/pub/lod-datasets_2009-02-27.png">Linking Open Data Cloud</a> to what libraries understand as their ready reference collection:</p>

<p><img width="450" src="http://inkdroid.org/images/readyref.png" /> 
<em>See p.9  of Dan's<a href="http://onebiglibrary.net/files/20090223-code4lib-linked-data.pdf"> slides</a></em></p>

<p>Dan explored how we need to think about the technical and administrative details of managing linked-data if linked-data is to be taken seriously by the library community. Relatedly the pre-conf gave me an opportunity to publicly apologize to <a href="http://blog.libris.kb.se/librisutv/">Anders Söderbäck</a> for yanking lcsh.info offline in such an abrupt manner, and disturbing his links from subject authority records at libris.kb.se to lcsh.info. Dan's ideas for consuming library linked data and Anders and mine experience publishing library linked data gelled nicely in my brain. Similar ideas from Jon Phipps (one of the authors of <a href="http://www.w3.org/TR/swbp-vocab-pub/">Best Practice Recipes for Publishing RDF Vocabularies</a>) have led me to believe this could be a nice little area for some research.</p>

<p>Prepping for the pre-conference itself was good fun, since it led me to discover a series of connections between the early development of the www and Brown University (where the conference was being held) and the history of hyperdata/text: in a nutshell it was <a href="">Tim Berners-Lee's </a><a href="http://www.w3.org/History/1989/proposal.html">proposal</a> for the web -> <a href="http://en.wikipedia.org/wiki/Dynatext">Dynatext</a> -> <a href="http://www.derose.net/">Steve DeRose</a> -> <a href="http://en.wikipedia.org/wiki/Andries_van_Dam">Andy van Dam</a> -> <a href="http://en.wikipedia.org/wiki/Hypertext_Editing_System">Hypertext Editing System</a> -> <a href="http://en.wikipedia.org/wiki/Ted_Nelson">Ted Nelson</a> -> <a href="http://en.wikipedia.org/wiki/Doug_Engelbart">Doug Engelbart</a> -> <a href="http://en.wikipedia.org/wiki/Vannevar_Bush">Vannevar Bush</a>.  Yeah, I guess you had to be there ... or maybe that didn't help. At any rate the slides, complete with breakdancing instructions are <a href="http://inkdroid.org/talks/c4links.pdf">available</a>.</p>

<iframe src='http://docs.google.com/EmbedSlideshow?docid=dv89m3d_26c6rrnmcq' frameborder='0' width='410' height='342'></iframe>

<p>I haven't even started talking about the main event yet. The things I took away from the 3 days of presentations and talks, in no particular order were:</p>

<ul>
    <li>I want to learn more about the <a href="http://www.crossref.org/CrossTech/2007/02/crossref_author_id_meeting.html">Author-ID</a> effort that Geoffrey Bilder talked about</li>
    <li><a href="http://www.betaversion.org/~stefano/linotype/">Stefano Mazzocchi</a>'s keynote and Sean Hannan's <a href="http://code4lib.org/files/freebase.pdf">presentation</a> convinced me that I <em>need</em> to understand and play with Freebase's JavaScript application development environment <a href="http://web.archive.org/web/20100801030219/http://dev.freebaseapps.com:80/">Acre</a> and the sparql-ish, query by example <a href="http://mql.freebaseapps.com/">Metaweb Query Language (MQL)</a>. It seems like Freebase is exploring some really interesting territory in building a shared knowledge base of machine readable, human editable data, which can sit behind a seemingly infinite amount of web presentation layers.</li>
<li>Terence Ingram's <a href="http://www.slideshare.net/eby/restafarianism-at-the-nla">presentation</a>, Ross Singer's <a href="http://www.slideshare.net/eby/like-a-can-opener-for-your-data-silo-simple-access-through-atompub-and-jangle">presentation</a> about<a href="http://jangle.org/"> Jangle</a>, me and Mike's SWORD <a href="http://docs.google.com/Presentation?docid=dv89m3d_3hfbjmbhq">presentation</a>, and a chat with <a href="http://web.archive.org/web/20090206124810/http://fedora-commons.org:80/confluence/display/FCR30/Fedora+REST+API">Fedora/REST</a> proponent <a href="http://flyingzumwalt.blogspot.com/">Matt Zumwalt</a>, and hearing about the <a href="http://www.talis.com/platform/">Talis Platform</a> have convinced me that real <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">REST</a> has got mind-share and traction in the library technology world.</li>
<li>Ian Davis' <a href="http://www.slideshare.net/iandavis/code4lib2009-keynote-1073812">keynote</a> on the second day captured for me, the constant challenge it is to stay true to the roots of the web, and how important it is to stay true to them. It was really interesting to hear how he emphasized the importance of data over code, and the necessity for decentralization compared with the centralization.</li>
<li>Chatting with <a href="http://www.jodischneider.com/">Jodi Schneider</a> and <a href="http://www.miskatonic.org/">William Denton</a> and listening to their <a href="http://code4lib.org/files/frbr_code4lib09.pdf">presentation</a> made me want to understand RDA and FRBR at a practical level. This includes getting into the vocabularies that are being developed, and <a href="http://code.google.com/p/code4rda/wiki/HomePage">trying to convert</a> some data. The history of FRBR in particular <a href="http://pi.library.yorku.ca/dspace/handle/10315/1250">as told by Bill</a> is also a gateway into a really fascinating history of cataloging. Also the work that Diane Hillman and Jon Phipps have been doing to enable vocabulary development like RDA/FRBR seems really important to keep abreast of.</li>
</ul>

<p>More tidbits will probably float into my blog or into my <a href="http://twitter.com/edsu">tweets</a> over the coming weeks, as the beer wears off, and the ideas sink in. But for now I'll leave you with some of my favorite photos from the conference. It's the people that makes code4lib what it is. It was great to connect up, and meet new folks in the field.</p>

<table>
<tr>
<td><a href="http://flickr.com/photos/bigdpix/3316707035/" title="mjg"><img src="http://inkdroid.org/images/c4l09/mjgiarlo.jpg" width="225" align="left"/></a></td>
<td><a href="http://flickr.com/photos/bigdpix/3317534966/" title="charper"><img src="http://inkdroid.org/images/c4l09/charper.jpg" width="225" align="left"/></a></td>
</tr>
<tr>
<td><a href="http://flickr.com/photos/bigdpix/3317462352/" title="rsinger"><img src="http://inkdroid.org/images/c4l09/rsinger.jpg" width="225" align="left"/></a></td>
<td><a href="http://flickr.com/photos/evergreen-ils/3304327331/" title="anagy"><img src="http://inkdroid.org/images/c4l09/anagy.jpg" width="225" align="left" /></a></td>
</tr>
<tr>
<td><a href="http://flickr.com/photos/schwartzray/3310167317/" title="kat3 &amp; ecorrado"><img src="http://inkdroid.org/images/c4l09/kat3_ecorrado.jpg" width="225" align="left"/></a></td>
<td><a href="http://flickr.com/photos/schwartzray/3310170905/" title="wtd"><img src="http://inkdroid.org/images/c4l09/wtd.jpg" width="225" align="left"/></a></td>
</tr>
<tr>
<td><a href="http://flickr.com/photos/schwartzray/3311002372/" title="dchud"><img src="http://inkdroid.org/images/c4l09/dchud.jpg" width="225" align="left"/></a></td>
<td><a href="http://flickr.com/photos/schwartzray/3311005112/" title="gsf &amp; MrDys"><img src="http://inkdroid.org/images/c4l09/gsf_mrdys.jpg" width="225" align="left"/></a></td>
</tr>
<tr>
<td><a href="http://flickr.com/photos/bigdpix/3316642473/" title="decasm"><img src="http://inkdroid.org/images/c4l09/decasm.jpg" width="225"/></a></td>
<td><a href="http://flickr.com/photos/bigdpix/3317472596/" title="anarchivist"><img src="http://inkdroid.org/images/c4l09/anarchivist.jpg" width="225" align="left"/></a></td>
</tr>
<tr>
<td><a href="http://flickr.com/photos/schwartzray/3310992788/" title="jrochkind &amp; co"><img src="http://inkdroid.org/images/c4l09/jrochkind.jpg" width="225" /></a></td>
<td><a href="http://flickr.com/photos/bigdpix/3316646551/" title="robcaSSon"><img src="http://inkdroid.org/images/c4l09/robcaSSon.jpg" width="225" align="left"/></a></td>
<td>&nbsp;</td>
</tr>
</table>

<p>Oh and in case you missed it, the <a href="http://search.twitter.com/search?q=%23c4l09">tweetstream</a> and the<a href="http://flickr.com/photos/tags/c4l09/"> other fine photos</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>768</wp:post_id>
		<wp:post_date>2009-03-03 14:22:00</wp:post_date>
		<wp:post_date_gmt>2009-03-03 21:22:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>c4l09</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="c4l09"><![CDATA[c4l09]]></category>
		<category domain="post_tag" nicename="code4lib"><![CDATA[code4lib]]></category>
		<category domain="post_tag" nicename="conferences"><![CDATA[conferences]]></category>
		<category domain="post_tag" nicename="hypertext"><![CDATA[hypertext]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkddata"><![CDATA[linkddata]]></category>
		<category domain="category" nicename="people"><![CDATA[people]]></category>
		<category domain="post_tag" nicename="providence"><![CDATA[providence]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81164</wp:comment_id>
			<wp:comment_author><![CDATA[gbilder]]></wp:comment_author>
			<wp:comment_author_email>gbilder@crossref.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>67.89.23.173</wp:comment_author_IP>
			<wp:comment_date>2009-03-05 06:54:51</wp:comment_date>
			<wp:comment_date_gmt>2009-03-05 13:54:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[You might be interested in some recent online conversations I've been involved in recently. Most of them linked to from here:

http://www.crossref.org/CrossTech/2009/02/an_interview_about_author_ids.html

And am happy to chat about author-id any time.

--G]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>87</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>APIs Suck</title>
		<link>http://inkdroid.org/2009/03/05/apis-suck/</link>
		<pubDate>Thu, 05 Mar 2009 15:57:01 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=831</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://yeswescan.org"><img src="http://inkdroid.org/images/yeswescan.png" style="float: left; border: 0; margin-right: 10px;" /></a></p>

<p>With <a href="http://www.transparencycamp.org/">TransparencyCamp</a> last weekend, <a href="http://groups.google.com/group/sunlightlabs/browse_thread/thread/dfd9fd76be9b6f1b">news</a> of the mandated use of feed syndication by Federal Agencies receiving funds from the Recovery Act, recent blog posts by <a href="http://radar.oreilly.com/2009/03/bulk-data-downloads-government-transparency-breakthrough.html">Tim O'Reilly</a> and the <a href="http://web.archive.org/web/20090415164126/http://sla-divisions.typepad.com:80/government_information/2009/03/newsweek-reports-on-the-peoples-data-oogl.html">Special Libraries Association</a>, an article in <a href="http://web.archive.org/web/20090416053255/http://www.newsweek.com:80/id/186991">Newsweek</a>, news of Carl Malamud's <a href="http://yeswescan.org/">bid</a> to become the Public Printer of the United States (aka head of the GPO), and the W3C eGov <a href="http://www.w3.org/2007/eGov/IG/wiki/F2F2">meeting</a> coming up next week it looks like issues related public access to government data (specifically Library of Congress bibliographic and legislative data) are hitting the mainstream media, and getting political mind-share. Exciting times.</p>

<p>One thing that bubbled up at <a href="http://code4lib.org/2009">code4lib2009</a> last week was the notion that <em>APIs Suck</em>.  Not that web2.0 APIs are wrong or bad...they're actually great, especially when compared to a world where no machine access to the data existed before.  The point is that sometimes just having access to the raw data in the 'lowest level format' is the ideal. Rather than service providers trying to guess what you are trying to do with their data, and absorbing the computational responsibility of delivering it, why not make the data readily available using a protocol like HTTP? Put the data in a directory, turn on Indexes, do some sensible caching, and maybe gzip compression and let people grab it, and robots crawl it. Or maybe use something like <a href="http://aws.amazon.com/publicdatasets/">Amazon Public Datasets</a>. It seems like a relatively easy first step, that involves very little custom software development, and one with the ability make a huge impact.</p>

<p>I'm a federal employee, so I really can't come out and formally advocate directly for political appointments. But I have to say it would great to see someone like Malamud at the helm of the GPO, since he's been doing just this kind of <a href="http://bulk.resource.org">work</a> for 20 years. Exciting times.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>831</wp:post_id>
		<wp:post_date>2009-03-05 08:57:01</wp:post_date>
		<wp:post_date_gmt>2009-03-05 15:57:01</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>apis-suck</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="apis"><![CDATA[apis]]></category>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="loc"><![CDATA[loc]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="post_tag" nicename="politics"><![CDATA[politics]]></category>
		<category domain="post_tag" nicename="publicdomain"><![CDATA[publicdomain]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"81219";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81220</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.50.234.42</wp:comment_author_IP>
			<wp:comment_date>2009-03-07 11:34:20</wp:comment_date>
			<wp:comment_date_gmt>2009-03-07 18:34:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah, agreed on it not being either/or. But my main point is that it's probably easiest to make the data available first in some raw form, and then make the shiny API later--after it's easier to see how people are using the data. 

I guess the title of my blog post sucks, much more than APIs :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81219</wp:comment_id>
			<wp:comment_author><![CDATA[dilettantes.blogspot.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://dilettantes.blogspot.com/</wp:comment_author_url>
			<wp:comment_author_IP>67.161.232.211</wp:comment_author_IP>
			<wp:comment_date>2009-03-07 10:24:53</wp:comment_date>
			<wp:comment_date_gmt>2009-03-07 17:24:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I know Anders was being kind of flip when he made this remark (since immediately after it he said that Libris had APIs and that's what they used), but I also think sentiment is a little disingenuous.  APIs that suck suck.  On the flipside, if all we had to work with from any organization where big dumps of data, <em>that would probably suck even more</em>.  The notion of having to bootstrap a heap of data into some usable form, just to even see if there's anything useful in it, seems inefficient and counter productive.

I guess my point is, this isn't an either/or proposition.  There are plenty of organizations that provide both an API <em>and</em> their entire data set.  Govtrack.us does this nicely, I think.  You can pick and choose between entire datasets and a la carte access as your needs determine.

I readily admit that I'm biased on this.  For the last year, majority of my time has been spent designing an API intended primarily to make all of your data available for reuse.  Still, it's an API.  There will be data (personal, borrower data, for example) that people will want to control.  At least, I hope people will want to control.

But just because it's an API, doesn't mean that it's the same thing as Facebook's or Worldcat's API.  An API doesn't <em>inherently</em> have to be restrictive.  There are just some organizations that feel less restriction on the use of their data is more disruptive than they feel comfortable with, with regard to their business model.

That's a totally different blog post comment, however.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>88</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>LibraryThing Ubuntu Screen Saver </title>
		<link>http://inkdroid.org/2009/03/11/librarything-ubuntu-screen-saver/</link>
		<pubDate>Wed, 11 Mar 2009 18:55:20 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=858</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I read about the <a href="http://www.librarything.com/blog/2009/03/librarything-mac-screensaver_11.php">LibraryThing Mac Screensaver</a> and of course wanted the same thing for my Ubuntu workstation at $work. Naturally, I'm supposed to be working on some high-priority tickets on a tight deadline...so I started to work right away on how to do this. Your tax dollars at work, etc...</p>

<p>I'm sure that there's a much more elegant way of doing this, but I basically created a simple python program <a href="http://web.archive.org/web/20101216223621/http://inkdroid.org/bzr/bin/extract-images">extract-images</a> that will pull image urls out of arbitrary text, suck down the images, and dump them to a directory. This can be combined with cron and the standard GLSlideshow screensaver, which displays a slideshow of images in a particular directory.</p>

<p>So you just download <a href="http://web.archive.org/web/20101216223621/http://inkdroid.org/bzr/bin/extract-images">extract-images</a>, put it in your path, add a crontab entry like (substituting edsu for your LibraryThing username):</p>

<pre>
00 14 * * * extract-images http://www.librarything.com/labs-screensaver.php?userid=edsu /home/ed/Pictures/covers
</pre>

<p>And then tell GLSlideshow where your images are by adding this to your ~/.xscreensaver</p>

<pre>
imageDirectory:   /home/ed/Pictures/covers
chooseRandomImages:   True
</pre>

<p><a href="http://inkdroid.org/images/screensaver-screenshot.png"><img src="http://inkdroid.org/images/screensaver-screenshot-small.png" width="450" /></a></p>

<p>Dear $manager, it really didn't take me that long to do this. Honest!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>858</wp:post_id>
		<wp:post_date>2009-03-11 11:55:20</wp:post_date>
		<wp:post_date_gmt>2009-03-11 18:55:20</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>librarything-ubuntu-screen-saver</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="images"><![CDATA[images]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="librarything"><![CDATA[librarything]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="ubuntu"><![CDATA[ubuntu]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"81374";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81374</wp:comment_id>
			<wp:comment_author><![CDATA[David Brunton]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawkSlg8D1WWGgYFINCTBA1lRQZY4vmBFUyU</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.195</wp:comment_author_IP>
			<wp:comment_date>2009-03-11 12:05:55</wp:comment_date>
			<wp:comment_date_gmt>2009-03-11 19:05:55</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Dear $techlead, the fact that I happened to be on google reader when this post came through should not be noted too closely either ;)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>65</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>VocabularySoup (1)</title>
		<link>http://inkdroid.org/2009/03/26/vocabularysoup-1/</link>
		<pubDate>Thu, 26 Mar 2009 13:20:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=923</guid>
		<description></description>
		<content:encoded><![CDATA[<p>It's been great to see <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a> being picked up by web2.0 publishers like Digg and MySpace.  You can use the <a href="http://www.w3.org/2007/08/pyRdfa/">RDFa Distiller</a> to extract the RDFa from a given web page <em>u</em> by constructing a URI like:</p>

<pre>
http://www.w3.org/2007/08/pyRdfa/extract?format=turtle&uri=<em>u</em>
</pre>

<p>Which translates kind of nicely into a command line utility to add to your ~/bin:</p>

<pre>
#!/bin/sh
curl "http://www.w3.org/2007/08/pyRdfa/extract?format=turtle&uri=$1"
</pre>

<p>So with that little shell script in hand I can now look at the RDFa something like Yo La Tengo's page on MySpace:</p>

<pre>
ed@rorty:~$ rdfa http://www.myspace.com/yolatengo

@prefix myspace: &lt;http://x.myspacecdn.com/modules/sitesearch/static/rdf/profileschema.rdf#&gt; .
@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .
@prefix xhv: &lt;http://www.w3.org/1999/xhtml/vocab#&gt; .
@prefix xml: &lt;http://www.w3.org/XML/1998/namespace&gt; .
@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .

&lt;http://www.myspace.com/YO LA TENGO&gt; a myspace:MusicProfile ;
     myspace:profileType "Music" .

&lt;http://www.myspace.com/yolatengo&gt; xhv:stylesheet
         &lt;http://x.myspacecdn.com/modules/common/static/css/global_j03fjftp.css&gt;,
         &lt;http://x.myspacecdn.com/modules/common/static/css/header/profileheader008.css&gt;,
         &lt;http://x.myspacecdn.com/modules/common/static/css/myspace_jvtnwmp4.css&gt;,
         &lt;http://x.myspacecdn.com/modules/common/static/css/profile_adl4r-y8.css&gt;,
         &lt;http://x.myspacecdn.com/modules/profiles/static/css/musicv2_wo4zzzd-.css&gt; ;
     myspace:addToFriends &lt;http://friends.myspace.com/index.cfm?fuseaction=invite.addfriend_verify&friendID=91362837&gt; ;
     myspace:friendCount "33993" ;
     myspace:headline "\"&lt;b&gt;YO LA TENGO IS MURDERING THE CLASSICS&lt;/b&gt;\""^^rdf:XMLLiteral ;
     myspace:photo &lt;http://viewmorepics.myspace.com/index.cfm?fuseaction=user.viewAlbums&friendID=91362837&gt; ;
     myspace:sendMessage &lt;http://messaging.myspace.com/index.cfm?fuseaction=mail.message&friendID=91362837&MyToken=62964687-f06b-4b8b-8227-ba97f133a029&gt; ;
     myspace:viewPictures &lt;http://viewmorepics.myspace.com/index.cfm?fuseaction=user.viewAlbums&friendID=91362837&gt; .
</pre>

<p>Today I <a href="http://lists.w3.org/Archives/Public/public-rdfa/2009Mar/0064.html">learned</a> that "the world's largest community for sharing presentations" <a href="http://slideshare.net">SlideShare</a> is now using RDFa as well. For example here is the metadata SlideShare makes available for <a href="http://derivadow.com">Tom Scott</a>'s recent presentation at CERN for the 20th birthday of the web:</p>

<pre>
ed@rorty:~$ rdfa http://www.slideshare.net/derivadow/www20-what-does-the-history-of-the-web-tell-us-about-its-future

@prefix dc: &lt;http://purl.org/dc/terms/&gt; .
@prefix hx: &lt;http://purl.org/NET/hinclude&gt; .
@prefix media: &lt;http://search.yahoo.com/searchmonkey/media/&gt; .
@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .
@prefix xhv: &lt;http://www.w3.org/1999/xhtml/vocab#&gt; .
@prefix xml: &lt;http://www.w3.org/XML/1998/namespace&gt; .
@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .

&lt;http://www.slideshare.net/derivadow/www20-what-does-the-history-of-the-web-tell-us-about-its-future&gt; dc:creator "Tom Scott"@en ;
     dc:description "Following my invitation to speak at the WWW@20 celebrations - this is my attempt to squash the interesting bits into a s"@en ;
     media:height "355"@en ;
     media:presentation &lt;http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=www20departmentatalpresentation-090325122157-phpapp02&stripped_title=www20-what-does-the-history-of-the-web-tell-us-about-its-future&gt; ;
     media:thumbnail &lt;http://cdn.slidesharecdn.com/www20departmentatalpresentation-090325122157-phpapp02-thumbnail?1238020296&gt; ;
     media:title "www@20 what does the history of the web tell us about its future?"@en ;
     media:width "425"@en ;
     xhv:alternate &lt;http://www.slideshare.net/rss/latest&gt; ;
     xhv:icon &lt;http://www.slideshare.net/favicon.ico&gt; ;
     xhv:stylesheet &lt;http://public.slidesharecdn.com/v3/styles/slideview.css?1238021672&gt; .
</pre>

<p>I guess it's nerdy but I find it really interesting to look at the vocabulary usage. You can see SlideShare is using Yahoo's media vocabulary as well as DublinCore; and MySpace has opted to create their own <a href="http://x.myspacecdn.com/modules/sitesearch/static/rdf/profileschema.rdf">vocabulary</a>. The really wonderful thing about RDF is that it allows you to reuse parts of someone else's vocabulary, in addition to creating your own, or doing both. As a technology RDF <em>encourages</em> this, as do documents like <a href="http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/#whichvocabs">How to Publish Linked Data on the Web</a> and the <a href="http://www.w3.org/2001/sw/SW-FAQ#findont">Semantic Web FAQ</a>.</p>

<p>A common perception of the 14 year <a href="http://dublincore.org">Dublin Core</a> effort is that it has largely been about coming to consensus about a set of vocabulary terms to use when describing web resources. I think it's important to recognize that the <a href="http://dublincore.org">Dublin Core</a> community has also been a role model for how to create and share your vocabulary on the web so it can be assembled, discovered, understood, used, and remixed.  More recently the <a href="http://microformats.org">Microformats</a> community has done something similar, but by targeting web developers (who are actually coding up the HTML) rather than library/infosci professionals. The real message of the Dublin Core and Microformats efforts aren't that there ought to be one vocabulary to describe information resources, but that we can use the web to collaboratively build and deploy the vocabularies we need.</p>

<p>As we see more and more metadata making it online as RDFa, LinkedData and Microformats the community really needs tool support for visualizing vocabulary use. These tools will aid data publishers in choosing what vocabularies they could use in their descriptions. They will also aid consumers, harvesters of the web to understand which vocabularies are important to understand (a.k.a write code for). How can we make this easier?</p>

<p>I guess the simplest visualization is the 'view source' feature that was built into early web browsers, and enabled the propagation of HTML--which is what my command line shell script approximates, and other plugins like <a href="https://addons.mozilla.org/en-US/firefox/addon/4106">Operator</a> and <a href="http://web.archive.org/web/20120120163224/http://rdfa.digitalbazaar.com:80/fuzz/trac/">Fuzz</a> make much more friendly. Another approach is to throw a query at an index like <a href="http://sindice.com/">Sindice</a> which indexes large swathes of linked data, rdfa and microformats, and easily click through to the "Ontologies" view for a search result that lists the vocabularies used.  Jay Luker covered some of these approaches in his <a href="http://docs.google.com/Present?docid=df2kgdvp_200cn99qqfq">Vocabularies for Linked Data: Finding, Selecting, Creating</a> presentation at code4lib last month.</p>

<p>But it would be really interesting to see more tools that detailed vocabulary usage in a more aggregated way--kind of like what Google did in 2005 for HTML in their <a href="http://code.google.com/webstats/">Web Authoring Statistics</a>. Are some people already doing this? I hope you know of something I don't.</p>

<p><em>Up next in part 2 (if I ever get the nerve to publish it) my insane ramblings about why I think XML Schema is nice, but not really web friendly enough to encourage metadata vocabulary use/reuse on the web.</em></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>923</wp:post_id>
		<wp:post_date>2009-03-26 06:20:23</wp:post_date>
		<wp:post_date_gmt>2009-03-26 13:20:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>vocabularysoup-1</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="dublincore"><![CDATA[dublincore]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="microformats"><![CDATA[microformats]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="category" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_800412348f26fb3207b1771b47cb1b35</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_d40a4c76ee3d56dccabe2073b19b5837</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>rest, the semantic web and my feeble brain</title>
		<link>http://inkdroid.org/2009/05/14/rest-the-semantic-web-and-my-feeble-brain/</link>
		<pubDate>Fri, 15 May 2009 03:29:40 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=974</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Imagine you were minting close to a million URIs for historic newspaper pages such as:</p>

<blockquote>
<a href="http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/">http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/</a>
</blockquote>

<p>for pages like:</p>

<p><a href="http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/"><img src="http://inkdroid.org/images/sn85066387_1898-01-01_ed-1_seq-1.png" width="450" border="0" /></a></p>

<p>The web page allows you to zoom in quite close and see lots of detail in the page:</p>

<p><a href="http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/"><img src="http://inkdroid.org/images/sn85066387_1898-01-01_ed-1_seq-1-b.png" width="450" border="0" /></a></p>

<p>Now lets say I want to describe this Newspaper Page in RDF. I need to decide what subject URI to hang the description off of. Should I consider this Newspaper Page resource an information resource, or a real world resource? The answer to this question determines whether or not I can hang my description of the page off the above URI, for example:</p>

<pre>
&lt;http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/&gt; 
  dcterms:issued "1898-01-01"^^&lt;http://www.w3.org/2001/XMLSchema#date&gt; .
</pre>

<p>Or if I need to mint a new URI for the page as a real world thing:</p>

<pre>
&lt;http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1#page&gt; 
  dcterms:issued "1898-01-01"^^&lt;http://www.w3.org/2001/XMLSchema#date&gt; .
</pre>

<p><a href="http://www.w3.org/TR/webarch/#id-resources">AWWW 1</a> provides some guidance:</p>

<blockquote>
By design a URI identifies one resource. We do not limit the scope of what might be a resource. The term "resource" is used in a general sense for whatever might be identified by a URI. It is conventional on the hypertext Web to describe Web pages, images, product catalogs, etc. as “resources”. The distinguishing characteristic of these resources is that all of their essential characteristics can be conveyed in a message. We identify this set as “information resources.”

This document is an example of an information resource. It consists of words and punctuation symbols and graphics and other artifacts that can be encoded, with varying degrees of fidelity, into a sequence of bits. There is nothing about the essential information content of this document that cannot in principle be transfered in a message. In the case of this document, the message payload is the representation of this document.
</blockquote>

<p>Can all of the <em>essential characteristics</em> of this newspaper page be sent down the wire as a message to a client? The text of the page is pretty legible after zooming in and you can see pictures, headlines, etc. You can't feel the texture of the page itself, but you can't in the microfilm that the page images were generated from. So I'm inclined to say yes.</p>

<p><a href="http://www.w3.org/TR/cooluris/#distinguishing">Cool URIs for the Semantic Web</a> also has some advice:</p>

<blockquote>
It is important to understand that using URIs, it is possible to identify both a thing (which may exist outside of the Web) and a Web document describing the thing. For example the person Alice is described on her homepage. Bob may not like the look of the homepage, but fancy the person Alice. So two URIs are needed, one for Alice, one for the homepage or a RDF document describing Alice. The question is where to draw the line between the case where either is possible and the case where only descriptions are available.

According to W3C guidelines ([AWWW], section 2.2.), we have a Web document (there called information resource) if all its essential characteristics can be conveyed in a message. Examples are a Web page, an image or a product catalog.

In HTTP, because a 200 response code should be sent when a Web document has been accessed, but a different setup is needed when publishing URIs that are meant to identify entities which are not Web documents.
</blockquote>

<p>This makes me think that I will need distinct identifiers for the abstract notion of the Newspaper Page, and the HTML document itself, if it is important to describe them separately. Say for example if I wanted to say the publisher of the web page was the Library of Congress, but the publisher of the Newspaper Page was Charles M. Shortridge. If I don't have distinct identifiers I will have to say:</p>

<pre>&lt;http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/&gt; 
  dc:publisher &lt;http://loc.gov&gt;, 
  &lt;http://www.joincalifornia.com/candidate/12338&gt; 
  .
</pre>

<p>Pondering this <em>Information Resource Sniff-Test</em> got me re-reading Xiaoshu Wang's paper <a href="http://web.archive.org/web/20110903110301/http://dfdf.inesc-id.pt:80/tr/web-arch">URI Identity and Web Architecture Revisited</a> again. And I've come away more convinced that maybe he's right: that the real issue lies in my vocabulary usage (dc:publisher in this example), and not with whether my URI identifies an Information Resource or not. So maybe new vocabulary is needed in order to describe the representation?</p>

<pre>&lt;http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/&gt; 
  web:repPublisher &lt;http://loc.gov&gt; ;
  dcterms:publisher &lt;http://www.joincalifornia.com/candidate/12338&gt; 
  .
</pre>

<p>But there isn't a community of practice behind Xiaoshu's position, at least not one like the Linked Data community.  Unless perhaps his position is closer to the REST community which is going strong at the moment, especially in AtomPub circles. Members of the linked-data/semweb community would most likely say that there needs to be either hash or 303'ing URIs for the Newspaper Page, distinct from the URIs for the document describing the Newspaper Page. As a late comer to the httpRange-14 debate I don't think I ever internalized how REST and the Semantic Web are slightly out of tune w/ each other regarding resources on the web.</p>

<p>So. Should I have two different URIs: one for the real-world Newspaper Page, and one for the HTML document that describes that page? Is the Newspaper Page an Information Resource? Am I muddling up something here? Am I thinking too much? Should I just let sleeping dogs lie? Your opinion, advice, therapy would be greatly appreciated.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>974</wp:post_id>
		<wp:post_date>2009-05-14 20:29:40</wp:post_date>
		<wp:post_date_gmt>2009-05-15 03:29:40</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>rest-the-semantic-web-and-my-feeble-brain</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="loc"><![CDATA[loc]]></category>
		<category domain="post_tag" nicename="newspapers"><![CDATA[newspapers]]></category>
		<category domain="post_tag" nicename="rest"><![CDATA[rest]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="uris"><![CDATA[uris]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{i:0;s:5:"81518";i:1;s:5:"81519";i:2;s:5:"81520";i:3;s:5:"81525";i:4;s:5:"81528";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81536</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-05-22 09:02:15</wp:comment_date>
			<wp:comment_date_gmt>2009-05-22 16:02:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@herbert yes, in fact we were playing around w/ ore. I just sent a <a href="http://groups.google.com/group/oai-ore/browse_thread/thread/4a71d09b6b5a6feb" rel="nofollow">message</a> to the oai-ore discussion list about the use of the oai-ore vocabulary in the linked data views at chronicling america.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81529</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.50.234.42</wp:comment_author_IP>
			<wp:comment_date>2009-05-16 05:18:30</wp:comment_date>
			<wp:comment_date_gmt>2009-05-16 12:18:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Jeff, yes ... but I thought you were talking about the link rel="alternate" type="text/plain" in the html ... which really does lead you to a text/plain representation. 

I am already aware of Topic Maps yes, but thanks for the link to your blog post. I am interested in exploring how linked data works in practice primarily at the moment.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81530</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.50.234.42</wp:comment_author_IP>
			<wp:comment_date>2009-05-16 05:26:17</wp:comment_date>
			<wp:comment_date_gmt>2009-05-16 12:26:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[John: interesting, so would it be appropriate to say Topic Maps are more in line with Xiaoshu’s point about the confusion being a vocabulary issue rather than an identity issue? Will need to review Topic Maps again.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81531</wp:comment_id>
			<wp:comment_author><![CDATA[hvdsomp]]></wp:comment_author>
			<wp:comment_author_email>hvdsomp@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>67.164.145.190</wp:comment_author_IP>
			<wp:comment_date>2009-05-19 08:46:28</wp:comment_date>
			<wp:comment_date_gmt>2009-05-19 15:46:28</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed: Regarding the vocab issue, you might be interested in this paper that was presented at LDOW2009 (where I presented on ORE):

An Ontology of Resources for Linked Data  (Harry Halpin, Valentina Presutti) - http://events.linkeddata.org/ldow2009/papers/ldow2009_paper19.pdf]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>77</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81532</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-05-19 09:10:11</wp:comment_date>
			<wp:comment_date_gmt>2009-05-19 16:10:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@hvdsomp thanks so much, I had seen your ldow2009 ore paper, but not this one on resources! 

I've been really pleased to see you and the rest of the OAI-ORE folks cross-fertilizing the digital-library/repository and linked-data/semweb crowds.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81533</wp:comment_id>
			<wp:comment_author><![CDATA[hvdsomp]]></wp:comment_author>
			<wp:comment_author_email>hvdsomp@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>67.164.145.190</wp:comment_author_IP>
			<wp:comment_date>2009-05-19 11:58:44</wp:comment_date>
			<wp:comment_date_gmt>2009-05-19 18:58:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@ed thanks for your kind words. 

regarding your above question, I was wondering whether you had actually considered an ORE approach? I see the following resources:

(*) Splash page that gives access to scanned image of newspaper - document resource
(*) Scanned image - one or more document resources depending on whether you give each format the same (conneg) or different (non conneg) URI
(*) Analog newspaper - non-document resource

The above 3 could be aggregated in an ORE Aggregation, itself a non-document resource.  Now the URI of the ORE Aggregation is the one to ship around ;-)

Anyhow, this is interesting because at a workshop a few months ago at the National Library of Sweden a related issue came up: the library there has the need to glue together the analog newspaper (issue of a day) and all its related digital products, such as the newspaper's website pages, the blog, the blog's comments, the videos, etc. In that context, ORE was also mentioned as a possible solution.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>77</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81522</wp:comment_id>
			<wp:comment_author><![CDATA[gluejar]]></wp:comment_author>
			<wp:comment_author_email>eric@hellman.net</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>173.70.68.54</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 08:46:39</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 15:46:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[inkdroid, don't worry, be happy; you work for the Library of Congress, not the US Mint. 

When I make an assertion about something digital you make available, I am the one who chooses the URI to use. I can be guided by your example- but I am free to reject your guidance. If I want to make an assertion about the representation you deliver distinct from the RWO, it's up to me to be clear what my subject is, and it's not your job to anticipate all my identifier needs.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>156</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81521</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 08:13:07</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 15:13:07</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Jeff, yeah you have a point. If there were a URI for the page it should content-negotiate to all the representations. I still think it's debatable based on the language in the AWWW  whether this is a RWO or not though. Not probably worthy of much debate though in the long run.

On the subject of the text/plain representation ... why do you say <a href="http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/ocr.txt" rel="nofollow">http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/ocr.txt</a> isn't text/plain?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81520</wp:comment_id>
			<wp:comment_author><![CDATA[Daniel Bennett]]></wp:comment_author>
			<wp:comment_author_email>daniel@citizencontact.com</wp:comment_author_email>
			<wp:comment_author_url>http://citizencontact.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>216.15.46.2</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 08:11:47</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 15:11:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I am still confused by all of the server side machinations for using URI for describing parts of XML documents of any sort. I was under the impression that anchors and ids within a document would allow for finding a portion of a document. XLINKS and especially XPointer provides fine grained exposure of an XML document. Then extra features, either server or client side, based on those standards can be built, allowing for gentle degrading to the link to the whole document. Fewer HTTP error messages, I assume, easier caching especially when supported with static documents that do not depend on server side processing and ...

And the result is still a URI (e.g. 
http://example.com/document1#part1_subsec5 )

Daniel Bennett]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>154</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81528</wp:comment_id>
			<wp:comment_author><![CDATA[Jeff Young]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://realworldobject.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>99.186.54.21</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 19:08:22</wp:comment_date>
			<wp:comment_date_gmt>2009-05-16 02:08:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed,

Look closer at the "View: Text" link at http://chroniclingamerica.loc.gov/lccn/sn85066387/1898-01-01/ed-1/seq-1/. It leads to a text/html representation.

If you plan to buy into the Linked Data principles, you should be beware that Topic Maps seem to have a competing philosophy. This blog entry may help you decide which URIs are appropriate for use in the RDF you produce: http://q6.oclc.org/2009/05/linked_datahttp.html.

Jeff]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>153</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81525</wp:comment_id>
			<wp:comment_author><![CDATA[jenitennison.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.jenitennison.com/</wp:comment_author_url>
			<wp:comment_author_IP>86.0.73.64</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 12:33:57</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 19:33:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I can tell you what we did in a similar situation with the London Gazette. We gave the notices (and issues, and editions) identifier URLs which 303 to an abstract document URL, which content negotiate to a number of different representation URLs.

I think that there is a difference between "page 1 of edition 1 of The Call dated 1st Jan 1898" and "a web page that provides information about page 1 of edition 1 of The Call dated 1st Jan 1898". The two items have different publishers and creation dates, for example. Therefore I'd give them separate URIs. If someone requests "page 1 of edition 1 of The Call dated 1st Jan 1898" you redirect them to the "web page about ...".]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>157</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81524</wp:comment_id>
			<wp:comment_author><![CDATA[johnwcowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>72.14.228.89</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 12:24:18</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 19:24:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Topic Maps gets this right: it distinguishes *in the links* between  the use of URIs as subject indicators and their use as information resources.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>44</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81523</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 11:53:17</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 18:53:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[gluejar, that makes me rest a bit easier...

But still: if I am publishing RDF assertions about the things I publish then I do need to think about the identifiers I use eh?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81519</wp:comment_id>
			<wp:comment_author><![CDATA[Jeff Young]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://realworldobject.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.40</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 07:02:59</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 14:02:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Note that this example indicates there are 2 text/html (one supposedly plain text but not really), 1 PDF, and 1 JP2 representation available. Even though these are separate resources with different URIs, they are still representations of some one thing. This something is the Real World Object and it deserves a URI of its own.Ideally, in this case, the RWO URI will do a 303 redirect to a Generic Document from which clients can negotiate for the representation of their choice. Some of the triples you refer to such as the publisher of the newspaper need to use this RWO URI as the subject which may be different from the publisher of the representation.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>153</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81518</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>96.234.239.79</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 06:10:58</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 13:10:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I have to say, and I guess I've said before, that I've always found http-range14 and the "real world" vs "information" thing to be somewhat confusing, and a pretty hacky "solution" that seems to make everything consistent but really just confuses as much as it answers. 

I think you should do whatever seems to be simplest and easiest while still supporting all the use cases you can think of fairly well. It seems like you have several options that support most of your use cases fairly well; in which case to me it comes down to whatever is simplest, easiest to implement, easiest to understand. Rather than whatever is most abstractly theoretically sound according to httpRange-14. 

We're all working this stuff out in practice, right?  So you'll try something, and people will learn from it.  Now, it would still be unfortunate if you tried something without understand what was going on and what other people were trying -- but I really believe this isn't a done deal answered question, smart intelligent reasonable people can disagree on the best way to do it. I've never completely bought into to the httpRange-14 stuff -- now, perhaps that's cause I still don't completely understand it, but as we frequently say critisizing overly complex library standards, if someone who's motivated and reasonably clever can't understand something even spending some time trying, that doens't say good things about it's eventual widespread adoption.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81887</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &rsaquo; web documents and axioms for linked data]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2010/02/22/web-documents-and-axioms-for-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>109.74.197.162</wp:comment_author_IP>
			<wp:comment_date>2010-02-22 19:06:53</wp:comment_date>
			<wp:comment_date_gmt>2010-02-23 02:06:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] email exchange with Richard Cyganiak (one of the architects of the Linked Data pattern) about some trouble I&#8217;ve had understanding what Information Resources and Documents are in the context of [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>canonical question</title>
		<link>http://inkdroid.org/2009/05/15/canonical-question/</link>
		<pubDate>Fri, 15 May 2009 18:50:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1015</guid>
		<description></description>
		<content:encoded><![CDATA[As the last post indicated I'm part of a team at loc.gov working on an <a href="http://chroniclingamerica.loc.gov">application</a> that serves up page views like <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1901-08-25/ed-1/seq-15/">this</a> for historic newspapers--almost a million of them in fact. For each page view there is another URL for a view of the OCR text gleaned from that image, such as <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1901-08-25/ed-1/seq-15/ocr/">this</a>. Yeah, kind of yuckster at the moment, but we're working on that.

Perhaps it's obvious, but the goal of making the OCR html view available is so that search engine crawlers can come and index it. Then when someone is searching for someone's name, say <a href="http://www.google.com/search?q=Dr.+Herbert+D.+Burnham">Dr. Herbert D. Burnham</a> in Google they'll come to <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1901-08-25/ed-1/seq-15/">page 3</a> in the <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1901-08-25/ed-1/">08/25/1901 issue</a> of the <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/">New York Tribune</a>. And this can happen without the searcher needing to know anything about the Chronicling America project beforehand. Classic <a href="http://en.wikipedia.org/wiki/Search_engine_optimization">SEO</a>...

The current OCR view at the moment is quite confusing, so we wanted to tell Google that when they link to the page in their search results they use the page zoom view instead. We reached for Google's (and now other major search engine's) <a href="http://googlewebmastercentral.blogspot.com/2009/02/specify-your-canonical.html">rel="canonical"</a>, since it seemed like a perfect fit. 

<blockquote>
... we now support a format that allows you to publicly specify your preferred version of a URL. If your site has identical or vastly similar content that's accessible through multiple URLs, this format provides you with more control over the URL returned in search results. It also helps to make sure that properties such as link popularity are consolidated to your preferred version.
</blockquote>

From our logs we can see that Google has indeed come and fetched both the page viewer and the ocr view for this particular page, and also the text/plain and application/xml views.

<pre>
66.249.71.166 - - [05/May/2009:23:31:51 -0400] "GET /lccn/sn83030214/1901-08-25/ed-1/seq-15/ HTTP/1.1" 200 15566 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" "*/*"
66.249.71.227 - - [06/May/2009:02:02:51 -0400] "GET /lccn/sn83030214/1901-08-25/ed-1/seq-15.pdf HTTP/1.1" 200 3119248 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" "*/*"
66.249.71.165 - - [06/May/2009:02:03:46 -0400] "GET /lccn/sn83030214/1901-08-25/ed-1/seq-15/ocr/ HTTP/1.1" 200 47075 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" "*/*"
66.249.71.172 - - [06/May/2009:04:34:02 -0400] "GET /lccn/sn83030214/1901-08-25/ed-1/seq-15/ocr.txt HTTP/1.1" 200 40300 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" "*/*"
66.249.71.202 - - [06/May/2009:04:36:07 -0400] "GET /lccn/sn83030214/1901-08-25/ed-1/seq-15/ocr.xml HTTP/1.1" 200 1447056 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" "*/*"
</pre>

But it doesn't look like the ocr html view is being indexed, at least based on the results for <a href="http://www.google.com/search?q=Dr.+Herbert+D.+Burnham">this</a> query I just ran. We can see the .txt file is showing up, which was harvested just after the OCR html view ... so it really ought to be in the search results.

A bit of text in a recent www2009 paper <a href="http://data.semanticweb.org/conference/www/2009/paper/100/html">Sitemaps: Above and Beyond the Crawl of Duty</a> by Uri Schonfeld and Narayanan Shivakumar made me think ...

<blockquote>
Amazon.com also suffers from URL canonicalization issues, multiple URLs reference identical or similar content. For example, our (Google's) Discovery crawl crawls both
<ul>
<li>http://.../B0000FEFEFW?showViewpoints=1</li>
<li>http://.../B0000FEFEFW?filterBy=addFiveStar</li>
</ul>
The two URLs return identical content and offer little value since these pages off two "different" views on an empty customer review list. Simple crawlers cannot detect these type of duplicate URLs without downloading all duplicate URLs first, processing their content, and <em>wasting resources in the process</em>.
</blockquote>

So. Could it be that google crawls and indexes <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1901-08-25/ed-1/seq-15/">http://chroniclingamerica.loc.gov/lccn/sn83030214/1901-08-25/ed-1/seq-15/</a>, where it discovers <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1901-08-25/ed-1/seq-15/ocr/">http://chroniclingamerica.loc.gov/lccn/sn83030214/1901-08-25/ed-1/seq-15/ocr/</a> which it crawls and sees the canonical URI, which it knows it has already indexed, so it doesn't waste any resources re-indexing?

It seems like a somewhat non-obvious (to me) side effect of asserting a canonical relationship with another URI is that Google will not index the document at the alternate URI. I guess I'm just learning to only use canonical when a site has "identical or vastly similar content that's accessible through multiple URLs" ... Does this seem about right to you?

(thanks <a href="http://dret.net/netdret/">Erik Wilde</a> for the pointer to the Schonfeld and Shivakuma paper) ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1015</wp:post_id>
		<wp:post_date>2009-05-15 11:50:23</wp:post_date>
		<wp:post_date_gmt>2009-05-15 18:50:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>canonical-question</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="crawlers"><![CDATA[crawlers]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="search"><![CDATA[search]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"81526";i:1;s:5:"81534";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81535</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-05-22 08:34:38</wp:comment_date>
			<wp:comment_date_gmt>2009-05-22 15:34:38</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[erik, funny you should mention that -- it's high on the list of features the stakeholders would like added, so it's likely to go in soon.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81534</wp:comment_id>
			<wp:comment_author><![CDATA[claimid.com/egh]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://claimid.com/egh</wp:comment_author_url>
			<wp:comment_author_IP>69.106.231.4</wp:comment_author_IP>
			<wp:comment_date>2009-05-20 22:45:32</wp:comment_date>
			<wp:comment_date_gmt>2009-05-21 05:45:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Can't help with the rel=canonical issue, just wanted to say thanks for making sure that google indexes your content. It's nice to see something like this being made available to ordinary searchers.

This might be a place where that annoying "feature" of pulling the search term out of the referer and highlighting it on the page might be useful. I clicked the link to the NY Trib but I can't find Dr Burnham.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>165</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81527</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 13:42:10</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 20:42:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yes, thanks Jeni -- two good options there. Sometimes the obvious isn't so obvious when you are working with a legacy website...so your advice is much appreciated.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>81526</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81526</wp:comment_id>
			<wp:comment_author><![CDATA[jenitennison.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.jenitennison.com/</wp:comment_author_url>
			<wp:comment_author_IP>86.0.73.64</wp:comment_author_IP>
			<wp:comment_date>2009-05-15 12:45:02</wp:comment_date>
			<wp:comment_date_gmt>2009-05-15 19:45:02</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I think you'd be hard pressed to get Google to index a page but have any search results that include that page actually point to a different page. Perhaps you'd be better off doing a bit of content negotiation and serving up the OCRed content when Google (or another 'bot) requests the page viewer URL? Or, of course, including the OCRed content within the page viewer page.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>157</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>American Memory is (almost) 20</title>
		<link>http://inkdroid.org/2009/07/08/american-memory-is-almost-20/</link>
		<pubDate>Wed, 08 Jul 2009 14:00:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1050</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://memory.loc.gov"><img src="http://inkdroid.org/images/ammem.png" style="margin-right: 10px; float: left;" /></a>Through an internal discussion list at the Library of Congress I learned that this year will mark the 20th Anniversary of <a href="http://memory.loc.gov">American Memory</a>. The exact date of the anniversary depends on how you want to mark it: either the beginning of FY90 on October 1st, <strike>1999</strike> 1989 (thanks <a href="http://davidbrunton.com">David</a>) when work officially began, or earlier in the year when the President signed the bill that included the Legislative Branch appropriations for that year (exact date yet to be determined).

Via the discussion list I was able to learn that Shirley Liang (with the help of Nancy Eichacker) was able to locate a <a href="http://inkdroid.org/data/american_memory_hearings.pdf">transcript of the hearings</a>, which includes the details of Carl Fleischhauer's demo of a <a href="http://en.wikipedia.org/wiki/HyperCard">Hypercard</a> / <a href="http://en.wikipedia.org/wiki/Laser_disk">Laser Video Disc</a> based system before the House and later the Senate. Yes, <em>HyperCard</em>. LoC was making a pitch for American Memory before Congress just a few months after Tim Berners-Lee made his <a href="http://www.w3.org/History/1989/proposal.html">proposal</a> to build a "web of notes with links" at <a href="http://public.web.cern.ch/public/">CERN</a>. Incidentally, I learned recently in <a href="http://twitter.com/fuzheado">Andrew Lih</a>'s <a href="http://www.librarything.com/work.php?book=46197761">Wikipedia Revolution</a>, that <a href="http://en.wikipedia.org/wiki/Ward_Cunningham">Ward Cunningham</a>'s first implementation of <a href="http://en.wikipedia.org/wiki/Wiki">Wiki</a> was written using Hypercard.

I digress...and I want to digress more.

As a Library School student in the mid 90s  I became a big fan of American Memory. It seemed like an audacious and exciting experiment right on the cutting edge of what the World Wide Web made (and continues to make) possible. The work that Caroline Arms and Carl Fleischhauer did to expose metadata about American Memory collections (with the technical expertise of <a href="http://twitter.com/davewoodward">Dave Woodward</a>) deepened my interest in what LoC was doing. In hindsight, I think seeing this work from afar is what got me interested in trying to find a job at the Library of Congress.

Seeing that American Memory is turning 20 this year made me fess up to a crazy idea of writing a history of the project. In conversation with those much more knowledgeable than me I think I've convinced myself that a good place to start would be compiling a bibliography of things that have been written about the project. It seems a relatively simple and logical place to start. 
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1050</wp:post_id>
		<wp:post_date>2009-07-08 07:00:19</wp:post_date>
		<wp:post_date_gmt>2009-07-08 14:00:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>american-memory-is-almost-20</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="american-memory"><![CDATA[american memory]]></category>
		<category domain="post_tag" nicename="congress"><![CDATA[congress]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[history]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="loc"><![CDATA[loc]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>flickr, digital curation and the web</title>
		<link>http://inkdroid.org/2009/07/09/flickr-digital-curation-and-the-web/</link>
		<pubDate>Thu, 09 Jul 2009 14:26:25 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1072</guid>
		<description></description>
		<content:encoded><![CDATA[The Library of Congress has started to put selected content from <a href="http://chroniclingamerica.loc.gov">Chronicling America</a> into <a href="http://flickr.com">Flickr</a> as part of the <a href="http://www.flickr.com/photos/library_of_congress/sets/72157619452486566/">Illustrated Newspaper Supplements</a> set. More details on the rationale and process involved can be found in a <a href="http://www.loc.gov/rr/news/flickr/flickrFAQ.html">FAQ</a> on the LC Newspapers and Current Periodical Reading Room website. 

So for example this newspaper page on Chronicling America:

<a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-12-26/ed-1/seq-13/"><img src="http://inkdroid.org/images/chronam_flickr_2.png" style="width: 500px; border: none;" /></a>

Is also available on Flickr:

<a href="http://www.flickr.com/photos/library_of_congress/3608741612"><img src="http://inkdroid.org/images/chronam_flickr_1.png" style="width: 500px; border: none;" /></a>




I haven't written about it here yet, but Chronicling America is just a regular vanilla Django/MySQL/Apache webapp which <a href="http://lists.w3.org/Archives/Public/public-lod/2009May/0301.html">exposes machine readable metadata</a> for the newspaper content using the <a href="http://www.w3.org/DesignIssues/LinkedData.html">Linked Data</a> pattern. It just so happens that <a href="http://twitter.com/davewoodward">Dave</a> was able to use these linked data views to determine the metadata to use when uploading content to Flickr. For example if a curator wants to have <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-12-26/ed-1/seq-13/">this newspaper page</a> uploaded to Flickr,  Dave's flickr uploading program is able to use the associated <a href="http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-12-26/ed-1/seq-13.rdf">metadata</a> (referenced in a link element inthe HTML) to get the newspaper title, issue, date, and other related metadata. The beauty of this was Dave was able to do the technical work on his own, and it didn't require any formal project coordination.

A few weeks ago, on learning of the Flickr / Chronicling America integration <a href="http://www.csc.liv.ac.uk/~azaroth/">Rob Sanderson</a> asked if we could possibly reference the Flickr content in our own data views. Rob became interested in Chronicling America because of its use of the Open Archives Initiative <a href="http://www.openarchives.org/ore/1.0/vocabulary.html">Object Reuse and Exchange Vocabulary</a> in the linked data views. Rob has written a rather nice oai-ore Greasemonkey visualization tool called <a href="http://foresite.cheshire3.org/explorer/foresite-explorer.user.js">foresite-explorer</a>, which can visualize oai-ore links to Flickr. It also makes sense from a curatorial perspective to want to capture these bi-directional links between Chronicling America and Flickr in the Chronicling America database.

After agreeing with Rob I've had it on my plate to get the list of Flickr URLs and their associated Chronicling America URLs from Dave, for loading into Chronicling America so that the links can be served up in the data views, and perhaps maybe in the HTML. But yesterday morning I had the realization that I didn't really need to ask (and keep asking every month) Dave for the list. Since Dave created a Flickr set for these pages, and has embedded the URI for the Chronicling America page as a machine tag I can get it right from Flickr. So I hacked together a quick <a href="http://gist.github.com/142889">script</a>, and now I have the list too.

<pre>
ed@rorty:~$ ./flickr_chronam.py
(u'http://www.flickr.com/photos/library_of_congress/3608399458', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-01-03/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608400834', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-01-10/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608402104', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-01-17/ed-1/seq-16/')
(u'http://www.flickr.com/photos/library_of_congress/3608403362', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-01-24/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607588861', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-01-31/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608405718', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-02-07/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608407068', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-02-14/ed-1/seq-16/')
(u'http://www.flickr.com/photos/library_of_congress/3608408274', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-02-21/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607593693', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-02-28/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608410606', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-03-07/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607596267', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-03-14/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607597927', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-03-21/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608414374', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-03-28/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608415708', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-04-04/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607601559', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-04-11/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608418042', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-04-18/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608419060', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-04-25/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607604705', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-05-02/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608421240', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-05-09/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608422694', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-05-16/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607608459', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-05-23/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608425436', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-05-30/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607611709', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-06-06/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607637819', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-06-13/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607638897', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-06-20/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608455948', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-06-27/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607641409', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-07-04/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607642551', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-07-11/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607889205', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-07-18/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608709982', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-07-25/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607894517', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-08-01/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607896027', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-08-08/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608713826', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-08-15/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608715804', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-08-22/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3607900561', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-08-29/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608718394', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-09-05/ed-1/seq-15/')
(u'http://www.flickr.com/photos/library_of_congress/3608719874', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-09-12/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608721302', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-09-19/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607906387', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-09-26/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608724542', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-10-03/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607909093', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-10-10/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607910739', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-10-17/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608728408', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-10-24/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607913989', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-10-31/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607915481', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-11-07/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607916497', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-11-14/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608734354', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-11-21/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3607919583', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-11-28/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608737124', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-12-05/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608738658', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-12-12/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608740242', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-12-19/ed-1/seq-17/')
(u'http://www.flickr.com/photos/library_of_congress/3608741612', u'http://chroniclingamerica.loc.gov/lccn/sn83030214/1909-12-26/ed-1/seq-13/')
</pre>

The point of this blog post is that Dave, Rob and I were able to independently work on tools for pushing the same repository content around without having to talk much at all. The World Wide Web, and URIs in particular, enables this sort of integration. I guess some people would argue that this is a Web2.0 Mashup, and I guess they would be right in a way. But really it is using the Web the way it was designed to be used--as a decentralized information space. Flickr can tell me about the Chronicling America content that's on Flickr; and Chronicling America can tell other people about the repository objects themselves.

Now I just need to make those links available for Rob in the data views :-)

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1072</wp:post_id>
		<wp:post_date>2009-07-09 07:26:25</wp:post_date>
		<wp:post_date_gmt>2009-07-09 14:26:25</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>flickr-digital-curation-and-the-web</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="flickr"><![CDATA[flickr]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81615</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Keane's Miscellanea &middot; Take Two]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blogs.law.harvard.edu/pkeane/2010/01/03/take-two/</wp:comment_author_url>
			<wp:comment_author_IP>128.103.64.114</wp:comment_author_IP>
			<wp:comment_date>2010-01-04 00:29:06</wp:comment_date>
			<wp:comment_date_gmt>2010-01-04 07:29:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Why authorities are not available as web resources is mind-boggling to me.  Oh wait, they are!  Ed Summers et. al. at the Library of Congress are at the forefront of this whole approach I am talking about.  Keep an eye on their work for ideas/inspiration. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85048</wp:comment_id>
			<wp:comment_author><![CDATA[Social Meta-data collection &laquo; The Journeyler]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://hugh.thejourneyler.org/social-meta-data-collection</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-07-02 16:42:51</wp:comment_date>
			<wp:comment_date_gmt>2011-07-02 23:42:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] model has been implemented by the Library of Congress in the Chronicling America project. Where the Library of Congress is putting images out on Flickr and the public is annotating [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1309787558.2358";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1309650171.9449";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>New York Times Topics as SKOS</title>
		<link>http://inkdroid.org/2009/08/18/new-york-times-topics-as-skos/</link>
		<pubDate>Wed, 19 Aug 2009 04:46:59 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1094</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Serves 23,376 SKOS Concepts</em></p>

<p><strong>INGREDIENTS</strong></p>

<ul>
        <li>Text editor: <a href="http://www.vim.org/">Vim</a>, <a href="http://www.gnu.org/software/emacs/">Emacs</a>, <a href="http://macromates.com/">TextMate</a>, etc</li>
    <li><a href="http://python.org">Python</a></li>
    <li><a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a></li>
    <li><a href="http://rdflib.net">rdflib</a></li>
        <li>Internet connection</li>
</ul>

<p><strong>DIRECTIONS</strong></p>

<ol>
        <li>Open a new file using your favorite text editor.</li>
        <li>Instantiate an RDF graph with a dash of rdflib.</li>
        <li>Use python's urllib to extract the HTML for each of the Times Topics Index Pages, e.g. for <a href="http://topics.nytimes.com/top/reference/timestopics/all/a">A</a>.</li>
    <li>Parse HTML into a fine, queryable data structure using BeautifulSoup.</li>
        <li>Locate topic names and their associated URLs, and gently add them to the graph with a pinch of SKOS.</li>
        <li>Go back to step 3 to fetch the next batch of topics, until you've finished <a href="http://topics.nytimes.com/top/reference/timestopics/all/z">Z</a>.</li>
        <li>Bake the RDF graph as an rdf/xml file.</li>
</ol>

<p><strong>NOTES</strong></p>

<p>If you don't feel like cooking up the rdf/xml yourself you can download it from <a href="http://web.archive.org/web/20101216220314/http://inkdroid.org/bzr/timestopics/timestopics.rdf">here</a> (might want to right-click to download, some browsers might have trouble rendering the xml), or download the 68 line <a href="http://web.archive.org/web/20101216220252/http://inkdroid.org/bzr/timestopics/timestopics.py">implementation</a> and run it yourself.</p>

<p>The point of this exercise was mainly to show how thinking of the New York Times Topics as a <a href="http://en.wikipedia.org/wiki/Controlled_vocabulary">controlled vocabulary</a>, that can be serialized as a file, and still present on the Web, could be useful. Perhaps to someone writing an application that needs to integrate with the New York Times and who want to be able to tag content using the same controlled vocabulary. Or perhaps someone wants to be able to link your own content with similar content at the New York Times. These are all use cases for expressing the Topics as SKOS, and being able to ship it around with resolvable identifiers for the concepts.</p>

<p>Of course there is one slight wrinkle. Take a look at this <a href="http://en.wikipedia.org/wiki/Turtle_(syntax)">Turtle</a> snippet for the concept of Ray Bradbury:</p>

<pre>
@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@Prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; .

&lt;http://topics.nytimes.com/top/reference/timestopics/people/b/ray_bradbury#concept&gt; a skos:Concept;
    skos:prefLabel "Bradbury, Ray";
    skos:broader &lt;http://topics.nytimes.com/top/reference/timestopics/people#concept&gt;;
    skos:inScheme &lt;http://topics.nytimes.com/top/reference/timestopics#conceptScheme&gt; 
    .
</pre>

<p>Notice the URI being used for the concept?</p>

<pre>
http://topics.nytimes.com/top/reference/timestopics/people/b/ray_bradbury#concept</pre>

<p>The wrinkle is that there's no way to get RDF back from this URI currently. But since NYT is already using XHTML, it wouldn't be hard to sprinkle in some RDFa such that:</p>

<pre lang="html">
<html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:skos="http://www.w3.org/2004/02/skos/core#">
...
<h1 about="http://topics.nytimes.com/top/reference/timestopics/people/b/ray_bradbury#concept" property="skos:prefLabel">Ray Bradbury</h1>
...
</html>
</pre>

<p>And <em>voila</em> you've got <a href="http://www.w3.org/DesignIssues/LinkedData.html">Linked Data</a>. I took the 5 minutes to mark up the HTML myself and put it <a href="http://inkdroid.org/data/bradbury.html">here</a> which you can run through the <a href="http://www.w3.org/2007/08/pyRdfa/">RDFa Distiller</a> to get some <a href="http://www.w3.org/2007/08/pyRdfa/extract?uri=http%3A%2F%2Finkdroid.org%2Fdata%2Fbradbury.html&format=turtle&warnings=false&parser=lax&space-preserve=true&submit=Go%21&text=">Turtle</a>. Of course if the NYT ever decided to alter their HTML to provide this markup this recipe would be simplified greatly: no more error prone scraping, the assertions could be pulled directly out of the HTML.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1094</wp:post_id>
		<wp:post_date>2009-08-18 21:46:59</wp:post_date>
		<wp:post_date_gmt>2009-08-19 04:46:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>new-york-times-topics-as-skos</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="beautifulsoup"><![CDATA[beautifulsoup]]></category>
		<category domain="post_tag" nicename="newspapers"><![CDATA[newspapers]]></category>
		<category domain="post_tag" nicename="nytimes"><![CDATA[nytimes]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="post_tag" nicename="rdflib"><![CDATA[rdflib]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="skos"><![CDATA[skos]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{i:0;s:5:"81543";i:1;s:5:"81545";i:2;s:5:"81566";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_651937bebc83c5986a63d84b3867bc43</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_a820dffae1f775c85a0c9327dbddbe62</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81543</wp:comment_id>
			<wp:comment_author><![CDATA[Ryan Shaw]]></wp:comment_author>
			<wp:comment_author_email>ryan.shaw@stanfordalumni.org</wp:comment_author_email>
			<wp:comment_author_url>http://people.ischool.berkeley.edu/~ryanshaw/wordpress/bio</wp:comment_author_url>
			<wp:comment_author_IP>128.32.226.133</wp:comment_author_IP>
			<wp:comment_date>2009-08-19 13:42:09</wp:comment_date>
			<wp:comment_date_gmt>2009-08-19 20:42:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[http://open.blogs.nytimes.com/2009/06/26/nyt-to-release-thesaurus-and-enter-linked-data-cloud/]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>242</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81544</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.0.59</wp:comment_author_IP>
			<wp:comment_date>2009-08-19 15:32:56</wp:comment_date>
			<wp:comment_date_gmt>2009-08-19 22:32:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks Ryan, I did see this announcement come out of SemTech a few months ago. It is great news, both in its potential and its enthusiasm. The point of my post was that it is achievable right now, with minimal technical effort ... and there might be a small first step they could take into the Linked Data arena, while they work on the big idea.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81545</wp:comment_id>
			<wp:comment_author><![CDATA[Karl Dubost]]></wp:comment_author>
			<wp:comment_author_email>karl@la-grange.net</wp:comment_author_email>
			<wp:comment_author_url>http://karl.dubost.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>70.81.32.202</wp:comment_author_IP>
			<wp:comment_date>2009-08-26 05:03:39</wp:comment_date>
			<wp:comment_date_gmt>2009-08-26 12:03:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[A long time ago (in 2004), Aaron had explored this topic using RDF and SVG for plotting relationships of NY Times See the work at http://aaronland.info/nytimes/]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>259</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81566</wp:comment_id>
			<wp:comment_author><![CDATA[Uche Ogbuji]]></wp:comment_author>
			<wp:comment_author_email>uche@ogbuji.net</wp:comment_author_email>
			<wp:comment_author_url>http://uche.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>70.90.116.203</wp:comment_author_IP>
			<wp:comment_date>2009-11-28 13:35:46</wp:comment_date>
			<wp:comment_date_gmt>2009-11-28 20:35:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Came across this and decided to spend 10 minutes porting it to Amara 2.x http://web.archive.org/web/20100620115354/http://wiki.xml3k.org:80/Amara2 - nice cut in LOC), and updating to changed site structure.  Purely a demo.  I know NYT have provided a better way now :)</p>

<p>http://bitbucket.org/uche/scatter-share/src/tip/demos/akara/amara/nyt2skos.py</p>

<p>http://web.archive.org/web/20100609160203/http://wiki.xml3k.org:80/Amara2/Recipes</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>332</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83264</wp:comment_id>
			<wp:comment_author><![CDATA[Internet Alchemy &raquo; links for 2009-09-14]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.iandavis.com/2009/09/links-for-2009-09-14</wp:comment_author_url>
			<wp:comment_author_IP>209.20.92.98</wp:comment_author_IP>
			<wp:comment_date>2010-09-21 17:58:09</wp:comment_date>
			<wp:comment_date_gmt>2010-09-22 00:58:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid » Blog Archive » New York Times Topics as SKOS (tags: skos rdf linkeddata python web howto semantic semweb) [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85387</wp:comment_id>
			<wp:comment_author><![CDATA[links for 2009-09-14 &laquo; Internet Alchemy]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.iandavis.com/2009/09/14/links-for-2009-09-14/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-08 11:28:25</wp:comment_date>
			<wp:comment_date_gmt>2012-05-08 18:28:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid &raquo; Blog Archive &raquo; New York Times Topics as SKOS (tags: skos rdf linkeddata python web howto semantic semweb) [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336501705.7601";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336503525.9508";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>open to view</title>
		<link>http://inkdroid.org/2009/08/13/open-to-view/</link>
		<pubDate>Thu, 13 Aug 2009 15:54:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1103</guid>
		<description></description>
		<content:encoded><![CDATA[I spent an hour checking out the <a href="http://www.hathitrust.org/data_api">HathiTrust API docs</a> this morning; mainly to see what the similarities and differences are with the as-of-yet undocumented API for <a href="http://chroniclingamerica.loc.gov">Chronicling America</a>. There are quite a few similarities in the general RESTful approach, and the use of Atom, METS and PREMIS in the metadata that is made available. 

Everyone's a critic right? Nevertheless, I'm just going to jot down a few thoughts about the API, mainly for my friend over in <a href="irc://chat.freenode.net/code4lib">#code4lib</a> <a href="http://billdueber.com/">Bill Dueber</a> who works on the project. Let me just say at the outset that I think it's awesome that HathiTrust are providing this API, especially given some of the licensing constraints around some of the content. The API is a good example of putting library data on the web using both general and special purpose standards. But there are a few minor things that could be tweaked I think, to make the API fit into the web and the repository space a bit better.

it would be nice if the <a href="http://opensearch.org">OpenSearch</a> description document referenced in the <a href="http://catalog.hathitrust.org">HTML</a> at 

<blockquote>
<a href="http://catalog.hathitrust.org/Search/OpenSearch?method=describe ">http://catalog.hathitrust.org/Search/OpenSearch?method=describe</a>
</blockquote>

worked. It should be pretty easy and non-invasive to add a basic description file for the HTML response since the search is already GET driven. Ideally it would be nice to see the responses also available as Atom and/or JSON with <a href="http://tools.ietf.org/html/rfc5005">Atom Feed Paging</a>. 

Another thing that would be nice to see is the API being merged more into the human usable webapp. The best way to explain this is with an example. Consider the HTML page for this 1914 edition of Walt Whitman's <a href="http://catalog.hathitrust.org/Record/00020629">Leaves of Grass</a>, available with this clean URI:

<blockquote>
<a href="http://catalog.hathitrust.org/Record/000206297">http://catalog.hathitrust.org/Record/000206297</a>
</blockquote>

Now, you can get a <a href="http://services.hathitrust.org/api/htd/meta/mdp.39015056032132">few</a> <a href="http://services.hathitrust.org/api/htd/structure/mdp.39015056032132">flavors</a> of metadata for this book, and an aggregated <a href="https://services.hathitrust.org/api/htd/aggregate/mdp.39015056032132">zip file</a> of all the page images and OCR if you are a HathiTrust member. Why not make these alternate representations discoverable right from the item display? It could be as simple as adding some &lt;link&gt; elements to the HTML, that use the link relations they've already established for their Atom:

<pre lang="html">
<head>
<link rel="http://schemas.hathitrust.org/htd/2009#meta" 
    type="application/atom+xml" 
    href="http://services.hathitrust.org/api/htd/meta/mdp.39015056032132" />
<link rel="http://schemas.hathitrust.org/htd/2009#structure " 
    type="application/atom+xml" 
    href="http://services.hathitrust.org/api/htd/structure/mdp.39015056032132" />
<link rel="http://schemas.hathitrust.org/htd/2009#aggregate" 
    type="application/zip" 
    href="https://services.hathitrust.org/api/htd/aggregate/mdp.39015056032132" />
</head>
</pre>

If you wanted to get fancy you could also put human readable links into the &lt;body&gt; and annotate them w/ <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a>. But this would just be icing on the cake. There are a few reasons for doing at least the bare minimum. The big one is to enable in browser applications (like <a href="http://zotero.org">Zotero</a>, etc) to be able to learn more about a given resource in a relatively straightforward and commonplace way. The other big one is to let automated agents like <a href="http://www.google.com/bot.html">GoogleBot</a> and <a href="http://help.yahoo.com/help/us/ysearch/slurp">YahooSlurp</a> and Internet Archive's <a href="http://crawler.archive.org/">Heritrix</a>, etc. discover the <a href="http://en.wikipedia.org/wiki/Deep_Web">deep web</a> data that's held behind your API. Another nice side effect is that it helps people who might ordinarily scrape your site automatically discover the API in a straightforward way.

Lastly, I was curious to know if HathiTrust considered adjusting their Atom response to use the <a href="http://www.openarchives.org/ore/1.0/atom.html">Atom pattern</a> recommended by the OAI-ORE folks. They are pretty close already, and in fact seem to have modeled their own aggregation vocabulary on OAI-ORE. It would be interesting to hear why they diverged if it was intentional, and if it might be possible to use a bit of oai-ore in there so we can bootstrap an oai-ore harvesting ecosystem.

I'm <a href="http://iandavis.com/blog/2009/07/the-linked-data-brand">not sure</a> that I can still call this approach to integrating web2.0 APIs into web1.x applications <em>Linked Data</em> anymore, since it doesn't really involve RDF directly. It does  involve thinking in a RESTful way about the resources you are publishing on the web, and how they can be linked together to form a graph. My colleague <a href="http://onebiglibrary.net">Dan</a> has been writing in Computers in Libraries recently about how perhaps thinking in terms of "building a better web" may be a more accurate way of describing this activity. 

For reasons I don't fully understand I've been reading a lot of Wittgenstein (well mainly books about Wittgenstein honestly) lately during the non-bike commute. The trajectory of his thought over his life is really interesting to me. He had this zen-like, controversial idea that 

<blockquote>
Philosophy simply puts everything before us, nor deduces anything. — Since everything lies open to view there is nothing to explain. For what is hidden, for example, is of no interest to us. <a href="http://books.google.com/books?id=ici7FXQZsFIC&lpg=PP1&dq=philosophical%20investigations&pg=PA43-IA1#v=onepage&q=126&f=false">(PI 126)</a>
</blockquote>

I really like this idea that our data APIs on the web could be "open to view" by checking out the HTML, following your nose, and writing scrapers, bots and browser plugins to use what you find. I think it's unfortunate that the recent changes to the <a href="http://www.w3.org/DesignIssues/LinkedData.html">Linked Data Design Issues</a>, and the ensuing <a href="http://cloudofdata.com/2009/07/does-linked-data-need-rdf/">discussion</a> seemed to create this dividing line about the use of RDF and SPARQL. I had always hoped (and continue to hope) that the Linked Data effort is bigger than a particular brand, or reformulation of the semantic web effort ... for me it's a pattern for building a better web. I think RDF is very well suited to expressing the core nature of the web, the <a href="http://dig.csail.mit.edu/breadcrumbs/node/215">Giant Global Graph</a>. I've served up RDF representations in applications I've worked on just for this reason. But I think Linked Data pattern will thrive most if it is thought of as an inclusive continuum of efforts, similar to what <a href="http://webofdata.wordpress.com/2009/07/20/what-else/#comment-132">Dan Brickley</a> has suggested. Us technology people strive for explicitness, it's an occupational hazard -- but there's sometimes quite a bit of strength in ambiguity.

Anyhow, my little review of the HathiTrust API turned into a bit of a soapbox for me to stand on and shout like a lunatic. I guess I've been wanting to write about what I think Linked Data is for a few weeks now, and it just kinda bubbled up when I least expected it. Sorry Bill!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1103</wp:post_id>
		<wp:post_date>2009-08-13 08:54:41</wp:post_date>
		<wp:post_date_gmt>2009-08-13 15:54:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>open-to-view</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="atom"><![CDATA[atom]]></category>
		<category domain="post_tag" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="hathitrust"><![CDATA[hathitrust]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="opensearch"><![CDATA[opensearch]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="rest"><![CDATA[rest]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:4:{i:0;s:5:"81538";i:1;s:5:"81540";i:2;s:5:"81541";i:3;s:5:"81542";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81541</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>69.251.254.56</wp:comment_author_IP>
			<wp:comment_date>2009-08-18 13:21:03</wp:comment_date>
			<wp:comment_date_gmt>2009-08-18 20:21:03</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[It's definitely a 'better web app', but it's better in a particular way it may be useful to be descriptive of. Better in that the data and services are more easily automatically discoverable by a machine, as they are exposed in a standard(ish) way.  

Accessible Data?  An Exposed Web App?  A web app which exposes it's data in standard ways?

I am not neccesarily myself convinced that RDF is always the 'best' way to do this -- at least not within a certain real world project where benefits and costs have to be balanced within externally set limits. But actually, I'm not even convinced that RDF is always the easiest for consumers to deal with either. Although I guess in an ideal world, we'd always have RDF _plus_ something else that might be easier for a consumer in a given context -- because I think RDF is an experiment well worth engaging in, although I'm less convinced of the ultimate success of this experiment than some Linked Data boosters.  But, back in the real world...  we don't always have the resources to do things ideally. The important thing to me is exposing your data, not how you do it.  Exposed Data?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81538</wp:comment_id>
			<wp:comment_author><![CDATA[dchud]]></wp:comment_author>
			<wp:comment_author_email>dchud@umich.edu</wp:comment_author_email>
			<wp:comment_author_url>http://onebiglibrary.net/</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-08-13 14:37:46</wp:comment_date>
			<wp:comment_date_gmt>2009-08-13 21:37:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm sure my employers at InfoToday would prefer me to point out that I'm writing for Computers in Libraries, not that other rag. :)  -Dan]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>66</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81539</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.0.59</wp:comment_author_IP>
			<wp:comment_date>2009-08-13 16:49:51</wp:comment_date>
			<wp:comment_date_gmt>2009-08-13 23:49:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[d'oh -- thanks dan :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81540</wp:comment_id>
			<wp:comment_author><![CDATA[Paul Miller]]></wp:comment_author>
			<wp:comment_author_email>paul.miller@cloudofdata.com</wp:comment_author_email>
			<wp:comment_author_url>http://paulmiller.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>213.249.187.213</wp:comment_author_IP>
			<wp:comment_date>2009-08-14 02:54:19</wp:comment_date>
			<wp:comment_date_gmt>2009-08-14 09:54:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA["I had always hoped (and continue to hope) that the Linked Data effort is bigger than a particular brand, or reformulation of the semantic web effort … for me it’s a pattern for building a better web."

I would, of course, have to agree. I am convinced that RDF is almost certainly the 'best' way to do this... but it is definitely not the 'only' way. To suggest otherwise is unnecessary, unwise, and - ultimately - unhelpful.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>233</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81542</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>69.251.254.56</wp:comment_author_IP>
			<wp:comment_date>2009-08-18 13:23:40</wp:comment_date>
			<wp:comment_date_gmt>2009-08-18 20:23:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Exhibitionist Data? heh.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81696</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &rsaquo; data.gov.uk and rdfa]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2010/01/26/data-gov-uk-and-rdfa/</wp:comment_author_url>
			<wp:comment_author_IP>109.74.197.162</wp:comment_author_IP>
			<wp:comment_date>2010-01-26 18:53:14</wp:comment_date>
			<wp:comment_date_gmt>2010-01-27 01:53:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] comforting about being able to crawl the open web to find the information that&#8217;s there in open to view.   This was written by ed. Posted on Tuesday, January 26, 2010, at 3:41 pm. Filed under publishing, [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85388</wp:comment_id>
			<wp:comment_author><![CDATA[links for 2009-08-23 &laquo; Internet Alchemy]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.iandavis.com/2009/08/23/links-for-2009-08-23/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-08 11:58:24</wp:comment_date>
			<wp:comment_date_gmt>2012-05-08 18:58:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid &raquo; Blog Archive &raquo; open to view (tags: linkeddata apis hathitrust) [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336503504.4585";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336503872.3949";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Documents</title>
		<link>http://inkdroid.org/2009/09/10/documents/</link>
		<pubDate>Fri, 11 Sep 2009 03:32:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1172</guid>
		<description></description>
		<content:encoded><![CDATA[<div xmlns:dct="http://purl.org/dc/terms/" xmlns:foaf="http://xmlns.com/foaf/0.1/" xmlns:bibo="http://purl.org/ontology/bibo/">
<a about="/images/otlet.jpg" rel="foaf:depicts" href="http://chroniclingamerica.loc.gov/lccn/sn84026749/1908-04-09/ed-1/seq-11#page">
<img src="/images/otlet.jpg" style="float: right; margin-left: 15px; width: 200px;" />
</a>
I've <a href="http://inkdroid.org/2009/05/14/rest-the-semantic-web-and-my-feeble-brain/">struggled</a> in the past with what constitutes an <em>Information Resource</em> in the context of <a href="http://www.w3.org/TR/webarch/#id-resources">Web Architecture</a>, <a href="http://www.w3.org/DesignIssues/LinkedData.html">Linked Data</a> and practical digital library applications such as the <a href="http://chroniclingamerica.loc.gov">National Digital Newspaper Project</a> I work on at the Library of Congress. So it was reassuring to see the issue come up a few months ago during a <a href="http://lists.w3.org/Archives/Public/www-tag/2009Jun/0056.html">review</a> of the effort to <a href="http://www.ietf.org/dyn/wg/charter/httpbis-charter.html">revise</a> the HTTP specification (<a href="http://www.w3.org/Protocols/rfc2616/rfc2616.html">RFC 2616</a>). It would be a major effort to summarize the entire conversation here. However an interesting sub-discussion circled around the idea of normalizing the language in the Architecture of the World Wide Web and RFC 2616  with respect to <em>Resources</em>.

Well into the multi-month thread Tim Berners-Lee offered up a very helpful, historical <a href="http://lists.w3.org/Archives/Public/www-tag/2009Aug/0000.html">recap</a> of the "what is a resource" issue , in which he said:

<blockquote about="#quote1" typeof="bibo:Quote">
<p id="quote1" property="bibo:content">I would like to see what the documents [AWWW and RFC 2616] all look like if edited to use the words Document and Thing, and eliminate Resource.</p>
<cite><a rel="dct:source" href="http://www.w3.org/DesignIssues/TermResource.html">A Short History of "Resource"</a></cite>

</blockquote>

Which, somewhat predictably, started a discussion of what a <em>Document</em> is. However this conversation seemed more tangible and earthy, and culminated in <a href="http://larry.masinter.net/">Larry Masinter</a> <a href="http://lists.w3.org/Archives/Public/www-tag/2009Aug/0010.html">recommending</a> David M. Levy's book <a href="http://openlibrary.org/b/OL3947422M/Scrolling_forward">Scrolling Forward</a>:

<blockquote about="#quote2" typeof="bibo:Quote">
<p id="quote2" property="bibo:content">... since much of the thought behind it informs a lot of my own thinking about the nature of "Document", "representation",  "Resource" and the like.</p>
<cite><a rel="dct:source" href="http://lists.w3.org/Archives/Public/www-tag/2009Aug/0010.html">www-tag email message</a></cite>
</blockquote>

Now Larry is a scientist at <a href="http://adobe.com">Adobe</a>, a company that knows a thing or two about electronic documents. He also works closely with the W3C and IETF on web architectural issues. So when he suggested reading a book to learn what he means by <em>Document</em> my ears perked up. The interjection of a book reference into this rapid-fire email exchange was like a magic spell, that made me pause, and consider that a working definition of <em>Document</em> was nuanced enough to be the subject matter of an entire book. 

<a href="http://openlibrary.org/b/OL3947422M">
<img src="http://covers.openlibrary.org/b/olid/OL3947422M-M.jpg" rel="foaf:depicts" resource="http://openlibrary.org/b/OL3947422M" style="float: left; margin-right: 15px; border: none;" /></a>

I've come to expect references to Michael Buckland's classic <a href="http://people.ischool.berkeley.edu/~buckland/whatdoc.html">What is a Document?</a> in discussions of documents. I hadn't run across David Levy's name before so Larry's recommendation was enough for me to request it from the stacks, and give it a read. I wasn't disappointed. Scrolling Forward is an ode to documents of all shapes and sizes, from all time periods. It's a joyful, mind expanding work, that explores the entire landscape of our documents: from cash register receipts, the multi-editioned Leaves of Grass, email messages, letters, books, photographs, papyrus scrolls, greeting cards and web pages. Since this takes place in 212 pages, it is not surprising that the analysis <!--more-->synthesizes rather than being exhaustive. Having received a doctorate in computer science from Stanford, obtained a diploma in calligraphy and bookbinding from the Roehampton Institute, and then worked at Xerox PARC studying the nature of documents for 15 years, Levy's own professional career is marked by a bringing together of scientific and humanistic disciplines. 

One of the key messages of the book is a working definition of the Document. Levy's draws out his definition largely in contrast to a statement made by David Weinberger in his 1996 Wired piece  <a href="http://www.wired.com/wired/archive/4.08/document.html">What's a Document?</a> where he says: 

<blockquote about="#quote3" typeof="bibo:Quote">
<p id="quote3" property="bibo:content">The fact that we can't even say what a document is anymore indicates the profundity of the change we are undergoing in how we interact with information and, ultimately, our world.</p>
<cite><a rel="dct:source" href="http://www.wired.com/wired/archive/4.08/document.html">What is a Document?</a></cite>
</blockquote>

To which Levy responds:

<blockquote about="#quote4">
<p id="quote4" property="bibo:content">We <strong>can</strong> say what a document is. Doing this, however, requires a somewhat different approach from that which dictionaries take. It requires going beyond word usage. It does require looking at the relevant technologies, but in such a way that we aren't fixated on them, that we don't fetishize them. Most of all, it requires immersing ourselves in the social roles these technologies play.</p>
<cite><a href="http://openlibrary.org/b/OL3947422M" rel="dct:source">Scrolling Forward</a> p. <span property="bibo:pages">23</span></cite> 
</blockquote>

So Scrolling Forward is a survey of sorts; a survey of document types that are inextricably linked to the social contexts in which they were created. This approach to <em>describing</em> rather than positing a theory of documents dove-tailed nicely with some reading of Wittgenstein I've been doing recently. In Wittgenstein's later period he eschewed positing philosophical theories, but instead attempted to resolve philosophical problems by exploring the richness of language and its use in social settings, or <em>language games</em>, to lay bare the problem in a therapeutic way. Levy takes a similar approach in simply laying out the complex, sometimes contradictory history of documents before us, instead of carving out a logical argument and selecting facts to support it.

Some parts of the book that were of particular interest to me (as a software developer working in the area of digital preservation) were the sections discussing document fixity:

<blockquote about="#quote5" typeof="bibo:Quote">
<p property="bibo:content">... paper documents, and indeed all documents are static <em>and</em> changing, fixed <em>and</em> fluid. There is a reason why text and graphics editors have a Save button, after all.</p>
<cite><a href="http://openlibrary.org/b/OL3947422M" rel="dct:source">Scrolling Forward</a> p. <span property="bibo:pages">36</span></cite> 
</blockquote>

Also of interest was Levy's analysis of why the idea of "digital libraries" is such a lightning rod of opinion (which perhaps applies to its sister concept "repositories").

<blockquote about="#quote6">
<p id="quote6" property="bibo:content">[The] ambiguity between institution and collection is carried through in the phrase "digital library". For some groups, most notably librarians, the phase refers most directly to institutions that oversee digital collections, while for other professionals, primarily computer and information scientists, it refers to digital collections, without regard to the institutional settings (if any) in which they might be managed ... Digital library, it seems to me, draws much of its power from this ambiguity: it provides a name for collections of digital materials that invokes the aura of the modern library and its social mission (library as social institution). But it does so without actually making any commitments to the public good (library as collection).</p>
<cite><a href="http://openlibrary.org/b/OL3947422M" rel="dct:source">Scrolling Forward</a> p. <span bibo:pages">135</span></cite> 
</blockquote>

And finally, Levy doesn't shy away from the big questions of how our psychological and religious impulses influence our notions of what documents are. 

<blockquote about="#quote7" typeof="bibo:Quote">
<p id="quote7" property="bibo:content">The human search for and construction of order [...] is our response to the profound mystery, and accompanying anxiety, of existence. Emerging into an unfathomable universe and fearing we are nothing within it, we strive to create a meaningful and ultimately immortal place for ourselves [...] Culture creates the conditions for a meaningful existence, for us to play out our games of physical and symbolic survival. But it is an ongoing performance, a play we can never stop performing, lest we see the back-stage gears and levers and be reminded of the mysterious and terrifying backdrop against which we are performing it. [Documents] are death-transcending, lack-filling artifacts of major proportions. Perhaps they can't literally prevent our physical demise or fill our deepest sense of lack. But they are the central participants in our attempts to do so. Every one of them -- each cash register receipt, each greeting card, each Post-it note -- makes a contribution to the collaborative edifice we call human culture. Although few carry the weight of the Bible or the Constitution, all of them inform us of "what is and what we should do". And in concert they help us create and sustain an orderly, and meaningful human lifeworld.</p>
<cite><a href="http://openlibrary.org/b/OL3947422M" rel="dct:source">Scrolling Forward</a> pp. <span property="bibo:pages">187-188</span></cite> 
</blockquote>

Heady stuff to be sure. And now I feel like I've traveled far from the beginning of this blog post, and the definition of information resources and the semantic web. Scrolling Forward has given me a very personal perspective on what documents are, and have been--and as a result I'm a bit more hopeful about the future of electronic documents. Working in digital preservation, it's sometimes pretty easy to give in to despair. I'm not sure what the the application of this perspective is towards the normalization of language in the Architecture of the World Wide Web and RFC 2616. But it seems certain that part of the answer lies in not taking our information technologies too seriously, and trying to stay focused on the roles that they play in our individual and collective lives:

    <blockquote about="#quote8" typeof="bibo:Quote">
        <p id="quote8" property="bibo:content">We make a mistake, I believe, when we fixate on particular forms and technologies, taking them, in and of themselves, to be the carriers of what we want either to embrace or resist. Not only do we fail to see the forms and technologies in their full complexity, but we use them, in their symbolic simplicity, as blunt instruments with which to beat one another over the head.</p>
        <cite><a rel="dct:source" href="http://openlibrary.org/b/OL3947422M">Scrolling Forward</a> p. <span property="bibo:pages">198</span></cite>

</blockquote></div>

PS. The bibliography is a great source of new material to read too.
PSS. This blog post was also a not-so-secret experiment in using <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a> and the<a href="http://bibliontology.com/"> Bibliographic Ontology</a> to mark up quotations. Check out the <a href="http://www.w3.org/2007/08/pyRdfa/extract?format=turtle&uri=http://inkdroid.org/2009/09/10/documents/">rdf assertions</a> you can extract from it using the <a href="http://www.w3.org/2007/08/pyRdfa/">RDFa Distiller</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1172</wp:post_id>
		<wp:post_date>2009-09-10 20:32:16</wp:post_date>
		<wp:post_date_gmt>2009-09-11 03:32:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>documents</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="documents"><![CDATA[documents]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[history]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="life"><![CDATA[life]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"81546";i:1;s:5:"81547";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81546</wp:comment_id>
			<wp:comment_author><![CDATA[troncy.myopenid.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://troncy.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>193.55.113.196</wp:comment_author_IP>
			<wp:comment_date>2009-09-11 00:31:02</wp:comment_date>
			<wp:comment_date_gmt>2009-09-11 07:31:02</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great post Ed!

Relevant to this post, I would also complement the bibliography with a long term work made a by a group of scientists in France under the surname Roger T. Pedauque. In particular, http://archivesic.ccsd.cnrs.fr/docs/00/06/22/28/PDF/sic_00000594.pdf is a MUST read.
Cheers.

  Raphaël]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>287</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81547</wp:comment_id>
			<wp:comment_author><![CDATA[Musebrarian]]></wp:comment_author>
			<wp:comment_author_email>musebrarian@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://musebrarian.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>98.222.56.37</wp:comment_author_IP>
			<wp:comment_date>2009-09-17 06:57:47</wp:comment_date>
			<wp:comment_date_gmt>2009-09-17 13:57:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed,

I'd also recommend taking a look at the work of my colleague Allen Renear who has been struggling with these questions for a long time, along with other scholars in the digital humanities.  A list of his publications is here:  http://people.lis.illinois.edu/~renear/renearcv.html

I'd recommend "What Is Text, Really?" (http://doi.acm.org/10.1145/264842.264843)   and "Three of the Four FRBR Group 1 Entity Types are Roles not Type" (http://is.gd/3nC0T) - which is really about how XML "documents" fit into FRBR.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>305</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81548</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-09-17 08:33:22</wp:comment_date>
			<wp:comment_date_gmt>2009-09-17 15:33:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@musebrarian funny you should mention that. I ran across @allenrenear and <strike>@sshreeves</strike> @karenwickett <a href="http://www.balisage.net/Proceedings/vol3/html/Renear01/BalisageVol3-Renear01.html" rel="nofollow">Documents Cannot Be Edited</a> while I was reading Scrolling Forward. Thanks for the reminder that I need to dig back into the references in the bibliography, some of which you mentioned. 

Part of the appeal, for me, of Levy's approach is that he really isn't trying to express a theory of documents, which can then be acted upon using logic (presumably by a computer) ... His goal instead is just to explore just how rich and shifting the Document landscape is, to get a sense of the intuitive semantics of "Document". I guess it's almost a quantitative/qualitative thing. We really do need both, and it was interesting that @allenrenear and <strike>@sshreeves</strike> @karenwickett paper ends with:

<blockquote>
One has to wonder whether all this logic-chopping subtlety is going to be worth it. It isn't clear exactly how to finish the job, and yet it is clear that some of our most familiar -- and effective -- ways of conceptualizing our domain will revised if we continue along this path. Particularly troublesome is the prospect that the revisions anticipated will add not only complexity in design and use, but increase computational complexity as well.
</blockquote>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81549</wp:comment_id>
			<wp:comment_author><![CDATA[KarenWickett]]></wp:comment_author>
			<wp:comment_author_email>karen.wickett@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>74.98.14.57</wp:comment_author_IP>
			<wp:comment_date>2009-09-17 10:30:46</wp:comment_date>
			<wp:comment_date_gmt>2009-09-17 17:30:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed,

Nice post, on a classic issue for information systems.  Just a note, I'm not sure @sshreeves would want to be associated with our logic-chopping extremism.  But I'm quite flattered to be confused with her at any rate.

I agree entirely that we need both approaches if we want the semantic web to do the things we hope it will do.  We were trying to draw attention to the unintuitive consequences of a simplistic approach based on mathematical notions.  But developing a fully correct ontology has it's own challenges.

-Karen.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>307</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81550</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.0.59</wp:comment_author_IP>
			<wp:comment_date>2009-09-17 18:21:45</wp:comment_date>
			<wp:comment_date_gmt>2009-09-18 01:21:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@karenwickett whoops! sorry about that ... I think I've mentally confused @sshreeves w/ you for almost a year now for some odd reason I don't quite understand. Thanks for taking the time to get me sorted out on this :-) And also for the comments about your paper. ]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85386</wp:comment_id>
			<wp:comment_author><![CDATA[links for 2009-09-11 &laquo; Internet Alchemy]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.iandavis.com/2009/09/11/links-for-2009-09-11/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-08 11:07:20</wp:comment_date>
			<wp:comment_date_gmt>2012-05-08 18:07:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid &raquo; Blog Archive &raquo; Documents (tags: documents) [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336500440.5432";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336503545.8197";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Think of Things</title>
		<link>http://inkdroid.org/2009/09/16/think-of-things/</link>
		<pubDate>Thu, 17 Sep 2009 02:32:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1245</guid>
		<description></description>
		<content:encoded><![CDATA[<div xmlns:bibo="http://purl.org/ontology/bibo/" xmlns:dct="http://purl.org/dc/terms/" xmlns:foaf="http://xmlns.com/foaf/0.1/">
<a about="http://covers.openlibrary.org/b/olid/OL619435M-M.jpg" rel="foaf:depicts" href="http://openlibrary.org/b/OL619435M"><img src="http://covers.openlibrary.org/b/olid/OL619435M-M.jpg" style="float: left; margin-right: 15px;"/></a>
<blockquote about="#thingQuote" typeof="bibo:Quote">
<p id="thingQuote" property="bibo:content">
Pooh began to feel a little more comfortable, because when you are a Bear of Very Little Brain, and you Think of Things, you find sometimes that a Thing which seemed very Thingish inside you is quite different when it gets out into the open and has other people looking at it.
</p>
<cite><a rel="dct:source" href="http://openlibrary.org/b/OL619435M">The Complete Tales of Winnie-the-Pooh</a></cite> p. <span property="bibo:pages">266</span>
</blockquote>
</div>

<br />
<br />
<br />

This page sent through the <a href="http://www.w3.org/2007/08/pyRdfa/extract?format=xml&uri=http://inkdroid.org/2009/09/16/think-of-things/">RDFa Distiller</a> yields some quotation data marked up with the <a href="http://bibliontology.com/">Bibliographic Ontology</a>, <a href="http://dublincore.org/documents/dcmi-terms/">Dublin Core</a> and <a href="http://xmlns.com/foaf/spec/">FOAF</a> vocabularies:

<pre>
@prefix bibo: &lt;http://purl.org/ontology/bibo/&gt; .
@prefix dct: &lt;http://purl.org/dc/terms/&gt; .
@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .

&lt;http://covers.openlibrary.org/b/olid/OL619435M-M.jpg&gt; foaf:depicts &lt;http://openlibrary.org/b/OL619435M&gt; .

&lt;http://inkdroid.org/2009/09/16/think-of-things/#thingQuote&gt; a bibo:Quote ;
     dct:source &lt;http://openlibrary.org/b/OL619435M&gt; ;  
     bibo:content """Pooh began to feel a little more comfortable, because when you are a Bear of Very Little Brain, and you Think of Things, you find sometimes that a Thing which seemed very Thingish inside you is quite different when it gets out into the open and has other people looking at it."""@en ;
     bibo:pages "266"@en .
</pre>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1245</wp:post_id>
		<wp:post_date>2009-09-16 19:32:00</wp:post_date>
		<wp:post_date_gmt>2009-09-17 02:32:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>think-of-things</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="pooh"><![CDATA[pooh]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>oai-pmh and xmpp</title>
		<link>http://inkdroid.org/2009/09/23/oai-pmh-and-xmpp/</link>
		<pubDate>Thu, 24 Sep 2009 04:14:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1262</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://xmpp.org"><img src="http://xmpp.org/images/xmpp.png" style="border: none; margin-right: 15px; float: left;"/></a>As an experiment to learn more about <a href="http://xmpp.org">xmpp</a> I created a little utility that will poll an <a href="http://www.openarchives.org/OAI/openarchivesprotocol.html">oai-pmh</a> server and send new records as a chunk of xml over xmpp.  The idea wasn't necessarily to see all the xml coming into my jabber client (although you can do that). I wanted  to enable downstream applications to have records pushed to them, instead of them having to constantly poll for updates.  So you could write a client that archived away metadata and potentially articles as they are found, or write a current awareness tool that listened for articles that matched a particular users research profile, etc...</p>

<p>Here's how you start it up:</p>

<pre>
oai2xmpp.py http://www.doaj.org/oai.article from@example.com to@example.org
</pre>

<p>which would poll <a href="http://doaj.org">Directory of Open Access Journals</a> for new articles every 10 minutes, and send them via xmpp to to@example.org. You can adjust the poll interval, and limit to records within a particular set with the --pollinterval and --set options, e.g.:</p>

<pre>
oai2xmpp.py http://export.arxiv.org/oai2 currents@jabber.org ehs@jabber.org --set cs --pollinterval 86400
</pre>

<p>It's a one file python hack in the spirit of Thom Hickey's <a href="http://web.archive.org/web/20090611032757/http://www.oclc.org:80/research/software/oai/2page.htm">2PageOAI</a> that has a few dependencies documented in the file (lxml, xmpppy, httplib2). I've run it for about a week against DOAJ and arxiv.org without incident (it does respect 503 HTTP status codes telling it to slow down). You can find it <a href="http://github.com/edsu/oai2xmpp/">here</a>.</p>

<p>If you try it out, have any improvements, or ideas let me know.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1262</wp:post_id>
		<wp:post_date>2009-09-23 21:14:57</wp:post_date>
		<wp:post_date_gmt>2009-09-24 04:14:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oai-pmh-and-xmpp</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81551</wp:comment_id>
			<wp:comment_author><![CDATA[Thom]]></wp:comment_author>
			<wp:comment_author_email>hickey@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>132.174.124.14</wp:comment_author_IP>
			<wp:comment_date>2009-09-25 11:46:18</wp:comment_date>
			<wp:comment_date_gmt>2009-09-25 18:46:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice Ed, but all that white space in the Python code makes me feel woozy.

--Th]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>80</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>ipres, iipc, pasig roundup/braindump</title>
		<link>http://inkdroid.org/2009/10/14/ipres-iipc-pasig-roundupbraindump/</link>
		<pubDate>Wed, 14 Oct 2009 21:15:08 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1273</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I spent last week in San Francisco attending 3 back-to-back conferences: the <a href="http://www.cdlib.org/iPres/">International Conference on Preservation of Digital Objects</a> (iPRES), <a href="http://web.archive.org/web/20120501044854/http://netpreserve.org/events/index.php">International Internet Preservation Consortium</a> (IIPC),  and the <a href="http://web.archive.org/web/20090830173258/http://sun-pasig.ning.com:80/events/pasig-san-francisco-oct-79">Sun Preservation and Archiving Special Interest Group</a> (PASIG)...thanks to the Library of Congress and to Kesa Summers for letting me go. Also, thanks to the 3 conferences for deciding to co-locate in San Francisco at the same time, which made this sort of tag-team-digital-preservation-event-week possible. I hadn't been to either iPRES, IIPC or PASIG before, so it was a lot of fun being able to take them all in at once...especially since given the nature of my group at the Library of Congress, these are my kind of people.</p>

<p>Each event had a different flavor, but the topic under discussion at each was digital preservation. iPRES focused generally on digital preservation, particularly from a research angle. IIPC also had a bit of a research flavor, but focused more specifically on the practicalities of archiving web content. And PASIG was less research oriented, and much more oriented around building/maintaining large scale storage systems. There was so much good content at these events, that it's kind of impossible to summarize it here. But I thought I would at least attempt to blurrily characterize some of the ideas from the three events that I'm taking back with me.</p>

<p><strong>Forever</strong></p>

<p>Long term digital preservation has many hard problems--so many that I think it is rational to feel somewhat overwhelmed and to some extent even paralyzed. It was important to see other people recognize the big problems of emulation, format characterization/migration, compression -- but continue working on pragmatic solutions, for today. Martha Anderson made the case several times for thinking of digital preservation in terms of 5-10 year windows, instead of <em>forever</em>. The phrase "to get to forever you have to get to 5 years first" got mentioned a few times, but I don't know who said it first. John Kunze brought up the notion of preservation as a "relay", where bits are passed along at short intervals--and how digital curation need to enable these hand offs to happen easily. It came to my <a href="http://twitter.com/cardcc/status/4742006590">attention</a> later that this relay idea is something that Chris Rusbridge <a href="http://www.ariadne.ac.uk/issue46/rusbridge/">written</a> about back in 2006.</p>

<p><strong>Access</strong></p>

<p>On a similar note, Martha Anderson indicated that making bits useful <em>today</em> is a key factor that the National Digital Information Infrastructure and Preservation Program (<a href="http://www.digitalpreservation.gov/">NDIIPP</a>) weighs when making funding decisions. Brewster Kahle in his keynote for IIPC struck a similar note that "preservation is driven by access".  Gary Wright gave an interesting <a href="http://lib.stanford.edu/files/pasig2009sf/pasig2009sf_familysearch_wright.pdf">presentation</a> about how the Church of Latter Day Saints had to adjust the Reference Model for Open Archival Information System (<a href="http://nost.gsfc.nasa.gov/isoas/">OAIS</a>) to enable thousands of concurrent users access to their archive of 3.1 billion genealogical image records. Jennifer Waxman was kind enough to give me a <a href="http://twitter.com/jwax55/status/4685881135">pointer</a> to some <a href="http://www.archives.gov/preservation/conferences/2009/presentations/">work</a> <a href="http://en.wikipedia.org/wiki/Paul_Conway_(professor)">Paul Conway</a> has done on this topic of access driven preservation. The topic of access in digital preservation is important to me, because I work in a digital preservation group at the Library of Congress, working primarily on access applications. We've had a series of pretty intense debates about the role of access in digital preservation ... so it was good to hear the topic come up in San Francisco. In a world where Lots of Copies Keeps Stuff Safe, access to copies is pretty important.</p>

<p><strong>Less is More (More or Less)</strong></p>

<p>Over the week I got several opportunities to hear details from John Kunze, Stephen Abrams, and Margaret Low about the California Digital Library's notion of <a href="http://www.cdlib.org/inside/diglib/">curation micro-services</a>, and how they enable digital preservation efforts at CDL. Several <a href="http://web.archive.org/web/20100218060222/http://lackoftalent.org:80/michael/blog/2009/09/27/exploring-curation-micro-services/">folk</a>s in my group at LC have been taking a close look at the CDL specifications recently, so getting to hear about the specs, and even see some implementation demos from Margaret was really quite awesome. The specs are interesting to me because they seem to be oriented around the fact that our digital objects ultimately reside on some sort of hierarchical file-system. Fileystem APIs are fairly ubiquitous. In fact, as David Rosenthal has <a href="http://vimeo.com/5407401">pointed out</a>, some file systems are even <a href="http://www.ieee.org/portal/pages/about/awards/bios/2009_Recips/2009rbjinfo_mckusick.html">designed</a> to resist change. As Kunze said at PASIG in his talk <a href="http://lib.stanford.edu/files/pasig2009sf/pasig-2009-pods.pdf">Permanent Objects, Evolving Services, and Disposable Systems: An Emergent Approach to Digital Curation Infrastructure</a></p>

<blockquote>
What is the thinnest smear of functionality that we can add to the filesystem so that it can act as an object storage system?
</blockquote>

<p>Approaches to building digital repository software thus far have been primarily aimed at software stacks (dspace, fedora, eprints) which offer particular services, or service frameworks. But the reality is that these systems come and go, and we are left with the bits. Why don't we try to get the bits in shape so that they can be handed off easily in the relay from application to application, filesystem to filesystem? What is nice about the micro-services approach is that:</p>

<ul>
<li>The services are compose-able, allowing digital curation activities to be emergent, rather than imposed by a pre-defined software architecture. Since I've been on a bit of a functional programming kick lately, I see compose-ability as a <a href="http://web.archive.org/web/20090803123913/http://www.cs.chalmers.se:80/~rjmh/Papers/whyfp.html">pretty</a> <a href="www.stanford.edu/class/cs242/readings/backus.pdf">big</a> <a href="http://www.joelonsoftware.com/items/2006/08/01.html">win</a>.</li>

<li>The services are defined by short specifications, not software--so they are ideas instead of implementations. The specifications are clearly guided by ease of implementation, but ultimately they could be implemented in a variety of languages, and tools. Having a 2-3 page spec that defines a piece of functionality, and can be read by a variety of people, and implemented by different groups seems to be an ideal situation to strive for. </li>
</ul>

<p><strong>Everything Else Is Miscellaneous</strong></p>

<p>Like I said, there was a ton of good content over the week...and it seems somewhat foolhardy to try to summarize it all in a single blog post. I tried to summarize the main themes I took home with me on the plane back to DC...but there were also lots of nuggets of ideas that came up in conversation, and in presentations that I want to at least jot down:</p>

<ul>
<li>While archival storage may not be best served by <a href="http://web.archive.org/web/20120513124017/http://hadoop.apache.org/common/docs/current/hdfs_design.html">HDFS</a>, jobs like <a href="http://web.archive.org/web/20110114005808/http://www.netpreserve.org/events/active_solutions/4_Holden_Here%20be%20Dragons.ppt">virus scanning huge web crawls</a> are well suited to distributed computing environments like <a href="http://hadoop.apache.org/">Hadoop</a>. We need to be able to operate at this scale at loc.gov.
</li><li>In Cliff Lynch's summary wrap up for PASIG he indicated that people don't talk so much about what we do when the inevitable happens, and bits are lost. The digital preservation community needs to share more statistics on bit loss, system failure modes, and software design patterns that let us build more sustainable storage systems.</li>
<li>Dave Tarrant's presentation on <a href="http://eprints.ecs.soton.ac.uk/17556/">Where the Semantic Web and Web 2.0 meet format risk management: P2 registry</a> was a welcome revelation about the intersection of my interest in linked data and digital preservation. His presentation of the <a href="http://www.nationalarchives.gov.uk/pronom/">PRONOM</a> format registry as <a href="http://web.archive.org/web/20120607071216/http://p2-registry.ecs.soton.ac.uk:80/">linked data</a>, and Kevin De Vorsey's talk about <a href="http://lib.stanford.edu/files/pasig2009sf/PASIG_2009_DeVorsey.pdf">Obsolescence, Risk Management, and Preservation Planning at the National Library of New Zealand</a> made me think that it might be interesting to explore how the LC's <a href="http://www.digitalpreservation.gov/formats/">Digital Formats</a> website could be delivered as linked data, and linked to something like PRONOM. David Pearson also <a href="http://web.archive.org/web/20120507072533/http://netpreserve.org/events/presenters.php">suggested</a> that collaborative wiki-spaces could be used by digital format specialists to collect information...which got me thinking of how a <a href="http://semantic-mediawiki.org/wiki/Semantic_MediaWiki">semantic media wiki</a> instance could be used in conjunction with Tarrant's ideas. How easy would it be to use the web to build a distributed network of preservation information, as opposed to some p2p solution?</li>
<li>I want to learn more about the (w)arc data format, and perhaps contribute to some of the existing <a href="http://code.google.com/p/warc-tools/">code</a> <a href="http://web.archive.org/web/20101123161017/http://code.google.com:80/p/search-tools/">bases</a> for working w/ (w)arc. I'm particularly interested in using harvesting tools and WARC to preserve linked data...which I believe some of the Sindice folks have worked on for their <a href="http://sindice.com/developers/bot">bot</a>.</li>
<li>It's long since time I understood how LOCKSS works as a technology. It was mentioned as the backbone of <a href="http://www.ohloh.net/p/dpp-lockss">several</a> <a href="http://www.metaarchive.org/">projects</a> <a href="http://www.clockss.org/clockss/Home">during</a> the week. I even overheard some talk about establishing rogue LOCKSS networks, which of course piqued my interest even more.</li>
<li>It would be fun to put a jython or jruby web front end on <a href="http://sourceforge.net/projects/droid/">DROID</a> for format identification, but it seems that Carol Chou of the Florida Center for Library Automation has already <a href="http://listserv.loc.gov/cgi-bin/wa?A2=ind0910&L=pig&T=0&P=55">done something similar</a>. Still, it would be neat to at least try it out, and perhaps have it conneg to Dave's <a href="http://web.archive.org/web/20120607071216/http://p2-registry.ecs.soton.ac.uk:80/">P2</a> registry or <a href="http://www.nationalarchives.gov.uk/pronom/">PRONOM</a>.
</li></ul>

<p>Ok, braindump complete. Thanks for reading this far!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1273</wp:post_id>
		<wp:post_date>2009-10-14 14:15:08</wp:post_date>
		<wp:post_date_gmt>2009-10-14 21:15:08</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>ipres-iipc-pasig-roundupbraindump</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="conferences"><![CDATA[conferences]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>hackability</title>
		<link>http://inkdroid.org/2009/11/03/hackability/</link>
		<pubDate>Tue, 03 Nov 2009 09:39:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1325</guid>
		<description></description>
		<content:encoded><![CDATA[Adam Bosworth has some good advice for would-be standards developers in the form of a <a href="http://adambosworth.net/2009/10/29/talking-to-dc/">7 item list</a>. It is strangely reassuring to know that someone in the US Federal Government called someone like Adam for advice about standards...even if it was at some inhuman hour. Number 5 really resonated with me:

<blockquote>
Always have real implementations that are actually being used as part of [the] design of any standard ... And the real implementations should be supportable by a single engineer in a few weeks.
</blockquote>

It is interesting because I think it could be argued that #1, #2, #4, #6 and #7 largely fall out from really doing #5.

<ul>
<li>Keep the standard as simple and stupid as possible.</li>
<li>The data being exchanged should be human readable and easy to understand.</li>
<li>Standards should have precise encodings.</li>
<li>Put in hysteresis for the unexpected.</li>
<li>Make the spec itself free, public on the web, and include lots of simple examples on the web site.</li>
</ul>

That leaves #3 <em>Standards work best when they are focused</em> -- which seems to be a really tricky one to get right. Maybe it comes down to being able to say <a href="http://jacobian.org/writing/the-power-of-no/">No</a> to the right things, to keep scope creep at bay. Or to be able to say:

<blockquote>
Stop It. Just Stop.
</blockquote>

to ward off complexity, like Joel Spolsky's <a href="http://www.joelonsoftware.com/items/2009/09/23.html">Duct Tape Programmer</a>. But I think #3 is really about being able to say <em>Yes</em> to the right things. The right things are the things that bind together the most people involved in the effort. Maybe it's the "rough consensus" in the <a href="http://www.ietf.org/tao.html">Tao of the IETF</a>:

<blockquote>
We reject kings, presidents and voting. We believe in <em>rough consensus</em> and running code.
</blockquote>

Whatever it is, it seems slippery and subtle -- a zen-like appreciation for what is best in people, mixed with being in the right place at the right time.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1325</wp:post_id>
		<wp:post_date>2009-11-03 02:39:04</wp:post_date>
		<wp:post_date_gmt>2009-11-03 09:39:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>hackability</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="standards"><![CDATA[standards]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81610</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Keane's Miscellanea &middot; Take Two]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blogs.law.harvard.edu/pkeane/2010/01/03/take-two/</wp:comment_author_url>
			<wp:comment_author_IP>128.103.64.114</wp:comment_author_IP>
			<wp:comment_date>2010-01-03 00:55:21</wp:comment_date>
			<wp:comment_date_gmt>2010-01-03 07:55:21</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] authorities are not available as web resources is mind-boggling to me.  Oh wait, they are!  Ed Summers et. al. at the Library of Congress are at the forefront of this whole approach I am talking about. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>alien vs predator: www-style</title>
		<link>http://inkdroid.org/2009/11/03/alien-vs-predator-www-style/</link>
		<pubDate>Tue, 03 Nov 2009 11:24:34 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1343</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://inkdroid.org/images/feed.png" style="float: left; margin: 15px; border: none;"/> I finally got around to reading <a href="http://escholarship.org/uc/item/0fv601z8">Web Services for Recovery.gov</a> by <a href="http://www.ischool.berkeley.edu/people/faculty/erikwilde">Erik Wilde</a>, <a href="http://www.ischool.berkeley.edu/people/faculty/erickansa">Eric Kansa</a> and <a href="http://www.ischool.berkeley.edu/people/faculty/raymondyee">Raymond Yee</a>. The authors wrote the report with funding from the <a href="http://www.sunlightfoundation.com/">Sunlight Foundation</a>, who are deeply engaged in improving the way the US Federal Government provides transparent access to its data assets.</p>

<p>I highly recommend giving it a read if you are interested in web services, REST, Linked Data, and simple things you can do to open up access to data. The practicality of the advice is clearly gleaned from the experience of an actual implementation over at <a href="http://web.archive.org/web/20101101232234/http://recovery.berkeley.edu:80/">recovery.berkeley.edu</a> where they kick the tires on their ideas.</p>

<p>Erik's <a href="http://dret.typepad.com/dretblog/2009/10/web-services-for-recoverygov.html">blog</a> has a succinct summary of the paper's findings, which for me boils down to:</p>

<blockquote>
any data source that is frequently updated must have simple ways for synchronizing data
</blockquote>

<p><a href="http://en.wikipedia.org/wiki/Web_syndication">Web syndication</a> is a widely deployed mechanism for presenting a list of updated web resources. The authors make a pretty strong case for <a href="http://www.ietf.org/rfc/rfc4287.txt">Atom</a> because of its pervasive use of identifiers for content, extensibility, <a href="http://www.iana.org/assignments/link-relations/link-relations.xhtml">rich linking semantics</a>, <a href="http://www.ietf.org/rfc/rfc5005.txt">paging</a>, the potential for <a href="http://www.ietf.org/rfc/rfc5023.txt">write-enabled</a> services, install base, and generally just good old Resource Oriented Architecture a.k.a. <a href="http://en.wikipedia.org/wiki/REST">REST</a>.</p>

<p>Because of my interest in Linked Data the paragraph that discusses why RDF/XML wasn't chosen as a data format is particularly interesting:</p>

<blockquote>
The approach described in this report, driven by a desire for openness and accessibility, uses the most widely established technologies and data formats to ensure that access to reporting data is as easy as possible. Recently, the idea of openly accessible data has been promoted under the term of “linked data”, with recent recommendations being centered around a very specific choice of technologies and data models (all centered around Semantic Web approaches focusing on RDF for data representation and centralized data storage). While it is possible to use these approaches for building Web applications, <span style="color: red;">our recommendation is to use better established and more widely supported technologies, thereby lowering the barrier-to-entry and choosing a simpler toolset for achieving the same goals</span> as with the more sophisticated technologies envisioned for the Semantic Web.
</blockquote>

<p>It could be argued that the growing amount of RDF/XML in the <a href="http://www4.wiwiss.fu-berlin.de/bizer/pub/lod-datasets_2009-07-14.html">Linked Data web</a> make it a contender for Atom's install base--especially when you consider RSS1.0. However I think the main point the authors are making is that the tools for working with XML documents far outnumber the tools that are available for processing RDF/XML graphs. Furthermore, most programmers I know are more familiar with the processing model and standards associated with XML documents (DOM, XSLT, XPath, XQuery) compared with RDF graphs (Triples, Directed Graph, GRDDL, SPARQL). Maybe this says more about the people I know ... and if I were to jump into the biomedical field I'd feel different. But perhaps the most subtle point is that whether or not developers know it, Atom expresses a Graph model just like RDF/XML ... but it does it in a much more straightforward, familiar document-centric way.</p>

<p>Of course the debate of whether RDF needed to be a part of Linked Data or not <a href="http://iandavis.com/blog/2009/07/the-linked-data-brand">rippled</a> through the semantic web community a few months ago--and there's little chance of resolving any of those issues here. In the pure-land of RDF model theory the choice between Atom and RDF/XML is a bit of a <a href="http://en.wikipedia.org/wiki/False_dilemma">false dilemma</a> since RDF/XML is minimally processable with, well, XML tools ... and idioms like GRDDL allow Atom to be explicitly represented as an RDF Graph. And in fact, REST and Content Negotiation would allow both serializations to co-exist nicely in the context of a single web application. However, I'd argue that this point isn't a particularly easy thing to explain, and it certainly isn't terrain that you would want to navigate in documentation on the <a href="http://recovery.gov">recovery.gov</a> website. The choice of whether RDF belongs in Linked Data or not has technical and business considerations -- but I'm increasingly seeing it as a cultural issue,that perhaps doesn't really even need resolving.</p>

<p>Even Tim Berners-Lee <a href="http://gcn.com/Articles/2009/10/30/Berners-Lee-Semantic-Web">recognizes</a> that there are quite large hurdles to modeling all government data on the Linked Data web in RDF and querying it with SPARQL. It's a bit unrealistic to expect the Federal Government to start modeling and storing their enterprise data in a fundamentally new and somewhat experimental way in order to support what amounts to arbitrary database queries from anyone on the web. If that's what the Linked Data brand is I'm not buying it.  That being said, I see a great deal of value in the RDF data model (<a href="http://dig.csail.mit.edu/breadcrumbs/node/215">the giant global graph</a>), especially as a <a href="http://www.xml.com/pub/a/2003/08/20/dive.html">tool</a> for seeing how your data fits the contours of the web.</p>

<p>The important message that Erik, Eric and Raymond's paper communicates is that the Federal Government should be focused on putting data out on the web in familiar ways, using sound web architecture practices that have allowed the web to grow and evolve into the wonderful environment it is today. Atom is a flexible, simple, commonly supported, well understood XML format for letting downstream applications know about newly published web resources. If the Federal Government is serious about the long term sustainability of efforts like recovery.gov and data.gov they should focus on enabling an ecosystem of visualization applications created by third parties, rather than trying to produce those applications themselves. I hope the <a href="http://data.gov">data.gov</a> folks also run across this important work. Thanks to Sunlight Foundation for funding the folks at Berkeley.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1343</wp:post_id>
		<wp:post_date>2009-11-03 04:24:34</wp:post_date>
		<wp:post_date_gmt>2009-11-03 11:24:34</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>alien-vs-predator-www-style</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="atom"><![CDATA[atom]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{i:0;s:5:"81554";i:1;s:5:"81555";i:2;s:5:"81567";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81552</wp:comment_id>
			<wp:comment_author><![CDATA[johnwcowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>72.14.228.129</wp:comment_author_IP>
			<wp:comment_date>2009-11-03 15:51:09</wp:comment_date>
			<wp:comment_date_gmt>2009-11-03 22:51:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[So invent an RDF serialization that is Atom-compliant.  It'll be a huge win.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>44</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81553</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.0.59</wp:comment_author_IP>
			<wp:comment_date>2009-11-04 02:02:24</wp:comment_date>
			<wp:comment_date_gmt>2009-11-04 09:02:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@johncowan "I will call him ... mini-RDF" :-) Seriously though Atom already seems to be a suitably good graph serialization for a graph of web resources doesn't it? It lets someone say here's a set of resources with URIs (atom:entry, atom:id), and here are the relationships between those resources and other resources (atom:link), and here's some core metadata about the resources (atom:title, atom:summary, etc), and here's some space to say whatever else you want about the collection of resources or the resources themselves (use of extensionElement in atom:feed and atom:entry)

There are a few proposals on the table for bringing the full expressive power of the RDF model to Atom: <a href="http://tools.ietf.org/id/draft-nottingham-atomtriples-00.txt" rel="nofollow">AtomTriples</a> from Mark Nottingham and Dave Beckett, and the work the Open Archives Initiative Object Reuse and Exchange group did on their <a href="http://www.openarchives.org/ore/1.0/atom#MetadataAR" rel="nofollow">Atom serialization</a> for graphs of web resources. I don't see people clamoring over themselves to use them however.

But (if you are being serious, heheheh I have my doubts) perhaps you see something simpler that can be done?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81554</wp:comment_id>
			<wp:comment_author><![CDATA[sgillies.net/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://sgillies.net/</wp:comment_author_url>
			<wp:comment_author_IP>147.99.64.78</wp:comment_author_IP>
			<wp:comment_date>2009-11-04 02:09:58</wp:comment_date>
			<wp:comment_date_gmt>2009-11-04 09:09:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yes. This feels like one of those situations where worse (Atom) is better.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>326</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81555</wp:comment_id>
			<wp:comment_author><![CDATA[me.yahoo.com/rossfsinger]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/rossfsinger#bc9c2</wp:comment_author_url>
			<wp:comment_author_IP>68.169.155.1</wp:comment_author_IP>
			<wp:comment_date>2009-11-06 09:01:47</wp:comment_date>
			<wp:comment_date_gmt>2009-11-06 16:01:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I think you're generally right about Atom being about as good of a compromise as you can get; it's the same basic reason I've been pushing for it for library data.

The danger is more what appears between the  tags.  Giving a 3GB blob of lots of discrete data in some arcane format a URI and saying, "here, we've made it available!  Just check out the Atom feed!" isn't really a huge improvement over just throwing them into a web-accessible directory.  It is also the sort of thing that really undermines the perception of Atom as a powerful and flexible format for making things available on the web.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>328</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81567</wp:comment_id>
			<wp:comment_author><![CDATA[danbri.org/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://danbri.org/</wp:comment_author_url>
			<wp:comment_author_IP>87.210.48.176</wp:comment_author_IP>
			<wp:comment_date>2009-12-23 01:33:38</wp:comment_date>
			<wp:comment_date_gmt>2009-12-23 08:33:38</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>See also DataRSS from Yahoo SearchMonkey folk -</p>

<p>http://web.archive.org/web/20120219171016/http://developer.yahoo.com:80/searchmonkey/smguide/datarss.html
http://web.archive.org/web/20090131231201/http://developer.yahoo.com:80/searchmonkey/smguide/understand_datarss.html</p>

<p>This uses rdfa. Are you an rdfa optimist?</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>333</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81568</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2009-12-23 08:44:25</wp:comment_date>
			<wp:comment_date_gmt>2009-12-23 15:44:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@danbri thanks for the pointer to DataRSS from Yahoo. I had seen that before but failed to make the connection between it and some things I had been thinking about at the time. Yes, I guess I'm an RDFa optimist -- but ultimately I'm an optimist about the RDF data model, and the Web :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>skos as atom</title>
		<link>http://inkdroid.org/2009/11/04/skos-as-atom/</link>
		<pubDate>Thu, 05 Nov 2009 03:39:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1398</guid>
		<description></description>
		<content:encoded><![CDATA[I'll be the first to admit the tone and content of my <a href="http://inkdroid.org/2009/11/03/alien-vs-predator-www-style/">last post</a> was a bit off kilter. I guess it was pretty clear immediately from the title of the post. Chalk it up to a second night of insomnia; and also to my unrealistic and probably unnecessary goal of bringing the Atom/REST camp in closer alignment with the RDF/LinkedData camp ... at least in my own brain if not on the web.

So, ever the pragmatist, Ian Davis called my bluff a bit on some of the crazier stuff I said:

<a href="http://twitter.com/iand/status/5434457944"><img src="http://inkdroid.org/images/iand-twitter.png" style="width: 400px;" /></a>

I know <a href="http://blogs.law.harvard.edu/pkeane/">Peter Keane</a> took a <a href="http://github.com/pkeane/lcsh-atom">stab</a> at this over the summer. But I couldn't find sample output lying around on the web, so I marked up one by hand to serve as a strawman. So here's the <a href="http://inkdroid.org/data/www.ttl">turtle</a> for the LCSH concept "World Wide Web":

<pre>
@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix dcterms: &lt;http://purl.org/dc/terms/&gt; .
@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .
@prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; .

&lt;http://id.loc.gov/authorities/sh95000541#concept&gt;
    a skos:Concept ;
    skos:prefLabel "World Wide Web"@en ;
    dcterms:modified "2001-10-01T09:56:06-04:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    skos:altLabel "W3 (World Wide Web)"@en, "WWW (World Wide Web)"@en, "Web (World Wide Web)"@en, "World Wide Web (Information retrieval system)"@en ;
    skos:broader &lt;http://id.loc.gov/authorities/sh88002671#concept&gt;, &lt;http://id.loc.gov/authorities/sh92002381#concept&gt; ;
    skos:narrower &lt;http://id.loc.gov/authorities/sh2002000569#concept&gt;, &lt;http://id.loc.gov/authorities/sh2003001415#concept&gt;, &lt;http://id.loc.gov/authorities/sh2007008317#concept&gt;, &lt;http://id.loc.gov/authorities/sh2007008319#concept&gt;, &lt;http://id.loc.gov/authorities/sh2008009697#concept&gt;, &lt;http://id.loc.gov/authorities/sh97003254#concept&gt; ;
    skos:related &lt;http://id.loc.gov/authorities/sh92002816#concept&gt; ;
    skos:closeMatch &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb13319953j&gt; .
</pre>

And here's the "corresponding" <a href="http://inkdroid.org/data/www.xml">atom</a>:

<pre>
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;entry xmlns="http://www.w3.org/2005/Atom" xmlns:skos="http://www.w3.org/2004/02/skos/core#"&gt;
    &lt;id&gt;http://id.loc.gov/authorities/sh95000541#concept&lt;/id&gt;
    &lt;title&gt;LCSH: World Wide Web&lt;/title&gt;
    &lt;author&gt;&lt;name&gt;Library of Congress&lt;/name&gt;&lt;/author&gt;
    &lt;updated&gt;2001-10-01T09:56:06Z&lt;/updated&gt;
    &lt;skos:prefLabel&gt;World Wide Web&lt;/skos:prefLabel&gt;
    &lt;skos:altLabel&gt;W3 (World Wide Web)&lt;/skos:altLabel&gt;
    &lt;skos:altLabel&gt;Web (World Wide Web)&lt;/skos:altLabel&gt;
    &lt;skos:altLabel&gt;World Wide Web (Information retrieval system)&lt;/skos:altLabel&gt;
    &lt;skos:altLabel&gt;WWW (World Wide Web)&lt;/skos:altLabel&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#broader" href="http://id.loc.gov/authorities/sh88002671#concept" title="Hypertext systems" /&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#broader" href="http://id.loc.gov/authorities/sh92002381#concept" title="Multimedia systems" /&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#narrower" href="http://id.loc.gov/authorities/sh2008009697#concept" title="Invisible web"/&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#narrower" href="http://id.loc.gov/authorities/sh2007008317#concept" title="Mashups (World Wide Web)" /&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#narrower" href="http://id.loc.gov/authorities/sh2002000569#concept" title="Semantic Web" /&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#narrower" href="http://id.loc.gov/authorities/sh2007008319#concept" title="Web 2.0" /&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#narrower" href="http://id.loc.gov/authorities/sh97003254#concept" title="WebDAV (Standard)" /&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#narrower" href="http://id.loc.gov/authorities/sh97003254#concept" title="WebTV (Trademark)" /&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#related" href="http://id.loc.gov/authorities/sh92002816#concept" title="Internet" /&gt;
    &lt;link rel="http://www.w3.org/2004/02/skos/core#closeMatch" href="http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb13319953j" title="Web" /&gt;
    &lt;link rel="alternate" href="http://id.loc.gov/authorities/sh95000541" type="text/html" /&gt;
    &lt;link rel="alternate" href="http://id.loc.gov/authorities/sh95000541.json" type="application/json" /&gt;
&lt;/entry&gt;
</pre>

Maybe I botched something? It could use a GRDDL stylesheet I suppose. At least the Atom <a href="http://validator.w3.org/feed/check.cgi?url=http%3A%2F%2Finkdroid.org%2Fdata%2Fwww.xml">validates</a>.  <a href="http://en.wikipedia.org/wiki/Tower_of_Babel"><img src="http://inkdroid.org/images/babel.jpg" style="width: 350px; float: left; margin-right: 10px;"/></a>  I really am a bit conflicted posting any of this here because there is so much about the Linked Data community that I like, and want to be a part of. But I'm finding it increasingly difficult to see a Linked Data future where RDF/XML is deployed all over. Instead I bleakly expect we'll see more fragmentation, and dueling idioms/cultures ... and I'm trying to see if perhaps things aren't as bleak as they seem by grasping at what the groups have in common. Maybe John Cowan's idea (in the <a href="http://inkdroid.org/2009/11/03/alien-vs-predator-www-style/#comments">comments</a>) of coming up with an RDF serialization that is valid Atom wasn't so bad after all? My apologies to any Linked Data folks who have helped me in the past who may have been rubbed the wrong way by my last blog post. 

<b>Update:</b> <a href="http://inamidst.com">Sean Palmer</a> clued me in to some earlier work he has done in the area of Atom and RDF, the <a href="http://inamidst.com/aefram/">Atom Extensibility Framework</a>. And Niklas Lindström let me know of some <a href="http://lists.w3.org/Archives/Public/semantic-web/2008Jul/0099.html">thinking</a> he's done on the topic that is grounded in some work he has been doing for legal information systems in Sweden.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1398</wp:post_id>
		<wp:post_date>2009-11-04 20:39:04</wp:post_date>
		<wp:post_date_gmt>2009-11-05 03:39:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>skos-as-atom</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"81564";i:1;s:5:"81565";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81564</wp:comment_id>
			<wp:comment_author><![CDATA[me.yahoo.com/a/dbdHXI4l3&hellip;]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/a/dbdHXI4l3Y7NYFmrmxReMclYyzOtdirIj2cuIqLKAg--#6201a</wp:comment_author_url>
			<wp:comment_author_IP>129.67.45.17</wp:comment_author_IP>
			<wp:comment_date>2009-11-16 04:44:27</wp:comment_date>
			<wp:comment_date_gmt>2009-11-16 11:44:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed,

I'm very interested to see this discussion of Atom. I'm currently working on a new system for managing data and metadata for studies of malaria. It's at an early stage (not much to show yet), but we've tentatively opted to use Atom as the common standard for web service APIs, and then to code the user interface in the browser (using gwt) making direct calls to the Atom API whenever possible.

FWIW we chose Atom and XML over anything based on RDF primarily because we needed a read-write API. SPARQL is great for querying data, but what we really need is to be able to CRUD arbitrary packages of data/metadata, and also define permissions over different collections of those packages ... in addition to querying the data in various ways. While Atom doesn't give you a query API, it is a good fit for the CRUD side of things.

(As an aside, we're also using eXist as the Atom implementation, which works more-or-less out of the box, and has cut down the amount of work we've had to do on the server side.)

I did take a good look at Talis' API, which has the necessary CRUD capabilities, and was very interested in it. I still think it's a great offering (the support for change sets instead of just bulk updates is potentially very useful). However, we went for Atom + eXist for two main reasons .. (1) Atom is a standard, so we had a vague hope that if eXist didn't work out we could swap to a different implementation, and (2) we're doing a lot of coding for the client-side, and there's better support for working with XML there (I just couldn't bring myself to write an rdf api for gwt then implement a gwt client for Talis' API which involves XML and RDF/XML and reification).

The main questions we've had to figure out with Atom are all to do with what best practice should be when extending Atom to convey a particular type of data entity. We use link elements to relate one entry to another, but what should we use for the rel attribute values to represent different types of relationship? Should we be using URIs? Where should we put the foreign markup that represents our data, directly in the atom:entry element, or inside an atom:content element with type application/xml? And what do you do when a client wants to pull down not just a single entry, but a graph of entries? Is it OK to include the related entries inline within the link elements?

The other challenge is how to implement query functionality over the data. I would love to use SPARQL, especially as the result set serialisation formats are very easy to work with in the browser. However, right now we're getting more mileage from writing XQuery scripts that implement simple HTTP query APIs. 

If someone designed a CRUD API for managing RDF graphs that was basically Atom, that was very convenient to use from browser code, and with an implementation that worked out of the box, things would get very interesting :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>331</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81565</wp:comment_id>
			<wp:comment_author><![CDATA[me.yahoo.com/a/dbdHXI4l3&hellip;]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/a/dbdHXI4l3Y7NYFmrmxReMclYyzOtdirIj2cuIqLKAg--#6201a</wp:comment_author_url>
			<wp:comment_author_IP>129.67.45.17</wp:comment_author_IP>
			<wp:comment_date>2009-11-16 04:45:22</wp:comment_date>
			<wp:comment_date_gmt>2009-11-16 11:45:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Sorry, I'm Alistair Miles, should have signed the previous comment - first time I've used an OpenID.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>331</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82102</wp:comment_id>
			<wp:comment_author><![CDATA[Reading List : Web, Linked Data, REST, Semantic Web &#8211; webr3.org]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20110609011418/http://webr3.org:80/blog/internet/reading-list-web-linked-data-rest-semantic-web/</wp:comment_author_url>
			<wp:comment_author_IP>74.86.25.114</wp:comment_author_IP>
			<wp:comment_date>2010-03-09 19:04:23</wp:comment_date>
			<wp:comment_date_gmt>2010-03-10 02:04:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...]  skos as atom [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>MARCetplace</title>
		<link>http://inkdroid.org/2009/11/10/marcetplace/</link>
		<pubDate>Tue, 10 Nov 2009 19:41:36 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1431</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Last Saturday I passed the time while waiting in line at the <a href="http://www.mva.maryland.gov/">DMV</a> by reading the recently <a href="http://www.loc.gov/today/pr/2009/09-219.html">released</a> <a href="http://www.loc.gov/bibliographic-future/news/MARC_Record_Marketplace_2009-10.pdf">Study of the North American MARC Records Marketplace</a>. The analysis of the survey results seem to focus on the role of the Library of Congress in the marketplace, which is understandable given that LC funded the report. But there seems to be a real effort to look at LC's role in the broader MARCetplace (sorry I couldn't resist).</p>

<p>Anyhow, I jotted down some random notes and questions in the margins, and
figured I'd add them here before my notes got tossed in the circular file.</p>

<p>So I found this kind of surprising at the time:</p>

<blockquote>
7 participating distributors report that they do not acquire MARC records from external sources, but the rest do.  Of those external sources, LC was predominant, followed by OCLC, LC record resellers, Library and Archives Canada, and the British National Library.  Approximately 14% of respondents acquire a significant portion of their records via Z39.50 protocols and various web crawlers. 

<br />
p. 19
</blockquote>

<p>Should I be surprised that there are more LC subscribers than OCLC subscribers
among the 70 distributors participating in the survey? I am surprised.</p>

<blockquote>
Much has changed since this <a href="http://www.law.cornell.edu/uscode/2/usc_sec_02_00000150----000-.html">law</a> was formulated. First, LC took on a community oriented role by underwriting the CIP program, which accounted for 53,000 new titles in 2008. Second, for the past 25 years or so, LC records have been distributed electronically. This has not only lowered the cost of distribution, but has made the records easily transferable from one institution to another, often without payment. One result is that <strong>LC records are significantly underpriced, since the cost of production is not included</strong>. Another is that an entire industry has developed around free (or at least very cheap) MARC records. Consider that an LC record for a single title might appear in thousands of library catalogs, while its MARC Distribution Service lists only 74 customers, 30 of them foreign. Most copies of LC records are obtained either free (via its Z39.50 servers and WebOPAC) or purchased from OCLC or vendors who supply those records in conjunction with the materials they sell. In short, many libraries and vendors benefit from a product for which production costs are not recovered.

<br />
pp. 26-27
</blockquote>

<p>It would've been nice to see how much money it costs to distribute MARC data from the LC FTP site compared with how much money LC gets through its MARC subscription program. The report points out elsewhere that LC catalogs items through the CIP program that it ends up discarding. So they aren't technically part of the operating cost of the library--if you don't consider the Copyright Office part of the Library of Congress. The last time I looked at the <a href="http://web.archive.org/web/20100528034700/http://www.loc.gov/about/lcorgsep06.pdf">LC organization chart</a> the Copyright Office was part of LC. Furthermore, unless I missed it there is no indication of how many records there are in that category. Extrapolating from the 74 customers and the <a href="http://web.archive.org/web/20100730095043/http://www.loc.gov:80/cds/mds.html">current price</a> of the subscription service (21,905) it would appear that LC gets approximately $1,620,970.00 a year in revenue from its distribution of the MARC data. It's difficult for me to imagine the generation of CIP records for items that LC discards added to the cost of operating an FTP site to equal this number.</p>

<blockquote>

Another major distribution channel involves direct downloads from LC’s Voyager database. At present, LC offers four separate interfaces: 
<ul>
     <li>A Web OPAC for bib records that supports 875 simultaneous users </li>
     <li>A Web OPAC for authority records that supports 500 simultaneous users </li>
     <li>Z39.50 direct access for users with Z39.50 clients, which supports 340 simultaneous users</li> 
     <li>Z39.50 gateway interface that supports up to 250 simultaneous users</li> 
</ul>
 
In total, these search interfaces process about 500,000 searches each business day. While not every  search leads to a download, the volume of searches is a clear indication of interest. Major users, to the degree that can be determined, include school libraries and small publics, who may not be OCLC members. In addition, vendors, open database providers, and firms such as Amazon regularly seek these  records.  

p. 35
</blockquote>

<p>Wow, half a million searches a day, that's bigger than I would've thought. It would be interesting to see how many actual MARC downloads there are through these services, and also to see a breakdown across services. Ironically, I think providing piecemeal access to records, and supporting these search interfaces such as Z39.50 have quite a high cost in practice, and that simply making bulk downloads available for free to the public via FTP or what have your would do a lot to mitigate these costs.</p>

<p>Lastly the findings with respect to copy cataloging were really interesting.</p>

<blockquote>
In looking at the median numbers of original catalogers reported, we estimated that well over 30,000 professional catalogers are at work in North America. In the earlier example, we suggested that if each of those catalogers were to produce one record per work day,  that would provide the capacity to create 6.8 million records per year.

p. 36
</blockquote>

<p>I probably missed it, but the report doesn't seem to estimate how much backlogged material there is in the United States. Presumably it is lower than 6.8 million? It is kind of staggering to think how much untapped potential there is for original cataloging by professional catalogers in the United States. I lay the blame for the lack of original cataloging at the doorstep of archaic and arcane systems, data formats, and rules for content generation. The barrier to entry is just too high. Unfortunately the barrier to entry for getting the bibliographic data that is generated using tax payers money is too high as well.</p>

<p>These are obviously my own rambling thoughts and not those of my employer, or anyone else I work with for that matter.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1431</wp:post_id>
		<wp:post_date>2009-11-10 12:41:36</wp:post_date>
		<wp:post_date_gmt>2009-11-10 19:41:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>marcetplace</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="marc"><![CDATA[marc]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81559</wp:comment_id>
			<wp:comment_author><![CDATA[johnwcowan]]></wp:comment_author>
			<wp:comment_author_email>cowan@ccil.org</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>72.14.228.129</wp:comment_author_IP>
			<wp:comment_date>2009-11-10 14:22:23</wp:comment_date>
			<wp:comment_date_gmt>2009-11-10 21:22:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Obviously you don't understand what "production costs" means in such a context.  It means "as much as we can conceivably squeeze out of each and every user without actually making them do without", just as it does to the record companies, and has nothing to do with, like, the costs of production.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>44</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81560</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>74.96.130.210</wp:comment_author_IP>
			<wp:comment_date>2009-11-10 14:24:40</wp:comment_date>
			<wp:comment_date_gmt>2009-11-10 21:24:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@johncowan yeah, but the record companies are legitimately trying to turn a profit :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>cloaking and fulltext</title>
		<link>http://inkdroid.org/2009/11/10/cloaking-and-fulltext/</link>
		<pubDate>Tue, 10 Nov 2009 15:16:50 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1432</guid>
		<description></description>
		<content:encoded><![CDATA[It's comforting to know that <a href="http://www.cdlib.org/">California Digital Library</a> are selectively serving up fulltext content in HTML from their <a href="http://escholarship.org">institutional repository</a> for search engines to chew on. For example, compare the output of:

<pre>
curl http://escholarship.org/uc/item/2896686x
</pre>

with:

<pre>
curl --header "User-Agent: Googlebot/2.1 (+http://www.google.com/bot.html)" http://escholarship.org/uc/item/2896686x
</pre>

You should see full-text content for the article in the latter and not in the former:

<pre>
...

<div>qt2896686x repo "Wholly Visionary": the American Library Association, the 
Library of Congress, and the Card Distribution Program wholly visionary the american
 library association the library of congress and the card distribution program 2009 
2009 2009 2009-04-01 2009-04-01 20090401 yee yy::Yee, Martha M Yee, Martha M 
American Library Association American Library Association Library of Congress 
Library of Congress card distribution program card distribution program shared 
cataloging shared cataloging cooperative cataloging cooperative cataloging national 
bibliography national bibliography cataloging rules and standards cataloging rules and 
standards library history united states library history united states This paper offers a 
historical review of the events and institutional influences in the nineteenth century 
that led to the
...

</div></pre>

The advantage to doing this is that when I was searching for a quote from Title 2, Chapter 5, Section 150 of the US Code:

<blockquote>
The Librarian of Congress is authorized to furnish to such institutions or individuals as may  desire to buy them
</blockquote>

I found Martha Yee's paper <a href="http://escholarship.org/uc/item/2896686x">"Wholly Visionary": the American Library Association, the Library of Congress, and the Card Distribution Program</a> as the 5th hit in the <a href="http://www.google.com/search?q=+The+Librarian+of+Congress+is+authorized+to+furnish+to+such+institutions+or+individuals+as+may++desire+to+buy+them&ie=utf-8&oe=utf-8&aq=t&rls=com.ubuntu:en-US:official&client=firefox-a">search results</a>.

We do this at the Library of Congress as well in <a href="http://chroniclingamerica.loc.gov">Chronicling America</a> to make the OCR text of historic newspaper pages available to search engines, while not burdening the UI search interface with all the (much noisier) textual content. Compare:

<pre>
curl http://chroniclingamerica.loc.gov/lccn/sn84026749/1908-04-09/ed-1/seq-11/
</pre>

with:

<pre>
curl --header "User-Agent: Googlebot/2.1 (+http://www.google.com/bot.html)" http://chroniclingamerica.loc.gov/lccn/sn84026749/1908-04-09/ed-1/seq-11/
</pre>

However we've got a ticket in our tracking system to revisit this practice in light of Google themselves <a href="http://www.google.com/support/webmasters/bin/answer.py?hl=en&answer=66355">frowning</a> on the practice of 'cloaking':

<blockquote>
Cloaking refers to the practice of presenting different content or URLs to users and search engines. Serving up different results based on user agent may cause your site to be perceived as deceptive and removed from the Google index. 
</blockquote>

We were thinking of returning the OCR text in all the responses and putting it in a background pane of some kind that can be selected. But this will most likely increase the size of the HTTP response, and may significantly impact the load time. As more and more fulltext content moves online it would be nice to have a pattern digital libraries could follow for minting URIs for books, articles, etc while still making the fulltext content available to UserAgents that can effectively use it. 

Google hasn't dropped Chronicling America's pages from its index yet, which is a good sign. After running across similar patter at CDL I'm wondering if it's OK to continue doing what we are doing. What do you think?

<em>Update: <a href="http://www.ldodds.com/blog">Leigh Dodds</a> let me know in <a href="http://twitter.com/ldodds/statuses/5594083546">twitter</a> that much of the content gets into Google Scholar via cloaking.</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1432</wp:post_id>
		<wp:post_date>2009-11-10 08:16:50</wp:post_date>
		<wp:post_date_gmt>2009-11-10 15:16:50</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>cloaking-and-fulltext</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{i:0;s:5:"81556";i:1;s:5:"81561";i:2;s:5:"81562";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81556</wp:comment_id>
			<wp:comment_author><![CDATA[ardvaark.net/identity/br&hellip;]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://ardvaark.net/identity/brian/</wp:comment_author_url>
			<wp:comment_author_IP>66.93.101.211</wp:comment_author_IP>
			<wp:comment_date>2009-11-10 10:31:25</wp:comment_date>
			<wp:comment_date_gmt>2009-11-10 17:31:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[How significantly will it impact page loads?  Just on my poking around, with a couple of test pages, I'm showing a text-heavy page with 41K bytes in the OCR text.  If GZIP encoding were enabled, that would knock it down to 17K.  Viewing that image with all caching working correctly is 158K, so you're talking about a 10% increase in size, although most of that is in the JPG and not the HTML.

This is one of those nasty situations for which there is no good answer.  The reality here is that, like a Major League Umpire, "correct" is whatever Google says it is on any given day.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>329</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81557</wp:comment_id>
			<wp:comment_author><![CDATA[gluejar]]></wp:comment_author>
			<wp:comment_author_email>eric@hellman.net</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>173.70.72.242</wp:comment_author_IP>
			<wp:comment_date>2009-11-10 10:38:01</wp:comment_date>
			<wp:comment_date_gmt>2009-11-10 17:38:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[What CA is doing is not cloaking- it's more akin to content-type negotiation. The one thing I'd worry about is that the OCR is poor enough for your example page that a human reviewing the text could get confused and think that it's not the ocr of the image.

Cloaking is when you give a spider juicy content and then give spam to a human.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>156</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81558</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>74.96.130.210</wp:comment_author_IP>
			<wp:comment_date>2009-11-10 11:31:32</wp:comment_date>
			<wp:comment_date_gmt>2009-11-10 18:31:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@gluejar I agree in principle -- but google's docs don't really say that. The worry at LC was that If google are trying to identify cloaked content at scale on the web they may inadvertently flag Chronicling America content as cloaked -- since determining juicy-ness could be infeasible.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81561</wp:comment_id>
			<wp:comment_author><![CDATA[Martin Haye]]></wp:comment_author>
			<wp:comment_author_email>martin.haye@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawkGjOoZiKwVvLN2fpf3oKco87tZXrxETgo</wp:comment_author_url>
			<wp:comment_author_IP>63.249.96.163</wp:comment_author_IP>
			<wp:comment_date>2009-11-10 17:14:49</wp:comment_date>
			<wp:comment_date_gmt>2009-11-11 00:14:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@aardvark: Trouble is some of our (CDL's) items are entire monographs. Even compressed these would add quite a bit of overhead to each page view. Admittedly we're not totally optimized for tiny downloads, but currently we *could* optimize for size. Serving up the entire OCR text would foil that.

This is a subtle point and we debated it quite a bit. In the end we went with what's practical, and put our hopes in Google recognizing that we're not cloaking -- we're giving them what they need. Search engines need text, people need the whole page experience.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>330</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81562</wp:comment_id>
			<wp:comment_author><![CDATA[sgillies.net/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://sgillies.net/</wp:comment_author_url>
			<wp:comment_author_IP>88.162.140.86</wp:comment_author_IP>
			<wp:comment_date>2009-11-11 06:37:24</wp:comment_date>
			<wp:comment_date_gmt>2009-11-11 13:37:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[But what about other non-Google, non-browser agents? Would you want them to masquerade as the Googlebot, or arrange with you for the same special treatment (which you'd readily provide, I don't mean to imply otherwise at all)? Either seems to break the web a bit, yes? Tricky situation.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>326</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81563</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.48.0.59</wp:comment_author_IP>
			<wp:comment_date>2009-11-11 07:17:15</wp:comment_date>
			<wp:comment_date_gmt>2009-11-11 14:17:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@sgillies in Chronicling America we do the same for all the big search engine bots. But, I agree: it does seem like the out-of-band coordination breaks the web a bit. Sometimes I try to rationalize it as a variant of content-negotiation, similar to what happens in practice on the mobile web...but it doesn't work for very long. I'm definitely open to other solutions. I wish that rel='canonical' could help here, but <a href="http://inkdroid.org/2009/05/15/canonical-question/" rel="nofollow">I don't think it does</a>. It would be nice if there were some rel="fulltext" or something that bots could follow. Perhaps @ardvaark is right and we should just bite the bullet now. But what does that do for @martin's problem? /me shrugs]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>thank you wikipedia</title>
		<link>http://inkdroid.org/2009/12/15/thank-you-wikipedia/</link>
		<pubDate>Tue, 15 Dec 2009 19:34:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1518</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="float: left; margin-right: 15px; margin-bottom: 15px;"><a href="http://wikimediafoundation.org/wiki/Support_Wikipedia/en"><img border="0" alt="Wikipedia Affiliate Button" src="http://wikimediafoundation.org/w/extensions/skins/Donate/images/banners/Banner_125x125_0000_A.jpg" /></a></span>I just donated to Wikipedia because I use it everyday. I work as a software developer at the Library of Congress. I'm not ashamed to admit that I've spent the last 10 years filling in gaps in my computer science, math and philosophy knowledge. Working in libraries makes this sort of self-education process easier because of all the access to books, journals and whatnot. But Wikipedia has made this process much more fun and collaborative. I don't think I could do my job without it. 

I also am a <a href="http://linkeddata.org">Linked Data</a> enthusiast, and appreciate the essential role that Wikipedia plays in data sets like <a href="http://dbpedia.org">dbpedia</a>, <a href="http://www.mpi-inf.mpg.de/yago-naga/yago/">yago</a> and <a href="http://freebase.com">freebase</a> in <a href="http://www4.wiwiss.fu-berlin.de/bizer/pub/lod-datasets_2009-03-05.html">bootstrapping</a> Linked Data around the world. Seeing Wikipedia pages float to the top of Google search results really brought home to me how important it is that we can use URLs as names for things in the world, and gather a shared understanding of what they are.

If you use Wikipedia I encourage you to take a moment to say <a href="http://wikimediafoundation.org/wiki/Support_Wikipedia/en">thank you</a> as well.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1518</wp:post_id>
		<wp:post_date>2009-12-15 12:34:16</wp:post_date>
		<wp:post_date_gmt>2009-12-15 19:34:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>thank-you-wikipedia</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="donations"><![CDATA[donations]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Hacking O&#039;Reilly RDFa</title>
		<link>http://inkdroid.org/2009/12/22/hacking-oreilly-rdfa/</link>
		<pubDate>Tue, 22 Dec 2009 18:17:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1535</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I recently learned from <a href="http://ivan-herman.name/2009/12/12/rdfa-usage-spreading…/">Ivan Herman</a>'s blog that O'Reilly has begun publishing RDFa in their online catalog of books. So if you go and install the <a href="http://www.w3.org/2001/sw/BestPractices/HTML/rdfa-bookmarklet/">RDFa Highlight</a> bookmarklet and then visit a page like <a href="http://oreilly.com/catalog/9780596516499/">this</a> and click on the bookmarklet you'll see something like:</p>

<p><a href="http://oreilly.com/catalog/9780596516499/">
<img src="http://inkdroid.org/images/oreilly-rdfa.png" style="width: 600px;"/>
</a></p>

<p>Those red boxes you see are graphical depictions of where metadata can be found interleaved in the HTML. In my screenshot you can maybe barely see an assertion involving the title being displayed:</p>

<pre>
&lt;urn:x-domain:oreilly.com:product:9780596516499.IP&gt; dc:title "Natural Language Processing with Python"
</pre>

<p>But there is actually quite a lot of metadata hiding in the page, which can be found by running the page through the <a href="http://www.w3.org/2007/08/pyRdfa">RDFa Distiller</a> (quickly skim over this if your eyes glaze over when you see Turtle):</p>

<pre style="height: 300px;">
@prefix dc: &lt;http://purl.org/dc/terms/&gt; .
@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .
@prefix frbr: &lt;http://vocab.org/frbr/core#&gt; .
@prefix gr: &lt;http://purl.org/goodrelations/v1#&gt; .
@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .
@prefix xml: &lt;http://www.w3.org/XML/1998/namespace&gt; .
@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .

&lt;urn:x-domain:oreilly.com:product:9780596516499.IP&gt; a frbr:Expression ;
     dc:creator &lt;urn:x-domain:oreilly.com:agent:pdb:3343&gt;, &lt;urn:x-domain:oreilly.com:agent:pdb:3501&gt;, &lt;urn:x-domain:oreilly.com:agent:pdb:3502&gt; ;
     dc:issued "2009-06-12"^^xsd:dateTime ;
     dc:publisher "O'Reilly Media"@en ;
     dc:title "Natural Language Processing with Python"@en ;
     frbr:embodiment &lt;urn:x-domain:oreilly.com:product:9780596516499.BOOK&gt;, &lt;urn:x-domain:oreilly.com:product:9780596803346.SAF&gt;, &lt;urn:x-domain:oreilly.com:product:9780596803391.EBOOK&gt; . 

&lt;http://customer.wileyeurope.com/CGI-BIN/lansaweb?procfun+shopcart+shcfn01+funcparms+parmisbn(a0130):9780596516499+parmqty(p0050):1+parmurl(l0560):http://oreilly.com/store/&gt; a gr:Offering ;
     gr:includesObject
         [ a gr:TypeAndQuantityNode ;
             gr:ammountOfThisGood "1"^^xsd:float ;
             gr:hasPriceSpecification
                 [ a gr:UnitPriceSpecification ;
                     gr:hasCurrency "GBP"@en ;
                     gr:hasCurrencyValue "34.50"^^xsd:float
                 ] ;
             gr:typeOfGood &lt;urn:x-domain:oreilly.com:product:9780596516499.BOOK&gt;
         ] . 

&lt;http://my.safaribooksonline.com/9780596803346&gt; a gr:Offering ;
     gr:includesObject
         [ a gr:TypeAndQuantityNode ;
             gr:ammountOfThisGood "1"^^xsd:float ;
             gr:typeOfGood &lt;urn:x-domain:oreilly.com:product:9780596803346.SAF&gt;
         ] . 

&lt;https://epoch.oreilly.com/shop/cart.orm?p=BUNDLE&prod=9780596516499.BOOK&prod=9780596803391.EBOOK&bundle=1&retUrl=http%3A%252F%252Foreilly.com%252Fstore%252F&gt; a gr:Offering ;
     gr:includesObject
         [ a gr:TypeAndQuantityNode ;
             gr:ammountOfThisGood "1"^^xsd:float ;
             gr:includesObject
                 [ a gr:TypeAndQuantityNode ;
                     gr:ammountOfThisGood "1"^^xsd:float ;
                     gr:hasPriceSpecification
                         [ a gr:UnitPriceSpecification ;
                             gr:hasCurrency "None"@en ;
                             gr:hasCurrencyValue "49.49"^^xsd:float
                         ] ;
                     gr:typeOfGood &lt;urn:x-domain:oreilly.com:product:9780596803391.EBOOK&gt;
                 ] ;
             gr:typeOfGood &lt;urn:x-domain:oreilly.com:product:9780596516499.BOOK&gt;
         ] . 

&lt;https://epoch.oreilly.com/shop/cart.orm?prod=9780596516499.BOOK&gt; a gr:Offering ;
     gr:includesObject
         [ a gr:TypeAndQuantityNode ;
             gr:ammountOfThisGood "1"^^xsd:float ;
             gr:hasPriceSpecification
                 [ a gr:UnitPriceSpecification ;
                     gr:hasCurrency "USD"@en ;
                     gr:hasCurrencyValue "44.99"^^xsd:float
                 ] ;
             gr:typeOfGood &lt;urn:x-domain:oreilly.com:product:9780596516499.BOOK&gt;
         ] . 

&lt;https://epoch.oreilly.com/shop/cart.orm?prod=9780596803391.EBOOK&gt; a gr:Offering ;
     gr:includesObject
         [ a gr:TypeAndQuantityNode ;
             gr:ammountOfThisGood "1"^^xsd:float ;
             gr:hasPriceSpecification
                 [ a gr:UnitPriceSpecification ;
                     gr:hasCurrency "USD"@en ;
                     gr:hasCurrencyValue "35.99"^^xsd:float
                 ] ;
             gr:typeOfGood &lt;urn:x-domain:oreilly.com:product:9780596803391.EBOOK&gt;
         ] . 

&lt;urn:x-domain:oreilly.com:agent:pdb:3343&gt; a foaf:Person ;
     foaf:homepage &lt;http://www.oreillynet.com/pub/au/3614&gt; ;
     foaf:name "Steven Bird"@en . 

&lt;urn:x-domain:oreilly.com:agent:pdb:3501&gt; a foaf:Person ;
     foaf:homepage &lt;http://www.oreillynet.com/pub/au/3615&gt; ;
     foaf:name "Ewan Klein"@en . 

&lt;urn:x-domain:oreilly.com:agent:pdb:3502&gt; a foaf:Person ;
     foaf:homepage &lt;http://www.oreillynet.com/pub/au/3616&gt; ;
     foaf:name "Edward Loper"@en . 

&lt;urn:x-domain:oreilly.com:product:9780596803346.SAF&gt; a frbr:Manifestation ;
     dc:type &lt;http://purl.oreilly.com/product-types/SAF&gt; . 

&lt;urn:x-domain:oreilly.com:product:9780596803391.EBOOK&gt; a frbr:Manifestation ;
     dc:identifier &lt;urn:isbn:9780596803391&gt; ;
     dc:issued "2009-06-12"^^xsd:dateTime ;
     dc:type &lt;http://purl.oreilly.com/product-types/EBOOK&gt; . 

&lt;urn:x-domain:oreilly.com:product:9780596516499.BOOK&gt; a frbr:Manifestation ;
     dc:extent """512"""@en ;
     dc:identifier &lt;urn:isbn:9780596516499&gt; ;
     dc:issued "2009-06-19"^^xsd:dateTime ;
     dc:type &lt;http://purl.oreilly.com/product-types/BOOK&gt; . 
</pre>

<p>So that's a lot of data. The nice thing about rdf is that you can look at the vocabularies that are being used to get an idea of the rough shape of the underlying data. Just looking at the namespace prefixes we can see that O'Reilly has chosen to use the following vocabularies:</p>

<ul>
<li><a href="http://dublincore.org/documents/dcmi-terms/">Dublin Core Terms</a>: for indicating the publisher, title, authors, issue date and identifiers for a book</li>
<li><a href="http://xmlns.com/foaf/spec/">Friend of a Friend (FOAF)</a>: for modeling authors as People</li>
<li><a href="http://vocab.org/frbr/core.html">Functional Requirements for Bibliographic Records (FRBR)</a>: for relating a particular book (Expression) to its various Manifestations of the title: ebook, printed book</li>
<li><a href="http://www.heppnetz.de/projects/goodrelations/">Good Relations</a>: for making pricing information available</li>
</ul>

<p>I was curious so I wrote a little <a href="https://gist.github.com/edsu/6214240">crawler</a> (41 lines of Python+rdflib) to collect all the metadata from the O'Reilly Catalog pages. Yes all the pages! It ended up pulling down 92,101 triples.</p>

<p>A nice side effect of having the data as a big ntriples file is you can do unix pipe tricks with sort, cut, uniq like <a href="https://gist.github.com/edsu/6214277">this</a> to get some ballpark numbers on what types of resources are in the rdf graph:</p>

<pre>
ed@curry:~/Projects/oreilly-crawler$ rdfsum catalog.nt
   6803 &lt;http://purl.org/goodrelations/v1#TypeAndQuantityNode&gt;
   5861 &lt;http://purl.org/goodrelations/v1#Offering&gt;
   4564 &lt;http://purl.org/goodrelations/v1#UnitPriceSpecification&gt;
   4065 &lt;http://vocab.org/frbr/core#Manifestation&gt;
   2100 &lt;http://vocab.org/frbr/core#Expression&gt;
   2023 &lt;http://xmlns.com/foaf/0.1/Person&gt;
</pre>

<p>Another nice thing about pulling the RDFa down with rdflib is you end up with a little berkeleydb triple store which you can query with SPARQL, say to pull out all the authors and titles:</p>

<pre>
    SELECT ?title ?author
    WHERE { 
      ?title_uri dct:title ?title .
      ?title_uri dct:creator ?author_uri .
      ?author_uri foaf:name ?author .
    }
</pre>

<p>And adding a little bit of <a href="http://networkx.lanl.gov/">networkx</a> <a href="https://gist.github.com/edsu/6214349">judo</a> you can get an <strong>xmas-friendly</strong> graph of authors (the green dots are books and the red ones are authors ; I limited author labels to authors who had written more than 2 books).</p>

<p><a href="http://inkdroid.org/images/oreilly-authorship.png"><img src="http://inkdroid.org/images/oreilly-authorship.png" /></a></p>

<p>Admittedly this is not very readable, but I imagine someone with more network visualization skillz could do something nicer in short order. There's a lot that could be done with the data. This exercise was mainly just to demonstrate how layering some new stuff into your HTML can really open up doors for how people use your website. Clearly O'Reilly did some deep thinking about what data they had, and what vocabularies they wanted to model it with. But once they'd done that they probably just had to go add 50 lines to an HTML template somewhere, and it was published (props to <a href="http://davidbrunton.com">David Brunton</a> for this turn of phrase). It's a really good sign that a tech publisher with the stature of O'Reilly is giving this method of data publishing a try.</p>

<p>My only suggestion (for anyone at O'Reilly who might be reading) would be that it would be nice if they used HTTP URLs instead of URNs for People, Works and Expressions. I understand why they did it: using URNs eases deployment somewhat since you don't have to worry about httpRange-14 stuff. But I think they could easily use a hash URI instead of an URN, and make it easy for people to link to their stuff in other data. The <a href="http://www.w3.org/TR/cooluris/">Cool URIs For the Semantic Web</a> has some other patterns they might want to consider, but simply adding a hash to their existing page URIs should do the trick. So for example, consider if <a href="http://openlibrary.org">OpenLibrary</a> wanted to link their notion of of a book to O'Reilly's notion of a book with owl:sameAs. If they used they URN they'd have:</p>

<pre>
&lt;http://openlibrary.org/b/OL23978297M&gt; owl:sameAs &lt;urn:x-domain:oreilly.com:product:9780596516499.IP&gt; .
</pre>

<p>but if O'Reilly identified their expressions with a URL they would enable something like:</p>

<pre>
&lt;http://openlibrary.org/b/OL23978297M&gt; owl:sameAs &lt;http://oreilly.com/catalog/9780596516499#expression&gt; .
</pre>

<p>This may seem like a minor point, but it's really important to be able to follow your nose on the web--particularly in <a href="http://linkeddata.org">Linked Data</a>. If a piece of software ran across the O'Reilly URL in a chunk of OpenLibrary RDF, the program could HTTP GET it, and learn more stuff about the book. <em>But if it got the URN it wouldn't really know how to fetch a representation for that resource without some special case logic that mapped the URN to a URL</em>. There is a reason why Tim Berners-Lee included the following as the second of his <a href="http://www.w3.org/DesignIssues/LinkedData.html">design principles</a> for Linked Data:</p>

<blockquote>
Use HTTP URIs so that people can look up those names.
</blockquote>

<p>Anyhow, hats off to O'Reilly for putting RDFa into practice. I hope the rest of the publishing (and library world) take note. If you are looking to learn more about RDFa <a href="http://adida.net/">Ben Adida</a> and <a href="http://web.archive.org/web/20111121115551/http://webbackplane.com/mark-birbeck/">Mark Birbeck</a>'s <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa Primer: Bridging the Human and Data Webs</a> is a really nice intro.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1535</wp:post_id>
		<wp:post_date>2009-12-22 11:17:03</wp:post_date>
		<wp:post_date_gmt>2009-12-22 18:17:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>hacking-oreilly-rdfa</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="graphs"><![CDATA[graphs]]></category>
		<category domain="post_tag" nicename="identifiers"><![CDATA[identifiers]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="oreilly"><![CDATA[oreilly]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81671</wp:comment_id>
			<wp:comment_author><![CDATA[links for 2010-01-22 &laquo; Claudio Bergamini Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://cbergamini.wordpress.com/2010/01/22/links-for-2010-01-22/</wp:comment_author_url>
			<wp:comment_author_IP>74.200.245.226</wp:comment_author_IP>
			<wp:comment_date>2010-01-22 03:03:55</wp:comment_date>
			<wp:comment_date_gmt>2010-01-22 10:03:55</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid › Hacking O’Reilly RDFa (tags: semantic rdfa) [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82435</wp:comment_id>
			<wp:comment_author><![CDATA[Hacking O&#8217;Reilly RDFa &laquo; ronjea&#39;s Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://ronjea.wordpress.com/2010/04/03/hacking-oreilly-rdfa/</wp:comment_author_url>
			<wp:comment_author_IP>74.200.245.176</wp:comment_author_IP>
			<wp:comment_date>2010-04-03 07:41:05</wp:comment_date>
			<wp:comment_date_gmt>2010-04-03 14:41:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] http://inkdroid.org/2009/12/22/hacking-oreilly-rdfa/ [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85248</wp:comment_id>
			<wp:comment_author><![CDATA[abdelazer]]></wp:comment_author>
			<wp:comment_author_email>abdelazer@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-12-01 12:46:30</wp:comment_date>
			<wp:comment_date_gmt>2011-12-01 19:46:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[An alternative way to get a lot of RDF data about these books is http://labs.oreilly.com/opmi.html  (maybe old data)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>424</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1322768790.7643";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:9:"abdelazer";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1322814500.542";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>5 Tunes for Gillian</title>
		<link>http://inkdroid.org/2010/01/11/5-tunes-for-gillian/</link>
		<pubDate>Mon, 11 Jan 2010 19:24:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1593</guid>
		<description></description>
		<content:encoded><![CDATA[Kesa's good friend Gillian from college days in NOLA sent around an email asking for people's favorite five <em>songs</em> of last year. 

For some reason picking individual songs is hard for me. I guess because I rarely put on a song, and almost always put on an album--as antiquated as that sounds. I do occasionally listen to suggestions on last.fm or random songs in my player-du-jour -- but then I don't really remember the song names. 

Anyhow here's the list I cobbled together, with links out to youtube (that'll probably break in 28 hrs):

<br />
<br />

<object width="445" height="364"><param name="movie" value="http://www.youtube.com/v/B5clBfEEiSw&hl=en_US&fs=1&border=1"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/B5clBfEEiSw&hl=en_US&fs=1&border=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="445" height="364"></embed></object>
<br />
<br />

<object width="445" height="364"><param name="movie" value="http://www.youtube.com/v/RrbGpvOulec&hl=en_US&fs=1&border=1"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/RrbGpvOulec&hl=en_US&fs=1&border=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="445" height="364"></embed></object>

<br />
<br />

<object width="445" height="364"><param name="movie" value="http://www.youtube.com/v/0UjsXo9l6I8&hl=en_US&fs=1&border=1"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/0UjsXo9l6I8&hl=en_US&fs=1&border=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="445" height="364"></embed></object>

<br />
<br />

<object width="445" height="364"><param name="movie" value="http://www.youtube.com/v/-UBDo2ExXqY&hl=en_US&fs=1&border=1"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/-UBDo2ExXqY&hl=en_US&fs=1&border=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="445" height="364"></embed>
</object>

<br />
<br />

<object width="445" height="364"><param name="movie" value="http://www.youtube.com/v/mjfY9lVBoTg&hl=en_US&fs=1&border=1"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/mjfY9lVBoTg&hl=en_US&fs=1&border=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="445" height="364"></embed></object><object>


</object>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1593</wp:post_id>
		<wp:post_date>2010-01-11 12:24:52</wp:post_date>
		<wp:post_date_gmt>2010-01-11 19:24:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>5-tunes-for-gillian</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"81641";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81641</wp:comment_id>
			<wp:comment_author><![CDATA[dbrunton]]></wp:comment_author>
			<wp:comment_author_email>dbrunton@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawkuMzjlM81pM-JvzFsoNviuHmLfn6YLydc</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2010-01-12 06:37:55</wp:comment_date>
			<wp:comment_date_gmt>2010-01-12 13:37:55</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I think it's possible you may have just displaced Dan C as the mayor of dbrun's new musical experiences.  I liked all five songs, and had only heard one of them before, making me perhaps the most musically sheltered person any of us know...  thanks for the links.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>327</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>data.gov.uk and rdfa</title>
		<link>http://inkdroid.org/2010/01/26/data-gov-uk-and-rdfa/</link>
		<pubDate>Tue, 26 Jan 2010 22:41:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1602</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The recent public release of the UK Government's <a href="http://data.gov.uk">data.gov.uk</a> site got picked up by the press last week in articles at <a href="http://news.bbc.co.uk/2/hi/technology/8470797.stm">The Guardian</a>, <a href="http://www.prospectmagazine.co.uk/2010/01/whitehalls-web-revolution-the-inside-story/">Prospect Magazine</a> and elswhere. These have been supplemented by some more technical discussions at  <a href="http://www.readwriteweb.com/archives/uk_launches_open_data_site_puts_datagov_to_shame.php">ReadWriteWeb</a>,  <a href="http://blog.okfn.org/2010/01/21/datagovuk-goes-public-and-its-using-ckan/">Open Knowledge Foundation</a>, <a href="http://web.archive.org/web/20110912091006/http://blogs.talis.com:80/nodalities/2009/11/data-gov-uk-and-the-talis-platform.php">Talis</a>, <a href="http://www.jenitennison.com/blog/node/140">Jeni Tennison's blog</a>, and some helpful emails from <a href="http://lists.w3.org/Archives/Public/public-egov-ig/2010Jan/0048.html">Leigh Dodds</a> (<a href="http://web.archive.org/web/20120528202833/http://www.talis.com:80/platform/">Talis</a>) and <a href="http://lists.w3.org/Archives/Public/public-egov-ig/2010Jan/0040.html">Jonathan Gray</a> (<a href="http://okfn.org">Open Knowledge Foundation</a>) on the <a href="http://lists.w3.org/Archives/Public/public-egov-ig/">w3c egovernment discussion list</a>.</p>

<p>One thing that I haven't seen mentioned so far in public (which I just discovered today) is that data.gov.uk is using <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a> to expose metadata about the datasets in a machine readable way. What this means is that in an HTML page for a dataset like <a href="http://data.gov.uk/dataset/agricultural_market_reports">this</a> there are some extra HTML attributes like <em>about</em>, <em>property</em>, <em>rel</em> that have been thoughtfully used to express some structured metadata about the dataset, which can be extracted from the HTML and expressed say as <a href="http://www.w3.org/TeamSubmission/turtle/">Turtle</a>:</p>

<pre>
&lt;http://data.gov.uk/id/dataset/agricultural_market_reports&gt; dct:coverage "Great Britain (England, Scotland, Wales)"@en ;
     dct:created "2009-12-04"@en ;
     dct:creator "Department for Environment, Food and Rural Affairs"@en ;
     dct:isReferencedBy &lt;http://data.gov.uk/wiki/index.php/Package:agricultural_market_reports&gt; ; 
     dct:license "Crown Copyright"@en ;
     dct:source &lt;http://statistics.defra.gov.uk/esg/publications/amr/default.asp&gt;, &lt;https://statistics.defra.gov.uk/esg/publications/amr/default.asp&gt; ;
     dct:subject
         &lt;http://data.gov.uk/data/tag/agriculture&gt;,
         &lt;http://data.gov.uk/data/tag/agriculture-and-environment&gt;,
         &lt;http://data.gov.uk/data/tag/environment&gt;,
         &lt;http://data.gov.uk/data/tag/farm-business&gt;,
         &lt;http://data.gov.uk/data/tag/farm-businesses&gt;,
         &lt;http://data.gov.uk/data/tag/farming&gt; .
</pre>

<p>In fact since data.gov.uk has a nice paging mechanism that lists all the datasets it's not hard to write a little <a href="http://web.archive.org/web/20101216201605/http://inkdroid.org/bzr/data-gov-uk/crawl.py">script</a> that scrapes <a href="http://inkdroid.org/bzr/data-gov-uk/data.rdf">all the metadata for the datasets</a> (35,478 triples) right out of the web pages.</p>

<p>I also <a href="http://buytaert.net/data-gov-uk-using-drupal">noticed</a> via <a href="http://twitter.com/scorlosquet/status/8242145459">Stéphane Corlosquet</a> today that data.gov.uk is using the <a href="http://drupal.org">Drupal</a> open-source content management system. To what extent Drupal7's new <a href="http://drupal.org/node/574624">RDFa features</a> are being used to layer in this RDFa isn't clear to me. But it is an exciting development. It's exciting because data.gov.uk is a great example of how to bubble up data that's typically locked away in databases of some kind into the HTML that's out on the web for people to interact with, and for crawlers to crawl and re-purpose.</p>

<p>For example I can now write a <a href="http://web.archive.org/web/20101216202008/http://inkdroid.org/bzr/data-gov-uk/link_check.py">utility</a> to check the status of the external dataset links, to make sure they are they are there (200 OK). The <a href="http://web.archive.org/web/20101216201928/http://inkdroid.org/bzr/data-gov-uk/link_check.txt">complete results by URL</a> can be summarized by rolling up by status code:</p>

<table style="border: thin gray solid; width: 60%;">
<tr><th>Status Code</th><th>Number of Datasets</th></tr>
<tr style='background: #eeeeee'><td>200</td><td>2977</td></tr>
<tr style='background: #ffffff'><td>404</td><td>106</td></tr>
<tr style='background: #eeeeee'><td>502</td><td>23</td></tr>
<tr style='background: #ffffff'><td>503</td><td>14</td></tr>
<tr style='background: #eeeeee'><td>[Errno socket error] [Errno -2] Name or service not known</td><td>8</td></tr>
<tr style='background: #ffffff'><td>500</td><td>3</td></tr>
<tr style='background: #eeeeee'><td>nonnumeric port: ''</td><td>1</td></tr>
<tr style='background: #ffffff'><td>[Errno socket error] [Errno 110] Connection timed out</td><td>1</td></tr>
<tr style='background: #eeeeee'><td>400</td><td>1</td></tr>
</table>

<p>Or I can <a href="http://web.archive.org/web/20101216201748/http://inkdroid.org/bzr/data-gov-uk/subjects.py">generate</a> a <a href="http://web.archive.org/web/20101216201813/http://inkdroid.org/bzr/data-gov-uk/subjects.txt">list of dataset subjects</a> (eventhough it's already <a href="http://data.gov.uk/data/tag">available</a> I guess). Here's the top 25:</p>

<table style="border: thin gray solid; width: 60%;">
<tr><th>Subject</th><th>Number of Datasets</th></tr><tr style='background: #eeeeee'><td>health </td><td>645</td></tr>
<tr style='background: #ffffff'><td>care </td><td>427</td></tr>
<tr style='background: #eeeeee'><td>child </td><td>398</td></tr>
<tr style='background: #ffffff'><td>population </td><td>341</td></tr>
<tr style='background: #eeeeee'><td>children </td><td>295</td></tr>
<tr style='background: #ffffff'><td>school </td><td>273</td></tr>
<tr style='background: #eeeeee'><td>health-and-social-care </td><td>271</td></tr>
<tr style='background: #ffffff'><td>health-well-being-and-care </td><td>205</td></tr>
<tr style='background: #eeeeee'><td>economy </td><td>202</td></tr>
<tr style='background: #ffffff'><td>economics-and-finance </td><td>189</td></tr>
<tr style='background: #eeeeee'><td>census </td><td>188</td></tr>
<tr style='background: #ffffff'><td>education </td><td>176</td></tr>
<tr style='background: #eeeeee'><td>communities </td><td>154</td></tr>
<tr style='background: #ffffff'><td>benefit </td><td>153</td></tr>
<tr style='background: #eeeeee'><td>road </td><td>144</td></tr>
<tr style='background: #ffffff'><td>children-education-and-skills </td><td>121</td></tr>
<tr style='background: #eeeeee'><td>people-and-places </td><td>111</td></tr>
<tr style='background: #ffffff'><td>government-receipts-and-expenditure </td><td>110</td></tr>
<tr style='background: #eeeeee'><td>education-and-skills </td><td>110</td></tr>
<tr style='background: #ffffff'><td>housing </td><td>108</td></tr>
<tr style='background: #eeeeee'><td>environment </td><td>107</td></tr>
<tr style='background: #ffffff'><td>tax </td><td>107</td></tr>
<tr style='background: #eeeeee'><td>life-in-the-community </td><td>106</td></tr>
<tr style='background: #ffffff'><td>employment </td><td>105</td></tr>
<tr style='background: #eeeeee'><td>tax-credit </td><td>96</td></tr>
</table>

<p>I realize it's early days but here are a few things it would be fun to see at data.gov.uk:</p>

<ul>
<li>add some RDFa and <a href="http://www.w3.org/TR/2009/REC-skos-reference-20090818/">SKOS</a> or <a href="http://www.commontag.org/Home">CommonTag</a> in tag pages like <a href="http://data.gov.uk/data/tag/education">education</a>: this would allow things to be hooked up a bit more explicitly, tags to be given nice labels, and encourage the reuse of the tagging vocabulary within and outside data.gov.uk</li>
<li>link the dataset descriptions to the dataset resources themselves (the pdfs, excel spreadsheets, etc) that are online using a vocabulary like the <a href="http://www.openarchives.org/ore/1.0/vocabulary">Open Archives Reuse and Exchange</a> and/or <a href="http://www.w3.org/TR/powder-dr/">POWDER</a>. This would allow for the harvesting and aggregation not only of the metadata, but the datasets as well.</li>
</ul>

<p>I imagine much of this sort of hacking around can be enabled by querying the <a href="http://web.archive.org/web/20130127043417/http://data.gov.uk/sparql">data.gov.uk SPARQL endpoint</a>. But it hasn't been very clear to me exactly what data is behind there. And there is something comforting about being able to crawl the open web to find the information that's there in <a href="http://inkdroid.org/2009/08/13/open-to-view/">open to view</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1602</wp:post_id>
		<wp:post_date>2010-01-26 15:41:19</wp:post_date>
		<wp:post_date_gmt>2010-01-26 22:41:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>data-gov-uk-and-rdfa</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="egov"><![CDATA[egov]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"81697";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81697</wp:comment_id>
			<wp:comment_author><![CDATA[jonathangray.org/]]></wp:comment_author>
			<wp:comment_author_email>jonathan.gray@okfn.org</wp:comment_author_email>
			<wp:comment_author_url>http://jonathangray.org/</wp:comment_author_url>
			<wp:comment_author_IP>92.225.85.156</wp:comment_author_IP>
			<wp:comment_date>2010-01-27 05:43:11</wp:comment_date>
			<wp:comment_date_gmt>2010-01-27 12:43:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the post!

Just a quick note to say the OKF's domain name (first paragraph) is okfn.org, not okfn.net.

Also you might be interested in this:

  http://blog.okfn.org/2010/01/26/sources-of-data-on-datagovuk/

Jonathan]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>334</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81721</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.138.160.67</wp:comment_author_IP>
			<wp:comment_date>2010-01-28 05:13:27</wp:comment_date>
			<wp:comment_date_gmt>2010-01-28 12:13:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Jonathan, thanks for the broken link report :-) I had seen that other post on the okfn blog. The relationship between the info at data.gov.uk and ckan.net still isn't very clear to me. But I was really pleased to see how easy it was to scrape the information out of the data.gov.uk pages.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81731</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid &rsaquo; data.australia.gov.au and rdfa]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2010/01/29/data-australia-gov-au-and-rdfa/</wp:comment_author_url>
			<wp:comment_author_IP>109.74.197.162</wp:comment_author_IP>
			<wp:comment_date>2010-01-29 07:37:08</wp:comment_date>
			<wp:comment_date_gmt>2010-01-29 14:37:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] my previous blog post I was trying to demonstrate the virtues of data.gov.uk making the descriptions of their datasets [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81791</wp:comment_id>
			<wp:comment_author><![CDATA[Luxcommons &raquo; Blog Archive &raquo; &#8220;les petites cases&#8221; sur les linked data]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.luxcommons.lu/?p=122</wp:comment_author_url>
			<wp:comment_author_IP>213.251.168.76</wp:comment_author_IP>
			<wp:comment_date>2010-02-10 02:54:58</wp:comment_date>
			<wp:comment_date_gmt>2010-02-10 09:54:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] des technlogies du Web sémantique. Le site en lui-même a été construit avec Drupal 6 avec du RDFa à l&#8217;intérieur, la plupart des donnnées a été convertie en RDF grâce, entre autres, aux bons soins de Jeni [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81793</wp:comment_id>
			<wp:comment_author><![CDATA[Do we have a Linked Data research agenda? « Web of Data]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://webofdata.wordpress.com/2010/02/13/linked-data-research-agenda/</wp:comment_author_url>
			<wp:comment_author_IP>66.135.48.211</wp:comment_author_IP>
			<wp:comment_date>2010-02-13 03:27:20</wp:comment_date>
			<wp:comment_date_gmt>2010-02-13 10:27:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] the process for drafting a &#8216;Research Agenda For Linked Data&#8217;. Since then, a couple of things [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>data.australia.gov.au and rdfa</title>
		<link>http://inkdroid.org/2010/01/29/data-australia-gov-au-and-rdfa/</link>
		<pubDate>Fri, 29 Jan 2010 14:36:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1656</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In my <a href="http://inkdroid.org/2010/01/26/data-gov-uk-and-rdfa/">previous blog post</a> I was trying to demonstrate the virtues of data.gov.uk making the descriptions of their datasets available as <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a>. Just this morning I <a href="http://groups.google.com/group/uk-government-data-developers/msg/16599323836791e5">learned</a> from <a href="http://web.archive.org/web/20111121115551/http://webbackplane.com/mark-birbeck/">Mark Birbeck</a> that the folks down under at <a href="http://data.australia.gov.au">data.australia.gov.au</a> did this last October!</p>

<p>For example <a href="http://web.archive.org/web/20110301044400/http://data.australia.gov.au/414">this</a> page describing a dataset for public Internet locations has this RDF metadata inside it:</p>

<pre>
&lt;http://data.australia.gov.au/80&gt; cc:attributionName "http://www.centrelink.gov.au/"@en-au ;
     cc:attributionURL &lt;http://www.centrelink.gov.au/&gt; ;
     dc:coverage.geospatial "Australia"@en-au ;
     dc:coverage.temporal "Not specified"@en-au ;
     dc:creator "Centrelink"@en-au ;
     dc:date.modified "2009-08-31"^^xsd:date ;
     dc:date.published "2009-08-31"^^xsd:date ;
     dc:description """&lt;p xml:lang="en-au" xmlns="http://www.w3.org/1999/xhtml"&gt;Location of Centrelink Offices&lt;/p&gt;
"""^^rdf:XMLLiteral ;
     dc:identifier "80"@en-au ;
     dc:keywords "&lt;a href=\"http://data.australia.gov.au/tag/social-security\" rel=\"tag\" xml:lang=\"en-au\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;Social Security&lt;/a&gt;"^^rdf:XMLLiteral ;
     dc:license "&lt;a href=\"http://creativecommons.org/licenses/by/2.5/au/\" rel=\"licence\" xml:lang=\"en-au\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;img alt=\"Creative Commons License\" class=\"licence\" src=\"http://i.creativecommons.org/l/by/2.5/au/88x31.png\"/&gt;Creative Commons - Attribution 2.5 Australia (CC-BY)&lt;/a&gt;"^^rdf:XMLLiteral ;
     dc:source "&lt;a href=\"http://www.centrelink.gov.au/\" rel=\"dc:source\" xml:lang=\"en-au\" xmlns=\"http://www.w3.org/1999/xhtml\"/&gt;"^^rdf:XMLLiteral ;
     dc:subject "&lt;a href=\"http://data.australia.gov.au/catalogue/community\" rel=\"category tag\" title=\"View all posts in Community\" xml:lang=\"en-au\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;Community&lt;/a&gt;,  &lt;a href=\"http://data.australia.gov.au/catalogue/employment\" rel=\"category tag\" title=\"View all posts in Employment\" xml:lang=\"en-au\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;Employment&lt;/a&gt;,  &lt;a href=\"http://data.australia.gov.au/catalogue/government\" rel=\"category tag\" title=\"View all posts in Government\" xml:lang=\"en-au\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;Government&lt;/a&gt;"^^rdf:XMLLiteral ;
     dc:title "Location of Centrelink Offices"@en-au ;
     dc:type &lt;http://purl.org/dc/dcmitype/Text&gt; ;
     agls:jurisdiction "[Commonwealth of] Australia (AU)"@en-au ;

&lt;http://www1.australia.gov.au/datasets/Federal/Centrelink/Location%20of%20Centrelink%20offices%2031_08_09/centrelink_offices_31_08_2009.CSV&gt; dc:format "CSV"@en-au . 
</pre>

<p>Now this data isn't without problems: notice the XML literals as objects in the assertions involving subject, keyword, license and source? But it's a Beta after all, and lots of us are learning this as we go, so Australia deserves a ton of credit. One really nice thing they are doing is making assertions about the format and URL location of the dataset itself. It would be even better if the dataset description was linked up with the dataset files using <a href="http://www.openarchives.org/ore/1.0/vocabulary">oai-ore</a> or some other vocabulary.</p>

<p>In about 5 minutes I adapted the simplistic data.gov.uk crawler to crawl the data.australia.gov.au data.  There aren't as many datasets, so the <a href="http://web.archive.org/web/20101216224228/http://inkdroid.org/bzr/data-australia-gov-au/crawl.py">crawler</a> only pulled down <a href="http://web.archive.org/web/20101216224353/http://inkdroid.org/bzr/data-australia-gov-au/data.rdf">1725 triples</a> (minus the xhtml triples)...but perhaps I missed some in my simplistic crawl.</p>

<p>Seeing both the data.gov.uk and data.australia.gov.au efforts to make dataset descriptions available makes me wonder if it could be useful for the <a href="http://www.w3.org/2007/eGov/">W3C eGov Working Group</a> to provide some lightweight guidance on how to make dataset descriptions available: what sorts of vocabularies to use, the kinds of assertions that are important, etc. It's hard not to daydream of trying to provide an aggregated view of both pools of data, which is kept in synch using the web, and which perhaps could pull down aggregated datasets and archive them, etc. Perhaps a little spot checking tool that took at look at your HTML and let you know if it can work as a dataset description would be useful too?</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1656</wp:post_id>
		<wp:post_date>2010-01-29 07:36:57</wp:post_date>
		<wp:post_date_gmt>2010-01-29 14:36:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>data-australia-gov-au-and-rdfa</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="australia"><![CDATA[australia]]></category>
		<category domain="post_tag" nicename="egov"><![CDATA[egov]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="w3c"><![CDATA[w3c]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>web documents and axioms for linked data</title>
		<link>http://inkdroid.org/2010/02/22/web-documents-and-axioms-for-linked-data/</link>
		<pubDate>Tue, 23 Feb 2010 02:06:38 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1676</guid>
		<description></description>
		<content:encoded><![CDATA[A few months ago I took part in a <a href="http://groups.google.com/group/pedantic-web/browse_thread/thread/eb65cce9df40abd4">discussion</a> on the <a href="http://pedantic-web.org/">pedantic-web</a> list, which started out as a relatively simple question about FOAF usage, and quickly evolved into a conversation about terms people use when talking about Linked Data, and more generally the Web. 

I ended up having a very helpful off-list email exchange with <a href="http://richard.cyganiak.de/">Richard Cyganiak</a> (one of the architects of the <a href="http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/">Linked Data</a> pattern) about <a href="http://inkdroid.org/2009/05/14/rest-the-semantic-web-and-my-feeble-brain/">some</a> <a href="http://inkdroid.org/journal/2009/09/10/documents/">trouble</a> I've had understanding what <em>Information Resources</em> and <em>Documents</em> are in the context of <a href="http://www.w3.org/TR/webarch/#id-resources">Web Architecture</a>. The trouble I had was in determining whether or not a collection of physical newspaper pages I was helping <a href="http://chroniclingamerica.loc.gov">put on the web</a> were <em>Information Resources</em> or not. I needed to know because I wanted to identify the newspaper pages with URIs, and describe them as Linked Data...and the resolvability of these URIs was largely <a href="http://www.w3.org/TR/cooluris/#semweb">dependent</a> on how I chose to answer the question.

Richard ended up offering up some advice that I've since found very useful, and I thought I would transcribe some of it down here just in case you might find it useful as well. My apologies to you (and Richard) if some of this seems out of context. It may really only be useful for people who are in the digital library domain, but perhaps it's useful elsewhere.

On the subject of what is a <em>Document</em> Richard offered up this way at looking at what are <em>Web Documents</em>:

<blockquote>
The Web is a new, blank information space that is, by definition, disjoint from anything else that exists in the world. By setting up and configuring a web server, you make things pop up in that information space (by creating resolvable URIs). By definition, the things that pop up in the information space are a different beast from anything that existed before. They are web pages. They are *not* the same as things that exist outside of the space, like files on your hard disk, or newspaper articles.

...

I would avoid the term "document" when talking about representations. Representations are those ephemeral things that go over the wire. A representation is a "byte streams with a media type (and possibly other meta data)". When I use the term "HTML document", I mean a resource, identified by a URI, that has (only) HTML representations.
</blockquote>

Richard encouraged me to think in terms of <em>Web Documents</em> and not generic Documents. I was getting tripped up by considering Newspaper Pages as Documents...which of course they are in the general sense, but characterized this way it became clear that the Newspaper Pages are not Web Documents. This view on Web Documents is supported in the <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a> that he co-authored. 

Richard also included some axioms that underpin how he thinks about resources in the Linked Data view:

<blockquote>
I'm using a few rules that I think should be considered axioms of web architecture:

First, if something exists independently from the Web, then it cannot be a Web Document. (hence two resources, one for the newspaper page and one for the web page)

Second, only Web Documents can have representations (hence the need to describe the newspaper page in a web page, rather than directly providing representations of the newspaper page).

I understand these rules as axioms, that is, they should be followed because they make the system work best, not because they somehow follow from the nature of the world (they don't).
</blockquote>

The pragmatist in me particularly liked how these aren't supposed to have anything to do with the Real World, but are just ways of thinking about the Web to make it work better.  Finally Richard offered some advice on how to reconcile the REST and Linked Data views on identity:

<blockquote>
I make sense of the REST worldview like this: In typical REST, all the URIs *always* identify web documents. The REST folks might claim that they identify other things, like users or items for sale or places on the earth, but actually they just identify a document that is *about* that thing. The thing itself doesn't have an identifier. This is perfectly fine for building certain kinds of systems, so the REST guys actually get away with pretending that the URI identifies the thing. But this doesn't allow you to do certain things, like using domain-independent vocabularies for metadata and coreference, and you get into deep trouble if you want to use this for describing *web pages* rather than *newspaper pages*.
</blockquote>

I hope I haven't take any liberties quoting my conversation with Richard out of context like this. I mainly wanted to transcribe Richard's points (which perhaps he has made elsewhere) so that I could revisit them, without having to dig through my email archive ... Comments welcome!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1676</wp:post_id>
		<wp:post_date>2010-02-22 19:06:38</wp:post_date>
		<wp:post_date_gmt>2010-02-23 02:06:38</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>web-documents-and-axioms-for-linked-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="rest"><![CDATA[rest]]></category>
		<category domain="post_tag" nicename="semanticweb"><![CDATA[semanticweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="webarch"><![CDATA[webarch]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:4:{i:0;s:5:"81894";i:1;s:5:"81898";i:2;s:5:"81899";i:3;s:5:"81900";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>81898</wp:comment_id>
			<wp:comment_author><![CDATA[pkeane.livejournal.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://pkeane.livejournal.com/</wp:comment_author_url>
			<wp:comment_author_IP>216.82.211.185</wp:comment_author_IP>
			<wp:comment_date>2010-02-22 22:16:20</wp:comment_date>
			<wp:comment_date_gmt>2010-02-23 05:16:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Good stuff, Ed. I am struck yet again, though, at the discordance between the Linked Data and REST worldviews. I think a REST-based rejoinder to the last quote would state that building a system that made the distinction too finely would be brittle (cf. http://roy.gbiv.com/untangled/2008/resource-resource-wherefore-art-thou-resource ).

My own take is that we are talking about two different kinds of architecture/approaches: Linked Data is concerned w/ compile time whereas REST is concerned w/ runtime. Put another way, linked data is strongly and statically typed and REST is weakly and dynamically typed.  Not sure how useful that metaphor is, but it seems to me to help distinguish systems that will fit well into the Linked Data approach (e.g. , LCSH) and those that might not.

I'm also struck that the two worldviews will continue to be at odds (there are essential differences), but that both will likely figure into the future of the web.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>336</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81894</wp:comment_id>
			<wp:comment_author><![CDATA[Andrew Ashton]]></wp:comment_author>
			<wp:comment_author_email>andyashton@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/a/xRkbRE8IqJIrabMSxbTe0.6TOqPB#e4298</wp:comment_author_url>
			<wp:comment_author_IP>24.38.172.185</wp:comment_author_IP>
			<wp:comment_date>2010-02-22 20:10:10</wp:comment_date>
			<wp:comment_date_gmt>2010-02-23 03:10:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I work a lot with TEI documents, though I don’t consider myself an expert or a zealot in that arena.  Your mention of the idea of web documents vs. non-web documents crystallizes a lot of tensions that have been floating around as I try to grok working with TEI in an RDF/OWL/LinkedData environment.  

My impression is that discussions about XML docs &amp; related technologies are running along a parallel, but rarely converging, track with discussions about “web” documents.  Philosophically, there is a big difference between the purity of the RDF/Linked Data world and the comparatively procedural XML world.  But I keep finding that I wish I had a better strategy for reconciling the two. 

So it raises the question: how do “web documents” express, or at least point to, the kind of semantic nuance that we can express in a single non-web document.  What sort of mechanism resolves a semantic concept expressed in TEI (for example) to a referenceable resource?  I’m not aware of any XML-native technologies (e.g. Xpointer) that are really suited to this.  It seems we’re stuck with creating RDF representations of semantic encoding within documents, but that level of abstraction is invariably going to introduce more noise into the already-noisy practice of text-encoding.  I’m just curious what ideas are out there for reconciling these technologies – or have all the Linked Data community given up on XML?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>335</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81899</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>12.21.216.106</wp:comment_author_IP>
			<wp:comment_date>2010-02-22 23:42:49</wp:comment_date>
			<wp:comment_date_gmt>2010-02-23 06:42:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This makes a hell of a lot of sense.

--jrochkind]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>81900</wp:comment_id>
			<wp:comment_author><![CDATA[jakoblog.de/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://jakoblog.de/</wp:comment_author_url>
			<wp:comment_author_IP>89.204.153.0</wp:comment_author_IP>
			<wp:comment_date>2010-02-23 00:13:16</wp:comment_date>
			<wp:comment_date_gmt>2010-02-23 07:13:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the summary! I read the whole thread and also <a href="http://lists.w3.org/Archives/Public/www-tag/2009Aug/0000.html" rel="nofollow">Tim Berners-Lee's historical explanation</a> how the term Resource slipped into the specifications. However "Web Document" is not much better than "Information Resource" and I disagree that only any of both can have representations. You can create byte streams to represent anything, a newspaper, a cat, or an HTML document. The question is only whether the representation is appropriate in a given context. But this question cannot be answered by technical architectures or axioms only. It always depends on. Funny how Semantic Web believers seem to think that you only need more standards and levels of abstraction to finally get rid of this fuzzy nasty human common sense that has less problems to handle with uncertain and contradicting information :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>337</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>a middle way for linked data at the bbc</title>
		<link>http://inkdroid.org/2010/03/02/a-middle-way-for-linked-data-at-the-bbc/</link>
		<pubDate>Tue, 02 Mar 2010 20:13:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1701</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I got the chance to attend the <a href="http://www.meetup.com/Web-Of-Data/calendar/12317420/">2nd London Linked Data Meetup</a> that was co-located with  <a href="http://dev8d.org">dev8d</a> last week, which turned out to be a whole lot of fun. I figured if I waited long enough other people would save me from having to write a good summary/discussion of the event...and they have: thanks <a href="http://efoundations.typepad.com/efoundations/2010/02/the-2nd-linked-data-london-meetup-trying-to-bridge-a-gap.html">Pete Johnston</a>, <a href="http://bens.me.uk/2010/london-linked-data-meetup">Ben Summers</a>, <a href="http://blogs.cetis.ac.uk/sheilamacneill/2010/02/26/2nd-linked-data-meetup-london/">Sheila Macneill</a>, <a href="http://www.currybet.net/cbet_blog/2010/03/linked_data_human_readable_uris.php">Martin Belam</a> and <a href="http://www.frankieroberto.com/weblog/1621">Frankie Roberto</a>.</p>

<p><img src="http://inkdroid.org/images/bbc.png" style="margin-right: 10px; margin-bottom: 5px; float: left;"/> The main thing that I took away is how much good work the <a href="http://bbc.com">BBC</a> is doing in this space. Given the recent news of <a href="http://www.nytimes.com/2010/03/03/business/media/03bbc.html">cuts</a> at the BBC, it seems like a good time to say publicly how important some of the work they are doing is to the web technology sector. As part of the Meetup <a href="http://derivadow.com/">Tom Scott</a> gave a  <a href="http://www.slideshare.net/derivadow/apis-and-apis-a-wildlife-ontology">presentation</a> on how the BBC are using Linked Data to integrate distinct web properties in the BBC enterprise, like their <a href="http://www.bbc.co.uk/programmes">Programmes</a> and the <a href="http://www.bbc.co.uk/wildlifefinder/">Wildlife Finder</a> web sites.</p>

<p>The basic idea is that they categorize (dare I say catalog?) <a href="http://www.bbc.co.uk/programmes">television and radio content</a> using wikipedia/dbpedia as a <a href="http://en.wikipedia.org/wiki/Controlled_vocabulary">controlled vocabulary</a>. Just doing this relatively simple thing means that they can create another site like the <a href="http://www.bbc.co.uk/wildlifefinder/">Wildlife Finder</a> that provides a topical guide to the natural world (and also happens to use wikipedia/dbpedia as a controlled vocabulary), that then links to their audio and video content. Since the two sites share a common topic vocabulary, they are able to automatically create links from the topic guides to all the radio and television content that are on a particular topic.</p>

<p>For a practical example take a look consider this page for the <a href="http://www.bbc.co.uk/nature/species/Pinus_longaeva">Great Basin Bristlecone Pine</a>:</p>

<p><a href="http://www.bbc.co.uk/nature/species/Pinus_longaeva"><img src="http://inkdroid.org/images/bristlecone.png" style="border: none; width: 80%;" /></a></p>

<p>If you scroll down on the page you'll see a link to a <a href="http://www.bbc.co.uk/programmes/p005fs5p">video clip</a> from David Attenborough's documentary <a href="http://www.bbc.co.uk/programmes/b00lbpcy">Life</a> on the Programmes portion of the website. Now take a step back and consider how these are two separate applications in the BBC enterprise that are able to build a rich network of links between each other. It's the shared controlled vocabulary (in this case dbpedia derived from wikipedia) which allows them to do this.</p>

<p>If you take a peak in the html you'll see the resource has an alternate RDF version:</p>

<pre>
&lt;link rel="alternate" type="application/rdf+xml" href="<a href="http://www.bbc.co.uk/nature/species/Pinus_longaeva.rdf">/nature/species/Pinus_longaeva.rdf</a>" /&gt;
</pre>

<p>The Resource Description Framework (RDF) is really just the best data model we have for describing stuff that's on the Web, and the type of links between resources that are on (and off) the Web. Personally, I prefer to look at RDF as <a href="http://www.w3.org/TeamSubmission/turtle/">Turtle</a> which is pretty easily done with <a href="http://www.dajobe.org/">Dave Beckett</a>'s handy <a href="http://librdf.org/raptor/rapper.html">rapper</a> utility (<code>aptitude install raptor-utils</code> if you are following from home).</p>

<pre>
rapper -o turtle http://www.bbc.co.uk/nature/species/Pinus_longaeva
</pre>

<p>The key bits of the RDF are the description of the Great Basin bristlecone pine:</p>

<pre> 
&lt;http://www.bbc.co.uk/nature/species/Pinus_longaeva&gt;
    rdfs:seeAlso &lt;http://www.bbc.co.uk/nature/species&gt; ;
    foaf:primaryTopic &lt;http://www.bbc.co.uk/nature/species/Pinus_longaeva#species&gt; .

&lt;http://www.bbc.co.uk/nature/species/Pinus_longaeva#species&gt;
    dc:description "Great Basin bristlecone pines are restricted to the mountain ranges of California, Nevada and Utah and have a remarkable ability to survive in this extremely harsh and challenging environment. They grow extremely slowly, and are some of the oldest living organisms in the world. With some aged at almost 5,000 years these amazing trees can reveal information about Earth's climate variations. Amazingly, the leaves, or needles, can remain green for over 45 years." ;
    wo:class &lt;http://www.bbc.co.uk/nature/class/Pinopsida#class&gt; ;
    wo:family &lt;http://www.bbc.co.uk/nature/family/Pinaceae#family&gt; ;
    wo:genus &lt;http://www.bbc.co.uk/nature/genus/Pinus#genus&gt; ;
    wo:growsIn &lt;http://www.bbc.co.uk/nature/habitats/Mountain#habitat&gt;, &lt;http://www.bbc.co.uk/nature/habitats/Temperate_coniferous_forest#habitat&gt; ;
    wo:kingdom &lt;http://www.bbc.co.uk/nature/kingdom/Plant#kingdom&gt; ;
    wo:name &lt;http://www.bbc.co.uk/nature/species/Pinus_longaeva#name&gt; ;
    wo:order &lt;http://www.bbc.co.uk/nature/order/Pinales#order&gt; ;
    wo:phylum &lt;http://www.bbc.co.uk/nature/phylum/Pinophyta#phylum&gt; ;
    a wo:Species ;
    rdfs:label "Great Basin bristlecone pine" ;
    <span style="color: red">owl:sameAs &lt;http://dbpedia.org/resource/Pinus_longaeva&gt; ;</span>
    foaf:depiction &lt;http://open.live.bbc.co.uk/dynamic_images/naturelibrary_640_credits/downloads.bbc.co.uk/earth/naturelibrary/assets/p/pi/pinus_longaeva/pinus_longaeva_1.jpg&gt; .
</pre>

<p>And then the description of the clip that is related to the topic of Great Basin bristlecone pine:</p>

<pre>
&lt;http://www.bbc.co.uk/programmes/p005fs5p#programme&gt;
    dc:title "Ancient bristlecones" ;
    po:subject &lt;http://www.bbc.co.uk/nature/species/Pinus_longaeva#species&gt; ;
    a po:Clip .
</pre>

<p>And we can follow our nose and fetch a description of the  <a href="http://www.bbc.co.uk/programmes/p005fs5p">Ancient bristelcones clip</a>:</p>

<pre>
rapper -o turtle http://www.bbc.co.uk/programmes/p005fs5p
</pre>

<p>Which tells us lots of stuff, like that it's a documentary part of the science and nature genre, gives us a synopsis, and even links the clip to the episode and series it is a part of:</p>

<pre>
&lt;http://www.bbc.co.uk/programmes/p005fs5p#programme&gt;
    dc:title "Ancient bristlecones" ;
    po:format &lt;http://www.bbc.co.uk/programmes/formats/documentaries#format&gt; ;
    po:genre &lt;http://www.bbc.co.uk/programmes/genres/factual/scienceandnature#genre&gt;, &lt;http://www.bbc.co.uk/programmes/genres/factual/scienceandnature/natureandenvironment#genre&gt; ;
    po:long_synopsis """Bristlecone pines live at the limit of life, above 3,000m in the mountains of  western America. Almost continuous freezing temperatures and savage winds make life so tough, that these bristlecones only grow for six weeks of the year.

Everything is about conserving energy.They hardly ever shed their needles which can last more than 30 years. After centuries of being blasted by storms a full grown tree still survives with only a strip of bark a few inches wide.

These trees live life at such a slow pace they can reach a great age. Some are over 5,000 years old. It has been said of the bristlecones that to live here is to take a very long time to die.""" ;
    po:medium_synopsis "Living above 3,000 metres, North America's bristlecones cope with freezing temperatures and battering winds by only growing for six weeks of the year. But seeing as they may live for more than 5,000 years, that's still a fair bit of growing in a single lifetime. Slowly but surely does it..." ;
    po:short_synopsis "The world's oldest trees have survived 5,000 years of harsh conditions." ;
    po:version &lt;http://www.bbc.co.uk/programmes/p005fs5r#programme&gt; ;
    a po:Clip .

&lt;http://www.bbc.co.uk/programmes/b00lbpcy#programme&gt;
    po:clip &lt;http://www.bbc.co.uk/programmes/p005fs5p#programme&gt; ;
    a po:Series .

&lt;http://www.bbc.co.uk/programmes/b00p90d6#programme&gt;
    po:clip &lt;http://www.bbc.co.uk/programmes/p005fs5p#programme&gt; ;
    a po:Episode .
</pre>

<p>Conspicuously missing from this description is something like:</p>

<pre>
&lt;http://www.bbc.co.uk/programmes/p005fs5p#programme&gt;
    dcterms:subject &lt;http://dbpedia.org/resource/Pinus_longaeva&gt; .
</pre>

<p>But presumably it's hiding underneath the covers in the Programmes database, and that's what lets them link stuff up?</p>

<p><img src="http://web.archive.org/web/20110130081539/http://uberblic.com/images/logo.png" style="float: right; margin-left: 10px" /> Also very interesting was <a href="http://blog.georgikobilarov.com/">Georgi Kobilarov</a>'s description of <a href="http://uberblic.org/2010/01/uberblic-release/">Uberblic</a>.  Since Georgi helped create <a href="http://dbpedia.org">dbpedia</a> and is now consulting with the BBC, it seems like uberblic is positioning itself to provide a platform for the BBC to have it's own local cache of the world of Linked Data. Having a local curated view of the world of linked data is something <a href="http://onebiglibrary.net">Dan Chudnov</a> identified as a real need at the <a href="http://wiki.code4lib.org/index.php/LinkedData">first Linked Data workshop at code4lib 2009</a> for <a href="http://onebiglibrary.net/story/code4lib-2009-talk-on-caching-and-proxying-linked-data">caching and proxying linked data</a>...so it is really cool to see solutions starting to appear in this space...and for them to be adopted by institutions like the BBC.</p>

<p>Georgi demo'd how an edit on wikipedia would be immediately reflected in the structured data available from uberblic. It was a real time update, and extremely impressive. It <a href="http://twitter.com/gkob/status/9734074624">looks like</a> part of the uberblic strategy is to crawl BBC's web site and other pockets of Linked Data to enable the sort of linking across web properties that Tom described. I'd also surmise given the realtime nature of this that Georgi is bypassing dbpedia dumps and using the Wikipedia <a href="http://en.wikipedia.org/w/index.php?title=Special:RecentChanges&feed=atom">changes atom feed</a> in conjunction with <a href="http://dbpedia.svn.sourceforge.net/viewvc/dbpedia/extraction/extractors/">extractors</a> that were built as part of the dbpedia project. But I'd love to know more of the mechanics of the update. It also would be interesting to know if uberblic has a notion of versions.</p>

<p>The really powerful message that the BBC is helping promote is this idea that good websites are APIs. Tom mentioned <a href="http://blog.whatfettle.com/">Paul Downey's</a> notion that <a href="http://blog.whatfettle.com/2007/01/11/good-web-apis-are-just-web-sites/">Web APIs Are Just Web Sites</a>. It's a subtle but extremely important point that I learned primarily working closely with <a href="http://eikeon.com">Dan Krech</a> for a year or so. It's an unfortunate side effect of lots market driven talk about web2.0, web3.0 and Linked Data in general that this simple REST message gets lost.  We took it seriously in the design of the <a href="http://chroniclingamerica.loc.gov/about/api/">"API"</a> at the Library of Congress' <a href="http://chroniclingamerica.loc.gov/">Chronicling America</a>. It's also something I tried to talk about later in the week at dev8d when I had to quickly put a presentation together:</p>

<iframe src="http://docs.google.com/present/embed?id=dv89m3d_374cpqzfnc9" frameborder="0" width="410" height="342"></iframe>

<p>The slides probably won't make much sense on their own, but the basic message was that we often hear about Linked Data in terms of pushing all your data to some triple store so you can start querying it with <a href="http://code.google.com/p/linked-data-api/wiki/Specification">SPARQL</a> and doing inferencing, and suddenly you're going to be sitting pretty, totally jacked up on the Semantic Web.</p>

<p>If you are like me, you've already got databases where things are modeled, and you've created little web apps that have extracted information from the databases and put them on the web as HTML docs for people around the world to read (queue some mid 1990s grunge music). Expecting people to chuck away the applications and technology stacks they have simply to say they do Linked Data is wishful thinking. What's missing is a simple migration strategy that would allow web publishers to easily recognize the value in publishing the contents of their database as Linked Data, and how it complements the HTML (and XML, JSON) publishing they are currently doing. My advice to folks at dev8d boiled down to:</p>

<ul>
    <li>Keep modelling your stuff how you like</li>
    <li>Identify your stuff with Cool URIs in your webapps</li>
    <li>Link your stuff together in HTML</li>
    <li>Link to machine friendly formats (RSS, Atom, JSON, etc)</li>
    <li>Use RDF to make your database available on the web using vocabularies other people understand.</li>
        <li>Start thinking about technologies like SPARQL that will let you query pools and aggregated views of your data.</li>
    <li>Consider joining the <a href="http://lists.w3.org/Archives/Public/public-lod/">public-lod discussion list and joining the conversation.</a></li>
</ul>

<p>I got some nice comments afterwards from <a href="http://www.ninebynine.org/">Graham Klyne</a>,  <a href="http://users.ecs.soton.ac.uk/hg/">Hugh Glaser</a>, <a href="http://blogs.ukoln.ac.uk/adrianstevenson/">Adrian Stevenson</a> and <a href="http://vphill.com/">Mark Phillips</a> so I felt pretty happy...granted most of the hard line Linked Data folks had already left a couple days earlier.</p>

<p>So some really exciting stuff is going on at the BBC. They are using Linked Data in a practical way that benefits their enterprise in real ways. I'm crossing my fingers and hoping that the value of what is going on here is recognized, and the various cuts that are going on won't affect any of the fine work they are doing on improving the Web.</p>

<p>For more information check out the <a href="http://www.w3.org/2001/sw/sweo/public/UseCases/BBC/">Semantic Web Case Study</a> they folks at the BBC wrote summarizing their approach for the W3C.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1701</wp:post_id>
		<wp:post_date>2010-03-02 13:13:00</wp:post_date>
		<wp:post_date_gmt>2010-03-02 20:13:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-middle-way-for-linked-data-at-the-bbc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bbc"><![CDATA[bbc]]></category>
		<category domain="post_tag" nicename="dbpedia"><![CDATA[dbpedia]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="radio"><![CDATA[radio]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="semantic-web"><![CDATA[semantic web]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="television"><![CDATA[television]]></category>
		<category domain="post_tag" nicename="uberblic"><![CDATA[uberblic]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82000</wp:comment_id>
			<wp:comment_author><![CDATA[Apis and APIS a wildlife ontology &laquo; Derivadow.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://derivadow.com/2010/03/02/apis-and-apis-a-wildlife-ontology/</wp:comment_author_url>
			<wp:comment_author_IP>72.233.2.71</wp:comment_author_IP>
			<wp:comment_date>2010-03-02 15:36:04</wp:comment_date>
			<wp:comment_date_gmt>2010-03-02 22:36:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Finder &#8211; how we&#8217;re starting to publish and consume data on the web. Ed Summers has a great write up of what we&#8217;re doing I&#8217;ve also published my slides [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86682</wp:comment_id>
			<wp:comment_author><![CDATA[A Symbiotic Relationship: Information Architecture and the Semantic Web | ANTELOPE AS DOCUMENT]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://antelopeasdocument.wordpress.com/2013/12/02/a-symbiotic-relationship-information-architecture-and-the-semantic-web/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-12-02 12:55:35</wp:comment_date>
			<wp:comment_date_gmt>2013-12-02 19:55:35</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Summers, E. (04/02/10). a middle way for linked data at the bbc. Inkdroid. Retrieved December 1, 2013, from http://inkdroid.org/2010/03/02/a-middle-way-for-linked-data-at-the-bbc/ [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386014135.9631540775299072265625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386014523.3231070041656494140625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>full link graph?</title>
		<link>http://inkdroid.org/2010/03/03/full-link-graph/</link>
		<pubDate>Wed, 03 Mar 2010 12:42:36 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1798</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://norvig.com/">Peter Norvig</a> of Google <a href="http://www.youtube.com/watch?v=hE7k0_9k0VA#t=25m38s">mentioned</a> Linked Data in his interview with <a href="http://www.youtube.com/watch?v=hE7k0_9k0VA">Reddit Ask Me Anything</a> (thanks <a href="http://twitter.com/gromgull/status/9918829831">Gunnar</a>)

<blockquote>
So right from the start researchers are writing code that use our main APIs that
are using the data that everyone else uses. If you want some web pages you use 
the full copy of the web. If you want some <b>link<strike>ed</strike> data</b> you use the full link 
graph.
</blockquote>

<em>Update:</em> Richard Cyganiak correctly <a href="http://twitter.com/cygri/status/9923214244">points out</a> that Norvig said "link data" not "linked data". :-)  At least we won't have to ask Google if they are using SPARQL and RDF now ...

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1798</wp:post_id>
		<wp:post_date>2010-03-03 05:42:36</wp:post_date>
		<wp:post_date_gmt>2010-03-03 12:42:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>full-link-graph</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"82016";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82016</wp:comment_id>
			<wp:comment_author><![CDATA[Michael Hausenblas]]></wp:comment_author>
			<wp:comment_author_email>michael.hausenblas@deri.org</wp:comment_author_email>
			<wp:comment_author_url>http://mhausenblas.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>140.203.154.11</wp:comment_author_IP>
			<wp:comment_date>2010-03-03 06:09:59</wp:comment_date>
			<wp:comment_date_gmt>2010-03-03 13:09:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice one, but I doubt he really was referring to Linked Data. Rather what Dave mentioned [1]?

Minor nit: the youtube link you provided doesn't really work (ie. it doesn't jump to the 25min 38s). The correct syntax would be: http://www.youtube.com/watch?v=hE7k0_9k0VA#t=25m38s

Cheers,
Michael


[1] http://twitter.com/der42/status/9922707578]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>338</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82017</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2010-03-03 06:53:28</wp:comment_date>
			<wp:comment_date_gmt>2010-03-03 13:53:28</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks Michael, I fixed the YouTube link. Apologies for the mis-information in my post. It would still be interesting to know if their <em>full link graph</em> supported different edge types. But I guess even if they are doing computations over a graph of nodes identified w/ URLs and linked together with different predicates that still wouldn't be RDF, SPARQL and consequently Linked Data... hohum]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>history and genealogy at semwebdc</title>
		<link>http://inkdroid.org/2010/03/22/history-and-genealogy-at-semwebdc/</link>
		<pubDate>Mon, 22 Mar 2010 08:41:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1809</guid>
		<description></description>
		<content:encoded><![CDATA[<div xmlns:cc="http://creativecommons.org/ns#" about="http://www.flickr.com/photos/spine/2076729686/" style="float: left; margin-right: 10px; text-align: center;"><a href="http://www.flickr.com/photos/spine/2076729686"><img src="http://inkdroid.org/images/familytree.jpg" style="height: 250px" /></a><br /><a href="http://www.flickr.com/photos/spine/">spine</a> <a rel="license" href="http://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a>
</div> 

Last week's <a href="http://semweb.meetup.com/31/">Washington DC Semantic Web Meetup</a> focused on <a href="http://semweb.meetup.com/31/calendar/12368900/">History and Genealogy Semantics</a>. It was a pretty small, friendly crowd (about 15-20) that met for the first time at the Library of Congress. The group included folks from <a href="http://pbs.org">PBS</a>, the <a href="http://archives.gov">National Archives</a>, the <a href="http://loc.gov">Library of Congress</a>, and the <a href="http://chnm.gmu.edu/">Center for History and New Media</a>--as well as some regulars from the <a href="http://www.eccnet.com/xmlug/">Washington DC SGML/XML Users Group</a>.

<a href="http://semweb.meetup.com/31/members/6323472/">Brian Eubanks</a> gave a <a href="http://files.meetup.com/987383/History%20and%20Genealogy%20Semantics.pdf">presentation</a> on what the Semantic Web, <a href="http://linkeddata.org">Linked Data</a> and specifically <a href="http://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> and <a href="http://www.w3.org/2004/03/trix/">Named Graphs</a> have to offer genealogical research. He took us on a tour through a variety of websites, such as <a href="http://www.blm.gov/or/landrecords">Land Records Database</a> at the Bureau of Land Management, <a href="http://www.ancestry.com/">Ancestry.com</a>, <a href="http://www.footnote.com/">Footnote</a> and <a href="http://books.google.com">Google Books</a> and made a strong case for using RDF to link these sorts of documents with a family tree. 

As more and more historic records make their way online as Web Documents with URIs, RDF becomes an increasingly useful data model for providing provenance and source information for a family tree. On sites like Ancestry.com it is important to understand the provenance of genealogical assertions, since Ancestry.com allows you to merge other people's family trees into your own, based on likely common ancestors. In situations like this researchers need to be able to evaluate the credibility or truthfulness of other people's trees--and being able to source the family tree links to the documents that support them is an essential part of the equation.

Along the way Brian let people know about a variety of vocabularies that are available for making assertions that are of value to genealogical research: 

<ul>
<li><a href="http://www.w3.org/TR/rdfcal/#Locations">rdfcal</a> : for Events</li>
<li><a href="http://vocab.org/bio/0.1/.html">BIO</a> : for biographical information</li>
<li><a href="http://vocab.org/relationship/.html">Relationship</a> : for describing the links between people</li>
<li><a href="http://xmlns.com/foaf/spec/">FOAF</a> : for describing people</li>
<li><a href="http://www4.wiwiss.fu-berlin.de/bizer/TriG/">TriG</a> : for identifying the assertions that a researcher makes and linking them to a given document</li>
</ul>

The beautiful thing about RDF for me, is that it's possible to find and use these vocabularies in concert, and I'm not tempted to create the-greatest-genealogy-vocabulary that does it all. In addition, Brian pointed out that sites like <a href="http://dbpedia.org">dbpedia</a> and <a href="http://geonames.org">geonames</a> are great sources of names (URIs) for people, places and events that can be used in building descriptions. Brian has started the <a href="http://groups.google.com/group/history-and-genealogy-semantics-wg/">History and Genealogy Semantics Working Group</a> which has an open membership, and encourages anyone with interest in this area to join. While writing this post I happened to run across a Wikipedia page about <a href="http://en.wikipedia.org/wiki/Family_tree_mapping">Family Tree Mapping</a>, which indicated that some genealogical software already supports geocoding family trees. As usual it seems like the geo community is leading the way in making semantics on the web down to earth and practical. 

I followed Brian by giving a brief talk about the <a href="http://chroniclingamerica.loc.gov">Chronicling America</a>, which is the web front-end for data collected by <a href="http://en.wikipedia.org/wiki/National_Digital_Newspaper_Program">National Digital Newspaper Program</a>, which in turn is a joint project of the <a href="http://loc.gov">Library of Congress</a> and the <a href="http://neh.gov">National Endowment for the Humanities</a>. After giving a brief overview of the program, I described how we were naturally led to using Linked Data and embracing a generally <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">RESTful</a> approach by a few factors:

<ul>
<li>a need to create persistent <a href="http://www.w3.org/Provider/Style/URI">Cool URIs</a> for newspaper titles, issues and pages so that people could reference them.</li>
<li>the desire to make views available for the <a href="http://chroniclingamerica.loc.gov/awardees/">institutions</a> around the United States that supply us with <a href="http://chroniclingamerica.loc.gov/batches/">data</a></li>
<li>the need to make our data available as a participant in the  <a href="http://www.diggingintodata.org/">Digging Into Data Challenge</a></li>
<li>a desire to kick the tires on the relatively new <a href="http://www.openarchives.org/ore/1.0/vocabulary">Open Archives Initiative Object Reuse and Exchange vocabulary</a> for describing aggregations of resources on the Web so that they can be meaningfully harvested
</li></ul>

One thing that I learned during Brian's presentation is that sites like <a href="http://footnote.com">Footnote</a> are not only <a href="http://fcw.com/articles/2009/09/29/online-collection-helps-people-remember-holocaust.aspx">going around</a> digitizing historic collections for inclusion in their service,  but they also give their subscribers a rich editing environment to search and <em>annotate</em> document text. These annotations are exactly the sort of stuff that would be perfect to represent as and RDF graph, if you wanted to serialize the data. In fact the NSF funded <a href="http://www.openannotation.org/">Open Annotation Collaboration</a> project is exploring patterns and emerging best practices in this area. I've had it in the back of my mind that allowing users to annotate page content in Chronicling America would be a really nice feature to have. If not at chroniclingamerica.loc.gov proper, then perhaps showing how it could be done by a 3rd party using the API. To some extent we're already seeing annotation happening in Wikipedia, where people are creating links to newspaper pages and titles in their entries, which we can see in the <a href="http://en.wikipedia.org/wiki/Referrer">referrer</a> information in our web server logs. <em>Update: and I just learned that wikipedia themselves provide a <a href="http://en.wikipedia.org/w/index.php?title=Special:LinkSearch">service</a> that allows you to discover entries that have outbound links to a particular site, like <a href="http://en.wikipedia.org/w/index.php?title=Special:LinkSearch&target=http://chroniclingamerica.loc.gov&limit=500&offset=0">chroniclingamerica.loc.gov</a>.</em>

Speaking of the API (which really is just REST) if you are interested in learning more about it check out the  <a href="http://chroniclingamerica.loc.gov/about/api/">API Document</a> that <a href="http://onebiglibrary.net">Dan Chudnov</a> prepared. I also made my <a href="http://docs.google.com/present/view?id=dv89m3d_38246f986km">slides</a> available, hopefully the speaker notes provide a bit more context for what I talked about when showing images of various things.

<iframe src="http://docs.google.com/present/embed?id=dv89m3d_38246f986km" frameborder="0" width="410" height="342" style="float: left; margin-right: 10px;"></iframe>

Afterwards a bunch of us headed across the street to have a drink. I was really interested to hear from <a href="http://twitter.com/samdeng">Sam Deng</a> that (like the group I work in at LC) <a href="http://pbs.org">PBS</a> are big <a href="http://python.org">Python</a> and <a href="http://djangoproject.com">Django</a> shop. We're going to try to get a little brown bag lunch going on between PBS and LC to talk about their use of Django on <a href="http://aws.amazon.com/ec2/">Amazon EC2</a>, as well as software like <a href="http://celeryproject.org/">Celery</a> for managing asynchronous task queues. 

Also, after chatting with <a href="http://twitter.com/GlennClatworthy">Glenn Clatworthy</a> of PBS, I learned that he has been experimenting with making Linked Data views available for their programs. It was great to hear Glenn describe how assigning each program a URI, and leveraging the nature of the web would make a perfect fit for distributing data in the PBS enterprise. It makes me think that perhaps having a session on what the <a href="http://bbc.co.uk">BBC</a> are <a href="http://derivadow.com/2009/03/31/linking-bbccouk-to-the-linked-data-cloud/">doing</a> with Linked Data would be timely?]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1809</wp:post_id>
		<wp:post_date>2010-03-22 01:41:57</wp:post_date>
		<wp:post_date_gmt>2010-03-22 08:41:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>history-and-genealogy-at-semwebdc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="dc"><![CDATA[dc]]></category>
		<category domain="post_tag" nicename="django"><![CDATA[django]]></category>
		<category domain="post_tag" nicename="genealogy"><![CDATA[genealogy]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[history]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="oai-ore"><![CDATA[oai-ore]]></category>
		<category domain="post_tag" nicename="pbs"><![CDATA[pbs]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"82432";i:1;s:5:"82433";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82433</wp:comment_id>
			<wp:comment_author><![CDATA[Ryan Shaw]]></wp:comment_author>
			<wp:comment_author_email>ryan.shaw@stanfordalumni.org</wp:comment_author_email>
			<wp:comment_author_url>http://people.ischool.berkeley.edu/~ryanshaw/wordpress/bio</wp:comment_author_url>
			<wp:comment_author_IP>169.229.122.132</wp:comment_author_IP>
			<wp:comment_date>2010-04-02 11:33:17</wp:comment_date>
			<wp:comment_date_gmt>2010-04-02 18:33:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Sorry, that should read "is intended to be more appropriate".]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>242</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82413</wp:comment_id>
			<wp:comment_author><![CDATA[Now I&#8217;M seeing the semantic web everywhere I look! :-) &laquo; Dr. MacCall&#8217;s Metadata Spot]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://daryl.slis.ua.edu/slis/courses/ls590/spring2009/maccall/01/35/wordpress/?p=175</wp:comment_author_url>
			<wp:comment_author_IP>130.160.88.101</wp:comment_author_IP>
			<wp:comment_date>2010-03-29 13:42:46</wp:comment_date>
			<wp:comment_date_gmt>2010-03-29 20:42:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Part of a genealogy community&#8217;s project: history and genealogy at semwebdc [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82432</wp:comment_id>
			<wp:comment_author><![CDATA[Ryan Shaw]]></wp:comment_author>
			<wp:comment_author_email>ryan.shaw@stanfordalumni.org</wp:comment_author_email>
			<wp:comment_author_url>http://people.ischool.berkeley.edu/~ryanshaw/wordpress/bio</wp:comment_author_url>
			<wp:comment_author_IP>169.229.122.132</wp:comment_author_IP>
			<wp:comment_date>2010-04-02 11:32:18</wp:comment_date>
			<wp:comment_date_gmt>2010-04-02 18:32:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<a href="http://linkedevents.org/ontology/" rel="nofollow">LODE</a>is a vocabulary that in intended to more appropriate for historical events than calendar-focused vocabularies. There's a <a href="http://escholarship.org/uc/item/4pd6b5mh" rel="nofollow">paper</a> with more information, if you're interested.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>242</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>research ideas for library linked data</title>
		<link>http://inkdroid.org/2010/04/18/research-ideas-for-library-linked-data/</link>
		<pubDate>Mon, 19 Apr 2010 04:49:26 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1909</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://commons.wikimedia.org/wiki/File:PoeCorbeau.png"><img src="http://inkdroid.org/images/crow.png" width="150" style="margin-right: 10px; float: left;" /></a></p>

<p>The past few weeks have seen some pretty big news for Library <a href="http://linkeddata.org">Linked Data</a>. On April 7th the <a href="http://www.oszk.hu/">Hungarian National Library</a> <a href="http://lists.w3.org/Archives/Public/public-lod/2010Apr/0155.html">announced</a> that its entire library catalog, digital library holdings, and name/subject authority data are now available as Linked Data. Then just a bit more than a week later, on April 16th the <a href="http://www.d-nb.de">German National Library</a> <a href="https://listserv.nd.edu/cgi-bin/wa?A2=ind1004&L=NGC4LIB&T=0&F=&S=&P=31709">announced</a> that it was making its name and subject authority files available as Linked Data.</p>

<p>This adds to the pioneering work that the <a href="http://www.kb.se/">Royal Library of Sweden</a> has already done in making all of its catalog and authority data available, which they <a href="http://article.gmane.org/gmane.culture.libraries.ngc4lib/4617">announced</a> almost two years ago now. Add to this that <a href="http://oclc.org">OCLC</a> is also <a href="http://outgoing.typepad.com/outgoing/2009/09/viaf-as-linked-data.html">publishing</a> the <a href="http://viaf.org">Virtual International Authority File</a> as Linked Data, and that the <a href="http://loc.gov">Library of Congress</a> also makes its subject authority data <a href="http://web.archive.org/web/20110720034058/http://id.loc.gov:80/authorities">available</a> as Linked Data and things are starting to get interesting.</p>

<p>About 16 months ago at the <a href="http://web.archive.org/web/20130722011956/http://dc2008.de/">Dublin Core Conference</a> in Berlin  <a href="http://alimanfoo.wordpress.com/">Alistair Miles</a> predicted that we'd see several implementations of Linked Data at major libraries within the year. I must admit, while I was sympathetic to the cause, I was also pretty skeptical that this would come to pass. But here we are, just a bit past a year and two national libraries and a major library data distributor have decided to publish some of their data assets as Linked Data.</p>

<p>Hey Al, <a href="http://en.wikipedia.org/wiki/Eating_crow">crow</a> never tasted so good...</p>

<p>So now it's starting to feel like there's enough extant library Linked Data to start looking at patterns of usage, to see if there are any emerging best practices we could work towards. In particular I think it would be interesting to take a look at:</p>

<ul>
<li>What vocabularies are being used, and is there emerging consensus about which to use?</li>
<li>What licenses (if any) are associated with the data?</li>
<li>How much linking and interlinking is going on?</li>
<li>What sorts of mechanisms does the publisher offer for getting the data: sitemap, feeds, SPARQL, bulk download?</li>
<li>What is the quality of the data: granularity, link integrity, vocabulary usage.</li>
<li>What approaches to identifiers for "real world things" have publishers taken: hash, slash, 303, PURLs, reuse of traditional identifiers, etc.</li>
<li>What are the relative sizes of the pools of library linked data?</li>
<li>How are updates being managed?

</li></ul>

<p>Tomorrow I'm meeting with some folks at the <a href="http://ils.unc.edu/mrc/">Metadata Research Center</a> at the <a href="http://sils.unc.edu/">School of Information and Library Science </a>at the University of North Carolina to talk about their <a href="https://www.nescent.org/sites/hive/">HIVE</a> project. Barbara Tillett and Libby Dechman of LC are also here to talk about the use of <a href="http://en.wikipedia.org/wiki/Library_of_Congress_Subject_Headings">LCSH</a>, <a href="http://viaf.org/">VIAF</a> and <a href="http://www.rdaonline.org/">RDA</a>. I'm hoping to convince some of the folks at the MRC that answering some of these questions about the use of Linked Data in libraries could be valuable to the library research community. The rumored W3C Incubator Group for Cultural Heritage Institutions and the Semantic Web couldn't come at a better time.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1909</wp:post_id>
		<wp:post_date>2010-04-18 21:49:26</wp:post_date>
		<wp:post_date_gmt>2010-04-19 04:49:26</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>research-ideas-for-library-linked-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="germany"><![CDATA[germany]]></category>
		<category domain="post_tag" nicename="hungary"><![CDATA[hungary]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="post_tag" nicename="research"><![CDATA[research]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="sweden"><![CDATA[sweden]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84332</wp:comment_id>
			<wp:comment_author><![CDATA[Describing the &#8220;things&#8221;: the RDF terms used (part 1) &laquo; LOCAH Project]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blogs.ukoln.ac.uk/locah/2011/03/08/describing-the-things-the-rdf-terms-used-part-1/</wp:comment_author_url>
			<wp:comment_author_IP>138.38.146.12</wp:comment_author_IP>
			<wp:comment_date>2011-03-08 06:36:35</wp:comment_date>
			<wp:comment_date_gmt>2011-03-08 13:36:35</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] quite a bit to go on, rather less so for the archives case. For a few pointers, see e.g. this review post by Ed Summers (itself already nearly a year [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1299621345.9761";s:7:"message";s:36:"ed reported this comment as not spam";s:5:"event";s:10:"report-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_user_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_user</wp:meta_key>
				<wp:meta_value><![CDATA[ed]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1299621358.9416";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[true]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1299591395.4206";s:7:"message";s:35:"Akismet caught this comment as spam";s:5:"event";s:10:"check-spam";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85339</wp:comment_id>
			<wp:comment_author><![CDATA[All about Linked Data &laquo; Feral Librarian]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://chrisbourg.wordpress.com/2010/06/15/all-about-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-02-06 10:13:51</wp:comment_date>
			<wp:comment_date_gmt>2012-02-06 17:13:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Sommers on research ideas for library linked data [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1328548431.3805";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1329807424.962";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Dear Footnote Bot</title>
		<link>http://inkdroid.org/2010/04/22/dear-footnote-bot/</link>
		<pubDate>Fri, 23 Apr 2010 02:35:54 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1945</guid>
		<description></description>
		<content:encoded><![CDATA[Thanks for taking an interest in the historic content on a <a href="http://chroniclingamerica.loc.gov">website</a> I help run. We want to see the <a href="http://www.loc.gov/ndnp/">NDNP</a> newspaper content get crawled, indexed and re-purposed in as many places as possible. So we appreciate the time and effort you are spending on getting the OCR XML and JPEG2000 files  into <a href="http://footnote.com">Footnote</a>. I am a big fan of <a href="http://footnote.com">Footnote</a> and what you are <a href="http://go.footnote.com/nara/">doing</a> to help historical/genealogical researchers who subscribe to your product.

But since I have your ear, it would be nice if you identified yourself as a bot. Right now you are pretending to be Internet Explorer:

<pre style="width: 95%;">
38.101.149.14 - - [22/Apr/2010:18:38:39 -0400] "GET /lccn/sn86069496/1909-09-08/ed-1/seq-8.jp2 HTTP/1.1" 200 3170304 "-" "Internet Explorer 6 (MSIE 6; Windows XP)" "*/*" "-" "No-Cache"
</pre>

Oh, and could you stop sending the <em>Pragma: No-Cache</em> header with every HTTP request? We have a <a href="http://en.wikipedia.org/wiki/Reverse_proxy">reverse-proxy</a> in front of our dynamic content so that we don't waste CPU cycles regenerating pages that haven't changed. It's what allows us to make our content available to well behaved web crawlers. But every request you send bypasses our cache, and makes our site to do extra work.

It's true, we can ignore your request to bypass our cache. In fact, that's what we're <a href="http://httpd.apache.org/docs/2.0/mod/mod_cache.html#cacheignorecachecontrol">doing now</a>. This means we can't shift-reload in our browser to force the content to refresh--but we'll manage. Maybe you could be a good citizen of the Web and send an <a href="http://diveintopython.org/http_web_services/http_features.html#d0e27689"><em>If-Modified-Since</em></a> header--or perhaps just don't send <em>Pragma: No Cache</em>?

Identifying yourself with a <em>User-Agent</em> string like <em>"footbot/0.1 +(http://footnote.com/footbot)"</em> would be neighborly too :-)

Yours Sincerely,
Ed

PS 

<pre>
ed@curry:~$ whois 38.101.149.14
...
%rwhois V-1.5:0010b0:00 rwhois.cogentco.com
38.101.149.14
network:ID:NET4-2665950018
network:Network-Name:NET4-2665950018
network:IP-Network:38.101.149.0/24
network:Postal-Code:84042
network:State:UT
network:City:Linden
network:Street-Address:355 South 520 West
network:Org-Name:iArchives Inc dba Footnote
network:Tech-Contact:ZC108-ARIN
network:Updated:2008-05-21 13:05:26
network:Updated-by:Gus Reese
</pre>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1945</wp:post_id>
		<wp:post_date>2010-04-22 19:35:54</wp:post_date>
		<wp:post_date_gmt>2010-04-23 02:35:54</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>dear-footnote-bot</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="crawling"><![CDATA[crawling]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="ndnp"><![CDATA[ndnp]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"82660";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82660</wp:comment_id>
			<wp:comment_author><![CDATA[eric]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://epugh.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>209.145.68.230</wp:comment_author_IP>
			<wp:comment_date>2010-04-26 06:22:40</wp:comment_date>
			<wp:comment_date_gmt>2010-04-26 13:22:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[A really wonderful post!  Entertaining and educational, post serves as a great overview of caching!   

Often I think crawlers mimic browsers to ensure they get what the browser gets.  What would you think of footnote declaring itself as "footnote" and then if the site rejects it, falling back to declaring itself as "IE"?   

I tried to write a crawler for the public pages of a Facebook group, and in that case Facebook was intentionally blocking access if you didn't look like a browser run by a person!   Lots of complex JavaScript calls, arbitrary ID's, etc to make scraping a page harder.  

Obviously you are happy to have the site be crawled, so it's a bit different.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>339</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>wee bit</title>
		<link>http://inkdroid.org/2010/04/30/wee-bit/</link>
		<pubDate>Fri, 30 Apr 2010 11:59:24 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1983</guid>
		<description></description>
		<content:encoded><![CDATA[As is my custom, this morning I asked Zoia (the bot in <a href="irc://freenode.net/code4lib">#code4lib</a>) for <a href="http://www.computerhistory.org/tdih/">this day in history</a> from the <a href="http://www.computerhistory.org">Computer History Musuem</a>. Lately I've been filtering it through the Pirate plugin, which transforms arbitrary text into something a pirate might say. Anyhow, today's was pretty humorous. 

<pre>
11:32 < edsu> @pirate [tdih]
11:32 < zoia> edsu: Claude Shannon be born in Gaylord, Michigan.  Known as th' 
              inventor 'o information theory, Shannon be th' first to use th' 
              word "wee bit."  Shannon, a contemporary 'o Johny-boy von 
              Neumann, Howard Aiken, 'n Alan Turin', sets th' stage fer th' 
              recognition 'o th' basic theory 'o information that could be 
              processed by th' machines th' other pioneers developed.  He 
              investigates information distortion, redundancy 'n noise, 'n (1 
              more message)
11:33 < edsu> @more
11:33 < zoia> edsu: provides a means fer information measurement.  He 
              identifies th' wee bit as th' fundamental unit 'o both data 'n 
              computation.
</pre>

Happy Birthday Cap'n Shannon.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1983</wp:post_id>
		<wp:post_date>2010-04-30 04:59:24</wp:post_date>
		<wp:post_date_gmt>2010-04-30 11:59:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wee-bit</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="computers"><![CDATA[computers]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[history]]></category>
		<category domain="post_tag" nicename="humor"><![CDATA[humor]]></category>
		<category domain="post_tag" nicename="pirates"><![CDATA[pirates]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{i:0;s:5:"82663";i:1;s:5:"82664";i:2;s:5:"82668";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82664</wp:comment_id>
			<wp:comment_author><![CDATA[Michael J. Giarlo]]></wp:comment_author>
			<wp:comment_author_email>michael.giarlo@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://lackoftalent.org/michael/</wp:comment_author_url>
			<wp:comment_author_IP>65.220.4.200</wp:comment_author_IP>
			<wp:comment_date>2010-04-30 05:44:53</wp:comment_date>
			<wp:comment_date_gmt>2010-04-30 12:44:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Are those sports people, David?

...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>67</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82663</wp:comment_id>
			<wp:comment_author><![CDATA[dbrunton]]></wp:comment_author>
			<wp:comment_author_email>dbrunton@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawkuMzjlM81pM-JvzFsoNviuHmLfn6YLydc</wp:comment_author_url>
			<wp:comment_author_IP>98.117.41.142</wp:comment_author_IP>
			<wp:comment_date>2010-04-30 05:21:55</wp:comment_date>
			<wp:comment_date_gmt>2010-04-30 12:21:55</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Happy Birthday Claude Shannon!  For what it's worth, in the paper where Shannon first used the word "bit" in print, he gave credit to John Tukey for coining the term, apparently also in the room was Johnny von Neumann.  Man, I would have liked to be in that room :-P]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>327</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82669</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2010-04-30 08:34:59</wp:comment_date>
			<wp:comment_date_gmt>2010-04-30 15:34:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<pre>
15:34 < edsu> @band add Fantasy Information Theory League
15:34 < zoia> edsu: Band 'Fantasy Information Theory League' added to list
</pre>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82668</wp:comment_id>
			<wp:comment_author><![CDATA[dbrunton]]></wp:comment_author>
			<wp:comment_author_email>dbrunton@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawkuMzjlM81pM-JvzFsoNviuHmLfn6YLydc</wp:comment_author_url>
			<wp:comment_author_IP>98.117.41.142</wp:comment_author_IP>
			<wp:comment_date>2010-04-30 08:31:57</wp:comment_date>
			<wp:comment_date_gmt>2010-04-30 15:31:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[They're in my Fantasy Information Theory league.  The league is run solely via Memex.

*You*.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>327</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>the 5 stars of open linked data</title>
		<link>http://inkdroid.org/2010/06/04/the-5-stars-of-open-linked-data/</link>
		<pubDate>Fri, 04 Jun 2010 20:44:01 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=1987</guid>
		<description></description>
		<content:encoded><![CDATA[<p>While perusing the <a href="http://www.w3.org/2010/06/04-egov-minutes.html">minutes</a> of today's <a href="http://www.w3.org/egov/">w3c egov</a> telecon I noticed mention of Tim Berners-Lee's <a href="http://www.youtube.com/watch?v=ga1aSJXCFe0">Bag of Chips</a> talk at the <a href="http://www.gov2expo.com/gov2expo2010">gov2.0 expo</a> last week in Washington, DC. I actually enjoyed the talk not so much for the bag-of-chips example (which is good), but for the examination of Linked Data as part of a continuum of web publishing activities associated with gold stars, like the ones you got in school. Here they are: <style>
  .stars {color: gold; font-size: 10pt; text-align: right; margin-right: 10px;}
</style></p>

<table style="font-size: smaller;">
  <tr>
    <td class="stars" style="width: 20%;">
      &#x2605;
    </td>
    
    <td>
      make your stuff available on the web (whatever format)
    </td>
  </tr>
  
  <tr>
    <td class="stars">
      &#x2605;&#x2605;
    </td>
    
    <td>
      make it available as structured data (e.g. excel instead of image scan of a table)
    </td>
  </tr>
  
  <tr>
    <td class="stars">
      &#x2605;&#x2605;&#x2605;
    </td>
    
    <td>
      non-proprietary format (e.g. csv instead of excel)
    </td>
  </tr>
  
  <tr>
    <td class="stars">
      &#x2605;&#x2605;&#x2605;&#x2605;
    </td>
    
    <td>
      use URLs to identify things, so that people can point at your stuff
    </td>
  </tr>
  
  <tr>
    <td class="stars">
      &#x2605;&#x2605;&#x2605;&#x2605;&#x2605;
    </td>
    
    <td>
      link your data to other people's data to provide context
    </td>
  </tr>
</table>

<p>I think it's helpful to think of Linked Data in this context, and not to minimize (or trivialize) the effort and the importance of getting the first 3 stars. It was interesting that he didn't mention RDF once (unless I missed it) and talked instead about Linked Data Format. <em>Correction he did mention it, thanks Anders.</em></p>

<iframe width="560" height="315" src="//www.youtube.com/embed/ga1aSJXCFe0" frameborder="0" allowfullscreen></iframe>

<p>The inclusiveness and ambiguity appeals to me.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1987</wp:post_id>
		<wp:post_date>2010-06-04 13:44:01</wp:post_date>
		<wp:post_date_gmt>2010-06-04 20:44:01</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-5-stars-of-open-linked-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="egov"><![CDATA[egov]]></category>
		<category domain="category" nicename="government"><![CDATA[government]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"82693";i:1;s:5:"83920";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82693</wp:comment_id>
			<wp:comment_author><![CDATA[brocadedarkness.myopenid.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://brocadedarkness.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>80.216.173.218</wp:comment_author_IP>
			<wp:comment_date>2010-06-04 23:47:37</wp:comment_date>
			<wp:comment_date_gmt>2010-06-05 06:47:37</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[He actually mentions RDF once, at 07:41.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>340</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82699</wp:comment_id>
			<wp:comment_author><![CDATA[the 5 stars of open linked data at sonagi&#39;s blog!]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.blogweb.co.kr/?p=769</wp:comment_author_url>
			<wp:comment_author_IP>210.102.207.2</wp:comment_author_IP>
			<wp:comment_date>2010-06-09 16:33:13</wp:comment_date>
			<wp:comment_date_gmt>2010-06-09 23:33:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] @source: inkdroid [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82708</wp:comment_id>
			<wp:comment_author><![CDATA[Inside E-Government Lehrgang, John Gotze &laquo; Digital Government 2.0]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://digitalgovernment.wordpress.com/2010/06/18/inside-e-government-lehrgang-john-gotze/</wp:comment_author_url>
			<wp:comment_author_IP>66.135.48.241</wp:comment_author_IP>
			<wp:comment_date>2010-06-18 03:17:07</wp:comment_date>
			<wp:comment_date_gmt>2010-06-18 10:17:07</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] the Britain. Berners-Lee presented a five star stystem for measuring the quality of open data. (cf. inkdroid) Getting the first star is usually most difficult because it includes the fundamental decision to [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82738</wp:comment_id>
			<wp:comment_author><![CDATA[Communities and Collaboration &raquo; Bookmarks for July 7th through July 15th]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://steve-dale.net/2010/07/15/bookmarks-for-july-7th-through-july-15th/</wp:comment_author_url>
			<wp:comment_author_IP>212.227.127.176</wp:comment_author_IP>
			<wp:comment_date>2010-07-15 10:04:47</wp:comment_date>
			<wp:comment_date_gmt>2010-07-15 17:04:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid &rsaquo; the 5 stars of open linked data &#8211; Tim Berners Lee explains the 5 stars of open linked data, where each star represents a further step in the journey towards publishing data which is compliant with open linked data standards. Also a neat and simple description of linked data, ontologies and vocabularies using a packet of chips (crisps) to illustrate the points. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82742</wp:comment_id>
			<wp:comment_author><![CDATA[links for 2010-07-25 &laquo; Social Stoke]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://socialstoke.wordpress.com/2010/07/25/links-for-2010-07-25/</wp:comment_author_url>
			<wp:comment_author_IP>72.233.96.150</wp:comment_author_IP>
			<wp:comment_date>2010-07-25 01:03:35</wp:comment_date>
			<wp:comment_date_gmt>2010-07-25 08:03:35</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid › the 5 stars of open linked data (tags: #hhhbrum) [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82793</wp:comment_id>
			<wp:comment_author><![CDATA[Start sharing public sector data online &laquo; Observations]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://wmro.wordpress.com/2010/08/19/start-sharing-public-sector-data-online/</wp:comment_author_url>
			<wp:comment_author_IP>72.233.96.139</wp:comment_author_IP>
			<wp:comment_date>2010-08-19 07:01:45</wp:comment_date>
			<wp:comment_date_gmt>2010-08-19 14:01:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] The presentation also relates to helping non-technical staff contribute to open data initiatives by making small changes to existing working habits – the first three of Ed Summers&#8217; five stars of open linked data: [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83175</wp:comment_id>
			<wp:comment_author><![CDATA[Open Data @ CTIC &raquo; Crece el “Open Government Data” a nivel mundial]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://datos.fundacionctic.org/2010/09/crece-el-%e2%80%9copen-government-data%e2%80%9d-a-nivel-mundial/</wp:comment_author_url>
			<wp:comment_author_IP>212.89.8.82</wp:comment_author_IP>
			<wp:comment_date>2010-09-06 01:18:32</wp:comment_date>
			<wp:comment_date_gmt>2010-09-06 08:18:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Los catálogos se representan usando distintos colores, los cuales muestran el nivel de madurez de la iniciativa de Open Government Data, basándose en la clasificación de 5 estrellas de Linked Government Data: [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83410</wp:comment_id>
			<wp:comment_author><![CDATA[Identifiers &laquo; Briefing Paper for eResearch &amp; IE Call &#8211; 10/2010]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inf11briefingoct2010.jiscpress.org/identifiers/</wp:comment_author_url>
			<wp:comment_author_IP>213.133.67.202</wp:comment_author_IP>
			<wp:comment_date>2010-10-11 07:56:04</wp:comment_date>
			<wp:comment_date_gmt>2010-10-11 14:56:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...]  ?     Tim Berners-Lee’s 5 Stars of Linked Data: http://inkdroid.org/2010/06/04/the-5-stars-of-open-linked-data/  [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83525</wp:comment_id>
			<wp:comment_author><![CDATA[You are what you eat: potato chips and linked Data at blog.humaneguitarist.org]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.humaneguitarist.org/2010/10/31/you-are-what-you-eat-potato-chips-and-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>173.201.216.42</wp:comment_author_IP>
			<wp:comment_date>2010-10-31 08:37:57</wp:comment_date>
			<wp:comment_date_gmt>2010-10-31 15:37:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] http://inkdroid.org/2010/06/04/the-5-stars-of-open-linked-data/ [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83920</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/profiles/da&hellip;]]></wp:comment_author>
			<wp:comment_author_email>dan.bolser@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.google.com/profiles/dan.bolser</wp:comment_author_url>
			<wp:comment_author_IP>92.233.142.235</wp:comment_author_IP>
			<wp:comment_date>2010-12-08 10:00:41</wp:comment_date>
			<wp:comment_date_gmt>2010-12-08 17:00:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Examples of websites using these?

Cheers,
Dan.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>356</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84412</wp:comment_id>
			<wp:comment_author><![CDATA[Open Data @ CTIC &raquo; No es Linked Data todo lo que reluce]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://datos.fundacionctic.org/2011/04/no-es-linked-data-todo-lo-que-reluce/</wp:comment_author_url>
			<wp:comment_author_IP>212.89.8.82</wp:comment_author_IP>
			<wp:comment_date>2011-04-15 03:07:46</wp:comment_date>
			<wp:comment_date_gmt>2011-04-15 10:07:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] abierto, usando formatos estándar. Todas buscan la excelencia tecnológica, con la vista puesta en las 5 estrellas de Tim Berners-Lee, lo que nos congratula enormemente y motiva para seguir apostando por la base de la Web del futuro [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1302862066.9381";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303390445.2321";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84369</wp:comment_id>
			<wp:comment_author><![CDATA[Why Open Data? | Stop]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://stop.zona-m.net/2011/01/why-open-data/</wp:comment_author_url>
			<wp:comment_author_IP>80.68.94.237</wp:comment_author_IP>
			<wp:comment_date>2011-03-19 11:58:01</wp:comment_date>
			<wp:comment_date_gmt>2011-03-19 18:58:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] have some of it, you can find other, related, data&#8221;. This concept is also explained in the &#8220;5 stars of open linked data&#8221; paper. The practical definitions that follow are, in a sense, a technical version of the &#8220;follow [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1300561081.556";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1300796745.6865";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84331</wp:comment_id>
			<wp:comment_author><![CDATA[Collaboration Workshop 2011 | DevCSI]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://devcsi.ukoln.ac.uk/blog/2011/03/07/collaboration-workshop-2011/</wp:comment_author_url>
			<wp:comment_author_IP>138.38.146.21</wp:comment_author_IP>
			<wp:comment_date>2011-03-07 07:42:01</wp:comment_date>
			<wp:comment_date_gmt>2011-03-07 14:42:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] not already know, the success of the UK government and Public Sector data publishing based around a &#8220;5 star&#8221; model and the CKAN catalogue, might have been helpful, in relation to getting people to publish [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1299593830.7872";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1299508922.2463";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85372</wp:comment_id>
			<wp:comment_author><![CDATA[Preparing Data &laquo; Themes to bring up in conversation at work]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://charleyramm.wordpress.com/2012/05/01/preparing-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-01 11:04:13</wp:comment_date>
			<wp:comment_date_gmt>2012-05-01 18:04:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Tim Berners-Lee describes open data by a system of five stars. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335895453.8908";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335896623.6905";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85373</wp:comment_id>
			<wp:comment_author><![CDATA[Preparing Data &laquo; Charley Ramm]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://charleyramm.com/2012/05/01/preparing-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-01 11:20:37</wp:comment_date>
			<wp:comment_date_gmt>2012-05-01 18:20:37</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Tim Berners-Lee describes open data by a system of five stars. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335896437.8062";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335896655.5165";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84414</wp:comment_id>
			<wp:comment_author><![CDATA[All recommendations lead back to Cool URIs &laquo; Linking You]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://linkingyou.blogs.lincoln.ac.uk/2011/04/19/all-recommendations-lead-back-to-cool-uris/</wp:comment_author_url>
			<wp:comment_author_IP>195.195.10.65</wp:comment_author_IP>
			<wp:comment_date>2011-04-19 09:17:40</wp:comment_date>
			<wp:comment_date_gmt>2011-04-19 16:17:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Data is common sense and fundamentally [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303229860.2818";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303390443.6156";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85407</wp:comment_id>
			<wp:comment_author><![CDATA[Tim Berners-Lee&#8217;s missing star &laquo; DataMarket blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.datamarket.com/2012/05/25/tim-berners-lees-missing-star-2/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-25 06:06:13</wp:comment_date>
			<wp:comment_date_gmt>2012-05-25 13:06:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] of you Open Data enthusiasts out there will be familiar with Tim Berners-Lee&#8217;s five star system, a no nonsense rating system for the usefulness and utility of a openly released data [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337951174.2468";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337959964.7624";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85391</wp:comment_id>
			<wp:comment_author><![CDATA[Publishing transport data | iRail]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.irail.be/2012/05/14/publishing-transport-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-14 03:28:42</wp:comment_date>
			<wp:comment_date_gmt>2012-05-14 10:28:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] in place. Use it to challenge your organisation. When you can say and prove that you have a 5-start data structure, you have something worth more than a certificate, for [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336991322.8678";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:13:"1336997563.45";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85356</wp:comment_id>
			<wp:comment_author><![CDATA[Interoperabilità degli Open Data governativi: il W3C pubblica i primi draft &raquo; International Webmasters Association]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.iwa.it/w3c/interoperabilita-degli-open-data-governativi-il-w3c-pubblica-i-primi-draft/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-11 08:50:50</wp:comment_date>
			<wp:comment_date_gmt>2012-04-11 15:50:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] maggiori motivi utili per far crescere il livello di riutilizzo e di interoperabilità dei dati, secondo la scala delle stelline promossa da Sir Tim Berners Lee. Inizia ad essere utile pensare alla Linked Open Data cloud, ed alla necessità di pubblicare non [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334165556.9628";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84792</wp:comment_id>
			<wp:comment_author><![CDATA[zaragoza.es Una Web de 5 estrellas &laquo; IAAA Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://webdiis.unizar.es/IAAA/wp/2011/05/zaragoza-es-una-web-de-5-estrellas/</wp:comment_author_url>
			<wp:comment_author_IP>155.210.152.200</wp:comment_author_IP>
			<wp:comment_date>2011-05-23 12:26:59</wp:comment_date>
			<wp:comment_date_gmt>2011-05-23 19:26:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] los días previos a la celebración del Bilbao Web Summit (http://inkdroid.org/2010/06/04/the-5-stars-of-open-linked-data/). Este nivel solamente ha sido conseguido por otras 3 instituciones públicas en el mundo: el [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1306271530.6579";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1306178819.5718";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85096</wp:comment_id>
			<wp:comment_author><![CDATA[L’Espagne, 2eme acteur européen de l’Open Data]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.dmph.net/?p=483</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-13 03:11:57</wp:comment_date>
			<wp:comment_date_gmt>2011-09-13 10:11:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Pour rappel, Tim Berners Lee avait proposé une méthodologie pour leur évaluation, basée sur 5 étoiles: [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1316435634.7099";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1315908717.7566";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85093</wp:comment_id>
			<wp:comment_author><![CDATA[Constructing the Open Data Landscape | ScraperWiki Data Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.scraperwiki.com/2011/09/07/constructing-the-open-data-landscape/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-07 04:02:04</wp:comment_date>
			<wp:comment_date_gmt>2011-09-07 11:02:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] of the data they release and the ways in which it is released. The goal should be to gain the 5 stars of open linked data. For this to be achieved the data needs to be pared down to its raw ingredients. In a research [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:12:"1315680281.4";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1315393325.1433";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85341</wp:comment_id>
			<wp:comment_author><![CDATA[#REDacciones: Linked mind (II) &laquo; vorpalina y yo]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://vorpalina.com/2012/02/13/redacciones-linked-mind-ii/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-02-13 13:46:19</wp:comment_date>
			<wp:comment_date_gmt>2012-02-13 20:46:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] el creador de la Red y del concepto de hipertexto, defendió hace poco su propuesta de las 5 estrellas del Linked Data para evaluar la calidad de los datos compartidos en la Red. Yo hoy quiero atreverme a hacer un [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1329165979.6689";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1329166814.334";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85335</wp:comment_id>
			<wp:comment_author><![CDATA[5 provocaciones para un debate sobre open data &laquo; Administraciones en red]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://eadminblog.net/2012/01/08/5-provocaciones-para-un-debate-sobre-open-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-01-08 14:07:48</wp:comment_date>
			<wp:comment_date_gmt>2012-01-08 21:07:48</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] cometido un grave error al categorizar la iniciativas RISP según el modelo de 5 estrellas de Berners-Lee. Primero, porque no se puede aplicar a catálogos completos, ya que lo normal es que haya cierta [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1326056868.7107";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1326057013.4034";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85203</wp:comment_id>
			<wp:comment_author><![CDATA[All about Linked Data « Feral Librarian]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://chrisbourg.wordpress.com/2010/06/15/all-about-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-11-04 09:42:14</wp:comment_date>
			<wp:comment_date_gmt>2011-11-04 16:42:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Short post by Ed Summers of Library of Congress on the 5 stars of linked data [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1320424935.5868";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1320435690.2529";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85230</wp:comment_id>
			<wp:comment_author><![CDATA[Directory Data &laquo; Development in Jeans and a T-Shirt]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://me2inict.blogs.lincoln.ac.uk/directory-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-11-24 14:16:40</wp:comment_date>
			<wp:comment_date_gmt>2011-11-24 21:16:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] a continued drive to making all our data 5-star quality, I&#8217;m pleased to announce that we&#8217;ve made a few improvements to our Staff Directory [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1322169400.9503";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1322185573.975";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85265</wp:comment_id>
			<wp:comment_author><![CDATA[La France entr&#8217;ouverte &raquo; OWNI, News, Augmented]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://owni.fr/2011/12/10/la-france-entrouverte-transparence-open-gov-open-data-etalab/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-12-10 03:17:04</wp:comment_date>
			<wp:comment_date_gmt>2011-12-10 10:17:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] l&#8217;équipe chargée de la libération des données publiques en France a donc choisi de suivre les recommandations indirectes [en] de &#8220;l&#8217;inventeur du Web&#8221;, Tim Berners-Lee : balancer les données en vrac, [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1323512224.6828";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1323528430.2225";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85465</wp:comment_id>
			<wp:comment_author><![CDATA[La France entr&#8217;ouverte | YRUQT]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://yruqt.com/2011/12/10/la-france-entrouverte/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-09-27 06:52:20</wp:comment_date>
			<wp:comment_date_gmt>2012-09-27 13:52:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] l&#8217;équipe chargée de la libération des données publiques en France a donc choisi de suivre les recommandations indirectes [en] de &laquo;&nbsp;l&#8217;inventeur du Web&nbsp;&raquo;, Tim Berners-Lee : balancer les [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1348768408.8504";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85466</wp:comment_id>
			<wp:comment_author><![CDATA[Keynote: &#8216;From Strings to Things&#8217;, LODLAM Melbourne workshop | mia ridge]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.miaridge.com/keynote-from-strings-to-things-lodlam-melbourne-workshop/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-09-30 11:16:50</wp:comment_date>
			<wp:comment_date_gmt>2012-09-30 18:16:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] [4]  http://inkdroid.org/2010/06/04/the-5-stars-of-open-linked-data/  [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1349029010.8888";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1349399358.0018";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85543</wp:comment_id>
			<wp:comment_author><![CDATA[Earning three stars from Tim Berners-Lee and other cool things you can do with Google Drive]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.accessola2.com/olita/insideolita/wordpress/?p=58410</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-01-30 07:16:46</wp:comment_date>
			<wp:comment_date_gmt>2013-01-30 14:16:46</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] of thinking of the work as a continuum of effort that should be recognized and rewarded. And so, he developed a five-star scale for Open Data efforts which I have I&#8217;ve included but modified slightly [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1359557193.6153509616851806640625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86975</wp:comment_id>
			<wp:comment_author><![CDATA[Publicación de datos energéticos en formato abierto | aulagreencities]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://aulagreencities.coamalaga.es/publicacion-de-datos-energeticos-en-formato-abierto/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-06-03 01:31:12</wp:comment_date>
			<wp:comment_date_gmt>2014-06-03 08:31:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Berners Lee, 2010: http://inkdroid.org/2010/06/04/the-5-stars-of-open-linked-data/ [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1401877149.834703922271728515625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1401784272.3505880832672119140625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87370</wp:comment_id>
			<wp:comment_author><![CDATA[Open Data Blog EU &raquo; Blog Archive &raquo; Tim-Berners Lee Euskadin izan dugu: datu irekien garrantzia]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://opendatabloga.korpoweb.com/blog-eu/jakingarri/tim-berners-lee-euskadin-izan-dugu-datu-irekien-garrantzia/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-06-10 02:28:04</wp:comment_date>
			<wp:comment_date_gmt>2015-06-10 09:28:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Berners-Leek biltzarreko paneletako bat, e-Gobernua gaitzat izan zuena. Eta grafikoki azaldu zigun, beste agerpen batzuetan ere erabili duen analogia batekin: linked data edo datu elkarlotuen bidean lortu beharreko bost [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1433928484.332395076751708984375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1436443883.206098079681396484375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87371</wp:comment_id>
			<wp:comment_author><![CDATA[Open Data Blog ES &raquo; Blog Archive &raquo; Tim Berners-Lee habló de Open Data en su visita a Euskadi]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://opendatabloga.korpoweb.com/blog-es/noticias/tim-berners-lee-hablo-de-open-data-en-su-visita-a-euskadi/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-06-10 02:34:03</wp:comment_date>
			<wp:comment_date_gmt>2015-06-10 09:34:03</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] en un panel sobre e-Gobierno. Y lo explico con un esquema gráfico que ya ha venido promoviendo en otras apariciones públicas últimamente: las cinco estrellas hacia los linked data, o los datos enlazados (un poquito más [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1433928843.9073660373687744140625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1436443881.861064910888671875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>bibliographic records on the web</title>
		<link>http://inkdroid.org/2010/06/08/bibliographic-records-on-the-web/</link>
		<pubDate>Tue, 08 Jun 2010 17:02:11 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2020</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://openlibrary.org"><img src="http://inkdroid.org/images/openlibrary.png" style="float:left; margin-right: 10px; margin-bottom: 10px;" /></a>There are a couple <a href="http://www.mail-archive.com/ol-tech@archive.org/msg00048.html">interesting</a> <a href="http://www.mail-archive.com/ol-tech@archive.org/msg00075.html">threads</a> (disclaimer I inadvertently started one) going on over on the <a href="http://www.mail-archive.com/ol-tech@archive.org/info.html">Open Library technical discussion list</a> about making Linked Data views available for authors. Since the topic was largely how to model people, part of the discussion <a href="http://lists.foaf-project.org/pipermail/foaf-dev/2010-June/010253.html">spilled over</a> to <a href="http://lists.foaf-project.org/mailman/listinfo/foaf-dev">foaf-dev</a> (also my fault). 

When making library Linked Data available my preference has been to follow the lead of <a href="http://twitter.com/geckomarma">Martin Malmsten</a>, <a href="http://twitter.com/brocadedarkness">Anders Söderbäck</a> and the <a href="http://libris.kb.se/">Royal Library of Sweden</a> by modeling authors as People using the <a href="http://xmlns.com/foaf/spec/">FOAF vocabulary</a>:

<pre>
&lt;http://libris.kb.se/resource/auth/317488&gt;
    libris:key "Berners-Lee, Tim" ;
    a foaf:Person ;
    rdfs:isDefinedBy &lt;http://libris.kb.se/data/auth/317488&gt; ;
    skos:exactMatch &lt;http://viaf.org/viaf/23002995&gt; ;
    foaf:name "Berners-Lee, Tim", "Lee, Tim Berners-", "Tim Berners- Lee", "Tim Berners-Lee" .
</pre>

It seems sensible enough right? But there is some desire in the library community to model an author as a <em>Bibliographic Resource</em> and then relate this resource to a <em>Person</em> resource. While I can understand wanting to have this level of indirection to assert a bit more control, and to possibly use some emerging vocabularies for RDA, I think (for now) using something like FOAF for modeling authors as people is a good place to start. 

<img src="http://inkdroid.org/images/rdf.gif" style="float: right; margin-left: 10px;" />

It will engage folks from the FOAF community who understand RDF and Linked Data, and get them involved in the Open Library Project. It will make library data fit in with other Linked Data out on the web. Plus, it just kind of fits my brain better to think of authors as people...isn't that what libraries were trying to do all along with their authority data? I'm not saying that FOAF will have everything the library world needs (it won't), but it's an open world and we <a href="http://ontologi.es/persona">can</a> add stuff that we need, collaborate, and make it a better place.

Anyway, that's not really what I wanted to talk about here. Over the course of this discussion Erik Hetzner <a href="http://www.mail-archive.com/ol-tech@archive.org/msg00092.html">raised</a> what I thought was an important question:

<blockquote>
Are you saying that there is a usable distinction between:

1. a bibliographic record, and
2. the data contained in that bibliographic record?

From above, my first notion would be to model things as, in
pseudo-Turtle::

  &lt;Victor Hugo&gt; a frbr:Person .
  &lt;Victor Hugo&gt; rdfs:isDefinedBy &lt;bib record&gt; .
  &lt;bib record&gt; dc:modified "..."^^xsd:date .

But it seems to me that you are adding a further distinction::

  &lt;Victor Hugo&gt; a frbr:Person .
  &lt;Victor Hugo&gt; rdfs:isDefinedBy &lt;bib record&gt; .
  &lt;bib record&gt; rdfs:isDefinedBy &lt;bib record data&gt;
  &lt;bib record data&gt; dc:modified "..."^^xsd:date .

Is this a usable or useful distinction? Are there times when we want to distinguish between the abstract bibliographic record and the representation of a bibliographic record? <strong>In linked data-speak, is a bibliographic record a non-information resource?</strong> My thinking has been that a bibliographic record is an information resource, and that one does not need to distinguish between (1) and (2) above.
</blockquote>

I think it's an important question because I don't think it's been really discussed much before, and has a direct impact on what sort of URL you can use to identify a <em>Bibliographic Record</em>, and what sort of HTTP response a client gets when it is resolved. This is the <a href="http://www.w3.org/2001/tag/issues.html#httpRange-14">httpRange-14</a> issue, which is covered in <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a>. If a <em>Bibliographic Record</em> is an <em>Information Resource</em> then its OK to identify the record with any old URL, and for the server to say 200 OK like normal. If it's not an Information Resource then the URL should either have a hash fragment in it, or the server should respond 303 See Other, and redirect to another location.

In my view if a <em>Bibliographic Record</em> is on the web with a URL, it is useful to think of it as an <em>Information Resource</em>...or (as <a href="http://richard.cyganiak.de/">Richard Cyganiak</a> dubs it) a <a href="http://inkdroid.org/2010/02/22/web-documents-and-axioms-for-linked-data/">Web Document</a>. I don't think it's worthwhile philosophizing about this, but instead to think about it pragmatically. I think it's useful to consider 

<ul>
<li><a href="http://lccn.loc.gov/99027665">http://lccn.loc.gov/99027665</a></li>
</ul>

as being an identifier for a bibliographic record that happens to be in HTML. Likewise 

<ul>
<li><a href="http://lccn.loc.gov/99027665/mods">http://lccn.loc.gov/99027665/mods</a></li>
<li><a href="http://lccn.loc.gov/99027665/dc">http://lccn.loc.gov/99027665/dc</a></li>
<li><a href="http://lccn.loc.gov/99027665/marcxml">http://lccn.loc.gov/99027665/marcxml</a></li> 
</ul>

are all identifiers for <em>Bibliographic Records</em> in MODS, Dublin Core and MARCXML respectively. It might be useful to link them together as they are with &lt;link&gt; elements in the HTML, or in some RDF serialization. It also could be useful to treat one as canonical, and content negotiate from one of the URLs (e.g. curl --header "Accept: application/marc+xml" http://lccn.loc.gov/99027665).  But I think it simplifies deployment of library Linked Data to think of bibliographic records as things that can be put on the web as documents, without worrying too much about httpRange-14. A nice side effect of this is that it would grandfather in all the OPAC record views out there. Maybe it'll be useful to distinguish between an abstract notion of a bibliographic record, and the actual document that is the bibliographic record -- but I'm not seeing it right now...and I think it would introduce a lot of unnecessary complexity in this fragile formative period for library Linked Data.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2020</wp:post_id>
		<wp:post_date>2010-06-08 10:02:11</wp:post_date>
		<wp:post_date_gmt>2010-06-08 17:02:11</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bibliographic-records-on-the-web</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="foaf"><![CDATA[foaf]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="urls"><![CDATA[urls]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:8:{i:0;s:5:"82696";i:1;s:5:"82697";i:2;s:5:"82700";i:3;s:5:"82701";i:4;s:5:"82703";i:5;s:5:"82704";i:6;s:5:"82706";i:7;s:5:"82709";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82707</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.50.54.15</wp:comment_author_IP>
			<wp:comment_date>2010-06-11 03:53:23</wp:comment_date>
			<wp:comment_date_gmt>2010-06-11 10:53:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Jakob, now I think we are in agreement. Our bibliographic records really should be just a class of web documents with URLs. I could imagine some linked data advocates making a case for an abstract notion of a bibliographic records (so called Real World Object), so that they could be described. You actually can see some of this approach in the use of viaf:EstablishedHeading, viaf:NameAuthorityCluster, etc in the <a href="http://viaf.org" rel="nofollow">Virtual International Authority File</a> (<a href="http://viaf.org/viaf/79454736/rdf.xml" rel="nofollow">example</a>). But I don't think this is necessary, and would ultimately make it difficult for people to put their records on the web. As Herbert van de Sompel <a href="http://twitter.com/hvdsomp/status/15777598515" rel="nofollow">put it</a>, this would be "one bridge too far".]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82700</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/profiles/Fe…]]></wp:comment_author>
			<wp:comment_author_email>felix.ostrowski@googlemail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>194.95.250.30</wp:comment_author_IP>
			<wp:comment_date>2010-06-10 02:06:10</wp:comment_date>
			<wp:comment_date_gmt>2010-06-10 09:06:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Agreed: End-users (whoever that may be) usually care about "data about things".  But this does not imply that the concept of a record is dispensable. In fact, those litte fellows are everywhere. Each and every information resource on the web can be considered a record (most of them containing rather unstructured data). And if we care about authorship, creation date etc. of a blog post, why should metadata for a bibliographic record be neglected?

I admit that metadata for a blog post is probably more interesting for a broad audience. But I still strongly disagree with Jakob saying that "[i]n practice there is no need to model 'a bibliographic record' because you should not care about bibliographic records".

First of all: there is no such thing as free-floating statements.  Assertions are always materialized. From an RDF point of view, triples are always asserted in graphs - be it the default graph or a named graph in a triplestore, or an RDF-Document available at some URL (I would actually also consider that a named graph). Since we have records, we should care about them.

Besides that, records are and will most likely continue to be very useful on an infrastructural level. They are neatly packed snippets of information that can be exchanged easily (e.g. by simply writing them to a file), merging is simple, and they provide a good boundary for version control. (I am assuming the "One Resource per Graph"-scenario discussed in Leigh Dodds' <a href="http://www.ldodds.com/blog/2009/12/approaches-to-publishing-linked-data-via-named-graphs/" rel="nofollow">Approaches to Publishing Linked Data via Named Graphs</a> here.) In a massively distributed environment such as librarianship, update notifications are also important.  Maybe bibliographic data would be a good playground to explore "<a href="http://www.ldodds.com/blog/2010/04/rdf-dataset-notifications/" rel="nofollow">notifications of graph structural changes to a dataset, e.g. adding or removing named graphs</a>".

The nice thing about a record model based on RDF and named graphs is the fact that it is easy to make the record "transparent". The way e.g. <a href="http://4store.org/" rel="nofollow">4store</a> handles named graphs is that it provides the aggregate of all assertions within all named graphs as the default graph. That would be the view on the data when "you are less interested in 'data about data about data ... about things'".]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>343</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82701</wp:comment_id>
			<wp:comment_author><![CDATA[Owen Stephens]]></wp:comment_author>
			<wp:comment_author_email>owen_stephens@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/owen_stephens#ed007</wp:comment_author_url>
			<wp:comment_author_IP>137.108.145.250</wp:comment_author_IP>
			<wp:comment_date>2010-06-10 05:49:47</wp:comment_date>
			<wp:comment_date_gmt>2010-06-10 12:49:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm going to comment on the first issue you raise, rather than your main point (sorry!)

The issue for me is the differentiation between the person who actually wrote the book, and how they are described by a particular book (and thought of by that books audience). I don't think this is about control but about how we model the data (I may be wrong - perhaps it is just the librarian in me coming through)

To take a relatively well known example, Iain (M) Banks writes under two names to differentiate his science fiction and non-genre fiction writings.

To take another example 'Devil May Care' is authored (according to the cover) by 'Sebastian Faulks, writing as Ian Fleming'

Another example might be a ghost written memoir.

I'm also reminded of the story that having read Alice in Wonderland, Queen Victoria asked for other books by the same author, and got delivered academic mathematics books by Charles Dodgson.

In these cases it seems to me that we have a persona that has been created to represent the author - and when we come to search for a book, or view the details of a book, we are as likely to be interested in that persona as we are in the person who actually wrote the book.

I do have a lot of sympathy with your argument that we need to take a pragmatic approach, and have some doubts about my own argument - perhaps I'm worrying about edge cases. On the otherhand the I think that there is a risk that we lose some of the information that is actually useful to people searching for this stuff if we aren't careful.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>344</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82704</wp:comment_id>
			<wp:comment_author><![CDATA[Owen Stephens]]></wp:comment_author>
			<wp:comment_author_email>owen_stephens@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/owen_stephens#ed007</wp:comment_author_url>
			<wp:comment_author_IP>137.108.145.250</wp:comment_author_IP>
			<wp:comment_date>2010-06-10 07:20:29</wp:comment_date>
			<wp:comment_date_gmt>2010-06-10 14:20:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[That's a good thread - sorry I should have read that first. Having gone through that discussion and given it at least 30 seconds thought, my immediate instinct is to go with the use of different foaf:Person resources, and then look at how we link them together.

Although I'm not entirely happy with this, I think it has some real advantages:

It is easier to understand an author as a person rather than a persona
In most cases the author is a person (or there is a one-to-one equivalence between person and persona)
Everything is there to do it now
Some people have already done it
It allows backwards compatibility for data already published

While I do like the 'persona' model proposed in the thread, and suspect it models the reality more accurately in some ways I suspect it will just put bib data on the outside again by adopting complexity beyond most people's interest.

Now will try to strongly hold this opinion for the next 24 hours :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>344</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82703</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>rossfsinger@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawlBhPsOb5GN8bfRnBAf9sp8p3TO9o2CYyA</wp:comment_author_url>
			<wp:comment_author_IP>68.169.158.195</wp:comment_author_IP>
			<wp:comment_date>2010-06-10 06:53:16</wp:comment_date>
			<wp:comment_date_gmt>2010-06-10 13:53:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Owen, I think there is quite a lot of value in your argument (and, in some ways, I have been making a similar argument in this regard, re: "personas").

I don't think modeling it would be terribly hard, but I am not sure how the execution would work.  More specifically, is this just yet another area where librarians will alienate themselves from the rest of the information universe simply because they get bogged down in the minutiae when the rest of the world just "does what feels right" (see also the schism between concept and thing).

On the flip side of that, it could also be viewed as an opportunity.  Were the library community to model this in a reusable way, it might help establish relevance.

Now on to Jakob's comment... Jakob, maybe it would be better to think of bibliographic records as "web documents" rather than "information resources"?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>345</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82706</wp:comment_id>
			<wp:comment_author><![CDATA[jakoblog.de/]]></wp:comment_author>
			<wp:comment_author_email>jakob@nichtich.de</wp:comment_author_email>
			<wp:comment_author_url>http://jakoblog.de/</wp:comment_author_url>
			<wp:comment_author_IP>128.214.20.122</wp:comment_author_IP>
			<wp:comment_date>2010-06-11 03:13:34</wp:comment_date>
			<wp:comment_date_gmt>2010-06-11 10:13:34</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Surely there are use cases that require you to model bibliographic records as entities with their own URL and make links from and to this specific records - but there is no end in adding another level of complexity. You could also put the whole record into a string and say

&lt;http://lccn.loc.gov/99027665&gt; format:hasmarcrecord "put record here" .

There is no "right" way to model it because it all depends on your use case. The most common use case is to express information about things (authors, publications etc.) but not about records. 

Records are only the method to pack and transport this information. You do not model TCP packets in RDF, do you? I do not argue against using URIs for records at all but records are not the goal - you should better first express what is really needed and this is not a bibliographic record, but its content. Aggregating and selecting records is based on other methods (HTTP Content negotiation, ATOM, OAI, unAPI ...) but not RDF.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>337</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82702</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2010-06-10 06:49:58</wp:comment_date>
			<wp:comment_date_gmt>2010-06-10 13:49:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey Owen. Yeah, it was kind of a mistake for me to have addressed both issues in the same post. 

The different names for people is a good example of where FOAF is currently lacking. This was actually the topic of the <a href="http://lists.foaf-project.org/pipermail/foaf-dev/2010-June/010253.html" rel="nofollow">discussion</a> over on  foaf-dev that I mentioned.

It seemed like there was general agreement that there are ways of using skos:altLabel (a variant on Leigh and Ian's <a href="http://patterns.dataincubator.org/book/preferred-label.html" rel="nofollow">preferred-label pattern</a>), or a new property foaf:alias, creating different foaf:Person resources and linking them together, or perhaps introducing the notion of a <a href="http://ontologi.es/persona" rel="nofollow">Persona</a>. 

But at the end of the day, I think we are describing authors who are people, and we should try to stay concrete if we can...instead of treating them as new special library resources.

I think worrying about edge-cases is an endemic problem in the library community -- not quite sure why that is. Probably because we end up dealing with so many of them :-) The library world is just starting out using RDF vocabularies, and I think it's important to stand on a few giant's heads, and look around instead of immediately embarking on vocabulary engineering.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82698</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2010-06-09 13:36:34</wp:comment_date>
			<wp:comment_date_gmt>2010-06-09 20:36:34</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I don't agree Jakob. I don't think it has to be terribly overthought and complicated myself. I think this assertion would work fine to indicate that a particular bibliographic record was created by someone named James Billington:

<pre>
&lt;http://lccn.loc.gov/99027665&gt; dc:creator "James Billington" .
</pre>

If this isn't good enough for asserting who the creator of a particular bibliographic record is, then maybe we need a better piece of vocabulary than dc:creator. <em>My apologies if your example somehow got garbled in the publication workflow.</em>

Also, for what it's worth, I completely disagree about bibliographic records not mattering. I care deeply about bibliographic records for books, people, subjects, places, etc being on the web where people can get them by resolving a URL. Of course it is the information that is contained in them that's most important. But unless you can get them using today's predominant publishing system (the Web) what good are they? Another way of putting this would be, don't you think encyclopedia entries still matter? Sure it's the information contained in the entry that matters most, but that encyclopedia entry resource on the web, say at Wikipedia, is tremendously important.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82697</wp:comment_id>
			<wp:comment_author><![CDATA[jakoblog.de/]]></wp:comment_author>
			<wp:comment_author_email>jakob@nichtich.de</wp:comment_author_email>
			<wp:comment_author_url>http://jakoblog.de/</wp:comment_author_url>
			<wp:comment_author_IP>128.214.20.122</wp:comment_author_IP>
			<wp:comment_date>2010-06-09 00:05:50</wp:comment_date>
			<wp:comment_date_gmt>2010-06-09 07:05:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The issue you and Erik Hetzner raised is not specific to bibliographic records on the web. It occurs as soon as you want to make assertions about data - for instance you want to express who created a MARC record and when it was changed. In RDF you could use reification but named graphs seem to become the more adopted solution. Moreover we have HTTP headers to express some of this metadata about serializations. This all adds additional layers of complexity which is nice in theory but stupid in practice.

In practice you are less interested in "data about data about data ... about things" but just in data about things - and in practice you do think about "information resources" vs "non-information resource". That's why a flat model is more useful in most situations:

<pre> a foaf:Person ;
  rdfs:isDefinedBy
     ,
     ,
     ,
     .</pre>

In practice there is no need to model "a bibliographic record" because you should not care about bibliographic records. You should care about the information that happens to be encoded in bibliographic records instead. Why do libraries always try to make users think like them instead of just providing useful service from the users point of view? Bibliographic records are irrelevant - it's the content that matters.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>337</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82709</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/profiles/Fe…]]></wp:comment_author>
			<wp:comment_author_email>felix.ostrowski@googlemail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>194.95.250.30</wp:comment_author_IP>
			<wp:comment_date>2010-06-18 03:29:24</wp:comment_date>
			<wp:comment_date_gmt>2010-06-18 10:29:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<blockquote>
I could imagine some linked data advocates making a case for an abstract notion of a bibliographic records (so called Real World Object), so that they could be described.
</blockquote>

With RDF you can describe anything that has a URI. This of course includes all information resources that have a URL, so "an abstract notion of a bibliographic record" is not a precondition to describing a (bibiographic) record in RDF. Yes, "[o]ur bibliographic records really should be just a class of web documents with URLs". Fortunately we can still use RDF to publish data about those records.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>343</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82696</wp:comment_id>
			<wp:comment_author><![CDATA[jakoblog.de/]]></wp:comment_author>
			<wp:comment_author_email>jakob@nichtich.de</wp:comment_author_email>
			<wp:comment_author_url>http://jakoblog.de/</wp:comment_author_url>
			<wp:comment_author_IP>128.214.20.122</wp:comment_author_IP>
			<wp:comment_date>2010-06-09 00:05:10</wp:comment_date>
			<wp:comment_date_gmt>2010-06-09 07:05:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The issue you and Erik Hetzner raised is not specific to bibliographic records on the web. It occurs as soon as you want to make assertions about data - for instance you want to express who created a MARC record and when it was changed. In RDF you could use reification but named graphs seem to become the more adopted solution. Moreover we have HTTP headers to express some of this metadata about serializations. This all adds additional layers of complexity which is nice in theory but stupid in practice.

In practice you are less interested in "data about data about data ... about things" but just in data about things - and in practice you do think about "information resources" vs "non-information resource". That's why a flat model is more useful in most situations:

<pre>
 a foaf:Person ;
  rdfs:isDefinedBy
     ,
     ,
     ,
     .
</pre>

In practice there is no need to model "a bibliographic record" because you should not care about bibliographic records. You should care about the information that happens to be encoded in bibliographic records instead. Why do libraries always try to make users think like them instead of just providing useful service from the users point of view? Bibliographic records are irrelevant - it's the content that matters.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>337</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82710</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.50.54.15</wp:comment_author_IP>
			<wp:comment_date>2010-06-18 03:51:32</wp:comment_date>
			<wp:comment_date_gmt>2010-06-18 10:51:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Agreed. It's easy to forget with all this talk about Linked Data for "real world things" that RDF really excels at describing documents on the web.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Confessions of a Graph Addict</title>
		<link>http://inkdroid.org/2010/06/24/confessions-of-a-graph-addict/</link>
		<pubDate>Thu, 24 Jun 2010 11:31:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2057</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Today I'm going to be at the annual conference of the <a href="http://www.ala.org">American Library Association</a> today for a pre-conference about <a href="http://web.archive.org/web/20100724135242/http://www.ala.org:80/ala/mgrps/divs/alcts/confevents/upcoming/ala/ac10/linked.cfm">Libraries and Linked Data</a>.  I'm going to try talking about how Linked Data, and particularly how the graph data structure fits the way catalogers have typically thought about bibliiographic information. Along the way I'll include some specific examples of Linked Data projects I've worked on at the Library of Congress--and gesture at work that remains to be done.</p>

<iframe src="http://docs.google.com/present/embed?id=dv89m3d_406g4kh34kx&size=m" frameborder="0" width="555" height="451"></iframe>

<p>Tomorrow there's an <a href="http://lists.w3.org/Archives/Public/public-lld/2010Jun/0047.html">unconference</a> style event at ALA to explore the what Linked Data means for Libraries. The pre-conference today is booked up, but the event tomorrow is open to the public, so please consider dropping by if you are interested and in the DC area.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2057</wp:post_id>
		<wp:post_date>2010-06-24 04:31:16</wp:post_date>
		<wp:post_date_gmt>2010-06-24 11:31:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>confessions-of-a-graph-addict</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="cataloging"><![CDATA[cataloging]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="marc"><![CDATA[marc]]></category>
		<category domain="post_tag" nicename="newspapers"><![CDATA[newspapers]]></category>
		<category domain="post_tag" nicename="oai-ore"><![CDATA[oai-ore]]></category>
		<category domain="post_tag" nicename="presentations"><![CDATA[presentations]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="urls"><![CDATA[urls]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>scoping intertwingularity</title>
		<link>http://inkdroid.org/2010/07/01/scoping-intertwingularity/</link>
		<pubDate>Thu, 01 Jul 2010 14:16:25 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2067</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.flickr.com/photos/danbri/4030764915/"><img src="http://inkdroid.org/images/share-what-we-know.jpg" style="float: right; border: none; margin-left: 10px; width: 200px;" /></a></p>

<p><a href="http://danbri.org/words/">Dan Brickley's</a> <a href="http://lists.w3.org/Archives/Public/public-lod/2010Jul/0020.html">recent post</a> to the public-lod discussion list about the <a href="http://www.w3.org/2010/06/rdf-work-items/table">future of RDF</a> is one of the best articulations of why I appreciate the practice of linking data:</p>

<blockquote>
And why would anyone care to get all this semi-related, messy Web data? Because problems don't come nicely <a href="http://en.wikipedia.org/wiki/Scope_(project_management)">scoped</a> and packaged into cleanly distinct <a href="http://en.wikipedia.org/wiki/Domain_knowledge">domains</a>. Whenever you try to solve one problem, it borders on a dozen others that are a higher priority for people elsewhere. You think you're working with '<a href="http://www.w3.org/TR/rdfcal/">events</a>' data but find yourself with information describing <a href="http://musicontology.com/">musicians</a>; you think you're describing musicians, but find yourself describing <a href="http://www.w3.org/2003/12/exif/">digital images</a>; you think you're describing digital images, but find yourself describing <a href="http://www.geonames.org/ontology/">geographic locations</a>; you think you're building a database of geographic locations, and find yourself modeling the opening hours of the <a href="http://www.epimorphics.com/public/vocabulary/org.html">businesses</a> based at those locations. To a poet or idealist, these interconnections might be beautiful or inspiring; to a project manager or product manager, they are as likely to be <a href="http://browsertoolkit.com/fault-tolerance.png">terrifying</a>.

Any practical project at some point needs to be able to say "Enough with all this <a href="http://www.youtube.com/watch v=iE2A95HXP4Y">intertwingularity</a>! this is our bit of the problem space, and forget the rest for now". In those terms, a linked Web of RDF data provides a kind of safety valve. By dropping in identifiers that link to a big pile of other people's data, we can hopefully make it easier to keep projects nicely scoped without needlessly restricting future functionality. An events database can remain an events database, but use <a href="http://musicbrainz.org/">identifiers for artists and performers</a>, making it possible to filter events by properties of those participants. A <a href="http://geonames.org">database of places</a> can be only a link or two away from records describing the opening hours or business offerings of the things at those places. <a href="http://linkeddata.org">Linked Data</a> (and for that matter <a href="http://www.foaf-project.org/">FOAF</a>...) is fundamentally a story about information sharing, rather than about triples. Some information is in RDF triples; but lots more is in documents, videos, spreadsheets, custom formats, or [hence FOAF] <a href="http://en.wikipedia.org/wiki/Thought">in people's heads</a>.
</blockquote>

<p>Dan's description is also a nice illustration of how the web can help us avoid <a href="http://joi.ito.com/weblog/2005/03/05/yak-shaving.html">Yak Shaving</a>, by leveraging the work of others:</p>

<blockquote>
Any seemingly pointless activity which is actually necessary to solve a problem which solves a problem which, several levels of recursion later, solves the real problem you're working on.
</blockquote>

<p>I'm just stashing that away here so I can find it again when I need it. Thanks danbri!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2067</wp:post_id>
		<wp:post_date>2010-07-01 07:16:25</wp:post_date>
		<wp:post_date_gmt>2010-07-01 14:16:25</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>scoping-intertwingularity</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="email"><![CDATA[email]]></category>
		<category domain="post_tag" nicename="intertwingularity"><![CDATA[intertwingularity]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="yaks"><![CDATA[yaks]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>linking things and common sense</title>
		<link>http://inkdroid.org/2010/07/07/linking-things-and-common-sense/</link>
		<pubDate>Wed, 07 Jul 2010 21:28:50 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2096</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://en.wikipedia.org/wiki/William_Shakespeare"><img src="http://inkdroid.org/images/qrcode.png" style="border: none; float: right; margin-left: 10px; width: 200px;" /></a>

<a href="http://derivadow.com">Tom Scott's</a> recent <a href="http://derivadow.com/2010/07/01/linked-things/">Linking Things</a> post got me jotting down what I've been thinking lately about URIs, Linked Data and the Web. First go read Tom's <a href="http://derivadow.com/2010/07/01/linked-things/">post</a> if you haven't already. He does a really nice job of setting the stage for why people care about using distinct URIs (web identifiers) for identifying web documents (aka information resources) and real world things (aka non-information resources). Tom's opinions are grounded in the experience of really putting these ideas into practice at the <a href="http://bbc.co.uk">BBC</a>. His key point, which he attributes to <a href="http://twitter.com/fantasticlife/">Michael Smethurst</a>, is that:

<blockquote>
Some people will tell you that the whole non-information resource thing isn’t necessary – we have a web of documents and we just don’t need to worry about URIs for non-information resources; others will claim that everything is a thing and so every URL is, in effect, a non-information resource.

Michael, however, recently made a very good point (as usual): all the interesting assertions are about real world things not documents. The only metadata, the only assertions people talk about when it comes to documents are relatively boring: author, publication date, copyright details etc.

If this is the case then perhaps we should focus on using RDF to describe real world things, and not the documents about those things.
</blockquote>

I think this is an important observation, but I don't really agree with the conclusion. I would conclude instead that the distinction between real world and document URIs is a non-issue. We should be able to tell if the thing being described is a document or a real world thing based on the vocabulary terms that are being used.

For example, if I assert:

<pre>
&lt;http://en.wikipedia.org/wiki/William_Shakespeare&gt; a foaf:Person ; foaf:name "William Shakespeare" .
</pre>

Isn't it reasonable to assume <code>http://en.wikipedia.org/wiki/William_Shakespeare</code> identifies a person whose name is <code>William Shakespeare</code>? I don't have to try to resolve the URL and see if I get a 303 or 200 response code do I? 

And if I also assert, 

<pre>
&lt;http://en.wikipedia.org/wiki/William_Shakespeares&gt; dcterms:modified "2010-06-28T17:02:41-04:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt;
</pre>

can't I can assume that <code>http://en.wikipedia.org/wiki/William_Shakespeare</code> identifies a document that was modified on <code>2010-06-28T17:02:41</code>? Does it really make sense to think that the person <code>William Shakespeare</code> was modified then? Not really...

Similarly if I said, 

<pre>
&lt;http://en.wikipedia.org/wiki/William_Shakespeare&gt; cc:license &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt; .
</pre>

Isn't it reasonable to assume that <code>http://en.wikipedia.org/wiki/William_Shakespeare</code> identifies a document that is licensed with the Attribution-ShareAlike 3.0 Unported license? It doesn't really make sense to say that the person <code>William Shakespeare</code> is licensed with Attribution-ShareAlike 3.0 Unported does it? Not really...

Why does the Linked Data community lean on using identifiers to do this common sense work?  Well, largely because <a href="http://norman.walsh.name/2005/06/19/httpRange-14">people argued about it for three years</a> and this is the resolution the W3C came to. In general I like the REST approach of saying a URL identifies a <em>Resource</em>, and that when you resolve one you get back a <em>Representation</em> (a document of some kind, html, rdf/xml, whatever). Why does it have to be more complicated than that? 

If it's not clear if an assertion is about a document or a thing, why isn't that a problem with the vocabulary in use being underspecified and vague? I believe this is essentially the point that Xiaoshu Wang made three years ago in his paper <a href="http://web.archive.org/web/20100126012209/http://dfdf.inesc-id.pt/tr/web-arch">URI Identity and Web Architecture Revisited</a>.

To get back to Tom's point, I agree that the really interesting assertions in Linked Data are about things, and their relations, or as Richard Rorty said a bit more expansively:

<blockquote>
There is nothing to be known about anything except an initially large, and forever expandable, web of relations to other things. Everything that can serve as a term of relation can be dissolved into another set of relations, and so on for ever. There are, so to speak, relations all the way down, all the way up, and all the way out in every direction: you never reach something which is not just one more nexus of relations.

Philosophy and Social Hope, pp 53-54.
</blockquote>

But assertions about a document, albeit being a bit more on the dry side, are also useful and important, such as: who created the web document, when they created it, a license associated with the document, its relation to previous versions, etc. As a software developer working in a library I'm actually really interested in this sort of administrivia. In fact the <a href="http://www.openarchives.org/ore/">Open Archives Initiative Object Reuse and Exchange</a> vocabulary, and the <a href="http://www.mementoweb.org/">Memento</a> efforts are largely about relating web documents together in meaningful and useful ways: to be able to harvest compound objects out of the web, and to navigate between versions of web documents. Heck, the Dublin Core vocabulary started out as an effort to describe <a href="http://dublincore.org/workshops/dc1/report.shtml">networked resources</a> (essentially documents), and the gist of the <a href="http://www.dublincore.org/documents/dcmi-terms/">Dublin Core Metadata Terms</a> retain much of this flavor. So I think RDF is also important for describing documents on the web, or (more accurately) representations.

So, in short:
<ol>
<li>URLs identify resources.</li>
<li>A resource can be anything.</li>
<li>When you resolve a URL you get a representation of that resource.</li>
<li>If a representation is some sort of flavor of RDF, the semantics of an RDF vocabulary should make it clear what is being described.</li>
<li>If it's not clear, maybe the vocabulary sucks.</li>
</ol>

I think this is basically the point that Harry Halpin and Pat Hayes were making in their paper <a href="http://www.ibiblio.org/hhalpin/homepage/publications/indefenseofambiguity.html">In Defense of Ambiguity</a>. A URL has a dual role: it identifies resources, and it allows us to access representations of resources. This ambiguity is the source of its great utility, expressiveness and power. It's why we see URLs on the sides of buses and buildings. It's why a <a href="http://en.wikipedia.org/wiki/QR_Code">QR Code</a> slapped on some real world thing has a URL embedded in it.

In an ideal world (where people agreed with Xiaoshu, Harry and Pat) I don't think this would mean we would have to redo all the <a href="http://richard.cyganiak.de/2007/10/lod/">Linked Data that we have already</a>. I think it just means that publishers who want the granularity of distinguishing between real world things and documents at the identifier level can have it. It would also mean that the Linked Data space can accommodate <a href="http://mikeschinkel.com/blog/whatisarestafarian/">RESTafarians</a>, and other mere mortals who don't want to ponder whether their resources are information resources or not.  And, of course, it would mean we could use a URL like <a href="http://en.wikipedia.org/wiki/William_Shakespeare">http://en.wikipedia.org/wiki/William_Shakespeare</a> to identify William Shakespeare in our RDF data ... 

Wouldn't that be nice? ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2096</wp:post_id>
		<wp:post_date>2010-07-07 14:28:50</wp:post_date>
		<wp:post_date_gmt>2010-07-07 21:28:50</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>linking-things-and-common-sense</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="httprange-14"><![CDATA[httpRange-14]]></category>
		<category domain="post_tag" nicename="identifiers"><![CDATA[identifiers]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="urls"><![CDATA[urls]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:9:{i:0;s:5:"82719";i:1;s:5:"82720";i:2;s:5:"82721";i:3;s:5:"82722";i:4;s:5:"82723";i:5;s:5:"82724";i:6;s:5:"82729";i:7;s:5:"82730";i:8;s:5:"82733";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82736</wp:comment_id>
			<wp:comment_author><![CDATA[Dilettante&#039;s Ball &raquo; Blog Archive &raquo; What we talk about when we talk about http://dbpedia.org/resource/Love]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://dilettantes.code4lib.org/blog/2010/07/what-we-talk-about-when-we-talk-about-love/</wp:comment_author_url>
			<wp:comment_author_IP>208.83.140.6</wp:comment_author_IP>
			<wp:comment_date>2010-07-09 11:21:56</wp:comment_date>
			<wp:comment_date_gmt>2010-07-09 18:21:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] status quo &#8220;just because that&#8217;s how we&#8217;ve always done it&#8221;, and (implicitly) an httpRange-14 apologist.  Quite frankly, none of these are true or quite what I mean (and I&#8217;m, of course, over [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82719</wp:comment_id>
			<wp:comment_author><![CDATA[Ian Davis]]></wp:comment_author>
			<wp:comment_author_email>nospam@iandavis.com</wp:comment_author_email>
			<wp:comment_author_url>http://iandavis.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>80.187.216.96</wp:comment_author_IP>
			<wp:comment_date>2010-07-07 15:10:30</wp:comment_date>
			<wp:comment_date_gmt>2010-07-07 22:10:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed, I am in complete agreement with you, but I will play devil's advocate. I think you have chosen to compare two things that are quite distinct: web pages and people. Clearly you can distinguish the context of statements made for those types of things because there is not much of an overlap. However, to choose something close to your heart, can I freely use this URI http://chroniclingamerica.loc.gov/lccn/sn99021999/1899-10-22/ed-1/seq-25/ to refer to the first page of issue 19 of the Omaha Illustrated Bee? Is the publisher of the resource identified by that URI the LoC or Edward Rosewater?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>346</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82720</wp:comment_id>
			<wp:comment_author><![CDATA[pkeane.livejournal.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://pkeane.livejournal.com/</wp:comment_author_url>
			<wp:comment_author_IP>128.83.27.7</wp:comment_author_IP>
			<wp:comment_date>2010-07-07 15:29:26</wp:comment_date>
			<wp:comment_date_gmt>2010-07-07 22:29:26</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great post, Ed.  Not only would it be nice, as you say, I think it is the key to making Linked Data work.  As you say: "This ambiguity is the source of its great utility, expressiveness and power."  The Web itself has that sort of ambiguity built  in, and it's what makes it possible to bend w/o breaking.  Good vocabularies are key, as you say, but every vocabulary has a context -- some vocabularies will be clearer w/in their intended domain/context.  Langauge &amp; meaning simply cannot be forced into universal unambiguity.  Being unambiguous w/in a particular domain or context *is* a worthy goal.  I'd contend that Ian's example can be answered by either "you better have a good vocabulary to make that distinction clear" OR "depends on the context..."]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>336</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82721</wp:comment_id>
			<wp:comment_author><![CDATA[Ian Davis]]></wp:comment_author>
			<wp:comment_author_email>nospam@iandavis.com</wp:comment_author_email>
			<wp:comment_author_url>http://iandavis.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>90.216.192.174</wp:comment_author_IP>
			<wp:comment_date>2010-07-07 17:29:52</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 00:29:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Peter, I guess one vocabulary oriented solution to my problem is to have a documentPublisher (or representationPublisher) property distinct from the usual publisher property.

Following on from Michael Smethurst's comment that there are only a limited number of interesting things to say about web documents, perhaps it's possible to create a good enough vocabulary to cover them. i.e. documentLicense, documentPublishDate etc.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>346</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82722</wp:comment_id>
			<wp:comment_author><![CDATA[dbrunton]]></wp:comment_author>
			<wp:comment_author_email>dbrunton@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawkuMzjlM81pM-JvzFsoNviuHmLfn6YLydc</wp:comment_author_url>
			<wp:comment_author_IP>98.117.63.151</wp:comment_author_IP>
			<wp:comment_date>2010-07-07 17:44:06</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 00:44:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This argument reminds me of a conversation I had when I was in China, when I asked a (Chinese) friend how they could distinguish the verb "shuo" which means "to say" in terms of past, present, and future.  There is no verb conjugation (at least not like we have in English) in Chinese.

The person, Dajie, looked at me like I was stupid, and told me that when someone said "Kong zi shuo" (loosely, "Confucius say"), that they meant it in the past, because Confucius is dead, and it had to be in the past.

The idea that there is ambiguity when using a URI is kind of similar.  If I'm talking about a person when I say http://davidbrunton.com, I probably mean David Brunton.  If I'm talking about an article that succinctly explains the whole universe in a word, it's probably that first post.

Anyway, nice post, Ed.  Love the Rorty quote, too :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>327</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82723</wp:comment_id>
			<wp:comment_author><![CDATA[dbrunton]]></wp:comment_author>
			<wp:comment_author_email>dbrunton@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawkuMzjlM81pM-JvzFsoNviuHmLfn6YLydc</wp:comment_author_url>
			<wp:comment_author_IP>98.117.63.151</wp:comment_author_IP>
			<wp:comment_date>2010-07-07 17:44:44</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 00:44:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[PS. I'll email Ross, too.  Just for good measure.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>327</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82724</wp:comment_id>
			<wp:comment_author><![CDATA[dbrunton]]></wp:comment_author>
			<wp:comment_author_email>dbrunton@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawkuMzjlM81pM-JvzFsoNviuHmLfn6YLydc</wp:comment_author_url>
			<wp:comment_author_IP>98.117.63.151</wp:comment_author_IP>
			<wp:comment_date>2010-07-07 17:45:30</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 00:45:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[PS. I'll email Ross, too.  Just in case he didn't read my singularly insightful comment here on http://inkdroid.org.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>327</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82725</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>68.50.54.15</wp:comment_author_IP>
			<wp:comment_date>2010-07-07 18:48:10</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 01:48:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@iand trust you to ask the hard questions :-) I should've have blamed^w credited you for putting the idea in my head with your various posts over the years, and your recent "what would it break" twitter convo with <a href="http://twitter.com/cygri" rel="nofollow">cygri</a>.

I think you nailed the answer in your later comment: there are probably a handful of things that we'd want to be able to assert about the representation, and if something like dcterms:publisher was too ambiguous we'd probably want to define a new term that was by definition about the representation.

As you know the URI http://chroniclingamerica.loc.gov/lccn/sn99021999/1899-10-22/ed-1/seq-25/ is served up by the host chroniclingamerica.loc.gov ; which DNS can tell us is owned by the Library of Congress. So I think it's safe to say that representations being generated by that hostname are effectively published by the Library of Congress. So I'd expect dc:publisher to be used to say something unique about the resource, in this case that a particular page of a newspaper was published by Edward Rosewater. As @pkeane pointed out, the context (named graph, surface?) of related assertions, such as one that said:

<pre>
&lt;http://chroniclingamerica.loc.gov/lccn/sn99021999/1899-10-22/ed-1/seq-25/&gt; a &lt;http://purl.org/ontology/bibo/Newspaper&gt; .
</pre>

could provide some guidance about what dc:publisher is meant to refer to.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82729</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>ad.pohl@googlemail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawl6ihZ1Xph3YoXs1slqKoKqflJvu1wNTV0</wp:comment_author_url>
			<wp:comment_author_IP>194.95.250.30</wp:comment_author_IP>
			<wp:comment_date>2010-07-08 01:37:03</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 08:37:03</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[While I think it is an elegant solution to shift the discrimination of information and non-information resources to vocabulary level by specifying the predicates, I see a problem arising with this approach which is similar to the existing problem:

Basically, I'm afraid people, instead of asking "Why do I need an extra URI for the information ressource?", will just ask "Why do I have to use a different predicate for the real newspaper than for the same scanned newspaper when those predicates actually mean the same?" In effect, the actual use might get inconsistent and the data's value diminishes.

So, I believe this approach wouldn't solve the practical problems Tom writes about because it as well requires people to distinguish information and non-information resources just as the minting of different URIs for information and non-information ressources does. 

I think the underlying problem is: People don't seeing any sense in distinguishing retrievable and non-retrievable ressources or even not being able to do so. So rather this is the task: Making clear why this distinction is useful and desirable.

Adrian]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>348</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82730</wp:comment_id>
			<wp:comment_author><![CDATA[Alan Dix]]></wp:comment_author>
			<wp:comment_author_email>alanjohndix@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/a/KJpchtNj3NXTKnYPc4OuQuW9gQtvt_zI#34670</wp:comment_author_url>
			<wp:comment_author_IP>82.68.252.134</wp:comment_author_IP>
			<wp:comment_date>2010-07-08 03:40:25</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 10:40:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[As well as documents that refer to documents such as the site @iand points to or URIBurner, in principle one might want to talk about the document meta-information itself - maybe we want to identify who added the meta-information such as dc:creator.  The reflective stack is unbounded!

In ordinary language we cope with this sort of thing in the way that Ed suggests for URIs: I can write, "Alan is writing this comment" or "Alan starts with the letter A" without even bothering to put quotes around 'Alan'.  Of course this can cause confusion or be used in verbal puns, so is not unambiguous, but works most of the time.

In some cases, like computing or philosophy these things start to become more complex and ONLY then do we resort to more precise languages "The first ASCII code in the representation of 'Alan' is 65" or "The use of 'Alan' as an example name and example string in the preceding paragraph."

Similar precise language is certainly needed for web resources to be able to distinguish the multiple levels, but maybe the solution would be to adopt Ed's more relaxed style with a set of well known disambiguation rules, and then only be more precise when the disambiguation would fail.

maybe in such cases:


   mymeta:document _:theDoc.
_:theDoc dc:creator "Library of Congress".
_:theDoc dc:title "Omaha daily bee. (Omaha [Neb.]) 187?-1922, October 22, 1899, Image 25 - Chronicling America - The Library of Congress:.

   mymeta:refers_to _:thePaper.
_:thePaper dc:title "Omaha daily bee".

... and then of course:


    mymeta:refers_to
        .

...

just for fun I looked at:

http://linkeddata.uriburner.com/about/html/http://linkeddata.uriburner.com/about/html/http://ontologi.es/rail/void.xhtml

;-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>347</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82733</wp:comment_id>
			<wp:comment_author><![CDATA[Karl Dubost]]></wp:comment_author>
			<wp:comment_author_email>karl@la-grange.net</wp:comment_author_email>
			<wp:comment_author_url>http://karl.dubost.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>209.104.75.194</wp:comment_author_IP>
			<wp:comment_date>2010-07-08 06:27:31</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 13:27:31</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[you said: "In general I think the REST approach of saying a URL identifies a Resource, and that when you resolve one you get back a Representation (a document of some kind, html, rdf/xml, whatever)."

The sentence is not very clear. Or I have a bit of difficulty to parse. Did you mean?

"In general, I agree with the REST approach i.e. A URL identifies a Resource and when you resolve the URL, you get back a Representation (a document of some kind, html, rdf/xml, whatever)."]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>349</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82734</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.245.65</wp:comment_author_IP>
			<wp:comment_date>2010-07-08 06:47:13</wp:comment_date>
			<wp:comment_date_gmt>2010-07-08 13:47:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks Karl, I was just noticing that goof when I was re-reading the post. s/think/like/ ; so yes, the meaning you struggled to get is what I meant to say :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>82733</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86661</wp:comment_id>
			<wp:comment_author><![CDATA[Give Me a Sign: What Do Things Mean on the Semantic Web? | AI3:::Adaptive Information]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.mkbergman.com/994/give-me-a-sign-what-do-things-mean-on-the-semantic-web/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-11-05 10:32:06</wp:comment_date>
			<wp:comment_date_gmt>2013-11-05 17:32:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Ed Summers, 2010. Linking Things and Common Sense, blog post of July 7, 2010. See  http://inkdroid.org/2010/07/07/linking-things-and-common-sense/.[13] Xiaoshu Wang, 2007.  URI Identity and Web Architecture Revisited, Word document posted on [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1383672726.7006909847259521484375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1383673044.511948108673095703125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>federal register embraces the web and opensource</title>
		<link>http://inkdroid.org/2010/07/27/federal-register-embraces-the-web-and-opensource/</link>
		<pubDate>Tue, 27 Jul 2010 15:05:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2174</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://federalregister.gov"><img style="border: none; margin-right: 10px; float: left;" src="http://inkdroid.org/images/federal-register.png"/></a><a href="http://sunlightfoundation.com/people/tlee/">Tom Lee</a> of the <a href="http://sunlightfoundation.com/">Sunlight Foundation</a> <a href="http://sunlightlabs.com/blog/2010/meet-the-new-federal-register/">blogged yesterday</a> about the <a href="http://www.federalregister.gov/">new Federal Register website</a>.  The facelift was also <a href="http://blogs.archives.gov/aotus/?p=1317">announced</a> a few days earlier by the Archivist of the United States, <a href="http://en.wikipedia.org/wiki/David_Ferriero">David Ferriero</a>. If you aren't familiar with it already, the Federal Register is basically <em>the</em> daily newspaper of the United States Federal Government, which details all the rules and regulations of the federal agencies. It is compiled by the <a href="http://www.archives.gov/federal-register/">Office of the Federal Register</a> located in the <a href="http://www.archives.gov/">National Archives</a>, and printed by the <a href="http://www.gpo.gov/">Government Printing Office</a>. As the <a href="http://www.youtube.com/watch?v=ADhP0KSmjkQ">video</a> describing the new site points out, the Federal Register began publication in 1936 in the depths of the <a href="http://en.wikipedia.org/wiki/Great_Depression_in_the_United_States">Great Depression</a> as a way to communicate <em>in one place</em> all that the agencies were <a href="http://en.wikipedia.org/wiki/New_Deal">doing</a> to try to jump start the economy. So it seems like a fitting time to be rethinking the role of the Federal Register.</p>

<p>I'm no usability expert, but just a few minutes browsing the <a href="http://www.federalregister.gov/">new site</a> and comparing it to the <a href="http://www.gpoaccess.gov/fr/">old one</a> make it clear what a leap forward this is. Hopefully the <a href="http://web.archive.org/web/20110117014406/http://www.federalregister.gov/policy/legal_status">legal status</a> of the new site will be ironed out shortly.</p>

<p>Most of all it's great to see that the Federal Register is now a single web application. The service it provides to the American public is important enough to deserve its own dedicated web presence. As the developers point out in <a href="http://www.youtube.com/watch?v=13fLdUyrd7A">their video</a> describing the effort, they wanted to make the Federal Register a "first class citizen of the web"...and I think they are certainly helping do that. This might seem obvious, but often there is a temptation to jam publications from the print world (like the Federal Register) into dumbed down <a href="http://www.gpo.gov/fdsys/">monolithic repositories</a> that treat all "objects" the same. Proponents of this approach tend to characterize one off websites like Federal Register 2.0 as "yet another silo". But I think it's important to remember that <a href="http://www.youtube.com/watch?v=OM6XIICm_qo#t=0m37s">the web was really created to break down the silo walls</a>, and that every well designed web site is actually the antithesis of a silo. In fact, monolithic repository systems that treat all publications as static documents to be uniformly managed are more like silos than these 'one off' dedicated web applications.</p>

<p>As a software developer working in the federal government there were a few things about the Federal Register 2.0 that I found really exciting:</p>

<ul>
    <li>Fruitful collaboration between federal employees and citizen activist/geeks initiated by a <a href="http://sunlightlabs.com/contests/appsforamerica2/">software development contest</a>.</li>
    <li>Extensive use of opensource technologies like <a href="http://www.ruby-lang.org/en/">Ruby</a>, <a href="http://rubyonrails.org/">Ruby on Rails</a>, <a href="http://www.mysql.com/">MySQL</a>, <a href="http://www.sphinxsearch.com/">Sphinx</a>, <a href="http://nginx.org/">nginx</a>, <a href="http://varnish-cache.org/">Varnish</a>, <a href="http://www.modrails.com/">Passenger</a>, <a href="http://httpd.apache.org/">Apache2</a>, <a href="http://www.ubuntu.com/">Ubuntu Linux</a>, <a href="http://wiki.opscode.com/display/chef/Home">Chef</a>. Opensource technologies encourage collaboration by allowing citizen activists/technologists to participate without having to drop a princely sum.</li>
    <li>Release of the <a href="http://github.com/criticaljuncture/fr2/">source code for the website itself</a>, using decentralized revision control (<a href="http://git-scm.com/">git</a>) so that people can easily contribute changes, and see how the site was put together.</li>
    <li>Extensive use of syndicated feeds to communicate how how content is being added to the site, <a href="http://www.federalregister.gov/events/search?conditions[term]=&conditions[location]=20901&conditions[within]=25&commit=Go">ical</a> feeds to keep on top of events going on in your area, and <a href="http://www.federalregister.gov/articles/xml/201/018/147.xml">detailed XML for each entry</a>.</li>
        <li>The <a href="http://federalregister.gov/robots.txt">robots.txt file for the site</a> makes the content fully crawlable by web indexers, except for search related portions of the website. Excluding dynamic search results is often important for performance reasons, but much of the article content can be discovered via links, see below about permalinks. They also have made a <a href="http://sitemaps.org">sitemap</a> available for crawlers to efficiently discover URLs for the content.</li>
    <li>Deployment of the web application to the cloud using Amazon's <a href="http://aws.amazon.com/ec2/">EC2</a> and <a href="http://aws.amazon.com/s3/">S3</a> services. Cloud computing allows computing resources to scale to meet demand. In effect this means that government IT shops don't have to make big up front investments in infrastructure to make new services available. I guess the jury is still out, but I think this will eventually prove to greatly lower the barrier to innovation in the egov sector. It also lets the more progressive developers in government leap frog ancient technologies and bureaucracies to get things done in a timely manner.</li> 
    <li>And last, but certainly not least ... <strong>now every entry in the Federal Register has a URL!</strong>. Permalinks for the Federal Register are incredibly important for citability reasons. I predict that we'll quickly see more and more people referencing specific parts of the Federal Register in social media sites like Facebook, Twitter and out on the open web in blogs, and in collaborative applications like Wikipedia.</li>
</ul>

<p>I would like to see more bulk access to XML data made available, for re-purposing on other websites--although I guess it might be able to walk from the syndicated feeds to the detailed XML. Also, the search functionality is so rich it would be useful to have an <a href="http://en.wikipedia.org/wiki/OpenSearch">OpenSearch</a> description that documents it, and perhaps provides some hooks for getting back JSON and/or XML representations. Perhaps even following the lead of the <a href="http://www.london-gazette.co.uk/">London Gazette</a> and trying to make some of the structured metadata available in the the HTML using RDFa. It also looks like content is only available for 2008 on, so it might be interesting to see how easy it would be to make more of the historic content available.</p>

<p>But the great thing about what these folks have done is now I can fork the project on github, see how easy it is to add the changes, and let the developers know about my updates to see if they are worth merging back into the production website. This is an incredible leap forward for egov efforts--so hats off to everyone who helped make this happen.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2174</wp:post_id>
		<wp:post_date>2010-07-27 08:05:52</wp:post_date>
		<wp:post_date_gmt>2010-07-27 15:05:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>federal-register-embraces-the-web-and-opensource</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="apache"><![CDATA[apache]]></category>
		<category domain="post_tag" nicename="egov"><![CDATA[egov]]></category>
		<category domain="post_tag" nicename="github"><![CDATA[github]]></category>
		<category domain="category" nicename="government"><![CDATA[government]]></category>
		<category domain="post_tag" nicename="gpo"><![CDATA[gpo]]></category>
		<category domain="post_tag" nicename="mysql"><![CDATA[mysql]]></category>
		<category domain="post_tag" nicename="nara"><![CDATA[nara]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"82744";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82747</wp:comment_id>
			<wp:comment_author><![CDATA[Political Mashup &raquo; Archief &raquo; Federal Register 2.0]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://politicalmashup.nl/2010/07/federal-register-20/</wp:comment_author_url>
			<wp:comment_author_IP>146.50.22.166</wp:comment_author_IP>
			<wp:comment_date>2010-07-28 00:03:03</wp:comment_date>
			<wp:comment_date_gmt>2010-07-28 07:03:03</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] [1] http://sunlightlabs.com/blog/2010/meet-the-new-federal-register/ [2] http://www.federalregister.gov/ [3] http://inkdroid.org/2010/07/27/federal-register-embraces-the-web-and -opensource/ [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82744</wp:comment_id>
			<wp:comment_author><![CDATA[Bob Burbach]]></wp:comment_author>
			<wp:comment_author_email>bob.burbach@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://rburbach.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>64.134.102.202</wp:comment_author_IP>
			<wp:comment_date>2010-07-27 09:39:56</wp:comment_date>
			<wp:comment_date_gmt>2010-07-27 16:39:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the great remarks on the new site. As one of the citizen/geeks who developed the project we are excited for where the site will evolve moving forward. We'll definitely keep in mind your comments regarding OpenSearch and RDFa.

I would like to clarify that content is available as far back as 1994 on the site (when the Register was first digitized) though the XML only is available as far back as 2000. 

XML for an article is made available on each article page. Also the bulk data remains available from the GPO where we pull from every morning - 
So for today:
http://www.gpo.gov/fdsys/bulkdata/FR/2010/07/FR-2010-07-27.xml
http://www.gpo.gov:80/fdsys/pkg/FR-2010-07-27/mods.xml

Also available on Data.gov (http://www.data.gov/raw/1325)

Keep the suggestions rolling!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>352</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86943</wp:comment_id>
			<wp:comment_author><![CDATA[The Archive as Data Platform | inkdroid]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2014/04/25/the-archive-as-data-platform/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-26 05:06:13</wp:comment_date>
			<wp:comment_date_gmt>2014-04-26 12:06:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] pointer to that Cassie) and the Government Printing Office&#8217;s Bulk Data website (see previous post and comments about the FederalRegister), where this activity can take place, and where a dialogue can happen [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1398847193.8367578983306884765625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1398513973.49390411376953125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>archival context on the web</title>
		<link>http://inkdroid.org/2010/08/12/archival-context-on-the-web/</link>
		<pubDate>Thu, 12 Aug 2010 22:16:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2210</guid>
		<description></description>
		<content:encoded><![CDATA[On <a href="http://matienzo.org/">Mark's</a> advice I attended the <a href="http://gslis.simmons.edu/blogs/jobs/2010/06/01/eac-cpf-moving-forward-with-authority-nara/">Moving Forward With Authority</a> Society of American Archivists pre-conference, which focused on the role of authority control in archival descriptions. There were lots of presentations/discussions over the day, but by and large most of them revolved around the recent release of the Encoded Archival Context: Corporate Bodies, Persons and Families (<a href="http://eac.staatsbibliothek-berlin.de/">EAC-CPF</a>) XML schema. Work on the EAC-CPF began in 2001 with the <a href="http://www.library.yale.edu/eac/torontotenets.htm">Toronto Tenets</a>, which articulated the need for encoding not only the contents of archival finding aids (w/ <a href="http://www.loc.gov/ead/">Encoded Archival Description</a>), but also the context (people, families, organizations) that the finding aid referenced:

<blockquote>
Archival context information consists of information describing the circumstances under which records (defined broadly here to include personal papers and records of organizations) have been created and used.  This context includes the identification and characteristics of the persons, organizations, and families who have been the creators, users, or subjects of records, as well as the relationships amongst them. 
<em><a href="http://www.library.yale.edu/eac/torontotenets.htm">Toronto Tenets</a></em>
</blockquote>


I'll admit I'm jaded. So much XML has <a href="http://www.dlib.indiana.edu/~jenlrile/metadatamap/">gone under the bridge</a>, that it's hard for me to get terribly excited (anymore) about yet-another-xml-schema. Yes, structured data is good. Yes, encouraging people make their data available similarly is important. But for me to get hooked I need a story for how this structured data is going to live, and be used on the Web. This is the main reason I found <a href="http://www.iath.virginia.edu/~dvp4c/">Daniel Pitti's</a> talk about the Social Networks of Archival Context (<a href="http://socialarchive.iath.virginia.edu/">SNAC</a>) to be so exciting.

SNAC is an NEH (and possibly IMLS soon) funded project of the University of Virginia, California Digital Library, and the Berkeley School for Information. The project's general goal is to:

<blockquote>
... unlock descriptions of people from descriptions of their records and link them together in exciting new ways.
</blockquote>

Where "descriptions of their records" are EAD XML documents, and "linking them together" means exposing the entities buried in finding aids (people, organizations, etc), assigning identifiers to them, and linking them together on the web. I guess I might be reading what I want a bit into the goal based on the presentation, and my interest in Linked Data. If you are interested, more accurate information about the project can be found in the <a href="http://socialarchive.iath.virginia.edu/proposal.html">NEH Proposal</a> that this quote came from. 

Even though SNAC was only very recently funded, Daniel was already able to demonstrate a prototype application that he and <a href="http://www.cdlib.org/contact/staff_directory/btingle.html">Brian Tingle</a> worked on. If I'm remembering this right, Daniel basically got a hold of the EAD finding aids from the Library of Congress, extracted contextual information (people, families, corporate names) from relevant EAD elements, and serialized these facts as EAC-CPF documents. Brian then imported the documents using an <a href="http://bitbucket.org/btingle/cpf2html/wiki/Home">extension</a> to the <a href="http://sourceforge.net/apps/trac/xtf">eXtensible Text Framwork</a>, which allowed XTF to be EAC-CPF aware.

The end result is a web application that lets you view distinct web pages for individuals mentioned in the archival material.  For example here's one for <a href="http://en.wikipedia.org/wiki/John_von_Neumann">John von Neumann</a>

<a href="http://inkdroid.org/images/neumann-snac.png"><img src="http://inkdroid.org/images/neumann-snac.png" style="border: none;" /></a>

All those names in the list on the right are themselves hyperlinks which take you to that person's page. If you were able to scroll down (the prototype hasn't been formally launched yet) you could see links to corporate names like Los Alamos Scientific Laboratory, The Institute for Advanced Study, US Atomic Energy Commission, etc. You would also see a Resources section that lists related finding aids and books, such as the <a href="http://lcweb2.loc.gov/cgi-bin/faidfrquery/r?faid/faidfr:@field(SOURCE+@band(von+neumann+john))">John von Neumann Papers finding aid</a> at the Library of Congress.

I don't think I was the only one in the audience to immediately see the utility of this. In fact it is territory well trodden by OCLC and the other libraries involved in the <a href="http://viaf.org">VIAF project</a> which essentially creates web pages for authority records for people like <a href="http://viaf.org/viaf/99899730/">John Von Neumann</a> who have written books. It's also similar to what <a href="http://trove.nla.gov.au/people">People Australia</a>, <a href="http://bibapp.org/">BibApp</a> and <a href="http://vivoweb.org/">VIVO</a> are doing to establish richly linked public pages for people. As Daniel pointed out: archives, libraries and museums do a lot of things differently; but ultimately they all have a deep and abiding interest in the intellectual output, and artifacts created by <em>people</em>. So maybe this is an area where we can see more collaboration across the cultural divides between cultural heritage institutions. The activity of putting EAD documents, and their transformed HTML cousins on the web is important. But for them to be more useful they need to be contextualized in the web itself using applications like this SNAC prototype.

I immediately found myself wondering if the URL say for this SNAC view of John von Neumann could be considered an identifier for the EAC-CPF record. And what if the HTML contained the structured EAC-CPF data using xml+xslt, a microformat, rdfa or a link rel pointing at an external XML document? Notice I said <em>an</em> instead of <em>the</em> identifier. If something like EAC-CPF is going to catch on lots of archives would need to start generating (and publishing) them. Inevitably there would be duplication: e.g. multiple institutions with their own notion of John von Neumann. I think this would be a good problem to have, and that having web resolvable identifiers for these records would allow them to be knitted together. It would also allow hubs of EAC-CPF to bubble up, rather than requiring some single institution to serve as *the* master database (as in the case of VIAF).

A few things that would be nice to see for EAC-CPF would be:
<ul>
	<li>Instructions on how to link EAD documents to EAC-CPF documents.</li>
        <li>Recommendations on how to make EAC-CPF data available on the Web.</li>
        <li>A wikipage or some low-cost page for letting people share where they are publishing EAC-CPF.</li>
</ul>

In addition I think it would be cool to see:

<ul>
	<li>A microformat for making EAC-CPF data available in HTML. If not an official <a href="http://microformats.org">Microformat</a> then <a href="http://microformats.org/wiki/posh">Plain Old Semantic HTML</a> will do. It might also be possible to leverage some existing Microformats like hCard.</li>
        <li>A EAC-CPF RDF schema for embedding EAC-CPF data in HTML using <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a> and other Linked Data environments. Heck, <a href="http://developers.facebook.com/docs/opengraph">Facebook is doing it</a>. It might also be useful to leverage existing vocabularies like <a href=''http://xmlns.com/foaf/spec/">FOAF</a>, <a href="http://www.epimorphics.com/public/vocabulary/org.html">Organization Ontology</a>, etc using a <a href="http://dublincore.org/usage/documents/profile-guidelines/">Dublin Core Application Profile</a>.</li>
        
</ul>

Anyhow I just wanted to take a moment to say how exciting it is to see the stuff hiding in finding aids making it out onto the Web, with URLs for resources like People, Corporations and Families. I hope to see more as the SNAC project integrates more with existing name authority files (work that <a href="http://people.ischool.berkeley.edu/~ray/">Ray Larson</a> at Berkeley is going to be doing), and importing finding aids from more institutions with different EAD encoding practices.






]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2210</wp:post_id>
		<wp:post_date>2010-08-12 15:16:19</wp:post_date>
		<wp:post_date_gmt>2010-08-12 22:16:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>archival-context-on-the-web</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="authorities"><![CDATA[authorities]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="people"><![CDATA[people]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<category domain="post_tag" nicename="xml-schema"><![CDATA[xml schema]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>83033</wp:comment_id>
			<wp:comment_author><![CDATA[SNAC: The Social Networks and Archival Context Project &laquo; Walker Sampson]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://wsampson.wordpress.com/2010/08/24/snac-the-social-networks-and-archival-context-project/</wp:comment_author_url>
			<wp:comment_author_IP>66.135.48.211</wp:comment_author_IP>
			<wp:comment_date>2010-08-24 10:05:25</wp:comment_date>
			<wp:comment_date_gmt>2010-08-24 17:05:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] 24, 2010   A post over at Inkdroid highlights the SNAC project, an effort to uncover and formalize person and agent data that is [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85346</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-02-27 04:43:55</wp:comment_date>
			<wp:comment_date_gmt>2012-02-27 11:43:55</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ryan, no I haven't seen any examples of how to use EAC in HTML...yet. As far as I know EAC is still XML schema based. What I was thinking is that it might be interesting to see how to use microdata and schema.org vocabularies or rdfa and existing rdf vocabularies to express the same XML data in HTML. Perhaps an experiment is in order?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1330343039.1937";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85345</wp:comment_id>
			<wp:comment_author><![CDATA[ryanprice]]></wp:comment_author>
			<wp:comment_author_email>rprice@ryanpricemedia.com</wp:comment_author_email>
			<wp:comment_author_url>http://ryanpricemedia.com</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-02-25 16:03:29</wp:comment_date>
			<wp:comment_date_gmt>2012-02-25 23:03:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi ed,

I found your post because I've been asked to create an obituary website that produces EAC - do you know of anyone who is embedding EAC in HTML as you discussed in your "cool to see" at the end of the post?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>429</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1330211009.4856";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:9:"ryanprice";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1330220943.6135";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>version control and digital curation</title>
		<link>http://inkdroid.org/2010/08/17/version-control-and-digital-curation/</link>
		<pubDate>Tue, 17 Aug 2010 17:32:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2243</guid>
		<description></description>
		<content:encoded><![CDATA[<p>For some time now I have been meaning to write about some of the issues related to version control in repositories as they relate to some projects going on at $work. Most repository systems have a requirement to maintain original data as submitted. But as we all know this content often changes over time--sometimes immediately. Change is in the very nature of digital preservation; as archaic formats are migrated to fresher, more usable ones, and the wheels of <a href="http://www.kk.org/thetechnium/archives/2008/12/movage.php">movage</a> keep turning. At the same time it's essential that when content is used and cited that the specific version in time is cited, or as Cliff Lynch is quoted as saying in the <a href="http://www.oclc.org/research/activities/past/rlg/trustedrep/default.htm">Attributes of a Trusted Repository</a>:</p>

<blockquote>
It is very easy to replace an electronic dataset with an updated copy, and ... the replacement can have wide-reaching effects. The processes of authorship ... produce different versions which in an electronic environment can easily go into broad circulation; if each draft is not carefully labeled and dated it is difficult to tell which draft one is looking at or whether one has the “final” version of a work.
</blockquote>

<p>For example at $work we have a pilot project to process journal content submitted by publishers. We don't have the luxury of telling them exactly what content to submit (packaging formats, xml schemas, formats, etc), but on receipt we want to normalize the content for downstream applications by <a href="http://purl.org/net/bagit">bagging</a> it up, and then extracting some of the content into a standard location.</p>

<p>So a publisher makes an issue of a journal available for pickup via FTP:</p>

<pre>
adi_v2_i2
|-- adi_015.pdf
|-- adi_015.xml
|-- adi_018.pdf
|-- adi_018.xml
|-- adi_019.pdf
|-- adi_019.xml
|-- adi_v2_i2_ofc.pdf
|-- adi_v2_i2_toc.pdf
`-- adi_v2_i2_toc.xml
</pre>

<p>The first thing we do is <a href="http://purl.org/net/bagit">bag</a> up the content, to capture what was retrieved (manifest + checksums) and stash away some metadata about where it came from.</p>

<pre>
adi_v2_i2
|-- bag-info.txt
|-- bagit.txt
|-- data
|   |-- adi_015.pdf
|   |-- adi_015.xml
|   |-- adi_018.pdf
|   |-- adi_018.xml
|   |-- adi_019.pdf
|   |-- adi_019.xml
|   |-- adi_v2_i2_ofc.pdf
|   |-- adi_v2_i2_toc.pdf
|   `-- adi_v2_i2_toc.xml
`-- manifest-md5.txt
</pre>

<p>Next we run some software that knows about the particularities of this publishers content, and persist it into the bag in a predictable, normalized way:</p>

<pre>
adi_v2_i2
|-- bag-info.txt
|-- bagit.txt
|-- data
|   |-- adi_015.pdf
|   |-- adi_015.xml
|   |-- adi_018.pdf
|   |-- adi_018.xml
|   |-- adi_019.pdf
|   |-- adi_019.xml
|   |-- adi_v2_i2_ofc.pdf
|   |-- adi_v2_i2_toc.pdf
|   |-- adi_v2_i2_toc.xml
|   `-- eco
|       `-- articles
|           |-- 1
|           |   |-- article.pdf
|           |   |-- meta.json
|           |   `-- ocr.xml
|           |-- 2
|           |   |-- article.pdf
|           |   |-- meta.json
|           |   `-- ocr.xml
|           `-- 3
|               |-- article.pdf
|               |-- meta.json
|               `-- ocr.xml
`-- manifest-md5.txt
</pre>

<p>The point of this post isn't the particular way these changes were layered into the filesystem--it could be done in a multitude of other ways (mets, oai-ore, mpeg21-didl, foxml, etc). The point is rather that the data has undergone two transformations very soon after the it was obtained. And if we are successful, it would undergo many more as it is preserved over time. The advantage of layering content like this into the filesystem is that makes some attempt to preserve the data by disentangling it from the code that operates on it, which will hopefully be swapped out at some point. At the same time we want to be able to get back to the original data, and to also possibly rollback changes which inadvertently corrupted or destroyed portions of the data. Shit happens.</p>

<p>So the way I see it we have three options:</p>

<ul>
<li>Ignore the problem and hope it will go away.</li>
<li>Create full copies of the content, and link them together</li>
<li>Version the content</li>
</ul>

<p>While tongue in cheek, option 1 is sometimes reality. Perhaps your repository content is such that it never changes. Or if it does, it happens in systems that are downstream from your repository. The overhead of managing versions isn't something that you need to worry about...yet. I've been told that <a href="http://dspace.org">DSpace</a> doesn't currently handle versioning, and that they are looking to upcoming <a href="http://web.archive.org/web/20110812055244/https://wiki.duraspace.org/display/DSPACE/DSpace-Fedora+Integration+FAQ">Fedora integration</a> to provide versioning options.</p>

<p>In option 2 version control is achieved by copying the files that make up a repository object, making modifications to the relevant files, and then linking this new copy to the older copy. We currently do this in our internal handling of the content in the National Digital Newspaper Program, where we work at what we call the <em>batch level</em>, which is essentially a directory of content shipped to the Library of Congress, containing a bunch of jp2, xml, pdf and tiff files. When we have accepted a batch, and later have discovered a problem, we then version the batch by creating a copy of the directory. When service copies of the batches are 50G or so, these full copies can add up. Sometimes just to make a few small modifications to some of the XML files we end up having what appears to be largely redundant copies of multi-gigabyte directories. Disk is cheap as they say, but it makes me feel a bit dirty.</p>

<p>Option 3 is to leverage some sort of version control system. One of the criteria I have for a version control system for data is that versioned content should not be dependent on a remote server. The reason for this is I want to be able to push the versioned content around to other systems, tape backup, etc and not have it become stale because some server configuration happened to change at some point. So in my mind, subversion and cvs are out. Revision control systems like <a href="http://www.gnu.org/software/rcs/">rcs</a>, <a href="http://git-scm.com/">git</a>, <a href="http://mercurial.selenic.com/">mercurial</a>, <a href="http://bazaar.canonical.com/en/">bazaar</a> on the other hand are possibilities. Some of the nice things about using git or mercurial:</p>

<ul>
    <li>they are very active projects, so there are lots of eyes on the code fixing bugs, making enhancements</li>
        <li>the content repositories are distributed, so they allow content to migrate into other contexts</li>
    <li>developers who come to inherit content will be familiar with using them</li>
        <li>you get audit logs for free</li>
        <li>they include functionality to publish content on the web, and to copy or clone repository content</li>
</ul>

<p>But there are trade offs with using git, mercurial, etc since these are quite complex pieces of software. Changes in them sometimes trigger backwards incompatible changes in repository formats--which require upgrading the repository. Fortunately there are tools for these upgrades, but one must stay on top of them. These systems also have a tendency to see periods of active use, followed by an exodus to a newer shinier version control system. But there are usually tools to convert your repositories from the old to the new.</p>

<p>To anyone familiar with digital preservation this situation should seem pretty familiar: format migration, or more generally <a href="http://en.wikipedia.org/wiki/Data_migration">data migration</a>.</p>

<p>The <a href="http://www.cdlib.org/">California Digital Library</a> have recognized the need for version control, but have taken a slightly different approach. CDL created a specification for a simplified revision control system called <a href="https://confluence.ucop.edu/display/Curation/ReDD">Reverse Directory Deltas (ReDD</a>), and built an implementation into their <a href="http://www.cdlib.org/services/uc3/curation/storage.html">Storage Service</a>. The advantage that ReDD has is that the repository format is quite a bit more transparent, and less code dependent than other repository formats like git, mercurial, etc. It is also a specification that CDL manages, and can change at will, rather than being at the mercy of an external opensource project. As an exercise about a year ago I wanted to see how easy it was to use ReDD by <a href="http://github.com/edsu/dflat">implementing it</a>. I ended up calling the tool dflat, since ReDD fits in with CDL's filesystem convention <a href="https://confluence.ucop.edu/display/Curation/D-flat">D-flat specification</a>.</p>

<p>In the pilot project I mentioned above I started out wanting to use ReDD. I didn't really want to use all of D-flat, because it did a bit more than I needed, and thought I could refactor the ReDD bits of my <a href="http://github.com/edsu/dflat">dflat code</a> into a ReDD library, and then use it to version the journal content. But in looking at the code a year later I found myself wondering about the trade offs again. Mostly I wondered about the bugs that might be lurking in the code, which nobody else was really using. And I thought about new developers that might work on the project, and who wouldn't know what the heck ReDD was and why I wasn't using something more common.</p>

<p>So I decided to be lazy, and use mercurial instead.</p>

<p>I figured this was a pilot project, so what better way to get a handle on the issues of using distributed revision control for managing data? The project was using the Python programming language extensively, and Mercurial is written in Python -- so it seemed like a logical choice. I also began to think of the problem of upgrading software and repository formats as another facet to the format migration problem...which we wanted to have a story for anyway.</p>

<p>I wanted to start a conversation about some of these issues at <a href="http://groups.google.com/group/digital-curation/web/curation-technology-sig">Curate Camp</a>, so I wrote a pretty simplistic tool that compares git, mercurial and dflat/redd in terms of the time it takes to initialize a repository, and commit changes to an arbitrary set of content in a directory called 'data'. For example I ran it against a gigabyte of mp3 content, and it generates the following output:</p>

<pre>
ed@curry:~/Projects$ git clone git://github.com/edsu/versioning-metrics.git
ed@curry:~/Projects/versioning-metrics$ cp ~/Music data
ed@curry:~/Projects/versioning-metrics$ ./compare.py 

create repository times
git        0:02:16.842117
hg         0:02:48.397038
dflat      0:01:21.196579

repository size
git        1233106981
hg         1287443209
dflat      1347108177

checkout sizes
git        2580197304
hg         2634533532
dflat      2694216030

commit one byte change to all files
git        0:01:03.694969
hg         0:00:48.456455
dflat      0:00:22.926949

repository sizes after commit
git        1233112423
hg         1287450930
dflat      1347166431
</pre>

<p>Anyhow this is just the start of some analysis. I'd like to see how it performs on <em>text content</em> where there are probably some compression gains to be had in the repository size. I think this also demonstrates how it might not be feasible to version large amounts of content as a synchronous operation in a web application. Hopefully this will be a useful seed for a discussion at Curate Camp. I don't want to pretend to have all the answers here, and want to have more conversations about approaches within the digital curation community.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2243</wp:post_id>
		<wp:post_date>2010-08-17 10:32:07</wp:post_date>
		<wp:post_date_gmt>2010-08-17 17:32:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>version-control-and-digital-curation</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="cdl"><![CDATA[cdl]]></category>
		<category domain="category" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="git"><![CDATA[git]]></category>
		<category domain="post_tag" nicename="mercurial"><![CDATA[mercurial]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="redd"><![CDATA[redd]]></category>
		<category domain="post_tag" nicename="version-control"><![CDATA[version control]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>top hosts referenced in english wikipedia</title>
		<link>http://inkdroid.org/2010/08/21/top-hosts-referenced-in-english-wikipedia/</link>
		<pubDate>Sun, 22 Aug 2010 06:01:51 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2277</guid>
		<description></description>
		<content:encoded><![CDATA[I've recently been <a href="http://github.com/edsu/linkypedia">experimenting a bit </a>to provide some tools to allow libraries, archives and museums to see how Wikipedians are using their content as primary source material. I didn't actually anticipate the interest in having a specialized tool like linkypedia to monitor who is using your institutions content on Wikipedia. So, the demo site is having some scaling problems--not the least of which is the feeble VM that it is running on. That's why I wanted to make the code available for other people to run where it made sense. At least before I had some time to think through how to scale it better.

Anyhow, I wanted to get a handle on <em>just how many</em> external links there are in the full snapshot of English Wikipedia. A month or so ago <a href="http://jakobvoss.de/">Jakob Voss</a> pointed me at the External Links SQL dump over at <a href="http://dumps.wikimedia.org/enwiki/latest/">Wikipedia</a> as a possible way to circumvent heavy use of the <a href="http://en.wikipedia.org/w/api.php">Wikipedia's API</a>, by providing a baseline to update against. So I thought to myself that I could just suck this down, and import it into MySQL and run some analysis on that to see how many links and what sorts of host name concentrations there were.

Sucking down the file didn't take too long. But the mysql import on the dump spent about 24hrs (on my laptop) before I killed it. On a hunch I peeked into the 4.5G sql file and noticed that the table had several indexes defined. So I went through some contortions with <a href="http://www.oreillynet.com/linux/cmd/cmd.csp?path=c/csplit">csplit</a> to remove the indexes from the DDL, and lo and behold it loaded in something like 20 minutes. Then I wrote a some python to query the database, get each external link url, extract the hostname from the url, and write it out through a unix pipeline to count up the unique hostnames:

<pre>
./hosts.py | sort -S 1g | uniq -c | sort -rn > enwiki-externallinks-hostnames.txt
</pre>

This is a little unix trick my old boss Fred Lindberg taught me years ago, and it stills works remarkably well: 30,127,734 urls were sorted into 2,162,790 unique domains in another 20 minutes or so. If you are curious the full output is available <a href="http://inkdroid.org/data/enwiki-externallinks-hostnames.txt.gz">here</a>. The number 1 host was <a href="http://toolserver.org">toolserver.org</a> with 3,169,993 links. This wasn't too surprising since it is a hostname heavily used by wikipedians as they go about their business. Next is <a href="http://www.google.com">www.google.com</a> at 2,117,967 links, which appeared to be quite a few canned searches. This wasn't terribly exciting either. So I removed toolserver.org and www.google.com (so as not to visually skew things too much), and charted the rest of the top 100:

<img src="http://inkdroid.org/images/en-wikipedia-externallinks-chart.png" alt="Top 100-2 Hostnames in Wikipedia External Links" />

I figured that could be of some interest to somebody, sometime. I didn't find similar current stats available anywhere on the web. But if you know of them please let me know. The high ranking of <a href="http://www.ncbi.nlm.nih.gov">www.ncbi.nlm.nih.gov</a> and <a href="http://dc.doi.org">dx.doi.org</a> were pleasant surprises. I did a little superficial digging and found some <a href="http://stats.wikimedia.org/EN/TablesWikipediaEN.htm#bots">fascinating bots</a> like <a href="http://en.wikipedia.org/wiki/User:Citation_bot">Citation Bot</a> and <a href="http://en.wikipedia.org/wiki/User:ProteinBoxBot">ProteinBoxBot</a>, which seem to trawl external article databases looking for appropriate Wikipedia pages to add links to. Kind of amazing. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2277</wp:post_id>
		<wp:post_date>2010-08-21 23:01:51</wp:post_date>
		<wp:post_date_gmt>2010-08-22 06:01:51</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>top-hosts-referenced-in-english-wikipedia</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="mysql"><![CDATA[mysql]]></category>
		<category domain="post_tag" nicename="unix"><![CDATA[unix]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>82993</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2010-08-23 06:39:23</wp:comment_date>
			<wp:comment_date_gmt>2010-08-23 13:39:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[From Jodi Schneider, whose OpenID doesn't work with my site:

<blockquote>
Did you get the dump of just article space, or everything (e.g. Talk, File, ....)?
http://inkdroid.org/2010/08/21/top-hosts-referenced-in-english-wikipedia/
I'm surprised that Google would be so highly-used in article space.

One possible audience for your stats: wiki-research-l@lists.wikimedia.org
</blockquote>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>82994</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2010-08-23 06:43:07</wp:comment_date>
			<wp:comment_date_gmt>2010-08-23 13:43:07</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Jodi: I pulled down the complete dump of external links from <a href="http://dumps.wikimedia.org/enwiki/latest/" rel="nofollow">here</a>. I'm pretty sure this includes all external links, not just from articles. The externallinks table only has a numeric page id, so it wasn't possible for me to limit to only articles. But I'm planning on pulling down the pages data, so I'll theoretically be able to join with that, and limit just to articles. Thanks for the great feedback!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>notes on retooling libraries</title>
		<link>http://inkdroid.org/2010/08/24/notes-on-retooling-libraries/</link>
		<pubDate>Tue, 24 Aug 2010 17:43:43 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2301</guid>
		<description></description>
		<content:encoded><![CDATA[<p>If you work in the digital preservation field and haven't seen Dorothea Salo's <a href="http://www.ariadne.ac.uk/issue64/salo/">Retooling Libraries for the Data Challenge</a> in the latest issue of <a href="http://www.ariadne.ac.uk/issue64/">Ariadne</a> definitely give it a read. Dorothea takes an unflinching look at the at the scope and characteristics of data assets currently being generated by scholarly research, and how equipped traditional digital library efforts are to deal with it. I haven't seen so many of the issues I've had to deal with (largely unconsciously) as part of my daily work so neatly summarized before. Having them laid out in such a lucid, insightful and succinct way is really refreshing--and inspiring.</p>

<p>In the section on <em>Taylorist Production Processes</em>, Dorothea makes a really good point that libraries have tended to optimize workflows for particular types of materials (e.g. photographs, books, journal articles, maps). When materials require a great deal of specialized knowledge to deal with, and the tools that are used to manage and provide access to the content are similarly specialized, it's not hard to understand why this balkanization has happened. On occasion I've heard folks (/me points finger at self) bemoan the launch of a new website as the creation of "yet another silo". But the technical silos that we perceive are largely artifacts of the social silos we work in: archives, special collections, rare books, maps, etc. The <em>collections</em> we break up our <em>libraries</em> into...the <em>projects</em> that mirror those collections. We need to work better together before we can build common digital preservation tools. To paraphrase something <a href="http://davidbrunton.com/">David Brunton</a> has said to me before: we need to think of our collections more as sets of things that can be rearranged at will, with ease and even impunity. In fact the architecture of the Web (and each website on it) is all about doing that.</p>

<p>Even though it can be tough (particularly in large organizations) I think we can in fact achieve some levels of common tooling (in areas like storage and auditing); but we must admit (to ourselves at least) that some levels of access will likely always be specialized in terms of technical infrastructure and user interface requirements:</p>

<blockquote>
Some, though not all, data can be shoehorned into a digital library not optimised for them, but only at the cost of the affordances surrounding them. Consider data over which a complex Web-based interaction environment has been built. The data can be removed from the environment for preservation, but only at the cost of loss of the specialised interactions that make the data valuable to begin with. If the dataset can be browsed via the Web interface, a static Web snapshot becomes possible, but it too will lack sophisticated interaction. <em>If the digital library takes on the not inconsiderable job of recreating the entire environment, it is committing to rewriting interaction code over and over again indefinitely as computing environments change.</em>
</blockquote>

<p>Dorothea's statement about committing to rewriting interaction code over and over again is important. I'm a software developer, and a web developer to boot -- so there's nothing I like more than yanking the data out of one encrusted old app, and creating it a-fresh using the web-framework-du-jour. But in my heart of hearts I know that while this may work for large collections of homogeneous data, it doesn't scale very well for a vast sea of heterogeneous data. However, all is not lost. As the JISC are fond of saying:</p>

<blockquote>
The coolest thing to do with your data will be thought of by someone else.
</blockquote>

<p>So why don't us data archivers get out of the business of building the "interaction code". Maybe our primary service should be to act as data wholesalers who collect it, and make it available in bulk to those who <em>do</em> want to build access layers on top of it. Lets make our data easy for other people to use (with clear licensing) and reference (with web identifiers) so that they can annotate it, and we can pull back those annotations and views. In a way this is kind of hearkening back to the idea of Data Providers and Service Providers that was talked about a lot in the context of <a href="http://www.dlib.org/dlib/february00/vandesompel-oai/02vandesompel-oai.html">OAI-PMH</a>. But in this case we'd be making the objects available as well as the metadata that describes them, similar to the use cases around <a href="http://www.openarchives.org/ore/">OAI-ORE</a>. I got a chance to chat with <a href="http://twitter.com/kzwa">Kate Zwaard</a> of the <a href="http://gpo.gov">GPO</a> at <a href="http://groups.google.com/group/digital-curation/web/curation-technology-sig">CurateCamp</a> a few weeks ago, and learned how the new <a href="http://inkdroid.org/2010/07/27/federal-register-embraces-the-web-and-opensource/">Federal Register</a> is a presentation application for raw XML data being made available by the GPO. Part of the challenge is making these flows of data public, and giving credit where credit is due -- not only to the creators of the shiny site you see, but to the folks behind the scenes who make it possible.</p>

<p>Another part of Dorothea's essay that stuck out a bit for me, was the advice to split ingest, storage and access systems.</p>

<blockquote>
Ingest, storage, and end-user interfaces should be as loosely coupled as possible. Ideally, the same storage pool should be available to as many ingest mechanisms as researchers and their technology staff can dream up, and the items within should be usable within as many reuse, remix, and re-evaluation environments as the Web can produce.
</blockquote>

<p>This is something we (myself and other folks at LC) did as part of the tooling to support the <a href="http://www.neh.gov/projects/ndnp.html">National Digital Newspaper Program</a>. Our initial stab at the software architecture was to use <a href="http://www.fedora-commons.org/">Fedora</a> to manage the full life cycle (from ingest, to storage, to access) of the newspaper content we receive from program awardees around the US. The only trouble was that we wanted the access system to support heavy use by researchers and also robots (Google, Yahoo, Microsoft, etc) building their own views on the content. Unfortunately the way we had put the pieces together we couldn't support that. Increasingly we found ourselves working around Fedora as much as possible to squeeze a bit more performance out of the system.</p>

<p>So in the end we (and by we I mean <a href="http://davidbrunton.com">David</a>) decided to bite the bullet and split off the inventory systems keeping track of where received content lives (what storage systems, etc) from the access systems that delivered content on the Web. Ultimately this meant we could leverage industry proven web development tools to deliver the newspaper content...which was a huge win. Now that's not saying that Fedora can't be used to provide access to content. I think the problems we experienced may well have been the result of our use of Fedora, rather than Fedora itself. Having to do multiple, large XSLT transforms to source XML files to render a page is painful. While it's a bit of a truism, a good software developer tries to pick the right tool for the job. Half the battle there is deciding on the right granularity for <em>the job</em> ... the single job we were trying to solve with Fedora (preservation and access) was too big for us to do either right.</p>

<p>Having a system that's decomposable, like the <a href="http://or2010.fecyt.es/Resources/documentos/GSabstracts/curationMicro-services.pdf">approach</a> that CDL is taking with <a href="http://www.cdlib.org/services/uc3/curation/">Microservices</a> is essential for long-term thinking about software in the context of digital preservation. I guess you could say "there's no-there-there" with Microservices, since there's not really a system to download--but in a way that's kind of the point.</p>

<p>I guess this is just a long way of saying, Thanks Dorothea! :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2301</wp:post_id>
		<wp:post_date>2010-08-24 10:43:43</wp:post_date>
		<wp:post_date_gmt>2010-08-24 17:43:43</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>notes-on-retooling-libraries</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="digital-perservation"><![CDATA[digital perservation]]></category>
		<category domain="post_tag" nicename="fedora"><![CDATA[fedora]]></category>
		<category domain="post_tag" nicename="gpo"><![CDATA[gpo]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="ndnp"><![CDATA[ndnp]]></category>
		<category domain="post_tag" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="tldr"><![CDATA[tl;dr]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>top hosts referenced in wikipedia (part 2)</title>
		<link>http://inkdroid.org/2010/08/25/top-hosts-referenced-in-wikipedia-part-2/</link>
		<pubDate>Wed, 25 Aug 2010 12:27:51 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2342</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://jodischneider.com/">Jodi Schneider</a> pointed out to me in an email that my <a href="http://inkdroid.org/2010/08/21/top-hosts-referenced-in-english-wikipedia/">previous post</a> about the top 100 hosts referenced in wikipedia may have been slightly off balance since it counted *all* pages on wikipedia (talk pages, files, etc), and was not limited to only links in articles. The indicator for her was the high ranking of <em>www.google.com</em>, which seemed odd to her in the article space.

So I downloaded the <a href="http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz">enwiki-latest-page.sql.gz</a>, loaded it in, and then joined on it in my query to come up with a <a href="http://inkdroid.org/data/enwiki-externallinks-hostnames-articles-only.txt.gz">new list</a>. Jodi was right, it's a lot more interesting:

<a href="http://inkdroid.org/images/en-wikipedia-externallinks-articles-only-chart.png"><img src="http://inkdroid.org/images/en-wikipedia-externallinks-articles-only-chart.png" style="border: none;"/></a>

This removed a lot of the interwiki links between the English wikipedia and other language wikipedias (which would be interesting to look at in their own right). It also removed administrative links to things like <a href="http://www.dnsstuff.com">www.dnsstuff.com</a>. Also interesting is that it removed <a href="http://www.facebook.com">www.facebook.com</a> from the list, which probably were linked to from user profile pages? The neat thing is it introduced new sites into the top 100 like the following:

<table>
<tr><td>adsabs.harvard.edu</td></tr>
<tr style="background-color: #EEEEEE;"><td>bioguide.congress.gov</td></tr>
<tr><td>cfa-www.harvard.edu</td></tr>
<tr style="background-color: #EEEEEE;"><td>eclipse.gsfc.nasa.gov</td></tr>
<tr><td>openjurist.org</td></tr>
<tr style="background-color: #EEEEEE;"><td>select.nytimes.com</td></tr>
<tr><td>ssd.jpl.nasa.gov</td></tr>
<tr style="background-color: #EEEEEE;"><td>worldcat.org</td></tr>
<tr><td>www1.arbitron.com</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.animenewsnetwork.com</td></tr>
<tr><td>www.cbc.ca</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.cricinfo.com</td></tr>
<tr><td>www.cricketarchive.com</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.discogs.com</td></tr>
<tr><td>www.expasy.org</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.fifa.com</td></tr>
<tr><td>www.gutenberg.org</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.history.navy.mil</td></tr>
<tr><td>www.hockeydb.com</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.imagesofengland.org.uk</td></tr>
<tr><td>www.independent.co.uk</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.jstor.org</td></tr>
<tr><td>www.leighrayment.com</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.mtv.com</td></tr>
<tr><td>www.nfl.com</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.nhm.ac.uk</td></tr>
<tr><td>www.nps.gov</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.racingpost.com</td></tr>
<tr><td>www.radio-locator.com</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.reuters.com</td></tr>
<tr><td>www.rollingstone.com</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.rsssf.com</td></tr>
<tr><td>www.soccerbase.com</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.usatoday.com</td></tr>
<tr><td>www.variety.com</td></tr>
</table>

We can see a lot more pop culture media present: newspapers, magazines, sporting information. Also we can see research oriented websites like worldcat.org, ssd.jpl.nasa.gov, adsabs.harvard.edu make it into the top 100.  

I work for the US federal government so I was interested to look at what .gov domains were in the top 100:

<table>
<tr><th>hostname</th><th>links</th></tr>
<tr style="background-color: #EEEEEE;"><td>www.ncbi.nlm.nih.gov</td><td>419816</td></tr>
<tr><td>www.pubmedcentral.nih.gov</td><td>62134</td></tr>
<tr style="background-color: #EEEEEE;"><td>geonames.usgs.gov</td><td>57423</td></tr>
<tr><td>factfinder.census.gov</td><td>48530</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.census.gov</td><td>33018</td></tr>
<tr><td>www.nr.nps.gov</td><td>25962</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.fcc.gov</td><td>25941</td></tr>
<tr><td>ssd.jpl.nasa.gov</td><td>20178</td></tr>
<tr style="background-color: #EEEEEE;"><td>eclipse.gsfc.nasa.gov</td><td>20063</td></tr>
<tr><td>bioguide.congress.gov</td><td>18880</td></tr>
<tr style="background-color: #EEEEEE;"><td>www.nlm.nih.gov</td><td>15115</td></tr>
<tr><td>www.nps.gov</td><td>12196</td></tr>
</table>

Which points to the importance of federal biomedical, geospatial, scientific, demographic and biographical information to wikipedians. It would be interesting to take a look at higher education institutions at some point. Doing these one off reports is giving me some ideas about what linkypedia could turn into. Thanks Jodi.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2342</wp:post_id>
		<wp:post_date>2010-08-25 05:27:51</wp:post_date>
		<wp:post_date_gmt>2010-08-25 12:27:51</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>top-hosts-referenced-in-wikipedia-part-2</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="statistics"><![CDATA[statistics]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"83043";i:1;s:5:"83100";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>83057</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2010-08-25 19:05:56</wp:comment_date>
			<wp:comment_date_gmt>2010-08-26 02:05:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Actually toolserver.org was in the top of the results when limiting to articles as well. I just removed it from my barchart again.

As far as I can see with curl, the content-type header for the <a href="http://inkdroid.org/data/enwiki-externallinks-hostnames-articles-only.txt.gz" rel="nofollow">list</a> is "application/x-gzip" which is right?

<pre>
ed@curry:~/Projects/openpub/examples$ curl -I http://inkdroid.org/data/enwiki-externallinks-hostnames-articles-only.txt.gz
HTTP/1.1 200 OK
Date: Thu, 26 Aug 2010 02:03:31 GMT
Server: Apache/2.2.14 (Ubuntu)
Last-Modified: Wed, 25 Aug 2010 11:30:30 GMT
ETag: "f8013-b44b97-48ea4357ce180"
Accept-Ranges: bytes
Content-Length: 11815831
Vary: Accept-Encoding
Connection: close
Content-Type: application/x-gzip
</pre>

Those are some good ideas for further analysis, thanks!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83100</wp:comment_id>
			<wp:comment_author><![CDATA[thekohser]]></wp:comment_author>
			<wp:comment_author_email>thekohser@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.google.com/profiles/thekohser</wp:comment_author_url>
			<wp:comment_author_IP>68.87.42.110</wp:comment_author_IP>
			<wp:comment_date>2010-08-30 12:24:32</wp:comment_date>
			<wp:comment_date_gmt>2010-08-30 19:24:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed, when you say "referenced in Wikipedia" do you mean actually used as a "References" or "Notes" link to substantiate a citation, or do you simply mean "external link" of any variety, anywhere on an article page?

If the latter, might I suggest you use some different terminology in this blog post?

Great work, by the way.  It's the sort of stuff that the Wikimedia Foundation itself should be doing, but their too busy hiring new staff to not do much.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>355</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83043</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>tfmorris@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawld-pz_ibfBuIZ2hkuRWFiHcZh9-2OSOCg</wp:comment_author_url>
			<wp:comment_author_IP>68.189.243.35</wp:comment_author_IP>
			<wp:comment_date>2010-08-25 11:22:14</wp:comment_date>
			<wp:comment_date_gmt>2010-08-25 18:22:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for posting these results.

I think you've posted the original list again rather than the updated one because toolserver.com is the top entry.  It also appears to be served with the wrong MIME type (or perhaps is twice compressed?)

This is just the tip of the iceberg and it would be interesting to see what kinds of additional insights could be teased out.  Some of the things that come to mind include identifying patterns used by bots and excluding/segregating them so that you can see more organic results, aggregating equivalent domains (e.g. Google Books in different TLDs), reversing domains and aggregating subtotals (gov.nih.*, gov.nps.*), aggregating by type/subject (e.g. recensement.insee.fr &amp; census.gov), etc, etc.  Days of fun for someone who wants to slice &amp; dice the data!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>353</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83058</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2010-08-25 19:08:07</wp:comment_date>
			<wp:comment_date_gmt>2010-08-26 02:08:07</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I just noticed that Finn Årup Nielsen did some similar <a href="http://fnielsen.posterous.com/top-news-cites-referenced-from-wikipedia" rel="nofollow">analysis</a> of the top news sites. In case you are interested in links from wikipedia to news outlets on the web.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83067</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid › top hosts referenced in wikipedia (part 2) | Google Wikipedia]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://googlewikipedia.com/inkdroid-%e2%80%ba-top-hosts-referenced-in-wikipedia-part-2/</wp:comment_author_url>
			<wp:comment_author_IP>74.54.218.98</wp:comment_author_IP>
			<wp:comment_date>2010-08-26 15:09:34</wp:comment_date>
			<wp:comment_date_gmt>2010-08-26 22:09:34</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Google history wikipedia &#8211; Google Blog Search  by Chris Devers [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83105</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2010-08-30 19:59:20</wp:comment_date>
			<wp:comment_date_gmt>2010-08-31 02:59:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@thekosher, well by external links I mean links included in enwiki-latest-externallinks.sql.gz made available by <a href="http://dumps.wikimedia.org/enwiki/latest/" rel="nofollow">en.wikipedia.org</a>. It's my understanding that these are referenced citations, and not just any link on an article page. The stats in this post also used the enwiki-latest-page.sql.gz dump to limit to only external links in <em>articles</em> with the following SQL:

<pre>
SELECT * FROM externallinks, page 
WHERE externallinks.el_from = page.page_id
AND page.page_namespace = 0
</pre>

I hope that clarifies things somewhat.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85354</wp:comment_id>
			<wp:comment_author><![CDATA[Wikipedia: Questionable Content – Oh Internet!]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20130509051903/http://blog.ohinternet.com/11618/wikipedia-questionable-content/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-02 03:56:09</wp:comment_date>
			<wp:comment_date_gmt>2012-04-02 10:56:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] &#8220;Anime News Network&#8221; has the dubious distinction of being one of the most-linked domains from Wikipedia pages. This is because anime is vastly more important than things like history, [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1333364170.3092";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1333884951.2129";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>bad xml smells</title>
		<link>http://inkdroid.org/2010/08/25/bad-xml-smells/</link>
		<pubDate>Wed, 25 Aug 2010 19:16:22 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2358</guid>
		<description></description>
		<content:encoded><![CDATA[I'm used to refactoring code smells, but sometimes you can catch a bad whiff in XML too. 

Before:

<pre lang="xml">
< ?xml version="1.0" encoding="UTF-8"?>
<mets TYPE="urn:library-of-congress:ndnp:mets:encyclopedia:encyclopediaEntry" PROFILE="urn:library-of-congress:mets:profiles:ndnp:encyclopediaEntry:v1.1" LABEL="The National Forum Scope Note" 
     xmlns:mods="http://www.loc.gov/mods/v3" xmlns="http://www.loc.gov/METS/" xmlns:dsig="http://www.w3.org/2000/09/xmldsig#" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">

     <!--METS HEADER-->
     <metshdr CREATEDATE="2007-01-10T09:00:00" ><!--CREATEDATE should be populated with creation date of the record.  RECORDSTATUS should only be set by Validation Application.-->
        <agent ROLE="CREATOR" TYPE="ORGANIZATION">
            <name><!--Awardee, if the awardee created the METS record.  If LC created the METS record, should be "Library of Congress"-->Library of Congress</name>
        </agent>
    </metshdr>

    <!--Descriptive metadata for encyclopedia entry-->
    <dmdsec ID="encyclopediaMods">                
        <mdwrap MDTYPE="MODS" LABEL="Encyclopedia entry metadata">
            <xmldata>
                <mods:mods>
                    <mods:name type="corporate">
                            <mods:displayform><!--Awardee-->Library of Congress</mods:displayform>
                    </mods:name>
                    <mods:relateditem>
                        <mods:identifier type="lccn"><!--LCCN-->sn82015056</mods:identifier>
                        <!-- mods:identifier elements are repeatable -->
                    </mods:relateditem>
                </mods:mods>
            </xmldata>
        </mdwrap>
    </dmdsec>
                            
   
    
    <!--FILE SECTION-->
    <filesec>
        <filegrp>
            <file ID="encyclopediaEntryText">
                <fcontent>
                    <xmldata>
                        <xhtml:html xmlns:xhtml="http://www.w3.org/1999/xhtml"  lang="en">
                           <xhtml:head>
                              <xhtml:title><!--Newspaper title-->The National Forum (Washington, DC), 1910-19??</xhtml:title>
                           </xhtml:head>
                           
                           <xhtml:body>
                              <xhtml:p>
The first issue of the <xhtml:cite>National Forum</xhtml:cite> was likely released on April 30, 1910 
and the newspaper ran through at least November 12 of that year. The four-page African-American 
weekly covered such local events as Howard University graduations and Baptist church activities, but its 
pages also included national news, sports, home maintenance, women's news, science, editorial 
cartoons, and reprinted stories from national newspapers. Its primary focus was on how the news 
affected the city's black community. A unique feature was its coverage of Elks Club meetings and 
activities.  Business manager John H. Wills contributed the community-centered "Vanity Fair" column that
 usually appeared on the front page of each issue. The publisher and editor was Ralph W. White, who 
went on to publish another African-American newspaper, the <xhtml:a href="info:lccn/sn86092050">
<xhtml:cite>McDowell Times</xhtml:cite></xhtml:a> of Keystone, West Virginia. Originally located at 
609 F St., NW, the newspaper's offices moved in August to 1022 U Street, N.W. to be closer to the 
African-American community it served.  No extant first issue of the <xhtml:cite>National
 Forum</xhtml:cite> exists.
                              </xhtml:p>
                           </xhtml:body>
                        </xhtml:html>
                    </xmldata>
                </fcontent>
            </file>
        </filegrp>
    </filesec>
        
    <!--STRUCTURAL MAP-->    
    <structmap xmlns:np="urn:library-of-congress:ndnp:mets:newspaper">
        <div TYPE="np:encyclopediaEntry" DMDID="encyclopediaMods">
            <fptr FILEID="encyclopediaEntryText"/>
        </div>
    </structmap>    
</mets>
</pre>

After:

<pre lang="xml">
< ?xml version="1.0" encoding="utf-8"?>
< !DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.0//EN" "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:dcterms="http://purl.org/dc/terms/">
  <head>
    <title>The National Forum (Washington, DC), 1910-19??</title>
    <meta content="The National Forum (Washington, DC), 1910-19??" property="dcterms:title"/>
    <meta content="2007-01-10T09:00:00" property="dcterms:created"/>
    <meta content="Library of Congress" property="dcterms:creator"/>
    <meta content="http://chroniclingamerica.loc.gov/lccn/sn82015056#title" property="dcterms:subject"/>
  </head>
  <body rel="dcterms:description">
    <p>The first issue of the <cite>National Forum</cite> was likely released on April 30, 1910 and the 
newspaper ran through at least November 12 of that year. The four-page African-American weekly 
covered such local events as Howard University graduations and Baptist church activities, but its pages 
also included national news, sports, home maintenance, women's news, science, editorial cartoons, and 
reprinted stories from national newspapers. Its primary focus was on how the news affected the city's black community. A unique feature was its coverage of Elks Club meetings and activities.  Business 
manager John H. Wills contributed the community-centered "Vanity Fair" column that usually appeared 
on the front page of each issue. The publisher and editor was Ralph W. White, who went on to publish 
another African-American newspaper, the <a href="http://chroniclingamerica.loc.gov/lccn/sn86092050">
<cite>McDowell Times</cite></a> of Keystone, West Virginia. Originally located at 609 F St., NW, the 
newspaper's offices moved in August to 1022 U Street, N.W. to be closer to the African-American 
community it served.  No extant first issue of the <cite>National Forum</cite> exists.</p>
  </body>
</html>
</pre>

I basically took a complicated METS wrapper around some XHTML, which was really just expressing metadata about the HTML, and refactored it as XHTML. Not that METS is a bad XML smell generally, but in this particular case it was overkill. If you look closely you'll see I'm using <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a>, similar to what Facebook are doing with their <a href="http://developers.facebook.com/docs/opengraph">OpenGraph Protocol</a>.  There's less to get wrong, what's there should look more familiar to web developers who aren't versed in arcane library standards, and I can now read the metadata from the XHTML with an RDFa aware parser, like Python's <a href="http://python.org/pypi/rdflib">rdflib</a>:

<pre lang="python">
>>> import rdflib
>>> g = rdflib.Graph()
>>> g.parse('essays/1.html', format='rdfa')
>>> for triple in g: print triple
... 
(rdflib.term.URIRef('file:///home/ed/Projects/essays/essays/1.html'), rdflib.term.URIRef('http://purl.org/dc/terms/creator'), rdflib.term.Literal(u'Library of Congress'))
(rdflib.term.URIRef('file:///home/ed/Projects/essays/essays/1.html'), rdflib.term.URIRef('http://purl.org/dc/terms/title'), rdflib.term.Literal(u'The National Forum (Washington, DC), 1910-19??'))
(rdflib.term.URIRef('file:///home/ed/Projects/essays/essays/1.html'), rdflib.term.URIRef('http://purl.org/dc/terms/description'), rdflib.term.Literal(u'\n    <p xmlns="http://www.w3.org/1999/xhtml">\nThe first issue of the <cite>National Forum</cite> was likely released on April 30, 1910 and the newspaper ran through at least November 12 of that year. The four-page African-American weekly covered such local events as Howard University graduations and Baptist church activities, but its pages also included national news, sports, home maintenance, women\'s news, science, editorial cartoons, and reprinted stories from national newspapers. Its primary focus was on how the news affected the city\'s black community. A unique feature was its coverage of Elks Club meetings and activities.  Business manager John H. Wills contributed the community-centered &quot;Vanity Fair&quot; column that usually appeared on the front page of each issue. The publisher and editor was Ralph W. White, who went on to publish another African-American newspaper, the <a href="http://chroniclingamerica.loc.gov/lccn/sn86092050"><cite>McDowell Times</cite></a> of Keystone, West Virginia. Originally located at 609 F St., NW, the newspaper\'s offices moved in August to 1022 U Street, N.W. to be closer to the African-American community it served.  No extant first issue of the <cite>National Forum</cite> exists.\n</p>\n  ', datatype=rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral')))
(rdflib.term.URIRef('file:///home/ed/Projects/essays/essays/1.html'), rdflib.term.URIRef('http://purl.org/dc/terms/subject'), rdflib.term.Literal(u'http://chroniclingamerica.loc.gov/lccn/sn82015056#title'))
(rdflib.term.URIRef('file:///home/ed/Projects/essays/essays/1.html'), rdflib.term.URIRef('http://purl.org/dc/terms/created'), rdflib.term.Literal(u'2007-01-10T09:00:00'))
</pre>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2358</wp:post_id>
		<wp:post_date>2010-08-25 12:16:22</wp:post_date>
		<wp:post_date_gmt>2010-08-25 19:16:22</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bad-xml-smells</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="mets"><![CDATA[mets]]></category>
		<category domain="post_tag" nicename="ndnp"><![CDATA[ndnp]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="post_tag" nicename="xhtml"><![CDATA[xhtml]]></category>
		<category domain="category" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>simplicity and digital preservation, sorta</title>
		<link>http://inkdroid.org/2010/08/27/simplicity-and-digital-preservation-sorta/</link>
		<pubDate>Fri, 27 Aug 2010 16:21:28 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2388</guid>
		<description></description>
		<content:encoded><![CDATA[Over on the <a href="http://groups.google.com/group/digital-curation">Digital Curation</a> discussion list <a href="http://www.cdlib.org/contact/staff_directory/ehetzner.html">Erik Hetzner</a> of the <a href="http://www.cdlib.org/">California Digital Library</a> <a href="http://groups.google.com/group/digital-curation/browse_thread/thread/ac208143d71f0ab7">raised</a> the topic of <em>simplicity</em> as it relates to <a href="http://en.wikipedia.org/wiki/Digital_preservation">digital preservation</a>, and specifically to CDL's notion of <a href="http://www.cdlib.org/services/uc3/curation/">Curation Microservices</a>. He referenced a recent bit of writing by <a href="http://lamp.epfl.ch/~odersky/">Martin Odersky</a> (the creator of <a href="http://www.scala-lang.org/">Scala</a>) with the title <a href="http://lamp.epfl.ch/~odersky/blogs/isscalacomplex.html">Simple or Complicated</a>. In one of the responses <a href="http://tingletech.tumblr.com/">Brian Tingle</a> (also of CDL) suggested that simplicity for an end user and simplicity for the programmer are often inversely related. My friend <a href="http://weblog.kevinclarke.info/">Kevin Clarke</a> prodded me in <a href="irc://irc.freenode.net/code4lib">#code4lib</a> into making my <a href="http://groups.google.com/group/digital-curation/browse_thread/thread/34a271426124f65c">response</a> to the discussion list into a blog post so, here it is (slightly edited).

For me, the <a href="http://lamp.epfl.ch/~odersky/blogs/isscalacomplex.html">Odersky piece</a> is a really nice essay on why <em>simplicity is often in the eye of the beholder</em>. Often the key to simplicity is working with people who see things in roughly the same way. People who have similar needs, that are met by using particular approaches and tools. Basically a shared and healthy culture to make emergent complexity palatable.

Brian made the <a href="http://groups.google.com/group/digital-curation/msg/b509d48ae3c97eb1">point</a> about simplicity for programmers having an inversely proportional relationship to simplicity for end users, or in his own words:

<blockquote>
I think that the simpler we make it for the programmers, usually the more complicated it becomes for the end users, and visa versa. 
</blockquote>

I think the only thing to keep in mind is that the distinction between programmers and end users isn't always clear. 

As a software developer I'm constantly using, or inheriting someone else's code: be it a third party library that I have a dependency on, or a piece of software that somebody wrote once upon a time, who has moved on elsewhere. In both these cases I'm effectively an end-user of a program that somebody else designed and implemented. The interfaces and abstractions that this software developer has chosen are the things I (as an end user) need to be able to understand and work with. Ultimately, I think that it's easier to keep software usable for end users (of whatever flavor) by keeping the software design itself  simple. 

Simplicity makes the software easier to refactor over time  when the inevitable happens, and someone wants some new or altered behavior. Simplicity also should make it clear when a suggested change to a piece of software doesn't fit the design of the software in question, and is best done elsewhere. One of the best rules of thumb I've encountered over the years to help get to this place is the <a href="http://en.wikipedia.org/wiki/Unix_philosophy">Unix Philosophy</a>: 

<blockquote>
Write programs that do one thing and do it well. Write programs to work together.
</blockquote>
 
As has been noted <a href="http://or2010.fecyt.es/Resources/documentos/GSabstracts/curationMicro-services.pdf">elsewhere</a>, composability is one of the guiding principles of the Microservices approach--and it's why I'm a big fan  (in principle). Another aspect to the Unix philosophy that Microservices seems to embody is: 

<blockquote>
Data dominates.
</blockquote>
 
The software can (and will) come and go, but we are left with the data. That's the reality of digital preservation. It could be argued that the programs themselves are data, which gets us into sci-fi virtualization scenarios. Maybe someday, but I personally don't think we're there yet. 

Another approach I've found that works well to help ensure code simplicity has been <a href="http://en.wikipedia.org/wiki/Unit_testing">unit testing</a>. Admittedly it's a bit of a <a href="http://www.codinghorror.com/blog/2006/07/i-pity-the-fool-who-doesnt-write-unit-tests.html">religion</a>, but at the end of the day, writing tests for your code encourages you to use the APIs, interfaces and abstractions that you are creating. So you notice sooner when things don't make sense. And of course, they let you refactor with a safety net, when the inevitable changes rear their head. 

And, <a href="http://c2.com/cgi/wiki?CodeForTheMaintainer">another</a> slightly more humorous way to help ensure simplicity:

<blockquote> 
Always code as if the person who ends up maintaining your code is a violent psychopath who knows where you live.
</blockquote>
 
Which leads me to a jedi mind trick my former colleague <strike><a href="http://en.wikipedia.org/wiki/Keyser_S%C3%B6ze">Keyser Söze</a></strike> <a href="http://andy.boyko.net">Andy Boyko</a> tried to teach me (I think): i<em>t's useful to know when you don't have to write any code at all</em>. Sometimes existing code can be used in a new context. And sometimes the perceived problem can be recast, or examined from a new perspective that makes the problem go away. I'm not sure what all this has to do with digital preservation. The  great thing about what CDL is doing with microservices is they are trying to focus on the *what*, and not the *how* of digital preservation. Whatever ends up happening with the implementation of<a href="https://confluence.ucop.edu/download/attachments/13860983/Merritt-latest.pdf"> Merritt</a> itself, I think they are discovering what the useful patterns of digital preservation are, trying them out, and documenting them...and it's incredibly important work that I don't really see happening much elsewhere. 
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2388</wp:post_id>
		<wp:post_date>2010-08-27 09:21:28</wp:post_date>
		<wp:post_date_gmt>2010-08-27 16:21:28</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>simplicity-and-digital-preservation-sorta</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="cdl"><![CDATA[cdl]]></category>
		<category domain="category" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="design"><![CDATA[design]]></category>
		<category domain="post_tag" nicename="digital-preservation"><![CDATA[digital preservation]]></category>
		<category domain="post_tag" nicename="microservices"><![CDATA[microservices]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="unix"><![CDATA[unix]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"83079";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>83079</wp:comment_id>
			<wp:comment_author><![CDATA[claimid.com/egh]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://claimid.com/egh</wp:comment_author_url>
			<wp:comment_author_IP>69.106.231.190</wp:comment_author_IP>
			<wp:comment_date>2010-08-27 12:48:23</wp:comment_date>
			<wp:comment_date_gmt>2010-08-27 19:48:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great comments, Ed. It just goes to show that simple is really complicated.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>165</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83080</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2010-08-27 13:06:41</wp:comment_date>
			<wp:comment_date_gmt>2010-08-27 20:06:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[That about sums it up eh? :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>lots of copies keeps epubs safe</title>
		<link>http://inkdroid.org/2010/08/30/lots-of-epub-copies-keeps-stuff-safe/</link>
		<pubDate>Mon, 30 Aug 2010 21:38:27 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2414</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Over the weekend you probably saw the <a href="http://booksearch.blogspot.com/2009/08/download-over-million-public-domain.html">announcements</a> <a href="http://www.readwriteweb.com/archives/google_opens_up_its_epub_archive_download_1_million_books_for_free.php">going</a> <a href="http://www.engadget.com/2009/08/26/google-makes-over-a-million-public-domain-books-available-in-epu/">around</a> about Google Books releasing +1 million public domain ebooks on the web as <a href="http://en.wikipedia.org/wiki/EPUB">epubs</a>. This is great news: epub is a web friendly, open format -- and having all this content available as epub is important.</p>

<p>Now I might be greedy, but when I saw that 1 million epubs are available my mind immediately jumps to thinking of getting them, indexing them and whatnot. Then I guiltily justified my greedy thoughts by pondering the conventional digital preservation wisdom that <a href="http://en.wikipedia.org/wiki/LOCKSS">Lots of Copies Keeps Stuff Safe</a> (LOCKSS). The books are in the public domain, so .... why not?</p>

<p>Google Books has a really nice <a href="http://web.archive.org/web/20120415030517/http://code.google.com:80/apis/books/docs/gdata/developers_guide_protocol.html">API</a>, which lets you get back search results as Atom, with lots of links to things like thumbnails, annotations, item views, etc. You also get a nice amount of <a href="http://dublincore.org/">Dublin Core</a> metadata. And you can limit your search to books published before 1923. For example here's a search for pre-1923 books that mention "Stevenson" (disclaimer: I don't think the 1923 limit is actually working):</p>

<pre>
curl 'http://books.google.com/books/feeds/volumes?tbs=cd_max:Jan%2001_2%201923&q=Stevenson' | xmllint --format -
</pre>

<p>which yields:</p>

<pre lang="xml">
< ?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:gbs="http://schemas.google.com/books/2008" xmlns:dc="http://purl.org/dc/terms" xmlns:batch="http://schemas.google.com/gdata/batch" xmlns:gd="http://schemas.google.com/g/2005">
  <id>http://www.google.com/books/feeds/volumes</id>
  <updated>2010-08-30T20:37:27.000Z</updated>
  <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
  <title type="text">Search results for Stevenson</title>
  <link rel="alternate" type="text/html" href="http://www.google.com"/>
  <link rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes"/>
  <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes?q=Stevenson"/>
  <link rel="next" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes?q=Stevenson&amp;start-index=11&amp;max-results=10"/>
  <author>
    <name>Google Books Search</name>
    <uri>http://www.google.com</uri>
  </author>
  <generator version="beta">Google Book Search data API</generator>
  <opensearch:totalresults>206</opensearch:totalresults>
  <opensearch:startindex>1</opensearch:startindex>
  <opensearch:itemsperpage>10</opensearch:itemsperpage>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/ENMWAAAAYAAJ</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">Kidnapped</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks4.books.google.com/books?id=ENMWAAAAYAAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;edge=curl&amp;sig=ACfU3U0dHqcPyQYmX-QBDY7DqGofAlUvvw&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=ENMWAAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=ENMWAAAAYAAJ&amp;printsec=frontcover&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/Kidnapped.acsm?id=ENMWAAAAYAAJ&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=ENMWAAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/ENMWAAAAYAAJ"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#enabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_all_pages"/>
    <dc:creator>Robert Louis Stevenson</dc:creator>
    <dc:date>1909</dc:date>
    <dc:format>308 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>ENMWAAAAYAAJ</dc:identifier>
    <dc:identifier>HARVARD:HN1JZ9</dc:identifier>
    <dc:title>Kidnapped</dc:title>
    <dc:title>being memoirs of the adventures of David Balfour in the year 1851 ...</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/WZ0vAAAAMAAJ</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">Treasure Island</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks8.books.google.com/books?id=WZ0vAAAAMAAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;edge=curl&amp;sig=ACfU3U3AusJAerfzUTAoo8iLZkeOL755rg&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=WZ0vAAAAMAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=WZ0vAAAAMAAJ&amp;printsec=frontcover&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=2&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/Treasure_Island.acsm?id=WZ0vAAAAMAAJ&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=WZ0vAAAAMAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/WZ0vAAAAMAAJ"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#enabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_all_pages"/>
    <dc:creator>Robert Louis Stevenson</dc:creator>
    <dc:creator>George Edmund Varian</dc:creator>
    <dc:date>1918</dc:date>
    <dc:description>CHAPTER I THE OLD SEA DOG AT THE &amp;quot;ADMIRAL BENBOW&amp;quot; SQUIRE Trelawney, Dr. Livesey, 
and the rest of these gentlemen having asked me to write down the whole ...</dc:description>
    <dc:format>306 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>WZ0vAAAAMAAJ</dc:identifier>
    <dc:identifier>NYPL:33433075793830</dc:identifier>
    <dc:subject>Fiction</dc:subject>
    <dc:title>Treasure Island</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/REUrAQAAIAAJ</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">Stevenson</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks6.books.google.com/books?id=REUrAQAAIAAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;sig=ACfU3U2pRkGJW1MpjNMZgSc6jDIp3HuT0Q&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=REUrAQAAIAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=REUrAQAAIAAJ&amp;q=Stevenson&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=3&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/Stevenson.acsm?id=REUrAQAAIAAJ&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=REUrAQAAIAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/REUrAQAAIAAJ"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#not_embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#disabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_no_pages"/>
    <dc:creator>Adlai Ewing Stevenson</dc:creator>
    <dc:creator>Grace Darling</dc:creator>
    <dc:creator>David Darling</dc:creator>
    <dc:date>1977-10</dc:date>
    <dc:format>127 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>REUrAQAAIAAJ</dc:identifier>
    <dc:identifier>STANFORD:36105037014342</dc:identifier>
    <dc:publisher>McGraw-Hill/Contemporary</dc:publisher>
    <dc:subject>Biography &amp; Autobiography</dc:subject>
    <dc:title>Stevenson</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/3ibdGgAACAAJ</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">Stevenson</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks1.books.google.com/books?id=3ibdGgAACAAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;sig=ACfU3U2ryyj2yW3FOO9-8WPQtihovQqQOA&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=3ibdGgAACAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=3ibdGgAACAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=4&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/Stevenson.acsm?id=3ibdGgAACAAJ&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=3ibdGgAACAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/3ibdGgAACAAJ"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#not_embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#disabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_no_pages"/>
    <dc:creator>Robert Louis Stevenson</dc:creator>
    <dc:date>2007-01-17</dc:date>
    <dc:description>This scarce antiquarian book is included in our special Legacy Reprint Series.</dc:description>
    <dc:format>128 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>3ibdGgAACAAJ</dc:identifier>
    <dc:identifier>ISBN:1430495375</dc:identifier>
    <dc:identifier>ISBN:9781430495376</dc:identifier>
    <dc:publisher>Kessinger Pub Co</dc:publisher>
    <dc:subject>Poetry</dc:subject>
    <dc:title>Stevenson</dc:title>
    <dc:title>Day by Day</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/3QI-AAAAYAAJ</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">A child's garden of verses</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks0.books.google.com/books?id=3QI-AAAAYAAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;edge=curl&amp;sig=ACfU3U1KKV9Y2E1jnnc83b4NA_GNAouwbA&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=3QI-AAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=3QI-AAAAYAAJ&amp;printsec=frontcover&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=5&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/A_child_s_garden_of_verses.acsm?id=3QI-AAAAYAAJ&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=3QI-AAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/3QI-AAAAYAAJ"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#enabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_all_pages"/>
    <dc:creator>Robert Louis Stevenson</dc:creator>
    <dc:date>1914</dc:date>
    <dc:description>IN winter I get up at night And dress by yellow candle-light. In summer, quite 
the other way, I have to go to bed by day. I have to go to bed and see The ...</dc:description>
    <dc:format>136 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>3QI-AAAAYAAJ</dc:identifier>
    <dc:identifier>CORNELL:31924052752262</dc:identifier>
    <dc:subject>Children's poetry, Scottish</dc:subject>
    <dc:title>A child's garden of verses</dc:title>
    <dc:title>by Robert Louis Stevenson; illustrated by Charles Robinson</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/Gmk-AAAAYAAJ</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">Travels with a donkey in the Cevennes</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks5.books.google.com/books?id=Gmk-AAAAYAAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;edge=curl&amp;sig=ACfU3U1CM_jn5vHlfF0va-U32zu2Rr2QIg&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=Gmk-AAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=Gmk-AAAAYAAJ&amp;printsec=frontcover&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=6&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/Travels_with_a_donkey_in_the_Cevennes.acsm?id=Gmk-AAAAYAAJ&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=Gmk-AAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/Gmk-AAAAYAAJ"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#enabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_all_pages"/>
    <dc:creator>Robert Louis Stevenson</dc:creator>
    <dc:date>1916</dc:date>
    <dc:description>THE DONKEY, THE PACK, AND THE PACK - SADDLE IN a little place called Le 
Monastier, in a pleasant highland valley fifteen miles from Le Puy, I spent 
about a ...</dc:description>
    <dc:format>287 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>Gmk-AAAAYAAJ</dc:identifier>
    <dc:identifier>HARVARD:HWP541</dc:identifier>
    <dc:subject>Cévennes Mountains (France)</dc:subject>
    <dc:title>Travels with a donkey in the Cevennes</dc:title>
    <dc:title>An inland voyage</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/f3A-AAAAYAAJ</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">St. Ives</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks6.books.google.com/books?id=f3A-AAAAYAAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;edge=curl&amp;sig=ACfU3U0HtuxZqcqgneZTtM0LH5rk4F2E4w&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=f3A-AAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=f3A-AAAAYAAJ&amp;printsec=frontcover&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=7&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/St__Ives.acsm?id=f3A-AAAAYAAJ&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=f3A-AAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/f3A-AAAAYAAJ"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#enabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_all_pages"/>
    <dc:creator>Robert Louis Stevenson</dc:creator>
    <dc:date>1906</dc:date>
    <dc:description>IVES CHAPTER IA TALE OF A LION RAMPANT IT was in the month of May,, that I was 
so unlucky as to fall at last into the hands of the enemy. ...</dc:description>
    <dc:format>528 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>f3A-AAAAYAAJ</dc:identifier>
    <dc:identifier>HARVARD:HWP61W</dc:identifier>
    <dc:title>St. Ives</dc:title>
    <dc:title>being the adventures of a French prisoner in England</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/4mb8LuKKwocC</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">Cruising with Robert Louis Stevenson</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks6.books.google.com/books?id=4mb8LuKKwocC&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;edge=curl&amp;sig=ACfU3U3VC0jTy4rEC4rjtFjLofSY7bAf2Q&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=4mb8LuKKwocC&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=4mb8LuKKwocC&amp;printsec=frontcover&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=8&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/Cruising_with_Robert_Louis_Stevenson.acsm?id=4mb8LuKKwocC&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=4mb8LuKKwocC&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/4mb8LuKKwocC"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#disabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_partial"/>
    <dc:creator>Oliver S. Buckton</dc:creator>
    <dc:date>2007</dc:date>
    <dc:description>Cruising with Robert Louis Stevenson: Travel, Narrative, and the Colonial Body is the first book-length study about the influence of travel on Robert Louis ...</dc:description>
    <dc:format>344 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>4mb8LuKKwocC</dc:identifier>
    <dc:identifier>ISBN:0821417568</dc:identifier>
    <dc:identifier>ISBN:9780821417560</dc:identifier>
    <dc:publisher>Ohio Univ Pr</dc:publisher>
    <dc:subject>Literary Criticism</dc:subject>
    <dc:title>Cruising with Robert Louis Stevenson</dc:title>
    <dc:title>travel, narrative, and the colonial body</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/4yo9AAAAYAAJ</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">New Arabian nights</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks0.books.google.com/books?id=4yo9AAAAYAAJ&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;edge=curl&amp;sig=ACfU3U1_NeiJKS1J279NoYk98NzRZURI3Q&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=4yo9AAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=4yo9AAAAYAAJ&amp;printsec=frontcover&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=9&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/New_Arabian_nights.acsm?id=4yo9AAAAYAAJ&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=4yo9AAAAYAAJ&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/4yo9AAAAYAAJ"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#enabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_all_pages"/>
    <dc:creator>Robert Louis Stevenson</dc:creator>
    <dc:date>1922</dc:date>
    <dc:description>THE SUICIDE CLUB STORY OF THE YOUNG MAN WITH THE CREAM TARTS DURING his 
residence in London, the accomplished Prince Florizel of Bohemia gained the ...</dc:description>
    <dc:format>386 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>4yo9AAAAYAAJ</dc:identifier>
    <dc:identifier>HARVARD:HWP51H</dc:identifier>
    <dc:subject>Fiction</dc:subject>
    <dc:title>New Arabian nights</dc:title>
  </entry>
  <entry>
    <id>http://www.google.com/books/feeds/volumes/z2Yf1FX02EkC</id>
    <updated>2010-08-30T20:37:27.000Z</updated>
    <category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/books/2008#volume"/>
    <title type="text">Robert Louis Stevenson</title>
    <link rel="http://schemas.google.com/books/2008/thumbnail" type="image/x-unknown" href="http://bks2.books.google.com/books?id=z2Yf1FX02EkC&amp;printsec=frontcover&amp;img=1&amp;zoom=5&amp;edge=curl&amp;sig=ACfU3U0gOHJkzL_2WLHxXwdINCOTt03bMw&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/info" type="text/html" href="http://books.google.com/books?id=z2Yf1FX02EkC&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/preview" type="text/html" href="http://books.google.com/books?id=z2Yf1FX02EkC&amp;printsec=frontcover&amp;dq=Stevenson&amp;ie=ISO-8859-1&amp;cd=10&amp;source=gbs_gdata"/>
    <link rel="http://schemas.google.com/books/2008/annotation" type="application/atom+xml" href="http://www.google.com/books/feeds/users/me/volumes"/>
    <link rel="http://schemas.google.com/books/2008/acsepubfulfillmenttoken" type="application/vnd.adobe.adept+xml" href="http://books.google.com/books/download/Robert_Louis_Stevenson.acsm?id=z2Yf1FX02EkC&amp;format=epub&amp;output=acs4_fulfillment_token"/>
    <link rel="alternate" type="text/html" href="http://books.google.com/books?id=z2Yf1FX02EkC&amp;dq=Stevenson&amp;ie=ISO-8859-1"/>
    <link rel="self" type="application/atom+xml" href="http://www.google.com/books/feeds/volumes/z2Yf1FX02EkC"/>
    <gbs:embeddability value="http://schemas.google.com/books/2008#embeddable"/>
    <gbs:openaccess value="http://schemas.google.com/books/2008#disabled"/>
    <gbs:viewability value="http://schemas.google.com/books/2008#view_partial"/>
    <dc:creator>Richard Ambrosini</dc:creator>
    <dc:creator>Richard Dury</dc:creator>
    <dc:date>2006</dc:date>
    <dc:description>As the editors point out in their Introduction, Stevenson reinvented the “personal essay” and the “walking tour essay,” in texts of ironic stylistic ...</dc:description>
    <dc:format>377 pages</dc:format>
    <dc:format>book</dc:format>
    <dc:identifier>z2Yf1FX02EkC</dc:identifier>
    <dc:identifier>ISBN:0299212246</dc:identifier>
    <dc:identifier>ISBN:9780299212247</dc:identifier>
    <dc:publisher>Univ of Wisconsin Pr</dc:publisher>
    <dc:subject>Literary Criticism</dc:subject>
    <dc:title>Robert Louis Stevenson</dc:title>
    <dc:title>writer of boundaries</dc:title>
  </entry>
</feed>
</pre>

<p>Now it would be nice if the Atom included &lt;link&gt; elements for the epubs themselves. Perhaps the feed could even use the <a href="http://opds-spec.org/blog/2010/08/29/opds-catalogs-v1/">recently released</a> "acquisition" link relation defined by OPDS v1.0. For example, by including something like the following in each <code>atom:entry</code> element:</p>

<pre lang="xml">
<link type="application/epub+zip" rel="http://opds-spec.org/acquisition/open-access" href="http://books.google.com/books/download/Kidnapped.epub?id=ENMWAAAAYAAJ&output=epub" />
</pre>

<p>Theoretically it should be possible to construct the appropriate link for the epub, based on what data is available in the Atom. But it would enable quite a bit of use of the epubs to make their URLs available explicitly in a programmatic way. Unfortunately we would still be limited to dipping into the full dataset using a query, instead of being able to crawl the entire archive, with something like a paged Atom feed. From a conversation over on <a href="http://groups.google.com/group/get-theinfo/browse_thread/thread/e151c1f69a974d21">get-theinfo</a> it appears that this approach might not be as easy as it sounds. Also, it turns out that magically, many of the books have been uploaded to the <a href="http://groups.google.com/group/get-theinfo/browse_thread/thread/e151c1f69a974d21">Internet Archive</a>. 902,188 of them in fact.</p>

<p>So maybe not that much work needs to be done. But presumably more public domain content will become available from Google Books, and it would be nice to be able to say there was at least one other copy of it elsewhere, for digital preservation purposes. It would be great to see Google step up and <em>do some good</em>, by making their API usable for folks wanting to replicate the public domain content. Still, at least they haven't of <em>done evil</em> by locking it away completely. <a href="http://danbri.org">Dan Brickley</a> had an interesting <a href="http://groups.google.com/group/get-theinfo/msg/0d6120b268aeba59">suggestion</a> to possibly collaborate on this work.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2414</wp:post_id>
		<wp:post_date>2010-08-30 14:38:27</wp:post_date>
		<wp:post_date_gmt>2010-08-30 21:38:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lots-of-epub-copies-keeps-stuff-safe</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="epub"><![CDATA[epub]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="google-books"><![CDATA[google books]]></category>
		<category domain="post_tag" nicename="internet-archive"><![CDATA[internet archive]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>edu, gov and tlds in en.wikipedia external links</title>
		<link>http://inkdroid.org/2010/08/30/edu-gov-and-tlds-in-en-wikipedia-external-links/</link>
		<pubDate>Tue, 31 Aug 2010 04:02:08 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2443</guid>
		<description></description>
		<content:encoded><![CDATA[Some folks over at <a href="http://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost">Wikipedia Signpost</a> asked if they could use some of the barcharts I've been posting here recently. They needed the graphs to be released with a free license, which was a good excuse to slap a <a href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution 3.0 license</a> on all the content here at inkdroid. I'm kinda ashamed I didn't think of doing this before...

I was also asked how easy it would be to generate the .gov and .edu graphs, as well as top level domains. I already had the hostnames exported, so it was just a few greps, sorts and uniqs away. I've included the graphs and the full data files below. My friend <a href="http://onebiglibrary.net">Dan Chudnov</a> suggested that plotting this data on a logarithmic scale would probably look better. I think he's probably right, but I just haven't gotten around to doing that yet. It's definitely something I will keep in mind for an app that allowed this slicing and dicing of the Wikipedia external links data.

<h3>top 100 .edu hosts in en.wikipedia external links</h3>
<a href="http://inkdroid.org/images/en-wikipedia-externallinks-edu.png"><img src="http://inkdroid.org/images/en-wikipedia-externallinks-edu.png" /></a>
<a href="http://inkdroid.org/data/enwiki-externallinks-edu.txt">download</a>

<h3>top 100 .gov hosts in en.wikipedia external links</h3>
<a href="http://inkdroid.org/images/en-wikipedia-externallinks-gov.png"><img src="http://inkdroid.org/images/en-wikipedia-externallinks-gov.png" /></a>
<a href="http://inkdroid.org/data/enwiki-externallinks-gov.txt">download</a>

<h3>top 100 top level domains in en.wikipedia external links</h3>
<a href="http://inkdroid.org/images/en-wikipedia-externallinks-tlds.png"><img src="http://inkdroid.org/images/en-wikipedia-externallinks-tlds.png" /></a>
<a href="http://inkdroid.org/data/enwiki-externallinks-tlds.txt">download</a>

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2443</wp:post_id>
		<wp:post_date>2010-08-30 21:02:08</wp:post_date>
		<wp:post_date_gmt>2010-08-31 04:02:08</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>edu-gov-and-tlds-in-en-wikipedia-external-links</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="dns"><![CDATA[dns]]></category>
		<category domain="post_tag" nicename="statistics"><![CDATA[statistics]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>triadomany</title>
		<link>http://inkdroid.org/2010/09/15/triadomany/</link>
		<pubDate>Wed, 15 Sep 2010 14:47:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2453</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
I fully admit that there is not uncommon craze for trichotomies. I do not know but the psychiatrists have provided a name for it. If not, they should ... it might be called <b>triadomany</b>. I am not so afflicted; but I find myself obliged, for truth's sake, to make such a large number of trichotomies that I could not [but] wonder if my readers, especially those of them who are in the way of knowing how common the malady is, should suspect, or even opine, that I am a victim of it ... I have no marked predilection for trichotomies in general. 

<em><a href="http://en.wikipedia.org/wiki/Charles_Sanders_Peirce">Charles S. Peirce</a> quoted in <a href="http://www.amazon.com/Sign-Three-Holmes-Advances-Semiotics/dp/0253204879">The Sign of Three</a>, edited by <a href="http://en.wikipedia.org/wiki/Umberto_Eco">Umberto Eco</a> and <a href="http://en.wikipedia.org/wiki/Thomas_Sebeok">Thomas A. Sebeok</a>.</em>
</blockquote>

<a href="http://inkdroid.org/empirical-cloud">
<img src="http://inkdroid.org/images/empirical-cloud.png" style="border: none; float: left;"/>
</a>

It's hard not to read a bit of humor and irony into this quote from Peirce. My friend <a href="http://onebiglibrary.net">Dan Chudnov</a> observed once that all this business with RDF and Linked Data often  seems like <a href="http://en.wikipedia.org/wiki/Fetishism">fetishism</a>. RDF colored glasses are kind of hard to take off when you are a web developer and have invested a bit of time in understanding the <strike>Semantic Web</strike> <a href="http://linkeddata.org">Linked Data</a> vision. I seem to go through phases of interest with the triples: ebbs and flows.  Somehow it's comforting to read of Peirce's predilections for triples at the remove of a couple hundred years.

Seeing the Linked Open Data Cloud for the <a href="http://inkdroid.org/2008/01/04/following-your-nose-to-the-web-of-data/">first time</a> was a revelation of sorts. It helped me understand concretely how the Web could be used to assemble a distributed, collaborative database. That same diagram is currently <a href="http://lists.w3.org/Archives/Public/public-lod/2010Sep/0009.html">being updated</a> to include new datasets. But a lot of Linked Data has been deployed since then ... and a lot of it has been collected as part of the annual <a href="http://challenge.semanticweb.org/">Billion Triple Challenge</a>. 

It has always been a bit mysterious to me how nodes get into the LOD Cloud, so I wondered how easy it would be create a cloud from the <a href="http://challenge.semanticweb.org/">2010 Billion Triple Challenge dataset</a>. It <a href="http://github.com/edsu/empirical-cloud">turns out</a> that with a bit of <a href="http://github.com/edsu/empirical-cloud/blob/master/README">unix pipelining</a> and the nice <a href="http://vis.stanford.edu/protovis/">ProtoVis</a> library it's not too hard to get something <a href="http://inkdroid.org/empirical-cloud">"working"</a>. It sure is nice to work in an <a href="http://loc.gov">environment</a> with helpful <a href="http://twitter.com/yellowmoss">folk</a> who can set aside a bit of storage and compute time for experimenting like this, without having to bog down my laptop for a long time.

If you click on the image you should be taken to the visualization. It's kind of heavy on JavaScript processing, so a browser like Chrome will probably render it best. 

But as <a href="http://blog.whatfettle.com/">Paul Downey</a> pointed out to me in Twitter:

<a href="http://twitter.com/psd/status/24559831612">
<img src="http://inkdroid.org/images/psd-tweet.png" style="border: none;"/>
</a>

Paul is so right. I find myself drawn to these graph visualizations for magical reasons. I can console myself that I did manage to find a new linked data supernode that I didn't know about before: <a href="http://www.bibsonomy.org/">bibsonomy</a>--which doesn't appear to be in the latest curated view of the Linked Open Data Cloud. And I did have a bit of fun making the underlying data available as <a href="http://inkdroid.org/empirical-cloud/sameas.rdf">rdf/xml</a> and <a href="http://inkdroid.org/empirical-cloud/sameas.ttl">Turtle</a> using the <a href="http://vocab.deri.ie/void">Vocabulary of Interlinked Datasets (VoID)</a>. And I generated a similar visualization for the <a href="http://inkdroid.org/empirical-cloud/2009.html">2009 data</a>. But it does feel a bit navel-gazy, so a sense of humor about the enterprise is often a good tonic. I guess this is the whole point of the Challenge, to get something generally useful (and not navel-gazy) out of the sea of triples.

Oh and <a href="http://www.amazon.com/Sign-Three-Holmes-Advances-Semiotics/dp/0253204879">Sign of Three</a> is an excellent read so far :-)


]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2453</wp:post_id>
		<wp:post_date>2010-09-15 07:47:07</wp:post_date>
		<wp:post_date_gmt>2010-09-15 14:47:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>triadomany</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="graphs"><![CDATA[graphs]]></category>
		<category domain="post_tag" nicename="humor"><![CDATA[humor]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="perl"><![CDATA[perl]]></category>
		<category domain="post_tag" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="triples"><![CDATA[triples]]></category>
		<category domain="post_tag" nicename="unix"><![CDATA[unix]]></category>
		<category domain="post_tag" nicename="visualizations"><![CDATA[visualizations]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Linked Library Data at the Deutschen Nationalbibliothek</title>
		<link>http://inkdroid.org/2010/10/19/linked-library-data-at-the-deutschen-nationalbibliothek/</link>
		<pubDate>Wed, 20 Oct 2010 00:17:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2503</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Just last week Lars Svensson from the Deutschen Nationalbibliothek (German National Library aka DNB) made a big <a href="http://lists.w3.org/Archives/Public/public-lod/2010Oct/0016.html">announcement</a> that they have released their <a href="http://en.wikipedia.org/wiki/Authority_control">authority data</a> as <a href="http://en.wikipedia.org/wiki/Linked_Data">Linked Data</a> for the world to use. What this means is that there are now unique URLs (and machine readable data at the other end of them) for:</p>

<ul>
    <li>1.8 million authors  from the <a href="http://web.archive.org/web/20120120051347/http://www.d-nb.de/standardisierung/normdateien/pnd.htm">Personennamendatei (PND)</a></li>
    <li>1.3 million corporate bodies from the <a href="http://web.archive.org/web/20120204073314/http://www.d-nb.de/standardisierung/normdateien/gkd.htm">Gemeinsame Körperschaftsdatei (GKD)</a></li>
    <li>187,000 subject headings from the <a href="http://web.archive.org/web/20120118174415/http://www.d-nb.de/standardisierung/normdateien/swd.htm">Schlagwortnormdatei (SWD)</a></li>
    <li>51,000 Dewey Decimal Classification categories</li>
</ul>

<p>The full dataset that the DNB has made available for <a href="https://wiki.d-nb.de/display/LDS/Dokumentation+des+Linked+Data+Services+der+DNB">download</a> amounts to 38,849,113 individual statements (aka triples). Linked Data <a href="http://linkeddata.org">enthusiasts</a> that are used to thinking in terms of <a href="http://challenge.semanticweb.org/">billions of triples</a> might not even blink when seeing these numbers. But it is important to remember that these data assets have been curated by a network of German, Austrian and Swiss libraries, for close to a hundred of years, as they documented (and continue to document) all known German-language publications.</p>

<p>The simple act of making each of these authority records URL addressable, means that they can now meaningfully participate in the global information space some call the Web of Data. It's true, the records were available as part of the DNB's <a href="https://portal.d-nb.de/">Online Catalog</a> before they were released as Linked Data. What's new is that the DNB has <em>commited</em> to using persistent URLs to identify these records, using a new host name <em>d-nb.info</em> in combination with their own record identifiers. This means that people can persistently link to these DNB resources in their own web applications and data. Another subtle thing, and really the heart of what Linked Data pattern offers, is the ability to use the same URL to retrieve the record as structured metadata. The important thing about having machine readable data is it allows other applications to easily re-purpose the information, much like libraries have done traditionally by shipping around batches of <a href="http://loc.gov/marc">Machine Readable Cataloging (MARC)</a> records. Here's a practical example:</p>

<p>The URL <a href="http://d-nb.info/gnd/119053071">http://d-nb.info/gnd/119053071</a> identifies the author <a href="http://en.wikipedia.org/wiki/Herta_M%C3%BCller">Herta Müller</a>, who won the Nobel Prize for Literature in 2009. If you load that URL in your web browser by clicking on it, you should see a web page (HTML) for the authority record describing Herta Müller. But if a web client requests that same URL asking for <a href="http://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> it will (via a redirect) get the same authority record as <a href="http://d-nb.info/gnd/119053071/about">RDF</a>. RDF is more a data model than a particular file format, so it has a variety of serializations ... The server at d-nb.info returns RDF/XML, and they have made their data dumps available in <a href="http://www.w3.org/2001/sw/RDFCore/ntriples/">N-Triples</a>...but I'm kind of fond of the <a href="http://www.w3.org/TeamSubmission/turtle/">Turtle</a> serialization which is kind of JSON-ish, and makes the RDF a bit more readable. Here is the RDF (as Turtle) for Herta Müller that the DBN makes available:</p>

<pre>
@prefix gnd: &lt;http://d-nb.info/gnd/&gt; .
@prefix rdaGr2: &lt;http://RDVocab.info/ElementsGr2/&gt; .
@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .
@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .

&lt;http://d-nb.info/gnd/119053071&gt;
    rdaGr2:biographicalInformation "Rumän.-dt. Schriftstellerin und Essayistin, lebt seit 1987 in Deutschland, Literaturnobelpreisträgerin 2009"@de ;
    rdaGr2:dateOfBirth "1953" ;
    rdaGr2:identifierForThePerson "(DE-588)119053071", "(DE-588c)4293331-6", "(DLC)n  86833524" ;
    rdaGr2:placeOfBirth "Nitzkydorf (Banat)"@de ;
    rdaGr2:placeOfResidence "Berlin"@de ;
    rdaGr2:professionOrOccupation &lt;http://d-nb.info/gnd/4053311-6&gt; ;
    gnd:countryCodeForThePerson "XA-RO" ;
    gnd:preferredNameForThePerson [
        gnd:foreName "Herta" ;
        gnd:surname "Müller" ;
        gnd:usedRules "RAK-WB"
    ], "Müller, Herta" ;
    gnd:studyPathsOfThePerson "Germanistik, Romanistik"@de ;
    gnd:variantNameForThePerson [
        gnd:foreName "Cherta" ;
        gnd:surname "Myller" ;
        gnd:usedRules "RAK-WB"
    ], [
        gnd:personalName "Heta-Mulei" ;
        gnd:usedRules "RAK-WB"
    ], [
        gnd:foreName "Heta" ;
        gnd:surname "Mulei" ;
        gnd:usedRules "RAK-WB"
    ], [
        gnd:foreName "Herta" ;
        gnd:surname "Müller" ;
        gnd:usedRules "AACR"
    ], [
        gnd:foreName "Heruta" ;
        gnd:surname "Myur?" ;
        gnd:usedRules "RAK-WB"
    ], "Heta-Mulei", "Mulei, Heta", "Müller, Herta", "Myller, Cherta", "Myur?, Heruta" ;
    owl:sameAs &lt;http://dbpedia.org/resource/Herta_M%C3%BCller&gt;, &lt;http://viaf.org/viaf/12324250&gt; ;
    foaf:page &lt;http://de.wikipedia.org/wiki/Herta_M%C3%BCller&gt; .


</pre>

<p>A few interesting things to note in this example are the use the <a href="http://rdvocab.info/ElementsGr2">RDA Group 2 Entities vocabulary</a> and the GND vocabulary to describe Herta Müller. RDF vocabularies are explicit ways of describing resources like people, places, topics, etc. When different things are described using the same vocabulary (or the vocabularies themselves are related together in a particular way) it becomes possible to merge the descriptions, and build software on top of it. So the DNB's choice of RDA and GND is quite significant. Normally the URL for an RDF schema will return a description of that schema known as a <a href="http://www.w3.org/TR/webarch/#namespace-document">Namespace Document</a>. Namespace Documents are handy for understanding what exactly the vocabulary means, and how it might relate to other RDF vocabularies on the web. This is the case for the RDA vocabulary, but the GND vocabulary namespace doesn't appear to be resolving to anything that describes the GND vocabulary.</p>

<p>Another really interesting thing to note about this RDF for Herta Müller are the links to Wikipedia (<a href="http://de.wikipedia.org/wiki/Herta_M%C3%BCller">http://de.wikipedia.org/wiki/Herta_M%C3%BCller</a>), VIAF (<a href="http://viaf.org/viaf/12324250">http://viaf.org/viaf/12324250</a>) and dbpedia (<a href="http://dbpedia.org/resource/Herta_M%C3%BCller">http://dbpedia.org/resource/Herta_M%C3%BCller</a>).  These are important because they contextualize the DNB record for Herta Müller by relating it to other records for her, thus allowing it to be disambiguated from records describing other people named Herta Müller. Another beneficial side effect of linking your own records to others out on the Web of Data is that you enrich your own data in the process. For example if a machine agent resolves the dbpedia URI it will get back RDF that includes 114 new assertions, some of which you can see below:</p>

<pre>
@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .
@prefix dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt; .
@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .
@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .

&lt;http://dbpedia.org/resource/Herta_M%C3%BCller&gt;
    dbpedia-owl:birthDate "1953-08-17"^^&lt;http://www.w3.org/2001/XMLSchema#date&gt; ;
    dbpedia-owl:birthPlace &lt;http://dbpedia.org/resource/Ni%C5%A3chidorf&gt; ;
    dbpedia-owl:spouse &lt;http://dbpedia.org/resource/Richard_Wagner_%28novelist%29&gt; ;
    dbpedia-owl:thumbnail &lt;http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Herta_M%C3%BCller_2007.JPG/200px-Herta_M%C3%BCller_2007.JPG&gt; ;
    rdfs:label "Herta Müller"@de, "Herta Müller"@en, "Herta Müller"@es, "Herta Müller"@fi, "Herta Müller"@fr, "Herta Müller"@it, "Herta Müller"@nl, "Herta Müller"@nn, "Herta Müller"@pl, "Herta Müller"@pt, "Herta Müller"@sv, "??????, ?????"@ru, "????????"@ja, "??·??"@zh ;
    owl:sameAs &lt;http://rdf.freebase.com/ns/guid.9202a8c04000641f8000000000dc69bb&gt;, &lt;http://umbel.org/umbel/ne/wikipedia/Herta_M%C3%BCller&gt; ;
    foaf:depiction &lt;http://upload.wikimedia.org/wikipedia/commons/2/2c/Herta_M%C3%BCller_2007.JPG&gt; .
</pre>

<p><a href="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Herta_M%C3%BCller_2007.JPG/200px-Herta_M%C3%BCller_2007.JPG"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Herta_M%C3%BCller_2007.JPG/200px-Herta_M%C3%BCller_2007.JPG" style="float: left; margin-right: 10px; padding-right: 10px;" /></a> So now we've enriched the DNB authority record with:</p>

<ul>
<li>a thumbnail picture of Herta Müller</li>
<li>her name in Japanese, Chinese and Russian</li>
<li>her birth day</li>
<li>her place of birth</li>
<li>a link to a similar record for her spouse Richard Wagner
</li><li>links to records for Herta Müller at <a href="http://freebase.org">Freebase</a> (recently <a href="http://googleblog.blogspot.com/2010/07/deeper-understanding-with-metaweb.html">purchased</a> by Google)
</li></ul>

<p>And that's just a sampling of the sorts of data that dbpedia returns. Another interesting one to look at is the <a href="http://viaf.org">Virtual International Authority File (VIAF)</a>, which links together the authority records for 18 National Libraries around the world. If you resolve the VIAF URL that DNB have linked to, you will get machine readable information for authority records from the the Library of Congress, NII (Japan), Biblioteca Nacional de Portugal, National Library of Sweden, Biblioteca Nacional de España, Bibliothèque nationale de France, National Library of the Czech Republic, and of course the Deutsche Nationalbibliothek. The information for the DNB and Sweden are particularly important because they in turn <em>link back to the records at the originating institution</em>: <a href="http://d-nb.info/gnd/119053071">http://d-nb.info/gnd/119053071</a> and <a href="http://libris.kb.se/auth/218085">http://libris.kb.se/auth/218085</a>. It might be worthwhile for the DNB to consider linking directly to their own record in VIAF http://viaf.org/viaf/12324250/#DNB%7C119053071 instead of http://viaf.org/viaf/12324250, but that's largely a technical matter. We've connected up the DNB's notion of Herta Müller with the Royal Library of Sweden's--just by <a href="http://inkdroid.org/2008/01/04/following-your-nose-to-the-web-of-data/">following our nose</a> on the World Wide Web. And this is an activity that computer software can perform as well.</p>

<p>So, it's clear there's a whole lot of <em>library</em> linking going on. I did some quick and dirty analysis of the full data dump from the DNB and found: 3,569,402 links to VIAF and 40,136 links to dbpedia (the Linked Data version of Wikipedia). What remains to be done to some extent is leveraging this contextual information around our data in Library Applications, both cataloging, metadata enrichment applications and end user facing discovery applications.</p>

<p>One challenge to building applications that use this Web of Library Data are the vocabularies that are used. I did some more rudimentary analysis on the full DNB data dump and came up with this count of property usage:</p>

<table style="word-wrap: break-word;">
<tr><th>RDF Property</th><th>Number of Assertions</th></tr>
<tr><td>http://www.w3.org/2002/07/owl#sameAs</td><td>3,609,878</td>
</tr><tr><td>http://d-nb.info/gnd/preferredNameForThePerson</td><td>3,609,753</td>
</tr><tr><td>http://d-nb.info/gnd/usedRules</td><td>3,476,879</td>
</tr><tr><td>http://d-nb.info/gnd/variantNameForThePerson</td><td>3,327,005</td>
</tr><tr><td>http://d-nb.info/gnd/surname</td><td>3,218,840</td>
</tr><tr><td>http://d-nb.info/gnd/foreName</td><td>3,218,125</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/identifierForTheCorporateBody</td><td>2,642,185</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/identifierForThePerson</td><td>2,163,258</td>
</tr><tr><td>http://d-nb.info/gnd/preferredNameForTheCorporateBody</td><td>1,320,711</td>
</tr><tr><td>http://d-nb.info/gnd/variantNameForTheCorporateBody</td><td>1,293,751</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/biographicalInformation</td><td>1,084,183</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/professionOrOccupation</td><td>1,059,570</td>
</tr><tr><td>http://d-nb.info/gnd/publicationOfThePerson</td><td>986,418</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/dateOfBirth</td><td>971,993</td>
</tr><tr><td>http://d-nb.info/gnd/countryCodeForThePerson</td><td>823,100</td>
</tr><tr><td>http://d-nb.info/gnd/countryCodeForTheCorporateBody</td><td>759,088</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/periodOfActivityOfThePerson</td><td>539,230</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/gender</td><td>404,247</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/dateOfDeath</td><td>381,888</td>
</tr><tr><td>http://purl.org/dc/terms/identifier</td><td>337,230</td>
</tr><tr><td>http://metadataregistry.org/uri/schema/RDARelationshipsGR2/hierarchicalSuperior</td><td>277,484</td>
</tr><tr><td>http://d-nb.info/gnd/personalName</td><td>258,214</td>
</tr><tr><td>http://d-nb.info/gnd/prefixName</td><td>233,481</td>
</tr><tr><td>http://d-nb.info/gnd/functionOfThePerson</td><td>211,045</td>
</tr><tr><td>http://d-nb.info/gnd/invalidIdentifierForThePerson</td><td>208,267</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/placeOfBirth</td><td>192,563</td>
</tr><tr><td>http://d-nb.info/gnd/qualifierName</td><td>169,284</td>
</tr><tr><td>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</td><td>168,615</td>
</tr><tr><td>http://www.w3.org/2004/02/skos/core#prefLabel</td><td>163,854</td>
</tr><tr><td>http://www.w3.org/2004/02/skos/core#altLabel</td><td>143,254</td>
</tr><tr><td>http://xmlns.com/foaf/0.1/page</td><td>123,569</td>
</tr><tr><td>http://d-nb.info/gnd/invalidIdentifierForTheCorporateBody</td><td>122,999</td>
</tr><tr><td>http://www.w3.org/2004/02/skos/core#broader</td><td>118,696</td>
</tr><tr><td>http://metadataregistry.org/uri/schema/RDARelationshipsGR2/predecessor</td><td>110,112</td>
</tr><tr><td>http://metadataregistry.org/uri/schema/RDARelationshipsGR2/successor</td><td>109,819</td>
</tr><tr><td>http://www.w3.org/2004/02/skos/core#narrower</td><td>102,850</td>
</tr><tr><td>http://d-nb.info/gnd/preferredNameAcronymForTheCorporateBody</td><td>102,517</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/dateOfEstablishment</td><td>88,470</td>
</tr><tr><td>http://d-nb.info/gnd/academicTitleOfThePerson</td><td>77,763</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/placeOfResidence</td><td>70,112</td>
</tr><tr><td>http://metadataregistry.org/uri/schema/RDARelationshipsGR2/relatedCorporateBodyPerson</td><td>65,319</td>
</tr><tr><td>http://www.w3.org/2004/02/skos/core#closeMatch</td><td>60,893</td>
</tr><tr><td>http://xmlns.com/foaf/0.1/homepage</td><td>59,065</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/dateOfTermination</td><td>38,997</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/dateOfTermination</td><td>38,997</td>
</tr><tr><td>http://www.w3.org/2004/02/skos/core#definition</td><td>37,086</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/placeOfDeath</td><td>35,266</td>
</tr><tr><td>http://d-nb.info/gnd/locQualifier</td><td>35,220</td>
</tr><tr><td>http://d-nb.info/gnd/studyPathsOfThePerson</td><td>33,307</td>
</tr><tr><td>http://www.w3.org/2004/02/skos/core#related</td><td>26,971</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/nameOfTheCorporateBody</td><td>20,009</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/languageOfThePerson</td><td>13,318</td>
</tr><tr><td>http://d-nb.info/gnd/variantNameAcronymForTheCorporateBody</td><td>12,786</td>
</tr><tr><td>http://www.w3.org/2004/02/skos/core#scopeNote</td><td>11,000</td>
</tr><tr><td>http://d-nb.info/gnd/useInsteadSWD</td><td>9,572</td>
</tr><tr><td>http://d-nb.info/gnd/useInsteadNoteSWD</td><td>9,522</td>
</tr><tr><td>http://d-nb.info/gnd/countryCodeForTheSubject</td><td>7,179</td>
</tr><tr><td>http://RDVocab.info/ElementsGr2/titleOfThePerson</td><td>6,798</td>
</tr><tr><td>http://purl.org/vocab/relationship/childOf</td><td>6,554</td>
</tr><tr><td>http://purl.org/vocab/relationship/parentOf</td><td>5,895</td>
</tr><tr><td>http://purl.org/vocab/relationship/spouseOf</td><td>5,613</td>
</tr><tr><td>http://d-nb.info/gnd/successorWithoutPredecessor</td><td>5,574</td>
</tr><tr><td>http://www.w3.org/2000/01/rdf-schema#label</td><td>4,761</td>
</tr><tr><td>http://d-nb.info/gnd/useConceptsInsteadSWD</td><td>4,761</td>
</tr><tr><td>http://d-nb.info/gnd/invalidIdentifierForTheSubject</td><td>4,635</td>
</tr><tr><td>http://purl.org/vocab/relationship/siblingOf</td><td>3,891</td>
</tr><tr><td>http://metadataregistry.org/uri/schema/RDARelationshipsGR2/relatedPersonPerson</td><td>2,764</td>
</tr><tr><td>http://d-nb.info/gnd/predecessorWithoutSuccessor</td><td>1,501</td>
</tr><tr><td>http://purl.org/vocab/relationship/grandchildOf</td><td>493</td>
</tr><tr><td>http://www.w3.org/2000/01/rdf-schema#seeAlso</td><td>484</td>
</tr><tr><td>http://purl.org/vocab/relationship/grandparentOf</td><td>416</td>
</tr><tr><td>http://purl.org/dc/terms/language</td><td>266</td>
</tr></table>

<p>So we see heavy usage of the http://d-nb.info/gnd/ vocabulary, but we don't know precisely how this vocabulary connects up with other vocabularies in use on the Web. We also see the new RDA vocabulary http://RDVocab.info/ElementsGr2 heavily used. Whereas the trailblazing Royal Library of Sweden chose to leverage the Friend of a Friend vocabulary more. It's very important that we see some convergence in vocabulary use, so that our distributed data is interoperable, and mashable. This will undoubtedly lead to changes in what vocabularies are used, and growing pains in any applications that are dependent on the data. But I think it is worth it. I have high hopes that some of this convergence may come about as a result of meetings later this week at the <a href="http://www.asis.org/Conferences/DC2010/">Dublin Core Metadata Initiative 2010</a> meeting in Pittsburgh. But if it's going to scale, we need to see this convergence going on all the time in online forums like the <a href="http://lists.w3.org/Archives/Public/public-lld/">Linked Library Data discussion list</a>, and via tools that allow library data managers to view the emerging web of library data.</p>

<p>Another niggling little problem is the need to synchronize these data sets. For example how am I to know when DNB has created, updated or deleted one of their authority records? I could wait for a database dump, and blow away what I knew before. But ideally there would be a mechanism to keep my own view of the DNB data synchronized. Of course there is the tried and true <a href="http://www.openarchives.org/OAI/openarchivesprotocol.html">OAI-PMH</a> which VIAF is using to collect MARC rocords, but it is showing its age and doesn't really fit the Linked Data pattern very well. There is the successor to OAI-PMH, <a href="http://www.openarchives.org/ore/1.0/toc">OAI-ORE</a> which better fits more recent notions of Web Architecture and Linked Data. But there are some issues to do with very large resource maps which kind of need ironing out. The <a href="http://groups.google.com/group/dataset-dynamics">Dataset Dynamics</a> has been doing some interesting work identifying the various mechanisms for performing synchronization with an emphasis on using <a href="http://tools.ietf.org/html/rfc4287">Atom</a>. Atom is a <em>standard</em> XML document format for describing sets of web resources. In fact OAI-ORE <a href="http://www.openarchives.org/ore/1.0/atom">leverage</a> Atom as one of the serialization formats for resource maps. But I'm personally hoping we'll see some stream lined guidelines for publishing feeds for Linked Data, that leverage Atom's <a href="http://tools.ietf.org/html/rfc5005">Feed Paging/Archiving</a> for making large lists of resources available. Maybe the <a href="http://data.semanticweb.org/conference/eswc/2008/paper/356/html">Semantic Sitemaps</a> (an Linked Data extension to traditional <a href="http://sitemaps.org">sitemaps</a> that the <a href="http://google.com">big</a> <a href="http://bing.com">web</a> <a href="http://search.yahoo.com">search engines</a> use to stay on top of things. I imagine we'll see a combination of these approaches, but I think it's important to see some convergence amongst Library Linked Data publishers to help the ecosystem flourish.</p>

<p><em>Update:  I shared some more pedantic thoughts about the d-nb.info URLs in <a href="http://groups.google.com/group/pedantic-web/browse_thread/thread/dc58b0b03723009d">another forum</a>. I didn't want these particular technical details/questions to detract from saying how important I think the DNB Linked Data release is.</em></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2503</wp:post_id>
		<wp:post_date>2010-10-19 17:17:23</wp:post_date>
		<wp:post_date_gmt>2010-10-20 00:17:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>linked-library-data-at-the-deutschen-nationalbibliothek</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="authority-records"><![CDATA[authority records]]></category>
		<category domain="post_tag" nicename="germany"><![CDATA[germany]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="people"><![CDATA[people]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btc_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>routers, webcams and thermometers</title>
		<link>http://inkdroid.org/2010/11/08/routers-webcams-and-thermometers/</link>
		<pubDate>Tue, 09 Nov 2010 06:16:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2569</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Router"><img src="http://inkdroid.org/images/linksys-router.jpg" style="width: 300px; margin-right: 10px; margin-bottom: 5px; float: left; border: medium solid grey;" /></a> If you have a local wi-fi network at home you probably use something like this Linksys wireless router on the left, to let your laptop and other devices connect to the Internet. When you bought it and plugged it in you probably followed the instructions and typed "http://192.168.1.1/" into your web browser and visited a page to configure the router: settings its name, admin password, etc.</p>

<p>Would you agree that this router sitting on top of your TV, or wherever it is, is a <em>real world thing</em>? It's not some abstract concept of a router: you can pick it up, turn it off and on, take it apart and try to put it back together again. And the router is identified with a URL: <a href="http://192.68.1.1.">http://192.168.1.1</a>. When your web browser resolves the URL for your router it gets back some HTML, that lets you see the router's current state, and make modifications to it. You don't get the <em>router itself</em>. That would be silly right?</p>

<p>In terms of <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">REST</a>, the router is a <em>Resource</em> that has a URL <em>Identifier</em>, which when resolved returns an HTML <em>Representation</em> of the <em>Resource</em>. But you don't really have to think about it much at all, because it's intuitively part of how you use the web every day.</p>

<p>In fact the Internet is strewn with online devices that have embedded web servers in them. A 5 year old BoingBoing article <a href="http://www.boingboing.net/2005/01/05/more_googleable_unse.html">More Googleable Unsecured Webcams</a> shows how you can drop a web search for <a href="http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=inurl:%22view/index.shtml%22#q=inurl:%22view/index.shtml%22">inurl:"view/index.shtml"</a> into Google, and get back thousands of webcams from around the world. You can zoom and pan these cameras using your web browser. These are URLs for <em>real world</em> cameras. When you put the URL in your browser you don't get the camera itself, that's crazy talk; instead you get some HTML describing the camera's current state, and some form controls for changing its position. Again all is well in the REST world, where the camera is the <em>Resource</em> identified with a URL, and your browser receives a <em>Representation</em> of the <em>Resource</em>.</p>

<p><a href="http://en.wikipedia.org/wiki/File:Arduino_Duemilanove_2009b.jpg"><img src="http://inkdroid.org/images/arduino.jpg" style="margin-left: 10px; margin-bottom: 5px; float: right; width: 400px; border: medium solid grey;" /></a></p>

<p>If you are an Arduino hacker you might follow some <a href="http://www.practicalarduino.com/projects/online-thermometer">instructions</a> to build an online thermometer. You wire up the temperature sensor, and configure the Arduino to listen for HTTP requests at a particular IP address. You can then visit a URL in your web browser, and the server returns a <em>Representation</em> of the current temperature. It doesn't return the Arduino board, the thermometer, or the thermodynamic state of its environment...that's crazy talk. It returns a <em>Representation</em> of the temperature.</p>

<p>So imagine I want to give myself a URL, say <a href="http://inkdroid.org">http://inkdroid.org</a>. Is this so different than the camera, the router and the thermometer? Sure, I don't have a web server embedded in me. But even if I did nobody would expect it to return <em>me</em> would they? Just as in the other cases, people would expect a <em>Representation</em> of me to be returned. Heck, there are millions of <a href="http://en.wikipedia.org/wiki/OpenID">OpenID</a> URLs deployed for people already. But this argument is used time, and time again in the Semantic Web, Linked Data community to justify the need for elaborate, byzantine, hard to explain HTTP behavior when making RDF descriptions of real world things available. The pattern has been best described in the <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a> W3C Note. I understand it. But if you've ever had to explain it to a web developer not already brainwashed^w familar with the pattern you will understand that it is hard to explain convincingly. It's even harder to implement correctly, since you are constantly asking yourself nonsensical questions like: "is this a Information Resource" when you are building your application.</p>

<p>I was pleased to see Ian Davis' recent <a href="http://iand.posterous.com/is-303-really-necessary">well</a> <a href="http://iand.posterous.com/a-guide-to-publishing-linked-data-without-red">articulated</a> posts about whether the complicated HTTP behavior is essential for deploying Linked Data. I know I am biased because I was introduced to much of the Semantic Web and Linked Data world when Ian Davis and <a href="http://twitter.com/beobal">Sam Tunnicliffe</a> visited the Library of Congress three years ago. I agree with Ian's position: the current situation with the 303 redirect is potentially wasteful, error prone and bordering on the absurd...and the Linked Data community could do a lot to make it easier to deploy Linked Data. At its core, Ian's advice in <a href="http://iand.posterous.com/a-guide-to-publishing-linked-data-without-red">Guide to Publishing Linked Data Without Redirects</a> does a nice job of making Linked Data publishing seem familiar to folks who have used HTTP's content-negotiation features to enable internationalization, or building RESTful web services. A URL for a resource that has a more constrained set of representations, allows for <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec12.html#sec12.2">Agent Driven Negotiation</a> in situations where custom tuning the Accept header in the client isn't convenient and practical. Providing a pattern for linking these resources together with something like wrds:describedby and/or the describedby relation that's now available in <a href="http://tools.ietf.org/html/rfc5988">RFC 5988</a> is helpful for people building REST APIs and Linked Data applications.</p>

<p><a href="http://www.flickr.com/photos/psd/5159730124/in/set-72157625161668113/"><img src="http://inkdroid.org/images/psd-resource.jpg" style="float: left; margin-right: 10px; margin-bottom: 5px;"/></a></p>

<p>At the end of the day, it would be useful if the W3C could de-emphasize httpRange-14,  simplify the <a href="http://www.w3.org/TR/webarch/">Architecture of the World Wide Web</a> (by removing the notion of Information Resources), and pave the cowpaths we already are seeing for Real World Objects on the Web. It would be great to have a W3C document that guided people on how to put URIs for things on the web, that fit with how people are already doing it, and made intuitive sense. We're already used to things like our routers, cameras and thermometers being on the web, and my guess is we're going to see <a href="http://www.webofthings.com/">much</a>, <a href="http://web.archive.org/web/20101203035826/http://panelpicker.sxsw.com/ideas/view/7607?return=%2Fideas%2Findex%2F7%2Fpresenter%3AGuinard">much</a> <a href="http://www.iot2010.org/">more</a> <a href="http://web.archive.org/web/20110123182233/http://www.inf.ethz.ch:80/personal/dguinard/publications/bibtex.html?file=/home/webvs/www/htdocs/publ/papers/dguinard-giving-2010">of it</a> in the coming years. I don't think a move like this would invalidate documents like <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a>, or make the existing Linked Data that is out there somehow wrong. It would simply lower the bar for people who want to publish Linked Data, who don't necessarily want to go through the process of using URIs to distinguish non-Information Resources from Information Resources.</p>

<p>If the W3C doesn't have the stomach for it, I imagine we will see the <a href="http://en.wikipedia.org/wiki/IETF">IETF</a> lead the way, or for innovation to happen elsewhere as with <a href="http://en.wikipedia.org/wiki/HTML5">HTML5</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2569</wp:post_id>
		<wp:post_date>2010-11-08 23:16:23</wp:post_date>
		<wp:post_date_gmt>2010-11-09 06:16:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>routers-webcams-and-thermometers</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="identifiers"><![CDATA[identifiers]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"83750";i:1;s:5:"83792";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>83792</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>68.50.223.109</wp:comment_author_IP>
			<wp:comment_date>2010-11-10 07:25:19</wp:comment_date>
			<wp:comment_date_gmt>2010-11-10 14:25:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for putting this out there, well said.  Converges nicely with Ian Davis and Andy Powell's posts on this topic. 

One point that I think is particularly clear from the view of we in libraries who work with "information resources that are not on the web", is how tricky that ontological/epistemological/metaphysical/whatever-fancy-word-you-want to use distinction actually is, it doesn't always make a lot of sense.  (Is an eBook an 'information resource', but a printed a book a 'non information resource' huh?). Something you blogged about first running into with the historical newspapers project. 

The way httpRange-14 prioritizes that binary distinction as THE fundamental ontological decision -- I think that will seem less and less useful as the internet continues to increase as an intrinsic part of daily life.  Not sure that sentance makes any sense, heh. 

There ARE some real problems with understanding and being clear about what the subjects of our assertions are -- but httpRange-14, in my opinion, does not actually succesfully solve them, at the cost of being a very confusing barrier to putting your open data on the web in a machine friendly way. (This will be laughable to some, but I think the FRBR ontology does a _better_ job of modelling the entities we make assertions about when dealing with information, then httpRange-14 does, with it's effective ontology of two entities, 'information resource' and 'non-information resource'. With FRBR instead your data can simply declare within itself (rather than as a transient side-effect of it's transfer protocol) whether it's subject is a 'manifestation' (a particular respresentation), or a 'work' (the abstract 'non-information resource' that is "my book about aardvarks" whether in ebook, html, or print form).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83750</wp:comment_id>
			<wp:comment_author><![CDATA[me.yahoo.com/danbri3]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/danbri3#3ecf3</wp:comment_author_url>
			<wp:comment_author_IP>87.210.48.176</wp:comment_author_IP>
			<wp:comment_date>2010-11-09 02:06:29</wp:comment_date>
			<wp:comment_date_gmt>2010-11-09 09:06:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I made much the same argument in, erm, 2003. I had a Tivo on the home network with an ethernet card added, and a Web server. It had /proc/therm so I could get the temperature in the room...

http://www.w3.org/2003/03/06-SW-Arch

[...]
timbl [at white board]
when someone makes up a property and uses it, how do we know what it means?
my view of web architecture, unformalized, but useful
when you have a URI, you look in a registry that points to another spec.
that spec points to the DNS spec which allows you to n IP
you use the HTTP spec to open a TCP connection and send a request based on that URI
you get back a string of bits that have a mime type.
that give you a pointer to the RDF spec.
patrick stickler
[re-expresses question]
timbl
specs says how bits are arranged. rest of this is all context
danbir
you get a triple set and want to see which are core and which are fluff.
timbl
if you understand part of it, that's ok.
[... lots more dicussion along this line...]
timbl
Roy and my models differ because it's not tested in the conventional web.
Roy maintains philosophically that foo.rdf could be a car.
That introduces a lot of inconsistancy for me.
red /w yellow stripe
why can't it be a car (or piece of hardware)
lynn
danbri
you run into the same problem with content negotiation.
why can't i give the thermometer in my tivo at home a URL.
[...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>341</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83777</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2010-11-09 09:31:09</wp:comment_date>
			<wp:comment_date_gmt>2010-11-09 16:31:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@danbri interesting historical details there. Are those actual notes you still have from the meeting? If so, colo(u)r me impressed :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83793</wp:comment_id>
			<wp:comment_author><![CDATA[Arguments about HTTP 303 Considered Harmful &#8211; Tom Heath&#8217;s Displacement Activities]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://tomheath.com/blog/2010/11/arguments-about-http-303-considered-harmful/</wp:comment_author_url>
			<wp:comment_author_IP>69.163.171.65</wp:comment_author_IP>
			<wp:comment_date>2010-11-10 07:34:42</wp:comment_date>
			<wp:comment_date_gmt>2010-11-10 14:34:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] to take a bit of responsibility for writing sensible RDF statements. Unfortunately, people like Ed seeming to conflate himself and his homepage (and his router and its admin console) don&#8217;t help with the general level of understanding. I&#8217;ve tried many times to explain to [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>me and my homepage</title>
		<link>http://inkdroid.org/2010/11/12/me-and-my-homepage/</link>
		<pubDate>Fri, 12 Nov 2010 17:16:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2651</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.flickr.com/photos/psd/sets/72157625161668113/with/5159730124/"><img src="http://inkdroid.org/images/resource-identifiers.jpg" style="float: left;" /></a></p>

<p>Thanks for all the positive feedback to my <a href="http://inkdroid.org/2010/11/08/routers-webcams-and-thermometers/">last post</a> about using URLs to identify resources. <a href="http://tomheath.com/home/html">Tom Heath</a> (one of the founding fathers of the <a href="http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/">Linked Data meme/pattern</a>) suggested that discussions about this topic are <a href="http://tomheath.com/blog/2010/11/arguments-about-http-303-considered-harmful/">harmful</a>, so of course I have to continue the conversation ... even if I don't have much new to say. Hell, I just wanted an excuse to re-publish another one of Paul Downey's lovely <a href="http://www.flickr.com/photos/psd/sets/72157625161668113/with/5159730124/">REST Tarot Cards</a> that he is doing for <a href="http://www.flickr.com/groups/1219303@N25/">NaNoDrawMo 2010</a>, and get some more hits on my backwater blog :-)</p>

<p>Anyhow, so Tom <a href="http://tomheath.com/blog/2010/11/arguments-about-http-303-considered-harmful/">said</a>:</p>

<blockquote>
Joe Developer ... has to take a bit of responsibility for writing sensible RDF statements. Unfortunately, people like Ed seeming to conflate himself and his homepage (and his router and its admin console) don’t help with the general level of understanding. I’ve tried many times to explain to someone that I am not my homepage, and as far as I know I’ve never failed. In all this frantic debate about the 303 mechanism, let’s not abandon certain basic principles that just make sense.
</blockquote>

<p>I'm glad Tom is able to explain this stuff about Information Resources better than me. I think I was probably one of the people he explained it to at some point. I understand the principles that Tom and other Linked Data advocates are promulgating well enough to throw together some Linked Data implementations at the Library of Congress, such as the <a href="http://web.archive.org/web/20110720034058/http://id.loc.gov:80/authorities">Library of Congress Subject Headings</a> and <a href="http://chroniclingamerica.loc.gov/about/api/">Chronicling America</a> which put millions of resources online using the principles that got documented in <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a>.</p>

<p>How do you know if someone understood something you said? Normally by what they do in response to what you say, right? The rapid growth of the <a href="http://richard.cyganiak.de/2007/10/lod/">Linked Data cloud</a> is a testament to the Linked Data folks ability to effectively communicate with software developers. No question. But lets face it, the principles of <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">web architecture</a> have seen way more adoption right? The successes that Linked Data have enjoyed so far have been a result of grounding the Semantic Web vision in the mechanics of the web we have now. And my essential point is that they didn't go far enough in making it easier.</p>

<p>So, yeah...I'm not my homepage. As someone would've said to me in grade school: "No shit Sherlock" :-) Although, our blogs sure seem to be having a friendly argument with each other at the moment (thanks Keith). What is a homepage anyhow? The Oxford English Dictionary defines a homepage as:</p>

<blockquote>
A document created in a hypertext system (esp. on the World Wide Web) which serves either as an introductory page for a visitor to a web site, or as a focus of information on a particular topic, and which usually contains hypertext links to related documents and other web sites. 
</blockquote>

<p>So my <em>homepage</em> is a hypertext document with a particular focus, in this case the person Ed Summers. If you are at your desk, and fire up your browser, and type in the URL for my homepage you would get an HTML document. If you were on the train, and typed in the same URL into the browser on your mobile device you might get a very different HTML document optimized for rendering on a smaller screen. <em>This is how the web was designed to work</em>, albeit a bit ex post facto (which is why it is awesome). A URL identifies a Resource, a Resource can be anything, when you request that Resource using HTTP you get back a Representation of the current state of the Resource. The Representation that you get back is determined by the way it was requested: in this case the User-Agent of the browser determined what HTML I got back.</p>

<p>It's very easy to look down over your bi-focals and say things like "surely Ed realizes he is not his homepage". But if we're going to go there, it kind of begs the question, what is a homepage ... and who am I?  <a href="http://plato.stanford.edu/entries/identity/">Identity</a> is hard. Tom should be pretty familiar with how hard identity as his <a href="http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/#links">instructions</a> on using owl:sameAs to link resources together <a href="http://www.webont.org/owled/2010/papers/owled2010_submission_12.pdf">proves</a> <a href="http://events.linkeddata.org/ldow2010/papers/ldow2010_paper09.pdf">to</a> <a href="http://journal.webscience.org/403/2/websci10_submission_123.pdf">be</a> a bit harder in practice than in theory.</p>

<p>But let's not go there. Who really wants to think about stuff like that when you are building a webapp that makes reusable machine readable data available?</p>

<p>My contention is that this whole line of discussion is largely academic, and gets in the way of actually putting resource descriptions out on the web. The reality is that people <a href="http://socialgraph.apis.google.com/lookup?q=http://inkdroid.org/&fme=1&pretty=1">can and do</a> use http://inkdroid.org/ as an identifier for me, Ed Summers. It is natural, and effortless, and doesn't require someone with a PhD in Knowledge Representation to understand it. If I want to publish some RDF that says:</p>

<pre>
&lt;http://inkdroid.org/&gt; a foaf:Person .
</pre>

<p>I can do that. It's my website, and I decide what the resources on it are. If someone puts that URL into their browser and gets some HTML that's cool. If someone's computer program likes RDF and gets back some "application/rdf+xml", all the better. If a script wants to nibble on some JSON, sure here's some "application/json" for ya. If someone wants to publish RDF about me, and use http://inkdroid.org/ as the identifier to hang their descriptions off of, I say, go right ahead. It's an Open Web still right (oh please say it still is).</p>

<p>And best of all, if someone wants different URLs for themselves and their homepage, that's fine too. The Linked Data we have deployed by following the rules to the best of our ability is still legit. It's all good. I don't mind following rules, but ultimately they have to make sense...to me. And this website is all about me, right? :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2651</wp:post_id>
		<wp:post_date>2010-11-12 10:16:19</wp:post_date>
		<wp:post_date_gmt>2010-11-12 17:16:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>me-and-my-homepage</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="rest"><![CDATA[rest]]></category>
		<category domain="post_tag" nicename="urls"><![CDATA[urls]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="webarch"><![CDATA[webarch]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{i:0;s:5:"83816";i:1;s:5:"83825";i:2;s:5:"83826";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>83816</wp:comment_id>
			<wp:comment_author><![CDATA[jakoblog.de/]]></wp:comment_author>
			<wp:comment_author_email>jakob@nichtich.de</wp:comment_author_email>
			<wp:comment_author_url>http://jakoblog.de/</wp:comment_author_url>
			<wp:comment_author_IP>93.246.18.179</wp:comment_author_IP>
			<wp:comment_date>2010-11-12 10:50:39</wp:comment_date>
			<wp:comment_date_gmt>2010-11-12 17:50:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The Arctic Monkeys have already solved the identity problem: <a href="http://en.wikipedia.org/wiki/Whatever_People_Say_I_Am,_That's_What_I'm_Not" rel="nofollow">Whatever People Say I Am, That's What I'm Not</a>. Ups, I broke the Semantic Web by linking to a Wikipedia article about the album, which is not the album itself! Let's dance to the music on the grave of Linked Data and solve people's issues instead of machine's problems. SCNR]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>337</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83825</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>68.50.223.109</wp:comment_author_IP>
			<wp:comment_date>2010-11-12 22:30:56</wp:comment_date>
			<wp:comment_date_gmt>2010-11-13 05:30:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I am not my home page. I am also not _any other_ representation you'll get on the web, including a bunch of triples about me. 

My home page is, arguably, one web representation of me. It may or may not be useful to consider it such.  Trying to make rules based on HTTP status codes to prohibit me from considering it as such probably isn't helpful. 

I get what httpRange-14 (or is it 13? the hard to remember number of this principle doens't help). Right, it says, jrochkind is not ANY representation of him on the web. That's why the URI representing jrochkind himself can't in fact return ANY representation at all, it has to 303 to another URI that returns a representation. 

I get it. But I think the distinction between things and representation quickly becomes much more confusing in many actual common use cases than it seems to be when you stick to only the least confusing examples. I am not my home page. But is a PDF of "Lucene in Action" actually "Lucene in Action", or is it just a representation of it?  Does the answer to this question _really_ depend on whether Lucene In Action is also published in print (or in non-PDF electronic formats?).  Convincing someone that I am not my home page is easy. Convincing them that a PDF of Lucene in Action is not Lucene in Action is harder. 

If you take httpRange-14 all the way, then almost EVERY URI used in an assertion should really be a URI that 303's.  Very few assertions are _really_ meant to be about representations themselves, they're almost all meant to be about things that may have a life beyond one particular representation.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83826</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>68.50.223.109</wp:comment_author_IP>
			<wp:comment_date>2010-11-12 23:11:16</wp:comment_date>
			<wp:comment_date_gmt>2010-11-13 06:11:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[You know what, I think reading "Cool URIs for the Semantic Web", that is actually is what they intend as the consequences of their rules:  Unless you really mean to be making an assertion about the particular representation returned by a URI, _always_ use a URI that 303 redirects. 

If this is indeed the intended consequence, that above sentence would, I think, be a LOT less confusing way to explain it than talking about "information resources" or "non-information resources". Leave the definition of such out of it entirely, since in addition to being confusing it's untenable. And indeed "Cool URIs for the Semantic Web" tries to use "Web Document", to I guess clarify that. 

I'm not sure that everyone has really understood that implication, that to follow those guidelines you hardly ever ever can make an assertion about a URI that is NOT a 303 redirect, it probably won't be what you mean to say.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>83832</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2010-11-14 05:52:45</wp:comment_date>
			<wp:comment_date_gmt>2010-11-14 12:52:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@jrochkind absolutely, the authors of Cool URIs for the Semantic Web wanted all real-world-things identified by URIs to HTTP 303 redirect to a document about the thing, or to be a Hash URI. The thing to remember about URIs is that they identify <em>Resources</em>, not <em>Representations</em>, at least in terms of REST. At the end of the day this incompatibility between Linked Data / Semantic Web and REST ways of looking at the Web seems to come down to an inability of Tim Berners-Lee and Roy Fielding to see eye to eye about the architecture of the web. I think Dan Brickley's <a href="http://inkdroid.org/2010/11/08/routers-webcams-and-thermometers/#comment-83750" rel="nofollow">earlier comment</a> confirms this.

You are quite right that <a href="http://inkdroid.org/2010/07/07/linking-things-and-common-sense/" rel="nofollow">we don't very often</a> want to describe representations. Although, we in the library world seem to want to more often than people in other domains tend to. Tom Scott of the BBC characterized this situation pretty nicely in <a href="http://derivadow.com/2010/07/01/linked-things/" rel="nofollow">a blog post</a> a few months ago.

There are certain situations where Linked Data folks have been tempted into wanting to describe what they call the Document as opposed to the Thing. The canonical example that gets used is assertions involving a license: they often want to license the "document" that describes the resource, but not the resource itself being described. Also, people think things can get murky when using dcterms:creator, dcterms:created, dcterms:modified, etc ... because they get wrapped around the axle about whether these assertions are about the HTML document or about the real-world-thing being described.

My main point (and I think <a href="http://iand.posterous.com/a-guide-to-publishing-linked-data-without-red" rel="nofollow">Ian's</a> as well) is that <em>you</em> as the publisher of this resource get to say what the Resource is. RDF is great because it gives you the machinery to explicitly do this. For example if Manning wants to assert that:

<pre>
&lt;http://www.manning.com/hatcher3/&gt; a bibo:Book .
</pre>

They can do that. If they want to later say:

<pre>
&lt;http://www.manning.com/hatcher3/&gt; dcterms:hasVersion &lt;http://www.manning.com/hatcher3/ebook.pdf&gt; .
&lt;http://www.manning.com/hatcher3/ebook.pdf&gt; dcterms:hasFormat "application/pdf" .
&lt;http://www.manning.com/hatcher3/ebook.pdf&gt; dcterms:license &lt;http://www.manning.com/about/license.html&gt; .
</pre>

They can do that too. No HTTP status code futzing around or arcane use of hash URIs is needed.

This is what tons of users of <a href="http://developers.facebook.com/docs/opengraph" rel="nofollow">Facebook's OpenGraph Protocol</a> are discovering now. If IMDB wants to say that http://www.imdb.com/title/tt0093822/ identifies a movie, they can and do. IMDB decides what the resources are that they are publishing, and they tell other people, machines running at Facebook, and anyone else who wants to crawl their site by explicitly asserting it; not by some funky use of HTTP status codes or hash URIs.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>iogdc ramblings</title>
		<link>http://inkdroid.org/2010/11/16/iogdc-ramblings/</link>
		<pubDate>Tue, 16 Nov 2010 15:19:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2705</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Yesterday I was at the first day of the <a href="http://www.data.gov/conference">International Open Government Data Conference</a> in Washington DC. It was an exciting day, with a great deal of enthusiasm being expressed by luminaries like <a href="http://en.wikipedia.org/wiki/Tim_Berners-Lee">Tim Berners-Lee</a>, <a href="http://en.wikipedia.org/wiki/Jim_Hendler">Jim Hendler</a> , <a href="http://en.wikipedia.org/wiki/Beth_Simone_Noveck">Beth Noveck</a>, and <a href="http://en.wikipedia.org/wiki/Vivek_Kundra">Vivek Kundra</a> for enabling participatory democracy by opening up access to government data. Efforts like <a href="http://data.gov/">data.gov</a>, <a href="http://data.gov.uk">data.gov.uk</a>, <a href="http://data.govt.nz/">data.govt.nz</a>, <a href="http://data.australia.gov.au/">data.australia.gov.au</a> to aggregate egov datasets from their jurisdictions were well represented, although it would've been great to hear more from places like Spain, Sweden as well as groups like the <a href="http://sunlightfoundation.com/">Sunlight Foundation</a> and <a href="http://okfn.org/">Open Knowledge Foundation</a> ... but there are two more days to go. Here are my reflections so far from the first day:</p>

<h3>Licensing</h3>

<p>New Zealand is embracing the use of <a href="http://creativecommons.org/">Creative Commons</a> licenses to release their datasets onto the web. Their <a href="http://web.archive.org/web/20110812114105/http://www.e.govt.nz/policy/nzgoal">NZGOAL</a> project got cabinet approval for using CC licenses in June of this year. They are now doing outreach within government agencies, and building tools to help data owners put these license into play, so that data can go out on the web. Where I work at the Library of Congress, the general understanding is that our data is public domain (in the US) ... except when its not. For example some of the high resolution images in the <a href="http://loc.gov/pictures/">Prints and Photographs Catalog</a> aren't available outside the physical buildings of the Library of Congress, due to licensing concerns. So I'm totally envious of New Zealand's coordinated efforts to iron out these licensing issues.</p>

<h3>Centralization/Decentralization</h3>

<p>Vivek Kundra and Alan Mallie of the data.gov touted the number of datasets that they are federating access to. But it remains unclear exactly how content is federated, and how datasets flow from agencies into data.gov itself. Perhaps some of these details are included in the v1.0 release of the data.gov <a href="http://www.usa.gov/Help/Free_Downloads.shtml">Concept of Operations</a> (which Kundra announced). An excellent question posed to Berners-Lee and Kundra concerned what role centralized and distributed approaches play in publishing data. While there is value in one-stop-shopping where you can find data aggregated in one place, Berners-Lee really stressed that the web grew because it was distributed. Aggregated collections of datasets like data.gov need to be able to efficiently pull data from places where it is collected. We need to use the web effectively to enable this.</p>

<h3>Legacy Data</h3>

<p>There are tons of datasets waiting to be put on the web. Steve Young of the EPA described a few datasets such as the <a href="http://www.epa.gov/tri/">Toxics Release Inventory</a>, which has the goal to:</p>

<blockquote>
provide communities with information about toxic chemical releases and waste management activities and to support informed decision making at all levels by industry, government, non-governmental organizations, and the public.
</blockquote>

<p>This data has been collected for 22 years after the <a href="http://www.epa.gov/superfund/contacts/infocenter/epcra.htm">Emergency Planning and Right to Know Act</a>. Young emphasized how important it is that this data be used in applications, and combined with other datasets. The data is available for download <a href="http://www.epa.gov/tri/tridata/preliminarydataset/index.html">directly</a> from the EPA, and is also <a href="http://www.data.gov/raw/1615">available</a> on data.gov. It would've been interesting to learn more about the mechanics of how the EPA gets data onto data.gov ; and how updates can flow.</p>

<p>But a really important question came from Young's colleague at the EPA (sorry I didn't note her name). She asked about how the data in their relational databases could be made available on the web. Should they simply dump the database? Or is there something else they could do? Young said that it's early days, but he hoped that Linked Data might have some answers. The issues came up later in the day at the Is the Semantic Web Ready Yet panel. There was a question about how to make Linked Data relevant to folks whose focus is Enterprise data. In my opinion Linked Data advocates <a href="http://semanticommunity.wik.is/Federal_Semantic_Interoperability_Community_of_Practice/RDF_Access_to_Structured_Databases">over emphasize</a> the importance of using RDF and SPARQL (standards), and converting all the data over without completely understanding how invasive these solutions are. Not enough is done to show enterprise data folks, who typically think in terms of relational databases, what they can do to put their lovingly crafted and hugged data on the web. Consider a primary key in a database: what does it identify, what relations does that thing have with other things? Why not use that key in constructing a URL for that thing, and link things together using the URLs? Then other people could use your URLs as well in their own data. I think the drumbeat to use SPARQL and triple stores often misses explaining this fundamental baby step that data owners could take. As Derek Willis said (on the 2nd day, when I'm writing this), people want to use your data, but not your database...people want to browse your data using their web browser. Assigning URLs to the important stuff in your databases is the first important step to make with Linked Data.</p>

<h3>Community</h3>

<p>Robert Schaefer of the Applied Physics Lab at Johns Hopkins University pointed out that enabling virtual communities around our data is an essential part of making data available and usable. In my opinion this is the true potential of platform, data aggregator sites like data.gov...they can allow users of government datasets to share what they have done, and learn from each other. Efforts like <a href="http://civiccommons.com/">Civic Commons</a> also promise to be places where this collaboration can take place. The communities may be born inside or outside of government, but they inevitably must include both. The W3C Egov effort might also be a good place to collaborate on standards possibly.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2705</wp:post_id>
		<wp:post_date>2010-11-16 08:19:16</wp:post_date>
		<wp:post_date_gmt>2010-11-16 15:19:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>iogdc-ramblings</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="conferences"><![CDATA[conferences]]></category>
		<category domain="post_tag" nicename="egov"><![CDATA[egov]]></category>
		<category domain="category" nicename="government"><![CDATA[government]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>dcat:distribution considered helpful</title>
		<link>http://inkdroid.org/2010/12/02/dcat-distribution-considered-helpful/</link>
		<pubDate>Thu, 02 Dec 2010 15:30:01 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2720</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The other day I happened to notice that the folks at <a href="http://data.gov.uk">data.gov.uk</a> have started using the <a href="http://vocab.deri.ie/dcat">Data Catalog Vocabulary</a> in the <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a> they have embedded in their dataset webpages. As an example <a href="https://gist.github.com/716228">here</a> is the RDF you can pull out of the HTML for the <a href="http://data.gov.uk/dataset/anonymised_mot_test">Anonymised MOT tests and results</a> dataset. Of particular <a href="http://inkdroid.org/2010/01/26/data-gov-uk-and-rdfa/">interest</a> to me is that the dataset description now includes an explicit link to the actual data being described using the <code><a href="http://vocab.deri.ie/dcat#distribution">dcat:distribution</a></code> property.</p>

<pre>
     &lt;http://data.gov.uk/id/dataset/anonymised_mot_test&gt; dcat:distribution
         &lt;http://www.dft.gov.uk/data/download/10007/DOC&gt;,
         &lt;http://www.dft.gov.uk/data/download/10008/ZIP&gt;,
         &lt;http://www.dft.gov.uk/data/download/10009/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10010/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10011/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10012/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10013/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10014/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10015/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10016/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10017/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10018/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10019/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10020/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10021/GZ&gt;,
         &lt;http://www.dft.gov.uk/data/download/10022/GZ&gt; .
</pre>

<p><a href="http://www.ecs.soton.ac.uk/people/cjg">Chris Gutteridge</a> happened to see a Twitter <a href="http://twitter.com/edsu/status/7992720853307392">message</a> of mine about this, and <a href="http://twitter.com/cgutteridge/status/8159811686367233">asked</a> what consumes this data, and why I thought it was important. So here's a brief illustration. I reran a little <a href="https://github.com/edsu/data-gov-uk-harvester/blob/master/crawl.py">python program</a> I have that crawls all of the data.gov.uk datasets, extracting the RDF using <a href="http://rdflib.net">rdflib's</a> RDFa support (thanks <a href="http://www.ivan-herman.net/">Ivan</a>). Now there are <a href="https://github.com/edsu/data-gov-uk-harvester/raw/master/data.ntriples">92,550 triples</a> (up from 35,478 triples <a href="http://inkdroid.org/2010/01/26/data-gov-uThanks!k-and-rdfa/">almost a year ago</a>).</p>

<p>So what can you do with the this metadata about datasets? I am a software developer working in the area where digital preservation meets the web. So I'm interested in not only getting the metadata for these datasets, but also the datasets themselves. It's important to enable 3rd party, automated access to datasets for a variety of reasons; but the biggest one for me can be summarized with the common-sensical: <a href="http://en.wikipedia.org/wiki/LOCKSS">Lots of Copies Keep Stuff Safe</a>.</p>

<p>It's kind of a no-brainer, but copies are important for digital preservation, when the <a href="http://www.dlib.org/dlib/july07/littman/07littman.html">unfortunate</a> happens. The subtlety is being able to know where the copies of a particular dataset are in the enterprise, in a distributed system like the Web, and the mechanics for relating them together. It's also important for <a href="http://www.dlib.org/dlib/september04/vandesompel/09vandesompel.html">scholarly communication</a>, so that researchers can cite datasets and follow citations of other research to the actual dataset it is based upon. And lastly aggregation services that collect datasets for dissemination on a particular platform, like data.gov.uk, need ways to predictably sweep domains for datasets that needs to be collected.</p>

<p>Consider this practical example: as someone interested in digital preservation I'd like to be able to know what format types are used within the data.gov.uk collection. Since they have used the <code>dcat:distribution</code> property to point at the referenced dataset, I was able to write a small <a href="https://github.com/edsu/data-gov-uk-harvester/blob/master/distributions.py">Python program</a> to crawl the datasets and log the media type and HTTP status code along the way, to generate some results like:</p>

<table>
<tr><th>media type</th><th>datasets</th></tr>
<tr style="background-color: #ffffff"><td>text/html</td><td style="text-align: right;">5898</td></tr>
<tr style="background-color: #eeeeee"><td>application/octet-stream</td><td style="text-align: right;">1266</td></tr>
<tr style="background-color: #ffffff"><td>application/vnd.ms-excel</td><td style="text-align: right;">874</td></tr>
<tr style="background-color: #eeeeee"><td>text/plain</td><td style="text-align: right;">234</td></tr>
<tr style="background-color: #ffffff"><td>text/csv</td><td style="text-align: right;">220</td></tr>
<tr style="background-color: #eeeeee"><td>application/pdf</td><td style="text-align: right;">167</td></tr>
<tr style="background-color: #ffffff"><td>text/xml</td><td style="text-align: right;">81</td></tr>
<tr style="background-color: #eeeeee"><td>text/comma-separated-values</td><td style="text-align: right;">51</td></tr>
<tr style="background-color: #ffffff"><td>application/x-zip-compressed</td><td style="text-align: right;">36</td></tr>
<tr style="background-color: #eeeeee"><td>application/vnd.ms-powerpoint</td><td style="text-align: right;">33</td></tr>
<tr style="background-color: #ffffff"><td>application/zip</td><td style="text-align: right;">31</td></tr>
<tr style="background-color: #eeeeee"><td>application/x-msexcel</td><td style="text-align: right;">28</td></tr>
<tr style="background-color: #ffffff"><td>application/excel</td><td style="text-align: right;">21</td></tr>
<tr style="background-color: #eeeeee"><td>application/xml</td><td style="text-align: right;">18</td></tr>
<tr style="background-color: #ffffff"><td>text/x-comma-separated-values</td><td style="text-align: right;">14</td></tr>
<tr style="background-color: #eeeeee"><td>application/x-gzip</td><td style="text-align: right;">13</td></tr>
<tr style="background-color: #ffffff"><td>application/x-bittorrent</td><td style="text-align: right;">12</td></tr>
<tr style="background-color: #eeeeee"><td>application/octet_stream</td><td style="text-align: right;">12</td></tr>
<tr style="background-color: #ffffff"><td>application/msword</td><td style="text-align: right;">10</td></tr>
<tr style="background-color: #eeeeee"><td>application/force-download</td><td style="text-align: right;">10</td></tr>
<tr style="background-color: #ffffff"><td>application/x-vnd.oasis.opendocument.presentation</td><td style="text-align: right;">9</td></tr>
<tr style="background-color: #eeeeee"><td>application/x-octet-stream</td><td style="text-align: right;">9</td></tr>
<tr style="background-color: #ffffff"><td>application/vnd.excel</td><td style="text-align: right;">9</td></tr>
<tr style="background-color: #eeeeee"><td>application/x-unknown-content-type</td><td style="text-align: right;">6</td></tr>
<tr style="background-color:Thanks! #ffffff"><td>application/xhtml+xml</td><td style="text-align: right;">6</td></tr>
<tr style="background-color: #eeeeee"><td>application/vnd.msexcel</td><td style="text-align: right;">5</td></tr>
<tr style="background-color: #ffffff"><td>application/vnd.google-earth.kml+xml kml</td><td style="text-align: right;">5</td></tr>
<tr style="background-color: #eeeeee"><td>application/octetstream</td><td style="text-align: right;">4</td></tr>
<tr style="background-color: #ffffff"><td>application/csv</td><td style="text-align: right;">3</td></tr>
<tr style="background-color: #eeeeee"><td>vnd.openxmlformats-officedocument.spreadsheetml.sheet</td><td style="text-align: right;">2</td></tr>
<tr style="background-color: #ffffff"><td>application/vnd.openxmlformats-officedocument.spreadsheetml.sheet</td><td style="text-align: right;">2</td></tr>
<tr style="background-color: #eeeeee"><td>application/octet-string</td><td style="text-align: right;">2</td></tr>
<tr style="background-color: #ffffff"><td>image/jpeg</td><td style="text-align: right;">1</td></tr>
<tr style="background-color: #eeeeee"><td>image/gif</td><td style="text-align: right;">1</td></tr>
<tr style="background-color: #ffffff"><td>application/x-mspowerpoint</td><td style="text-align: right;">1</td></tr>
<tr style="background-color: #eeeeee"><td>application/vnd.google-earth.kml+xml</td><td style="text-align: right;">1</td></tr>
<tr style="background-color: #ffffff"><td>application/powerpoint</td><td style="text-align: right;">1</td></tr>
<tr style="background-color: #eeeeee"><td>application/msexcel</td><td style="text-align: right;">1</td></tr>
</table>

<p>Granted some of these aren't too interesting. The predominance of <code>text/html</code> is largely an artifact of using <code>dcat:distribution</code> to link to the splash page for the dataset, not to the dataset itself. This is allowed by the dcat vocabulary ... but dcat's approach kind of assumes that the domain of the assertion is suitably typed as a <code>dcat:Download</code>, <code>dcat:Feed</code> or <code>dcat:WebService</code>. I personally think that dcat has some <a href="http://web.archive.org/web/20121107034506/http://www.w3.org/egov/IG/track/products/19">issues</a> that make it a bit more difficult to use than I'd like. But it's extremely useful that data.gov.uk are kicking the tires on the vocabulary, so that kinks like this can be worked out.</p>

<p>The <code>application/octet-stream</code> media-type (and its variants) are also kind of useless for these purposes, since it basically says the dataset is made of bits. It would be more helpful if the servers in these cases could send something more specific. But it ought to be possible to use something like <a href="https://confluence.ucop.edu/display/JHOVE2Info/Home">JHOVE</a> or <a href="http://sourceforge.net/apps/mediawiki/droid/index.php?title=Main_Page">DROID</a> to do some post-hoc analysis of the bitstream to figure out just what this data is, if it is valid etc.</p>

<p>The nice thing about using the Web to publish these datasets and their descriptions is that this sort of format analysis application could be decoupled from the data.gov.uk web publishing software itself. data.gov.uk becomes a clearinghouse for information and whereabouts of datasets, but a format verification service can be built as an orthogonal application. I think it basically fits the <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">RESTful</a> style of <a href="http://www.cdlib.org/services/uc3/curation/">Curation Microservices</a> being promoted by the California Digital Library:</p>

<blockquote>
Micro-services are an approach to digital curation based on devolving curation function into a set of independent, but interoperable, services that embody curation values and strategies. Since each of the services is small and self-contained, they are collectively easier to develop, deploy, maintain, and enhance. Equally as important, they are more easily replaced when they have outlived their usefulness. Although the individual services are narrowly scoped, the complex function needed for effective curation emerges from the strategic combination of individual services.
</blockquote>

<p>One last thing before you are returned to your regular scheduled programming. You may have noticed that the URI for the dataset being described in the RDF is different from the URL for the HTML view for the resource. For example:</p>

<blockquote>
<a href="http://data.gov.uk/id/dataset/anonymised_mot_test">http://data.gov.uk/id/dataset/anonymised_mot_test</a>
</blockquote>

<p>instead of:</p>

<blockquote>
<a href="http://data.gov.uk/dataset/anonymised_mot_test">http://data.gov.uk/dataset/anonymised_mot_test</a>
</blockquote>

<p>This is understandable given some of the dictums about Linked Data and trying to separate the Information Resource from the Non-Information Resource. But it would be nice if the URL resolved via a 303 redirect to the HTML as the <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a> document prescribes. If this is going to be the identifier for the dataset it's important that it resolves so that people and automated agents can follow their nose to the dataset. I think this highlights some of the difficulties that people typically face when deploying Linked Data.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2720</wp:post_id>
		<wp:post_date>2010-12-02 08:30:01</wp:post_date>
		<wp:post_date_gmt>2010-12-02 15:30:01</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>dcat-distribution-considered-helpful</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="data-gov-uk"><![CDATA[data.gov.uk]]></category>
		<category domain="post_tag" nicename="digital-preservation"><![CDATA[digital preservation]]></category>
		<category domain="post_tag" nicename="digital-curation"><![CDATA[digital-curation]]></category>
		<category domain="category" nicename="government"><![CDATA[government]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="post_tag" nicename="rest"><![CDATA[rest]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[datasets-on-the-web]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>on preserving bookmarks</title>
		<link>http://inkdroid.org/2010/12/22/on-preserving-bookmarks/</link>
		<pubDate>Wed, 22 Dec 2010 15:57:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2774</guid>
		<description></description>
		<content:encoded><![CDATA[<p>While it's not exactly clear what the future of <a href="http://delicious.com">Delicious</a> is, the <a href="http://www.readwriteweb.com/archives/deliciouss_data_policy_is_like_setting_a_museum_on.php">recent news</a> about <a href="http:/http://www.spinellis.gr/pubs/jrnl/2003-CACM-URLcite/html/urlcite.html">Yahoo</a> closing the doors or <a href="http://blog.delicious.com/">selling the building</a> prompted me to look around at other social bookmarking tools, and to revisit some old stomping grounds.</p>

<p><a href="http://onebiglibrary.net">Dan Chudnov</a> has been running <a href="http://unalog.com">Unalog</a> since 2003 (roughly when Delicious started). In fact I can remember Dan and <a href="http://en.wikipedia.org/wiki/Joshua_Schachter">Joshua Schacter</a> having some conversations about the idea of social bookmarking as both of the services co-evolved. So my first experience with social bookmarking was on Unalog, but a year or so later I ended up switching to Delicious in 2004 for reasons I can't quite remember. I think I liked some of the tools that had sprouted up around Delicious, and felt a bit guilty for abandoning Unalog.</p>

<p>Anyhow, I wanted to take the exported Delicious bookmarks and see if I could get them into Unalog. So I set up a dev Unalog environment, created a <a href="https://bitbucket.org/edsu/unalog2">friendly fork</a> of Dan's code, and added the ability to POST a chunk of JSON:</p>

<pre lang="text">curl --user user:pass \
         --header "Content-type: application/json" \
         --data '{"url": "http://example.com", "title": "Example"}' \
         http://unalog.com/entry/new
</pre>

<p>Here's a fuller example of the JSON that you can supply:</p>

<pre lang="json">{
      "url": "http://zombo.com",
      "title": "ZOMBO",
      "comment": "found this awesome website today",
      "tags": "website awesome example",
      "content": "You can do anything at Zombo.com. The only limit is yourself. Etc...",
      "is_private": true
    }
</pre>

<p>The nice thing about Unalog is that if you supply it (content), Unalog will index the text of the resource you've bookmarked. This allows you to do a fulltext search over your bookmarked materials.</p>

<p>So yeah, to make a long story a bit shorter, I created a <a href="https://bitbucket.org/edsu/unalog2/src/d1297c57471c/scripts/d2u.py">script</a> that reads in the bookmarks from a Delicious bookmark export (an HTML file) and pushes them up to a Unalog instance. Since the script GETs the bookmark URL to send Unalog the content to index you also get a log which contains the <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP Status Code</a> that provides the fodder for a linkrot report like:</p>

<table style="width: 90%; border: thin solid gray;">
  <tr style="background-color: #EEEEEE;">
    <td>
      200 OK
    </td>
    
    <td style="text-align: right;">
      4546
    </td>
  </tr>
  
  <tr>
    <td>
      404 Not Found
    </td>
    
    <td style="text-align: right;">
      367
    </td>
  </tr>
  
  <tr style="background-color: #EEEEEE;">
    <td>
      403 Forbidden
    </td>
    
    <td style="text-align: right;">
      141
    </td>
  </tr>
  
  <tr>
    <td>
      DNS failure
    </td>
    
    <td style="text-align: right;">
      81
    </td>
  </tr>
  
  <tr style="background-color: #EEEEEE;">
    <td>
      Connection refused
    </td>
    
    <td style="text-align: right;">
      28
    </td>
  </tr>
  
  <tr>
    <td>
      500 Internal Server Error
    </td>
    
    <td style="text-align: right;">
      19
    </td>
  </tr>
  
  <tr style="background-color: #EEEEEE;">
    <td>
      503 Service Unavailable
    </td>
    
    <td style="text-align: right;">
      10
    </td>
  </tr>
  
  <tr>
    <td>
      401 Unauthorized
    </td>
    
    <td style="text-align: right;">
      9
    </td>
  </tr>
  
  <tr style="background-color: #EEEEEE;">
    <td>
      410 Gone
    </td>
    
    <td style="text-align: right;">
      5
    </td>
  </tr>
  
  <tr>
    <td>
      302 Found
    </td>
    
    <td style="text-align: right;">
      5
    </td>
  </tr>
  
  <tr style="background-color: #EEEEEE;">
    <td>
      400 Bad Request
    </td>
    
    <td style="text-align: right;">
      4
    </td>
  </tr>
  
  <tr>
    <td>
      502 Bad Gateway
    </td>
    
    <td style="text-align: right;">
      2
    </td>
  </tr>
  
  <tr style="background-color: #EEEEEE;">
    <td>
      412 Precondition Failed
    </td>
    
    <td style="text-align: right;">
      1
    </td>
  </tr>
  
  <tr>
    <td>
      300 Multiple Choices
    </td>
    
    <td style="text-align: right;">
      1
    </td>
  </tr>
  
  <tr style="background-color: #EEEEEE;">
    <td>
      201 Created
    </td>
    
    <td style="text-align: right;">
      1
    </td>
  </tr>
</table>

<p>That was 5,220 bookmarks total collected over 5 years--which initially seemed low, until I did the math and figured I did 3 bookmarks a day on average. If we lump all the non-200 OK responses, that amounts to 13% linkrot. At first blush this seems significantly different compared to the <a href="http://www.spinellis.gr/pubs/jrnl/2003-CACM-URLcite/html/urlcite.html">research</a> done by Spinelli from 2003 (thanks <a href="http://lackoftalent.org/michael/blog/">Mike</a>) which reported 28% linkrot. I would've expected the Spinelli results to be better than my haphazard bookmark collection since he was sampling academic publications on the Web. But he was also sampling links from the 1995-1999 period, while I had links from 2005-2010. I know this is mere conjecture, but maybe we're learning to do things better on the web w/ <a href="http://www.w3.org/Provider/Style/URI">Cool URIs</a>. I'd like to think so at least. Maybe a comparison with <a href="http://www2003.org/cdrom/papers/refereed/p097/P97%20sources/p97-fetterly.html">some work</a> done by folks at HP and Microsoft would provide some insight.</p>

<p>At the very least this was a good reminder of how important this activity of pouring data from one system into another is to digital preservation. What <a href="http://www.alexandria.ucsb.edu/~gjanee/">Greg Janée</a> calls <a href="http://www.ijdc.net/index.php/ijdc/article/view/102">relay-supporting preservation</a>.</p>

<p>Most of all I want to echo the comments of former Yahoo employee <a href="http://uniquehazards.tumblr.com/">Stephen Hood</a> who <a href="http://uniquehazards.tumblr.com/post/2377362882/we-can-save-delicious-but-probably-not-in-the-way-you">wrote</a> recently about the value of this unique collection of bookmarks to the web community. If for some reason Yahoo were to close the doors on Delicious it would be great if they could donate the public bookmarks to the Web somehow, either via a public institution like the Smithsonian or the Library of Congress (full disclosure I work at the Library of Congress in a Digital Preservation group), or to an organization dedicated to the preservation of the Web like the <a href="http://netpreserve.org/">International Internet Preservation Consortium</a> of which LC, other National Libraries, and the <a href="http://archive.org">Internet Archive</a> are <a href="http://web.archive.org/web/20120722084143/http://www.netpreserve.org/about/memberList.php">members</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2774</wp:post_id>
		<wp:post_date>2010-12-22 08:57:04</wp:post_date>
		<wp:post_date_gmt>2010-12-22 15:57:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-preserving-bookmarks</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="delicious"><![CDATA[delicious]]></category>
		<category domain="post_tag" nicename="digital-preservation"><![CDATA[digital preservation]]></category>
		<category domain="post_tag" nicename="linkrot"><![CDATA[linkrot]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="unalog"><![CDATA[unalog]]></category>
		<category domain="post_tag" nicename="urls"><![CDATA[urls]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web-archiving"><![CDATA[web archiving]]></category>
		<category domain="post_tag" nicename="yahoo"><![CDATA[yahoo]]></category>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[on-archiving-bookmarks]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_d619281ef12786bf9cdbc9417fd5d509</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84006</wp:comment_id>
			<wp:comment_author><![CDATA[Delicious and the Preservation of &#8220;Platforms&#8221; &#8212; thesecretmirror.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://thesecretmirror.com/blog/2010/12/22/delicious-and-the-preservation-of-platforms/</wp:comment_author_url>
			<wp:comment_author_IP>67.23.6.175</wp:comment_author_IP>
			<wp:comment_date>2010-12-22 21:57:55</wp:comment_date>
			<wp:comment_date_gmt>2010-12-23 04:57:55</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] his post about preserving Delicious bookmarks through migration, Ed Summers advocates for releasing the Delicious data to the Web somehow. As he writes, this could [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Wikipedia 10</title>
		<link>http://inkdroid.org/2011/01/06/wikipedia-10/</link>
		<pubDate>Thu, 06 Jan 2011 14:28:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2827</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://ten.wikipedia.org/"><img src="http://inkdroid.org/images/10.png" style="float: left; margin-right: 10px;" /></a></p>

<p><br /></p>

<p>Wikipedia is turning ten years old on January 15th, and celebratory gatherings are going around the globe, including one in my area (<a href="http://ten.wikipedia.org/wiki/Washington,_D.C.">Washington DC</a>) on January 22 at the <a href="http://archives.gov">National Archives</a>.</p>

<p>Like you, I've been an accidental user of Wikipedia when searching for a topic to learn more about. Over time I have started actively searching on Wikipedia for things, linking to Wikipedia articles (in email and HTML), and even saying <a href="http://inkdroid.org/2009/12/15/thank-you-wikipedia/">thank you</a> with a small donation. I've only attended one of the local DC chapter events before, but am definitely planning on attending the <a href="http://ten.wikipedia.org/wiki/Washington,_D.C.">DC event</a> to meet other people who value Wikipedia as a resource.</p>

<p>Perhaps also like you (since you are reading this blog) I also work in a cultural heritage organization, well a <a href="http://loc.gov">library</a> to be precise. I wasn't able to attend the recent <a href="http://uk.wikimedia.org/wiki/GLAM-WIKI">Galleries, Libraries, Archives, Museums &amp; Wikimedia</a> conference at the <a href="http://www.britishmuseum.org/">British Museum</a> last November. But I have been listening to the audio that they <a href="http://uk.wikimedia.org/wiki/GLAM-WIKI#Schedule">kindly provided</a> recently with great interest. If you are interested in the role that cultural heritage organizations can play on Wikipedia, and the Web in general definitely check it out if you've got a long commute (or time to kill) and a audio device of some kind. There are lots of museums, galleries, archives and libraries in the DC area, so I'm hoping that this event on the 22nd will be an opportunity for folks to get together across institutional lines to talk about how they are engaging with the Wikipedia community. Who knows maybe it could be a precursor to a similar to <a href="http://uk.wikimedia.org/wiki/GLAM-WIKI">GLAM-WIKI</a> here in DC?</p>

<p>I'm planning on doing a lightning talk about this side/experimental project I've been <a href="http://inkdroid.org/2010/08/21/top-hosts-referenced-in-english-wikipedia/">working</a> <a href="http://inkdroid.org/journal/2010/08/30/edu-gov-and-tlds-in-en-wikipedia-external-links/">on</a> called <a href="http://github.com/edsu/linkypedia">linkypedia</a>. The basic idea is to give web publishers (and specifically cultural heritage organizations like libraries, museums, archives, etc) an idea of how their content is being used as primary resource material on Wikipedia. The goal is to validate the work that these institutions have done to make this content available, and for them to do more...and also to engage with the Wikipedia community. Version 1 of the (<a href="https://github.com/edsu/linkypedia">opensource</a>) software is running at <a href="http://linkypedia.inkdroid.org">here</a> on my minimal <a href="http://linode.com">Linode</a> <a href="http://en.wikipedia.org/wiki/Virtual_private_server">VPS</a>. But I'm also working on version 2, which will hopefully scale a bit better, and provide a more global (not just English Wikipedia) and real time picture of how your stuff is being used on Wikipedia. Part of the challenge is figuring out how to pay for it, given the volume of external links in the major language Wikipedias. I'm hoping a tip-jar and thoughtful use of Amazon EC2 will be enough.</p>

<p>If you are interested in learning more about the event on the 22nd check out <a href="http://twitter.com/filbertkm">Katie Filbert's</a> recent <a href="http://groups.google.com/group/sunlightlabs/browse_thread/thread/28eb06910639a7f3">post</a> to the Sunlight Labs Discussion List, and the (ahem) <a href="http://ten.wikipedia.org/wiki/Washington,_D.C.">wiki page</a> to sign up! Thanks <a href="http://matienzo.org/">Mark</a> for letting me know about the birthday celebration last week in <a href="irc://freenode.net/code4lib">IRC</a>. Sometimes with all the discussion lists, tweets, and blogs things like this slip by without me noticing them. So a direct prod in IRC helps :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2827</wp:post_id>
		<wp:post_date>2011-01-06 07:28:52</wp:post_date>
		<wp:post_date_gmt>2011-01-06 14:28:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wikipedia-10</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="dc"><![CDATA[dc]]></category>
		<category domain="post_tag" nicename="linkypedia"><![CDATA[linkypedia]]></category>
		<category domain="post_tag" nicename="meetings"><![CDATA[meetings]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<category domain="post_tag" nicename="wikixdc"><![CDATA[wikixdc]]></category>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>wikipedia external links: a redis database</title>
		<link>http://inkdroid.org/2011/01/19/wikipedia-external-links-a-redis-database/</link>
		<pubDate>Wed, 19 Jan 2011 15:47:15 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2855</guid>
		<description></description>
		<content:encoded><![CDATA[<p>As part of my continued meandering <a href="https://github.com/edsu/linkypedia/wiki/linkypedia-v2">linkypedia v2</a> experiments I created a Redis database of high level statistics about host names and top-level-domain names in external links from Wikipedia articles. <a href="http://web.archive.org/web/20110201040356/http://tom.opiumfield.com:80/">Tom Morris</a> happened to <a href="http://twitter.com/tommorris/status/27717927000088576">mention</a> he has been loading the external links as well (thanks <a href="http://twitter.com/invisiblecomma/status/27735169200824321">Alf</a>), so I thought I'd make the <a href="http://inkdroid.org/data/wikipedia-extlinks.rdb" rel="nofollow">redis database dump</a> available to anyone that is interested in looking at it. If you want to give it a whirl try this out:</p>

<pre>
% wget http://inkdroid.org/data/wikipedia-extlinks.rdb
% sudo aptitude install redis-server
% sudo mv wikipedia-extlinks.rdb /var/lib/redis/dump.rdb
% sudo chown redis:redis /var/lib/redis/dump.rdb
% sudo /etc/init.d/redis-server restart
% sudo pip install redis # or easy_install (version in apt is kinda old)
% python 
Python 2.6.5 (r265:79063, Apr 16 2010, 13:57:41) 
[GCC 4.4.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import redis
>>> r = redis.Redis()
>>> r.zrevrange("hosts", 0, 25, True)
[('toolserver.org', 2360809.0), ('www.ncbi.nlm.nih.gov', 508702.0), ('dx.doi.org', 410293.0), ('commons.wikimedia.org', 408986.0), ('www.imdb.com', 398877.0), ('www.nsesoftware.nl', 390636.0), ('maps.google.com', 346997.0), ('books.google.com', 323111.0), ('news.bbc.co.uk', 214738.0), ('tools.wikimedia.de', 181215.0), ('edwardbetts.com', 168102.0), ('dispatch.opac.d-nb.de', 166322.0), ('web.archive.org', 165665.0), ('www.insee.fr', 160797.0), ('www.iucnredlist.org', 155620.0), ('stable.toolserver.org', 155335.0), ('www.openstreetmap.org', 154127.0), ('d-nb.info', 141504.0), ('ssd.jpl.nasa.gov', 137200.0), ('www.youtube.com', 133827.0), ('www.google.com', 131011.0), ('www.census.gov', 124182.0), ('www.allmusic.com', 117602.0), ('maps.yandex.ru', 114978.0), ('news.google.com', 102111.0), ('amigo.geneontology.org', 95972.0)]
>>> r.zrevrange("hosts:edu", 0, 25, True)
[('nedwww.ipac.caltech.edu', 28642.0), ('adsabs.harvard.edu', 25699.0), ('animaldiversity.ummz.umich.edu', 21747.0), ('www.perseus.tufts.edu', 20438.0), ('genome.ucsc.edu', 20290.0), ('cfa-www.harvard.edu', 14234.0), ('penelope.uchicago.edu', 9806.0), ('www.bucknell.edu', 8627.0), ('www.law.cornell.edu', 7530.0), ('biopl-a-181.plantbio.cornell.edu', 5747.0), ('ucjeps.berkeley.edu', 5452.0), ('plato.stanford.edu', 5243.0), ('www.fiu.edu', 5004.0), ('www.volcano.si.edu', 4507.0), ('calphotos.berkeley.edu', 4446.0), ('www.usc.edu', 4345.0), ('ftp.met.fsu.edu', 3941.0), ('web.mit.edu', 3548.0), ('www.lpi.usra.edu', 3497.0), ('insects.tamu.edu', 3479.0), ('www.cfa.harvard.edu', 3447.0), ('www.columbia.edu', 3260.0), ('www.yale.edu', 3122.0), ('www.fordham.edu', 2963.0), ('www.people.fas.harvard.edu', 2908.0), ('genealogy.math.ndsu.nodak.edu', 2726.0)]
>>> r.zrevrange("tlds", 0, 25, True)
[('com', 11368104.0), ('org', 7785866.0), ('de', 1857158.0), ('gov', 1767137.0), ('uk', 1489505.0), ('fr', 1173624.0), ('ru', 897413.0), ('net', 868337.0), ('edu', 793838.0), ('jp', 733995.0), ('nl', 707177.0), ('pl', 590058.0), ('it', 486441.0), ('ca', 408163.0), ('au', 387764.0), ('info', 296508.0), ('br', 276599.0), ('es', 242767.0), ('ch', 224692.0), ('us', 179223.0), ('at', 163397.0), ('be', 132395.0), ('cz', 92683.0), ('eu', 91671.0), ('ar', 89856.0), ('mil', 87788.0)]
>>> r.zscore("hosts", "www.bbc.co.uk")
56245.0
</pre>

<p>Basically there are a few sorted sets in there:</p>

<ul>
    <li>"hosts": all the hosts sorted by the number of externallinks</li>
    <li>"hosts:%s": where %s is a top level domain ("com", "uk", etc)</li>
    <li>"tlds": all the tlds sorted by the number of externallinks</li>
    <li>"wikipedia": the wikipedia langauges sorted by total number of externallinks</li>
</ul>

<p>I'm not exactly sure how portable redis databases are but I was able to move it between a couple Ubuntu machines and <a href="http://ryaneby.com/">Ryan</a> successfully looked at it on a Gentoo box he had available. You'll need roughly 300MB of RAM available. I must say I was impressed with redis and in particular sorted sets for this stats collection task. Thanks to <a href="http://chris.improbable.org/">Chris Adams</a> for pointing me in the direction of redis in the first place.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2855</wp:post_id>
		<wp:post_date>2011-01-19 08:47:15</wp:post_date>
		<wp:post_date_gmt>2011-01-19 15:47:15</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wikipedia-external-links-a-redis-database</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="dns"><![CDATA[dns]]></category>
		<category domain="post_tag" nicename="redis"><![CDATA[redis]]></category>
		<category domain="post_tag" nicename="urls"><![CDATA[urls]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>wikixdc</title>
		<link>http://inkdroid.org/2011/01/23/wikixdc/</link>
		<pubDate>Mon, 24 Jan 2011 04:10:37 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2868</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.flickr.com/photos/inkdroid/5379054114/"><img src="http://inkdroid.org/images/wikipedia-cupcake.jpg" style="float: left; margin-right: 10px; margin-bottom: 10px;"/></a>

Wikipedia's <a href="http://ten.wikipedia.org/wiki/Washington,_D.C">10th Birthday Party</a> at the <a href="http://archives.gov">National Archives</a> in Washington DC on Saturday was a lot of fun. Far and away, the most astonishing moment for me came early in the opening remarks by <a href="http://www.archives.gov/about/archivist/archivist-biography-ferriero.html">David Ferriero</a>, the <a href="http://en.wikipedia.org/wiki/Archivist_of_the_United_States">Archivist of the United States</a>, when he stated (in no uncertain terms) that he was a big fan of Wikipedia, and that it was often his first go-to for information. Not only that, but when discussion about a bid for a DC <a href="http://wikimediafoundation.org/wiki/Wikimania">WikiMania</a> (the Wikipedia Annual Conference) came up later in the morning, Ferriero suggested that the <a href="http://archives.gov">National Archives</a> would be willing to host it if it came to pass. I'm not sure if anything actually came of this later in the day--a WikiMania in DC would be incredible. It was just amazing to hear the Archivist of the United States be supportive of Wikipedia as a reference source...especially as stories of schools, colleges and universities rejecting Wikipedia as a source are still common. Ferriero's point was even more poignant with several high schoolers in attendance. Now we all can say: 

<blockquote>
If Wikipedia is good enough for the Archivist of the United States, maybe it should be good enough for you.
</blockquote>

<a href="http://www.flickr.com/photos/inkdroid/5383171812/in/photostream/"><img src="http://farm6.static.flickr.com/5124/5383171812_e49c06526e.jpg" style="float: right; margin-left: 10px; margin-bottom: 10px;"/></a>

Another highlight for me was meeting <a href="http://phoebeayers.info/">Phoebe Ayers</a>, who is a <a href="http://people.lib.ucdavis.edu/~psayers/">reference librarian at UC Davis</a>, member of the <a href="http://wikimediafoundation.org/wiki/Board_of_Trustees">Wikimedia Foundation Board of Trustees</a>, and author of <a href="http://oreilly.com/catalog/9781593271763">How Wikipedia Works</a>. I strong armed Phoebe into signing my copy (I bought this copy on Amazon after it was de-accessioned from <a href="http://www.cuyahoga.lib.oh.us/">Cuyahoga County Public Library in Parma, Ohio </a>). Phoebe has some exciting ideas for creating collaborations between libraries and Wikipedia, which I think fit quite well into the <a href="http://en.m.wikipedia.org/wiki/Wikipedia:GLAM/">Galleries, Libraries, Archives and Museuems (GLAM)</a> effort within Wikipedia. I think she is still working on how to organize the effort.

Later in the day we heard how the National Archives is thinking of following <a href="http://www.nytimes.com/2010/06/05/arts/design/05wiki.html">the lead</a> of the <a href="http://www.britishmuseum.org/">British Museum</a> and establishing a <a href="http://twitter.com/#!/Sarah_Stierch/statuses/28846067348938754">Wikipedian in Residence</a>. <a href="http://www.wittylama.com/">Liam Wyatt</a>, the first Wikipedian in Residence, put a human face on Wikipedia for the British Museum, and familiarized museum staff with editing Wikipedia, through activities like the <a href="http://en.wikipedia.org/wiki/Wikipedia:GLAM/BM/Hoxne_challenge">Hoxne Challenge</a>. Having a Wikipedia in Residence at the National Archives (and who knows maybe the <a href="http://si.edu">Smithsonian</a> and the <a href="http://loc.gov">Library of Congress</a>) would be extremely useful I think.

In a similar vein, <a href="http://ragesoss.com/blog/">Sage Ross</a> spoke at length about the <a href="http://outreach.wikimedia.org/wiki/Wikipedia_Ambassador_Program">Wikipedia Ambassador Program</a>. The Ambassador Program is a formal way for folks to represent Wikipedia in academic settings (universities, high schools, etc). Ambassadors can get training in how to engage with Wikipedia (editing, etc) and can help professors and teachers who want to integrate Wikipedia into their curriculum, and scholarly activities.

I got to meet <a href="http://www.bls.gov/dpr/meyer.htm">Peter Benjamin Meyer</a> of the <a href="http://www.bls.gov">Bureau of Labor Statistics</a>, who has some interesting ideas for aggregating statistical information from federal statistical sources, and writing some bots that will update article info-boxes for places in the United States. The impending release of the 2010 US Census Data has the Wikipedia community <a href="http://en.wikipedia.org/wiki/Wikipedia_talk:2010_US_Census">discussing</a> the best way to update the information that was added by a bot for the 2000 census. It seemed like Peter might be able to piggy back some of his efforts on this work that is going on at Wikipedia for the 2010 Census.



<iframe src="https://docs.google.com/present/embed?id=dv89m3d_497c98tp7g5" frameborder="0" width="410" height="342" style="float: left; margin-right: 10px;"></iframe>

<a href="http://en.wikipedia.org/wiki/User:Jyothis">Jyothis Edthoot</a> an Oracle employee and <a href="http://meta.wikimedia.org/wiki/Stewards">Wikipedia Steward</a> gave me a behind the scenes look at the tools he and others in <a href="http://en.wikipedia.org/wiki/Wikipedia:Counter-Vandalism_Unit">Counter Vandalism Unit</a> use to keep Wikipedia open for edits from anyone in the world. I also got to meet <a href="http://en.scientificcommons.org/harihar_shankar">Harihar Shankar</a> from <a href="http://public.lanl.gov/herbertv/home/">Herbert van de Sompel's</a> team at <a href="http://lanl.gov">Los Alamos National Lab</a>, and to learn more about the latest developments with <a href="http://www.mementoweb.org/">Memento</a>, which he gave a lightning talk about. I also ran into <a href="http://www.spellboundblog.com/">Jeanne Kramer-Smyth</a> of the <a href="http://www.worldbank.org/">World Bank</a>, and got to hear about their efforts to provide meaningful access to their document collections to web crawlers using their metadata.


I did end up giving a lightning talk about <a href="http://github.com/edsu/linkypedia">Linkypedia</a> (slides on the left). I was kind of rushed, and I wasn't sure that this was exactly the right audience for the talk (being mainly Wikipedians instead of folks from the GLAM sector). But it helped me think through some of the challenges in expressing what Linkypedia is about, and who it is for. All in all it was a really fun day, with a lot of friendly folks interested in the Wikipedia community. There must've been at least 70 people there on a very cold Saturday--a promising sign of good things to come for collaborations between Wikipedia and the DC area. 

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2868</wp:post_id>
		<wp:post_date>2011-01-23 21:10:37</wp:post_date>
		<wp:post_date_gmt>2011-01-24 04:10:37</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wikixdc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="conferences"><![CDATA[conferences]]></category>
		<category domain="post_tag" nicename="dc"><![CDATA[dc]]></category>
		<category domain="post_tag" nicename="glam"><![CDATA[glam]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikimania"><![CDATA[wikimania]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>@andypowe11</title>
		<link>http://inkdroid.org/2011/01/27/andypowe11/</link>
		<pubDate>Thu, 27 Jan 2011 14:50:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2922</guid>
		<description></description>
		<content:encoded><![CDATA[<em><a href="http://twitter.com/#!/andypowe11">Andy Powell</a> has a <a href="http://efoundations.typepad.com/efoundations/2011/01/metadata-guidelines-for-the-uk-resource-discovery-taskforce.html">post</a> over on the eFoundations blog about some metadata guidelines he and <a href="http://twitter.com/#!/repetej">Pete Johnston</a> are working on for the UK Resource Discovery Taskforce. I got to rambling in a text area on his blog, but I guess I wrote too much, or included too many external URLs, so I couldn't post it in the end. So I thought I'd just post it here, and let trackback do the rest.</em>

<em>So uh, please s/you/Andy/g in your head as you are reading this ... </em>

A bit of healthy skepticism, from a 15-year vantage point is definitely warranted. Bearing in mind that often times its hard to move things forward without taking a few risks. I imagine constrained fiscal resources could also be a catalyst to improving access to data flows that cultural heritage institutions participate in, or want to participate in. I wonder if it would be useful to factor in the money that organizations can save by working together better?

As I've heard you argue persuasively in the past, the success of the WWW as a platform for delivery of information is hard to argue with. One of the things that the WWW did right (from the beginning) was focus the technology on people actually doing stuff...in their browsers. It seems really important to make sure whatever this metadata is, that users of the Web will see it (somehow) and will be able to use it. Ian Davis' points in <a href="http://blog.iandavis.com/2007/11/21/is-the-semantic-web-destined-to-be-a-shadow/">Is the Semantic Web Destined to be a Shadow</a> are still very relevant today I think. 

My friend <a href="http://eikeon.com">Dan Krech</a> calls this an "alignment problem". So I was really pleased to see this in the vision document:

<blockquote>
Agreed core standards for metadata for 
the physical objects and digital objects in 
aggregations ensuring the aggregations 
are open to all major search engines
</blockquote>

Aligning with the web is a good goal to have. Relatively recent service offerings from <a href="http://www.google.com/support/webmasters/bin/answer.py?hl=en&answer=99170">Google</a> and <a href="http://developers.facebook.com/docs/opengraph">Facebook</a> indicate their increased awareness of the utility of metadata to their users. And publishers are recognizing how important they are for getting their stuff before more eyes. It's a kind of virtuous cycle I hope. 

This must feel like it has been a long time in coming for you and Pete. Google's approach encourages a few different mechanisms: <a href="http://www.alistapart.com/articles/introduction-to-rdfa/">RDFa</a>, <a href="http://diveintohtml5.org/extensibility.html">Microdata</a> and <a href="http://microformats.org">Microformats</a>. Similarly, Google Scholar parses a handful of metadata vocabularies present in the HTML head element. The web is a big place to align with I guess.

I imagine there will be hurdles to get over, but I wonder if your task-force could tap into this virtuous cycle. For example it would be great if cultural heritage data could be aggregated using techniques that big Search companies also use: e.g. RDFa, microformats and microdata; and sitemaps and Atom for updates. This would assume a couple things: publishers could allow (and support) crawling, and that it would be possible to build aggregator services to do the crawling. An important step would be releasing the aggregated content in an open way too. This seems to be an approach that is very similar to what I've heard Europeana is doing...which may be something else to align with.

I like the idea of your recommendations providing a sliding scale, for people to get their feet wet in providing some basic information, and then work their way up to the harder stuff. Staying focused on what sorts of services moving up the scale provides seems to be key. Part of the vision document mentions that the services are intended for staff. There is definitely a need for administrators to manage these systems (I often wonder what sort of white-listing functionality Google employs with its Rich Snippets service to avoid spam). But keeping the ultimate users of this information in mind is really important.

Finally I'm a bit curious about the use of 'aggregations' in the <a href="http://ie-repository.jisc.ac.uk/475/1/JISC%26RLUK_VISION_FINAL.pdf">RLUK vision</a>. Is that some  OAI-ORE terminology percolating through? ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2922</wp:post_id>
		<wp:post_date>2011-01-27 07:50:07</wp:post_date>
		<wp:post_date_gmt>2011-01-27 14:50:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>andypowe11</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="facebook"><![CDATA[facebook]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="microdata"><![CDATA[microdata]]></category>
		<category domain="post_tag" nicename="microformats"><![CDATA[microformats]]></category>
		<category domain="post_tag" nicename="museums"><![CDATA[museums]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="post_tag" nicename="uk"><![CDATA[uk]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"84174";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84171</wp:comment_id>
			<wp:comment_author><![CDATA[Tweets that mention inkdroid › @andypowe11 -- Topsy.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://topsy.com/inkdroid.org/2011/01/27/andypowe11/?utm_source=pingback&amp;utm_campaign=L2</wp:comment_author_url>
			<wp:comment_author_IP>208.74.66.43</wp:comment_author_IP>
			<wp:comment_date>2011-01-27 08:29:29</wp:comment_date>
			<wp:comment_date_gmt>2011-01-27 15:29:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] This post was mentioned on Twitter by Ben O&#039;Steen, Ed Summers. Ed Summers said: New blog post: @andypowe11 http://inkdroid.org/2011/01/27/andypowe11/ [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84174</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>owen@ostephens.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawmejB2BjPAhKKrRn-vwum2QphgCE5r7zfU</wp:comment_author_url>
			<wp:comment_author_IP>137.108.145.10</wp:comment_author_IP>
			<wp:comment_date>2011-01-27 10:24:55</wp:comment_date>
			<wp:comment_date_gmt>2011-01-27 17:24:55</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>"I’m a bit curious about the use of ‘aggregations’ in the RLUK vision. Is that some OAI-ORE terminology percolating through?"</p>

<p>I was involved in the writing of the RDTF vision, and as far as I remember OAI-ORE wasn't mentioned. Think this blog post from Paul Walk might clarify the thinking behind the need for 'aggregations' http://web.archive.org/web/20110809022857/http://www.ukoln.ac.uk/jisc-ie/blog/2010/08/19/aggregation-and-the-resource-discovery-taskforce-vision/</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>357</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>OCLC&#039;s mapFAST and CORS</title>
		<link>http://inkdroid.org/2011/02/09/oclcs-mapfast-and-cors/</link>
		<pubDate>Wed, 09 Feb 2011 18:31:11 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2936</guid>
		<description></description>
		<content:encoded><![CDATA[Yesterday at <a href="http://code4lib.org/conference/2011">Code4lib 2011</a> <a href="http://www.librarywebchic.net/">Karen Coombs</a> gave a <a href="http://code4lib.org/conference/2011/coombs">talk</a> where (among other things) she demonstrated <a href="http://www.oclc.org/research/activities/mapfast/">mapFAST</a> that lets you find <a href="http://experimental.worldcat.org/mapfast/">relevant subject headings</a> for a given location, and then click on a subject heading and find relevant books on the topic. Go check out the <a href="http://www.indiana.edu/~video/stream/launchflash.html">archived video</a> of her talk (note you'll have to jump 39 minutes or so into the stream). Karen mentioned that the demo UI uses the <a href="http://www.oclc.org/developer/services/mapFAST">mapFAST REST/JSON API</a>. The service lets you construct a URL like this to get back subjects for any location you can identify with lat/lon coordinates:

<pre>
http://experimental.worldcat.org/mapfast/services?geo={lat},{lon}";crs=wgs84&radius={radius-in-meters}&mq=&sortby=distance&max-results={num-results}"
</pre>

For example: 

<pre>
ed@curry:~$ curl -i 'http://experimental.worldcat.org/mapfast/services?geo=39.01,-77.01;crs=wgs84&radius=100000&mq=&sortby=distance&max-results=1'
HTTP/1.1 200 OK
Date: Wed, 09 Feb 2011 14:07:39 GMT
Server: Apache/2.0.63 (Unix)
<span style="color: red;">Access-Control-Allow-Origin: *</span>
Transfer-Encoding: chunked
Content-Type: application/json

{
  "Status": {
    "code": 200, 
    "request": "geocode"
  }, 
  "Placemark": [
    {
      "point": {
        "coordinates": "39.0064,-77.0303"
      }, 
      "description": "", 
      "ExtendedData": [
        {
          "name": "NormalizedName", 
          "value": "maryland silver spring woodside park"
        }, 
        {
          "name": "Feature", 
          "value": "ppl"
        }, 
        {
          "name": "FCode", 
          "value": "P"
        }
      ], 
      "id": "fst01324433", 
      "name": "Maryland -- Silver Spring -- Woodside Park"
    }
  ], 
  "name": "FAST Authority Records"
}
</pre>

Recently I have been reading Mark Pilgirm's wonderful <a href="http://diveintohtml5.org/">Dive into HTML5</a> book, so I got it into my head that it would be fun to try out some of the <a href="http://diveintohtml5.org/geolocation.html">geo-location features</a> in modern browsers to display subject headings that are relevant for wherever you are. A short time later I now have a simple HTML/JavaScript HTML5 application (dubbed <a href="http://inkdroid.org/subjects-here/">subjects-here</a>) that does just that. The application itself is really just a toy. Part of Karen's talk was emphasizing the importance of using more than just text in Library Applications...and subjects-here kind of misses that key point.

What I wanted to highlight is the text in red in the HTTP response above:

<pre>
Access-Control-Allow-Origin: *
</pre>

The <code>Access-Control-Allow-Origin</code> HTTP header is a <a href="http://en.wikipedia.org/wiki/Cross-Origin_Resource_Sharing">Cross-Origin Resource Sharing</a> (CORS) header. If you've developed JavaScript applications before, you probably have run into situations where you wanted to load some JavaScript from a service elsewhere on the web. But you were prevented from doing this by <a href="http://en.wikipedia.org/wiki/Same_origin_policy">Same Origin Policy</a>, which prevents your JavaScript code from talking to a website that is different from the one it loaded from. So normally you hack around this by creating a proxy for that web service in your own application, which is a bit of work. Sometimes license agreements frown on you re-exposing their service, so you have to jump through a few more hoops to make sure it's not an open proxy for the web service. 

Enter CORS.

What the folks at OCLC did was add a <code>Access-Control-Origin</code> header to their JSON response. This basically means that my JavaScript served up at <code>inkdroid.org</code> is able to run in your browser and talk to the server at <code>experimental.worldcat.org</code>. OCLC has decided to allow this, to make their Web Service easier to use. So to create <a href="http://inkdroid.org/subjects-here/">subjects-here</a> I didn't have to write a single bit of server side code, it's just static HTML and JavaScript:

<pre lang="javascript">
function main() {
    if (Modernizr.geolocation) {
        navigator.geolocation.getCurrentPosition(lookup_subjects);
    }
    else {
        display_error();
    }
}

function lookup_subjects(position) {
    lat = parseFloat(position.coords.latitude);
    lon = parseFloat(position.coords.longitude);
    url = "http://experimental.worldcat.org/mapfast/services?geo=" + lat + "," + lon + ";crs=wgs84&radius=100000&mq=&sortby=distance&max-results=15";
    $.getJSON(url, display_subjects);
}

function display_subjects(data) {
    // putting results into the DOM left as exercise to the reader
}
</pre>

Nice and simple right? The full code is on <a href="http://github.com/edsu/subjects-here">GitHub</a>, which seemed a bit superfluous since there is no server-side piece (it's all in the browser). So the big wins are:

<ul>
<li>OCLC gets to see who is actually using their web service, not who is proxying it.</li>
<li>I don't have to write some proxy code.</li>
</ul>

The slight drawbacks are:

<ul>
<li>My application has a runtime dependency on <code>experimental.worldcat.org</code>, but it kinda did already when I was proxying it.</li>
<li>Most modern browsers support CORS headers, but <a href="http://en.wikipedia.org/wiki/Cross-Origin_Resource_Sharing#Browser_Support">not all of them</a>. So you would need to evaluate whether that matters to you.</li>
</ul>

I guess this is just a long way of saying <strong>USE CORS!!</strong> and help make the web a better place (pun intended).

<em>Update: and also, it is a good example where something like <a href="http://geojson.org/">GeoJSON</a> and <a href="http://www.opensearch.org/Specifications/OpenSearch/Extensions/Geo/1.0/Draft_1">OpenSearch Geo</a> could've been used to help spread common patterns for data on the Web. Thanks to <a href="http://twitter.com/sgillies/">Sean Gillies</a> for <a href="http://twitter.com/#!/sgillies/status/35411146542419968">pointing</a> that out.</em>

<em>Update: and <a href="http://chris.improbable.org/">Chris</a> is absolutely right, <a href="http://en.wikipedia.org/wiki/JSON#JSONP">JSONP</a> is another pattern in the Web Developer community that is a bit of a hack, but is an excellent fallback for older browsers.</em>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2936</wp:post_id>
		<wp:post_date>2011-02-09 11:31:11</wp:post_date>
		<wp:post_date_gmt>2011-02-09 18:31:11</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oclcs-mapfast-and-cors</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="cors"><![CDATA[cors]]></category>
		<category domain="post_tag" nicename="geo"><![CDATA[geo]]></category>
		<category domain="post_tag" nicename="html5"><![CDATA[html5]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web-services"><![CDATA[web services]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"84276";i:1;s:5:"84280";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84276</wp:comment_id>
			<wp:comment_author><![CDATA[sgillies.net/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://sgillies.net/</wp:comment_author_url>
			<wp:comment_author_IP>71.208.62.90</wp:comment_author_IP>
			<wp:comment_date>2011-02-09 11:51:13</wp:comment_date>
			<wp:comment_date_gmt>2011-02-09 18:51:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Looks like mapFAST's response is KML with curly braces. I'd recommend GeoJSON [1] for expressing locations (which are numerical, not strings( and OpenSearch-Geo [2] for the protocol/API (laziness and reuse being a virtue), but it's cool what you've done with it.

1. http://geojson.org/geojson-spec.html
2. http://www.opensearch.org/Specifications/OpenSearch/Extensions/Geo/1.0/Draft_1#Parameters]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>326</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84280</wp:comment_id>
			<wp:comment_author><![CDATA[Chris Adams]]></wp:comment_author>
			<wp:comment_author_email>chris@improbable.org</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawnjYt4eA4hzgDgYRfMpqMFitgKISVvzTc8</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2011-02-09 12:04:54</wp:comment_date>
			<wp:comment_date_gmt>2011-02-09 19:04:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[One alternative, which is definitely a hack, is JSONP, where you request the URL using a script tag with a callback parameter and the server emits code of the form "callback_name(data)".

The main reason for thinking about that is that it's widely supported because for a long time it was the only viable option - e.g. jQuery has built-in support as do many server-side API frameworks like django-tastypie. In particular, the example at http://experimental.worldcat.org/mapfast/api_docs/example.html is using jQuery.getJSON with a callback parameter to get a response like this:

jQuery1506183844415936619_1297277994400({    "name": "FAST Authority Records",    …

It's not necessary, of course, since they support CORS but it is a viable fallback for old browsers and services which haven't jumped on CORS yet because you should need to do is add "&amp;callback=?" to https://github.com/edsu/subjects-here/blob/master/js/subjects-here.js#L13]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>354</wp:comment_user_id>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84283</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2011-02-09 12:45:22</wp:comment_date>
			<wp:comment_date_gmt>2011-02-09 19:45:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the great comments Chris and Sean. JSONP and GeoJSON/OpenSearchGeo are definitely important things to mention in this context.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>release early, release often</title>
		<link>http://inkdroid.org/2011/02/15/release-early-release-often/</link>
		<pubDate>Tue, 15 Feb 2011 23:17:36 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2970</guid>
		<description></description>
		<content:encoded><![CDATA[<iframe src="http://player.vimeo.com/video/19990116" width="400" height="300" frameborder="0" style="float: left; margin-right: 10px;"></iframe>

The <a href="http://www.loc.gov/ndnp/">National Digital Newspaper Program</a> (NDNP) went live with a new JavaScript viewer today (as well as a lot of other stylistic improvements) in the <a href="http://chroniclingamerica.loc.gov/beta/">Beta</a> area portion of <a href="http://chroniclingamerica.loc.gov/">Chronicling America</a>. 

Being able to smoothly zoom in on images, and go into fullscreen mode is really reminiscent (for me) of the visceral experience of using a microfilm reader. We joked about adding whirring sound effects when moving between pages. But you'll be glad to know we showed restraint :-) It's all kind of deeply ironic given the Web's roots in the <a href="http://en.wikipedia.org/wiki/Memex">Memex</a>.

Hats off to <a href="http://twitter.com/eikeon">Dan Krech</a>. Risa Ohara and <a href="http://twitter.com/acdha">Chris Adams</a> for really digging into things like dynamically rendering tiles from our JPEG2000 access copies using Python (more maybe on that later).

I hacked together a brief video demonstration (above) of looking up the day the <a href="http://en.wikipedia.org/wiki/American_Civil_War">American Civil War</a> ended (April 9, 1865) in the <a href="http://chroniclingamerica.loc.gov/beta/lccn/sn83030213/">New York Daily Tribune</a>, to show off the viewer. One thing I forgot to do was go into headless mode (F11 w/ Firefox, Chrome, etc), which amplifies the effect somewhat.

Aside from the improvements on the site, this is a real milestone for the project and (I believe) the Library of Congress generally, since it is a 'beta' preview of what we would like to replace the existing site with. Given the nature of what they do, libraries are typically fairly conservative and slow moving organizations. So knowing how to use a beta/experimental area has proven to be a challenge. Hopefully a little space for experimentation will pay off. I don't think we could've gotten this far without the help of our fearless leader in all things tech, <a href="http://davidbrunton.com">David Brunton</a>.

If you have ideas, feel free to leave feedback using the little widget on the lower-right of pages at Chronicling America, or using our new <a href="http://sourceforge.net/mailarchive/forum.php?forum_name=loc-ndnp-mail">mailing list</a> that is devoted to the <a href="http://loc-ndnp.sf.net/">Open Source software project</a> that makes Chronicling America available. Open Source too, imagine that!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2970</wp:post_id>
		<wp:post_date>2011-02-15 16:17:36</wp:post_date>
		<wp:post_date_gmt>2011-02-15 23:17:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>release-early-release-often</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"84325";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84325</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>hellmatic@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawmmCwosruxR143Gl3p1axqjVKsKNIeWDu4</wp:comment_author_url>
			<wp:comment_author_IP>200.198.196.129</wp:comment_author_IP>
			<wp:comment_date>2011-02-18 09:04:41</wp:comment_date>
			<wp:comment_date_gmt>2011-02-18 16:04:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I browsed a few of the newspapers, and it feels fantastic! Congratulations on an excellent job!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>358</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1298045081.9443";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawmmCwosruxR143Gl3p1axq";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1298058059.4388";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>on &quot;good&quot; repositories</title>
		<link>http://inkdroid.org/2011/03/08/on-good-repositories/</link>
		<pubDate>Tue, 08 Mar 2011 18:44:42 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=2999</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://twitter.com/#!/cardcc">Chris Rusbridge</a> kicked off an interesting <a href="https://www.jiscmail.ac.uk/cgi-bin/webadmin?A2=ind1103&L=JISC-REPOSITORIES&F=&S=&P=9578">thread</a> on JISC-REPOSITORIES with the tag line <em>What makes a good repository?</em> Implicit in this question, and perhaps the discussion list, is that he is asking about "digital" repositories, and not the brick n' mortar libraries, archives, etc that are arguably also repositories. 

The question of what a repository is, is pretty much verboten in the group I work in. This is kind of amusing since I work in a group whose latest name (in a string of names, and no-names) is the Repository Development Center. Well, maybe saying it's "verboten" is putting it a bit too strongly. It's not as if the "repository" is equivalent to <a href="http://en.wikipedia.org/wiki/Lord_Voldemort">He Who Shall Not Be Named</a> or anything. It's just that the word means so many things, to so many different people, and encompasses so much of what we do, that it's hardly worth talking about. At our best (IMHO) we focus on what staff and researchers want to do with digital materials, and building out services that help them do that. Getting wrapped around the axle about what set of technologies we are using, and whether they model data in a particular way, is putting the cart before the horse.

As <a href="http://onebiglibrary.net">Dan</a> <a href="http://twitter.com/#!/dchud/status/11447075990">penned</a> one April 1st: "if you seek a pleasant repository, look about you". I guess this largely depends on where you are sitting. But seriously, if there's one thing that the <a href="http://www.crl.edu/sites/default/files/attachments/pages/trac_0.pdf"> Trustworthy Repositories Audit & Certification: Criteria and Checklist</a>, the <a href="http://www.crl.edu/archiving-preservation/digital-archives/metrics-assessing-and-certifying/core-re">Ten Principles for Digital Preservation Repositories</a>, and the <a href="http://brtf.sdsc.edu/">Blue Ribbon Task Force on Sustainable Digital Preservation and Access</a> make abundantly clear (after you've clawed out your eyes) it's that the fiscal and social dimension of repositories are a whole lot more important in the long run than the technical bits of how a repository is assembled in the now. I'm a software developer, and by nature I reach for technical solutions to problems, but in my heart of hearts I know it's true.

Back to Chris' question. Perhaps the "digital" is a red-herring. What if we consider his question in light of traditional libraries? This got me thinking: could <a href="http://en.wikipedia.org/wiki/S._R._Ranganathan">Ranganthan</a> and his <a href="http://en.wikipedia.org/wiki/Five_laws_of_library_science">Five Laws of Library Science</a> serve as a touchstone? Granted, bringing Ranganathan into library discussions is a bit of a cliché. But asking ethical questions like the "goodness" of something is a great excuse to dip into the canon. So put on your repository colored glasses, which magically substitute Repository Object for Book, and ...

<h3><a href="http://en.wikipedia.org/wiki/Five_laws_of_library_science#First_law:_Books_are_for_Use">Repository Objects Are For Use</a></h3>

We can build repositories that function as dark archives. But it kind of rots your soul to do it. It rots your soul because no matter what awesome technologies you are using to enable digital preservation in the repository, the repository needs to be used by <em>people</em>. If it isn't used, the stuff rots. And the digital stuff rots a whole lot faster than the physical materials. Your repository should have a raison d'être. It should be anchored in a community of people that want to use the materials that it houses. If it doesn't the repository is likely to <del datetime="2011-03-08T17:22:29+00:00">suck</del> not be good.

<h3><a href="http://en.wikipedia.org/wiki/Five_laws_of_library_science#Second_Law:_Every_reader_his_or_her_book">Every Reader His/Her Repository Object</a></h3>

Depending on their raison d'être (see above) repositories are used by a wide variety of people: researchers, administrators, systems developers, curators, etc. It does a disservice to these people if the repository doesn't support their use cases. A researcher probably doesn't care when fixity checks were last performed, and an administrator generating a report on fixity checks doesn't care about how an repository object was linked to and tagged in Twitter. Does your repository allow these different views, for different users to co-exist for the same object? Does it allow new classes of users to evolve?

<h3><a href="http://en.wikipedia.org/wiki/Five_laws_of_library_science#Third_Law:_Every_book_its_reader">Every Repository Object Its Reader</a></h3>

Are the objects in your repository discoverable? Are there multiple access pathways to them? For example, can someone do a search in Google and wind up looking at an item in your repository? Can someone link to it from a Wikipedia article? Can someone do a search within your repository to find an object of interest? Can they browse a controlled vocabulary or subject guide to find it? Are repository objects easily identified and found by automated agents like web crawlers and software components that need to audit them? Is it easy to extend, enhance and refine your description of what the repository object is as new users are discovered?

<h3><a href="http://en.wikipedia.org/wiki/Five_laws_of_library_science#Fourth_Law:_Save_the_time_of_the_reader">Save the Time of the Reader</a></h3>

Is your repository collection meaningfully on the Web? If it isn't, it should be, because that's where a lot of people are doing research today...in their web browser. If it can't be open access on the web, that's OK ... but the collection and its contents should be discoverable so that someone can arrange an onsite visit. For example, can a genealogist do a search for a person's name in a search engine and end up in your repository? Or do they have to know to come to your application to type in a search there? Once they are in your repository can they easily limit their search along familiar dimensions such as who, what, why, when, and where? Is it easy for someone to bookmark a search, or an item for later use. Do you allow your repository objects to be reused in other contexts like Facebook, Twitter, Flickr, etc which put the content where people are, instead of expecting them to come to you?

<h3><a href="http://en.wikipedia.org/wiki/Five_laws_of_library_science#Fifth_Law:_The_library_is_a_growing_organism">The Repository is a Growing Organism</a></h3>

This is my favorite. Can you keep adding numbers and types of objects, and scale your architecture linearly? Or are you constrained in how large the repository can grow? Is this constraint technical, social and/or financial? Can your repository change as new types or numbers of users (both human and machine) come into existence? When the limits of a particular software stack are reached, is it possible to throw it away and build another without losing the repository objects you have?  How well does your repository fit into the web ecosystem? As the web changes do you anticipate your repository will change along with it? How can you retire functionality and objects; to let them naturally die, with respect, and make way for the new?

<h3>So ... </h3>

I guess there are more questions here than answers. I hadn't thought of framing repository questions in terms of Ranganathan's laws before, but I imagine it has occurred to other people before. They still seem to be quite good principles to riff on, even in the digital repository realm--at least for a blog post. If you happen to run across similar treatment elsewhere I would appreciate hearing about them.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2999</wp:post_id>
		<wp:post_date>2011-03-08 11:44:42</wp:post_date>
		<wp:post_date_gmt>2011-03-08 18:44:42</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-good-repositories</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="digital-preservation"><![CDATA[digital preservation]]></category>
		<category domain="post_tag" nicename="digital-curation"><![CDATA[digital-curation]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="ranganathan"><![CDATA[ranganathan]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>xhtml, wayback</title>
		<link>http://inkdroid.org/2011/03/09/xhtml-wayback/</link>
		<pubDate>Wed, 09 Mar 2011 23:33:25 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3063</guid>
		<description></description>
		<content:encoded><![CDATA[The <a href="http://archive.org">Internet Archive</a> gave the <a href="">Wayback Machine</a> a <a href="http://iawebarchiving.wordpress.com/2011/01/24/updated-wayback-machine-in-beta-testing/">facelift</a> back in January. It actually looks really nice, but I noticed something kinda odd. I was looking for old archived versions of the lcsh.info site. Things <a href="http://replay.waybackmachine.org/20090308030621/http://lcsh.info//">work fine</a> for the latest archived copies:

<img src="http://inkdroid.org/images/wayback-lcsh-info.png"/>

But during part of lcsh.info's brief lifetime the site was serving up XHTML with the <em>application/xhtml+xml</em> media type. Now Wayback rightly (I think) remembers the media type, and serves it up that way:

<pre>
ed@curry:~$ curl -I http://replay.waybackmachine.org/20081216020433/http://lcsh.info/
HTTP/1.1 200 OK
Server: Apache-Coyote/1.1
X-Archive-Guessed-Charset: UTF-8
X-Archive-Orig-Connection: close
X-Archive-Orig-Content-Length: 6497
X-Archive-Orig-Content-Type: application/xhtml+xml; charset=UTF-8
X-Archive-Orig-Server: Apache/2.2.8 (Ubuntu) DAV/2 SVN/1.4.6 PHP/5.2.4-2ubuntu5.4 with Suhosin-Patch mod_wsgi/1.3 Python/2.5.2
X-Archive-Orig-Date: Tue, 16 Dec 2008 02:04:31 GMT
<span style="color: red;">Content-Type: application/xhtml+xml;charset=utf-8</span>
X-Varnish: 1458812435 1458503935
Via: 1.1 varnish
Date: Wed, 09 Mar 2011 23:09:47 GMT
X-Varnish: 903390921
Age: 0
Via: 1.1 varnish
Connection: keep-alive
X-Cache: MISS
</pre>

But to add navigation controls and branding, Wayback also splices in its own HTML into the display, which unfortunately is not valid XML. And since the media type and doctype trigger standards mode in browsers, the pages render in Firefox like this:

<img src="http://inkdroid.org/images/wayback-lcsh-info-ffx.png"/>

And in Chrome like this:

<img src="http://inkdroid.org/images/wayback-lcsh-info-chrome.png"/>

Now I don't quite know what the solution should be here. Perhaps the HTML that is spliced in should be valid XML. Or maybe Wayback should just serve up the HTML as <em>text/html</em>. Or maybe this is a good use case for frames (gasp). But I imagine it will similarly afflict any other XHTML that was served up as <em>application/xhtml+xml</em> when Heretrix crawled it. 

Sigh. I sure am glad that HTML5 is arriving on the scene and XHTML is riding off into the sunset. Although it's kind of the Long Goodbye given Internet Archive has archived it.

<em>Update: just a couple hours later I got an <a href="http://sourceforge.net/mailarchive/message.php?msg_id=27179259">email</a> that a fix for this was deployed. And sure enough now it works. I quickly eyeballed the response and didn't see what the change was. Thanks very much Internet Archive!</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3063</wp:post_id>
		<wp:post_date>2011-03-09 16:33:25</wp:post_date>
		<wp:post_date_gmt>2011-03-09 23:33:25</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>xhtml-wayback</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="browsers"><![CDATA[browsers]]></category>
		<category domain="post_tag" nicename="heretrix"><![CDATA[heretrix]]></category>
		<category domain="category" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="internet-archive"><![CDATA[internet archive]]></category>
		<category domain="post_tag" nicename="wayback"><![CDATA[wayback]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="xhtml"><![CDATA[xhtml]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>geeks bearing gifts</title>
		<link>http://inkdroid.org/2011/03/22/geeks-bearing-gifts/</link>
		<pubDate>Tue, 22 Mar 2011 13:51:09 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3084</guid>
		<description></description>
		<content:encoded><![CDATA[<div style="float: right; margin-left: 15px; text-align: center;"><a href="http://commons.wikimedia.org/wiki/File:Trojan_Horse_in_Stuttgart_2001.jpg"><img src="http://inkdroid.org/images/trojan-horse-stuttgart.jpg" width="250" /></a><br /><a href="http://commons.wikimedia.org/wiki/File:Trojan_Horse_in_Stuttgart_2001.jpg">Trojan Horse in Stuttgart</a> by <a href="http://www.webkuehn.de/">Stefan Kühn</a></div>

I recently received some correspondence about the <a href="http://n2t.net/ezid">EZID</a> identifier service from the <a href="http://www.cdlib.org/">California Digital Library</a>. EZID is a relatively new service that aims to help cultural heritage institutions manage their identifiers. Or as the EZID website says:

<blockquote>
EZID (easy-eye-dee) is a service that makes it simple for digital object producers (researchers and others) to obtain and manage long-term identifiers for their digital content. EZID has these core functions:

Create a persistent identifier: DOI or ARK
<ul>
<li>Add object location</li>
<li>Add metadata</li>
<li>Update object location</li>
<li>Update object metadata</li>
</ul>
</blockquote>

I have some serious concerns about a group of cultural institutions relying on a single service like EZID for managing their identifier namespaces. It seems too much like a <a href="http://en.wikipedia.org/wiki/Single_point_of_failure">single point of failure</a>. I wonder, are there any plans to make the software available, and to allow multiple EZID servers to operate as peers? 

URLs are a globally deployed identifier scheme that depend upon <a href="http://en.wikipedia.org/wiki/HTTP">HTTP</a> and <a href="http://en.wikipedia.org/wiki/Domain_Name_System">DNS</a>. These technologies have software implementations in many different computer languages, for diverse operating systems. Bugs and vulnerabilities associated with these software libraries are routinely discovered and fixed, often because the software itself is available as open source, and there are "many eyes" looking at the source code. Best of all, you can put a URL into your web browser (which are now ubiquitous), and view a document that is about the identified resource.

In my humble opinion, cultural heritage institutions should make every effort to work with the grain of the Web, and taking URLs seriously is a big part of that. I'd like to see more guidance for cultural heritage institutions on effective use of URLs, what Tim Berners-Lee has called <a href="http://www.w3.org/Provider/Style/URI">Cool URIs</a>, and what the Microformats and blogging community call <a href="http://en.wikipedia.org/wiki/Permalink">permalinks</a>. When systems are being designed or evaluated for purchase, we need to think about the URL namespaces that we are creating, and how we can migrate them forwards. Ironically, identifier schemes that don't fit into the DNS and HTTP landscape have their own set of risks; namely that organizations become dependent on niche software and services. Sometimes it's prudent (and cost effective) to seek safety in numbers. 

I did not put this discussion here to try to shame CDL in any way. I think the EZID service is well intentioned, clearly done in good spirit, and already quite useful. But in the long run I'm not sure it pays for institutions to go it alone like this. As another crank (I mean this with all due respect) Ted Nelson <a href="http://geeks-bearing-gifts.com/">put it</a>:

<blockquote>
Beware Geeks Bearing Gifts.
</blockquote>

On the surface the EZID service seems like a very useful gift. But it comes with a whole set of attendant assumptions. Instead of investing time & energy getting your institution to use a service like EZID, I think most cultural heritage institutions would be better off thinking about how they manage their URL namespaces, and making resource metadata available at those URLs.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3084</wp:post_id>
		<wp:post_date>2011-03-22 06:51:09</wp:post_date>
		<wp:post_date_gmt>2011-03-22 13:51:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>geeks-bearing-gifts</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ark"><![CDATA[ark]]></category>
		<category domain="category" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="doi"><![CDATA[doi]]></category>
		<category domain="post_tag" nicename="handle"><![CDATA[handle]]></category>
		<category domain="post_tag" nicename="identifiers"><![CDATA[identifiers]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="purl"><![CDATA[purl]]></category>
		<category domain="post_tag" nicename="url"><![CDATA[url]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>snac hacks</title>
		<link>http://inkdroid.org/2011/03/31/snac-hacks/</link>
		<pubDate>Thu, 31 Mar 2011 13:57:38 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3137</guid>
		<description></description>
		<content:encoded><![CDATA[A few months ago <a href="http://www.cdlib.org/contact/staff_directory/btingle.html">Brian Tingle</a> <a href="http://www.mail-archive.com/code4lib@listserv.nd.edu/msg09694.html">posted</a> some exciting news that the <a href="http://socialarchive.iath.virginia.edu/">Social Networks and Archival Context</a> (SNAC) project was releasing the data that sits behind their initial <a href="http://socialarchive.iath.virginia.edu/xtf/search">prototype</a>:

<blockquote>
As a part of our work on the Social Networks and Archival Context Project, the SNAC team is please to release more early results of our ongoing research.

A property graph of correspondedWith and associatedWith relationships between corporate, personal, and family identities is made available under the <a href="http://www.opendatacommons.org/licenses/by/">Open Data Commons Attribution License</a> in the form of a <a href="http://graphml.graphdrawing.org/">graphML</a> file. The graph expresses 245,367 relationships between 124,152 named entities.

The graphML file, as well as the scripts to create and load a graph database from EAC or graphML, are available on google code [5]

We are still researching how to map from the property graph model to RDF, but this graph processing stack will likely power the interactive visualization of the historical social networks we are developing.
</blockquote>

The SNAC project have aggregated <a href="http://en.wikipedia.org/wiki/Finding_aid">archival finding aid</a> data for manuscript collections at the <a href="http://findingaids.loc.gov/">Library of Congress</a>, <a href="http://nwda.wsulibs.wsu.edu//">Northwest Digital Archives</a>, <a href="http://www.oac.cdlib.org/">Online Archive of California</a> and <a href="http://ead.lib.virginia.edu/vivaead/">Virginia Heritage</a>. They then used <a href="http://en.wikipedia.org/wiki/Authority_control">authority control data</a> from <a href="http://authorities.loc.gov/">NACO/LCNAF</a>, <a href="http://www.getty.edu/research/conducting_research/vocabularies/ulan/">Getty Union List of Artist Names Online (ULAN)</a> and <a href="http://viaf.org/">VIAF</a> to knit these archival finding aids using the <a href="http://eac.staatsbibliothek-berlin.de/">Encoded Archival Context – Corporate bodies, Persons, and Families (EAC-CPF)</a>.

I <a href="http://inkdroid.org/2010/08/12/archival-context-on-the-web/">wrote</a> about SNAC here about 9 months ago, and how much potential there is in the idea of visualizing archival collections across institutions, along the axis of identity. I had also privately encouraged Brian to look into releasing some portion of the data that is driving their <a href="http://socialarchive.iath.virginia.edu/xtf/search">prototype</a>. So when Brian delivered I felt some obligation to look at the data and try to do something with it. Since Brian indicated that the project was interested in an RDF serialization, and <a href="http://matienzo.org/">Mark</a> had pointed me at <a href="http://lod-lam.net/summit/author/rubinsztajn/">Aaron Rubenstein's</a> <a href="http://purl.org/archival/vocab/arch">arch</a> vocabulary, I decided to take a stab at converting the GraphML data to some Arch flavored RDF.

So I <a href=https://bitbucket.org/edsu/eac-graph-load"">forked</a> Brian's <a href="http://code.google.com/p/eac-graph-load/source/checkout">mercurial repository</a>, and wrote a <a href="https://bitbucket.org/edsu/eac-graph-load/src/8e193cf83805/rdfizer.py">script</a> that parses the GraphML XML that Brian provided, and writes RDF (using arch:correspondedWith, arch:primaryProvenanceOf, arch:appearsWith) to a local triple store using <a href="http://rdflib.net">rdflib</a>. Since RDF has URLs cooked in pretty deep, part of this conversion involved reverse-engineering the SNAC URLs in the prototype, which wasn't terribly clean, but it seemed good enough for demonstration purposes.

Once I had those triples (877,595 of them) I learned from <a href="http://lists.w3.org/Archives/Public/public-lld/2011Mar/0178.html">Cory Harper</a> that the SNAC folks had matched up the archival identities with entries in the <a href="http://viaf.org">Virtual International Authority File</a>. The VIAF URLs aren't present in their GraphML data (GraphML is not as expressive as RDF), but they are available in the prototype HTML, which I had URLs for. So, again, in the name of demonstration and not purity, I wrote a little <a href="https://bitbucket.org/edsu/eac-graph-load/src/8e193cf83805/add_viaf.py">scraper</a> that would use the reverse-engineered SNAC URL to pull down the VIAF id. I tried to be respectful and not do this scraping in parallel, and to sleep a bit between requests. A few days of running and I had  40,237 owl:sameAs assertions that linked the SNAC URLs with the VIAF URLs.

With the VIAF URLs in hand I thought it would be useful to have a graph of only the VIAF related resources. It seemed like a VIAF centered graph of archival information could demonstrate something we've been talking about some in the <a href="http://www.w3.org/2005/Incubator/lld/charter">Library Linked Data W3C Incubator Group</a>: that Linked Data actually provides a technology that lets the archival and bibliographic description communities cross-pollinate and share. This is the real insight of the SNAC effort, that these communities have a lot in common, in that they both deal with people, places, organizations, etc. So I wrote another little <a href="https://bitbucket.org/edsu/eac-graph-load/src/8e193cf83805/infer.py">script</a> that created a named graph within the larger triple store, and used the owl:sameAs assertions to do some brute force inferencing, to generate triples relating VIAF resources with Arch.

I realize that Turtle isn't probably the most compelling example of the result, but in the absence of an app (maybe more on that forthcoming) that uses it, it'll have to do for now. So here are the assertions for Vannevar Bush, for the Linked Data <a href="http://inkdroid.org/2010/09/15/triadomany/">fetishists</a> out there:

<pre>
@prefix foaf &lt;http://xmlns.com/foaf/0.1/&gt; .
@prefix arch &lt;http://purl.org/archival/vocab/arch#&gt; .

&lt;http://viaf.org/viaf/15572358/#foaf:Person&gt;
    a foaf:Person ;
    foaf:name "Bush, Vannevar, 1890-1974." ;
    arch:appearsWith &lt;http://viaf.org/viaf/21341544/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/30867998/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/5076979/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/6653121/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/79397853/#foaf:Person&gt; ;
    arch:correspondedWith &lt;http://viaf.org/viaf/13632081/#foaf:Person&gt;,
        &lt;http://viaf.org/viaf/16555764/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/18495018/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/20482758/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/20994992/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/32065073/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/41170431/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/44376228/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/46092803/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/49966637/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/51816245/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/52483290/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/54269107/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/54947702/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/56705976/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/63110426/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/64014369/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/64087560/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/6724310/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/71767943/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/75645270/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/76361317/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/77126996/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/77903683/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/8664807/#foaf:Person&gt;, 
        &lt;http://viaf.org/viaf/92419478/#foaf:Person&gt; ;
    arch:primaryProvenanceOf &lt;http://hdl.loc.gov/loc.mss/eadmss.ms001043&gt;, 
        &lt;http://hdl.loc.gov/loc.mss/eadmss.ms007098&gt;, 
        &lt;http://hdl.loc.gov/loc.mss/eadmss.ms010024&gt;,
        &lt;http://hdl.loc.gov/loc.mss/eadmss.ms998004&gt;, 
        &lt;http://hdl.loc.gov/loc.mss/eadmss.ms998007&gt;, 
        &lt;http://hdl.loc.gov/loc.mss/eadmss.ms998009&gt;, 
        &lt;http://nwda-db.wsulibs.wsu.edu/findaid/ark:/80444/xv42415&gt;,
        &lt;http://www.oac.cdlib.org/findaid/ark:/13030/kt5b69p6zq&gt;, 
        &lt;http://www.oac.cdlib.org/findaid/ark:/13030/kt8w1014rz&gt; ;
    owl:sameAs &lt;http://socialarchive.iath.virginia.edu/xtf/view?docId=Bush+Vannevar+1890-1974-cr.xml&gt; .
</pre>

I've made a <a href="http://dl.dropbox.com/u/2797650/snac-rdf.tar.bz2">full dump of the data</a> I created available if you are interested in taking a look. The nice thing is that the URIs are already published on the web, so I didn't need to mint any identifiers myself to publish this Linked Data. Although I kind of played fast and loose with the SNAC URIs for people since they don't do the <a href="http://www.w3.org/TR/cooluris/">httpRange-14 dance</a>. It's interesting that it doesn't seem to have immediately broken anything. It would be nice if the SNAC Prototype URIs were a bit cleaner I guess. Perhaps they could use some kind of identifier instead of baking the heading into the URL?

So maybe I'll have some time to build a simple app on top of this data. But hopefully I've at least communicated how useful it could be for the cultural heritage community to share web identifiers for people, and use them in their data. RDF also proved to be a nice malleable data model for expressing the relationships, and serializing them so that others could download them. Here's to the emerging (hopefully) <a href="http://dig.csail.mit.edu/breadcrumbs/node/215">Giant Global GLAM Graph</a>!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3137</wp:post_id>
		<wp:post_date>2011-03-31 06:57:38</wp:post_date>
		<wp:post_date_gmt>2011-03-31 13:57:38</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>snac-hacks</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="eac"><![CDATA[eac]]></category>
		<category domain="post_tag" nicename="ead"><![CDATA[ead]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="viaf"><![CDATA[viaf]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"84402";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84402</wp:comment_id>
			<wp:comment_author><![CDATA[Jeff Young]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://realworldobject.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>99.186.54.21</wp:comment_author_IP>
			<wp:comment_date>2011-03-31 18:29:11</wp:comment_date>
			<wp:comment_date_gmt>2011-04-01 01:29:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed,

This would be a valid Linked Data assertion:

 foaf:page  .

I agree it would be better if they coined an http URI that conformed to Cool URI behavior so that owl:sameAs was justified. If this URI was opaque it would also insulate them occasional but inevitable changes in the preferred form of the name.

Jeff]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>153</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1301621351.3127";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:28:"realworldobject.myopenid.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1301652538.2357";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84405</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>140.147.236.194</wp:comment_author_IP>
			<wp:comment_date>2011-04-05 10:28:00</wp:comment_date>
			<wp:comment_date_gmt>2011-04-05 17:28:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Jeff, yes foaf:page is better. When building up this dataset I needed to hang my SNAC EAC triples off of a subject URI prior to inferring the VIAF triples, so it was easier to just pretend that the socialarchive.iath.virginia.edu were httpRange-14 compliant. I guess it would make sense to rewrite them as foaf:page in the final named graph to make things correct.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1302024480.4255";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>DOIs as Linked Data</title>
		<link>http://inkdroid.org/2011/04/25/dois-as-linked-data/</link>
		<pubDate>Mon, 25 Apr 2011 20:25:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3176</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Last week <a href="http://dilettantes.code4lib.org/blog/">Ross Singer</a> alerted me to some pretty<a href="http://www.crossref.org/CrossTech/2011/04/content_negotiation_for_crossr.html"> big news</a> for folks interested in Library Linked Data: <a href="http://crossref.org">CrossRef</a> has made the metadata for 46 million <a href="http://en.wikipedia.org/wiki/Digital_object_identifier">Digital Object Identifiers</a> (DOI) available as Linked Data. DOIs are heavily used in the publishing space to uniquely identify electronic documents (largely scholarly journal articles). CrossRef is a consortium of roughly 3,000 publishers, and is a big player in the academic publishing marketplace.</p>

<p>So practically what this means is that all the places in the scholarly publishing ecosystem where DOIs are present (caveat below), it's now possible to use the Web to retrieve metadata associated with that electronic document. Say you've got a DOI in the database backing your institutional repository:</p>

<blockquote>
doi:10.1038/171737a0
</blockquote>

<p>you can use the DOI to construct a URL:</p>

<blockquote>
http://dx.doi.org/<span style="color: red;">10.1038/171737a0</span>
</blockquote>

<p>and then do an HTTP GET (what your Web browser is doing all the time as you wander around the Web) to ask for metadata about that document:</p>

<blockquote>
curl --location --header "Accept: text/turtle" http://dx.doi.org/10.1038/171737a0
</blockquote>

<p>At which point you will get back some <a href="http://www.w3.org/TeamSubmission/turtle/">Turtle flavored RDF</a> that looks like:</p>

<pre>
&lt;http://dx.doi.org/10.1038/171737a0&gt;
    <span style="color: red;">a &lt;http://purl.org/ontology/bibo/Article&gt;</span> ;
    <span style="color: blue;">&lt;http://purl.org/dc/terms/title&gt; "Molecular Structure of Nucleic Acids: A Structure for Deoxyribose Nucleic Acid"</span> ;
    <span style="color: green;">&lt;http://purl.org/dc/terms/creator&gt; &lt;http://id.crossref.org/contributor/f-h-c-crick-367n8iqsynab1&gt;, &lt;http://id.crossref.org/contributor/j-d-watson-367n8iqsynab1&gt;</span> ;
    &lt;http://prismstandard.org/namespaces/basic/2.1/doi&gt; "10.1038/171737a0" ;  
    <span style="color: purple;">&lt;http://prismstandard.org/namespaces/basic/2.1/endingPage&gt; "738"</span> ;
    <span style="color: purple;">&lt;http://prismstandard.org/namespaces/basic/2.1/startingPage&gt; "737"</span> ;
    <span style="color: purple;">&lt;http://prismstandard.org/namespaces/basic/2.1/volume&gt; "171"</span> ;
    <span style="color: #ffcc99;">&lt;http://purl.org/dc/terms/date&gt; "1953-04-25Z"^^&lt;http://www.w3.org/2001/XMLSchema#date&gt;</span> ;
    &lt;http://purl.org/dc/terms/identifier&gt; "10.1038/171737a0" ;
    <span style="color: #00cccc;">&lt;http://purl.org/dc/terms/isPartOf&gt; &lt;http://id.crossref.org/issn/0028-0836&gt;</span> ;
    &lt;http://purl.org/dc/terms/publisher&gt; "Nature Publishing Group" ;
    &lt;http://purl.org/ontology/bibo/doi&gt; "10.1038/171737a0" ;
    &lt;http://purl.org/ontology/bibo/pageEnd&gt; "738" ;
    &lt;http://purl.org/ontology/bibo/pageStart&gt; "737" ;
    &lt;http://purl.org/ontology/bibo/volume&gt; "171" ;
    &lt;http://www.w3.org/2002/07/owl#sameAs&gt; &lt;doi:10.1038/171737a0&gt;, &lt;info:doi/10.1038/171737a0&gt; .

<span style="color: green;">
&lt;http://id.crossref.org/contributor/f-h-c-crick-367n8iqsynab1&gt;
    a &lt;http://xmlns.com/foaf/0.1/Person&gt; ;
    &lt;http://xmlns.com/foaf/0.1/familyName&gt; "CRICK" ;
    &lt;http://xmlns.com/foaf/0.1/givenName&gt; "F. H. C." ;
    &lt;http://xmlns.com/foaf/0.1/name&gt; "F. H. C. CRICK" .

&lt;http://id.crossref.org/contributor/j-d-watson-367n8iqsynab1&gt;
    a &lt;http://xmlns.com/foaf/0.1/Person&gt; ;
    &lt;http://xmlns.com/foaf/0.1/familyName&gt; "WATSON" ;
    &lt;http://xmlns.com/foaf/0.1/givenName&gt; "J. D." ;
    &lt;http://xmlns.com/foaf/0.1/name&gt; "J. D. WATSON" .
</span>
</pre>

<p>Well without all the funky colors...I put them there to help illustrate how the RDF includes some useful information, such as:</p>

<ul>
    <li style="color: red;">the document is an Article</li>
    <li style="color: blue;">it has the title "Molecular Structure of Nucleic Acids: A Structure for Deoxyribose Nucleic Acid"</li>
        <li style="color: #ffcc99;">the article was published on April 25th, 1953</li>
        <li style="color: #00cccc;">the article was published in the journal Nature</li>
    <li style="color: green;">the article was written by two people: J. D. Watson and F. H. C. Crick</li>
    <li style="color: purple;">it can be found in volume 171, on pages 737-738</li>
</ul>

<p>It's also interesting that both the <a href="http://bibliontology.com/">Bibliographic Ontology</a> and the <a href="http://www.idealliance.org/specifications/prism/">Publishing Requirements for Industry Standard Metadata (PRISM)</a> vocabularies being used. RDF lets you mix in different vocabularies like this. Some people might see this description as partly redundant, but it allows a data publisher to play the field a bit in its descriptions, while still committing to a particular URL for the resource.</p>

<p>Anyhow, the whole point of Linked Data is that you (or your software) can <a href="http://inkdroid.org/2008/01/04/following-your-nose-to-the-web-of-data/">follow your nose</a> by noticing links to related resources of interest in the data. If you are familiar with Turtle and RDF (a more visual diagram is below) you'll see that the article "Molecular Structure of Nucleic Acids" is "part of" another resource:</p>

<blockquote>
http://id.crossref.org/issn/0028-0836
</blockquote>

<p>If we follow our nose to this URL we get another bit of RDF:</p>

<pre>
&lt;http://id.crossref.org/issn/0028-0836&gt;
    &lt;http://purl.org/dc/terms/publisher&gt; &lt;http://periodicals.dataincubator.org/organization/nature-publishing-group&gt; ;
    <span style="color: red;">&lt;http://purl.org/dc/terms/sameAs&gt; &lt;http://periodicals.dataincubator.org/issn/0028-0836&gt;</span>, &lt;urn:issn:0028-0836&gt; ;
    &lt;http://purl.org/dc/terms/title&gt; "Nature" ;
    a "http://purl.org/ontology/bibo/Journal" .
</pre>

<p>Which tells us that the article is part of the journal Nature, which is the "same as" link to a resource in <a href="http://periodicals.dataincubator.org/.html">Linked Periodicals Data</a> at the <a href="http://dataincubator.org">Data Incubator</a>. When we resolve that URL we eventually get some more RDF:</p>

<pre>
&lt;http://periodicals.dataincubator.org/journal/nature&gt;
    dc:identifier &lt;info:pmid/0410462&gt;, &lt;info:pmid/0410463&gt; ;
    dc:subject "BIOLOGY", "Biologie", "CIENCIA", "NATURAL HISTORY", "Natuurwetenschappen", "Physique", "SCIENCE", "Science", "Sciences" ;
    dct:publisher &lt;http://periodicals.dataincubator.org/organization/nature-publishing-group&gt; ;
    <span style="color: red;">dct:subject &lt;http://id.loc.gov/authorities/sh85014203&gt;</span>, &lt;http://id.loc.gov/authorities/sh00007934&gt;, &lt;http://id.loc.gov/authorities/sh85015263&gt;, &lt;http://id.loc.gov/authorities/sh85090222&gt;, &lt;http://id.loc.gov/authorities/sh85118553&gt; ;
    dct:title "Nature" ;
    bibo:eissn "1476-4687" ;
    bibo:issn "0028-0836", "0090-0028" ;
    bibo:shortTitle "Nat New Biol", "Nature", "Nature New Biol." ;
    a bibo:Journal ;
    owl:sameAs &lt;http://periodicals.dataincubator.org/eissn/1476-4687&gt;, &lt;http://periodicals.dataincubator.org/issn/0028-0836&gt;, &lt;http://periodicals.dataincubator.org/issn/0090-0028&gt; ;
    foaf:isPrimaryTopicOf &lt;http://locatorplus.gov/cgi-bin/Pwebrecon.cgi?DB=local&v1=1&ti=1,1&Search_Arg=0410462&Search_Code=0359&CNT=20&SID=1&gt;, &lt;http://locatorplus.gov/cgi-bin/Pwebrecon.cgi?DB=local&v1=1&ti=1,1&Search_Arg=0410463&Search_Code=0359&CNT=20&SID=1&gt;, &lt;http://www.ncbi.nlm.nih.gov/sites/entrez?Db=nlmcatalog&doptcmdl=Expanded&cmd=search&Term=0410462%5BNlmId%5D&gt;, &lt;http://www.ncbi.nlm.nih.gov/sites/entrez?Db=nlmcatalog&doptcmdl=Expanded&cmd=search&Term=0410463%5BNlmId%5D&gt; .
</pre>

<p>Which (among other things) tells us that the journal Nature publishes content with the topic of "Biology" from the <a href="http://web.archive.org/web/20110720034058/http://id.loc.gov:80/authorities">Library of Congress Subject Headings</a>:</p>

<pre>
&lt;http://id.loc.gov/authorities/sh85014203#concept&gt;
    skos:prefLabel "Biology"@en ;
    dcterms:created "1986-02-11T00:00:00-04:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    dcterms:modified "1990-10-09T11:20:35-04:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
    a skos:Concept ;
    owl:sameAs &lt;info:lc/authorities/sh85014203&gt; ;
    skos:broader &lt;http://id.loc.gov/authorities/sh85076841#concept&gt; ;
    <span style="color: red;">skos:closeMatch &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119440835&gt;</span> ;
    skos:inScheme &lt;http://id.loc.gov/authorities#conceptScheme&gt;, &lt;http://id.loc.gov/authorities#topicalTerms&gt; ;
    skos:narrower &lt;http://id.loc.gov/authorities/sh00003440#concept&gt;, &lt;http://id.loc.gov/authorities/sh2001012327#concept&gt;, &lt;http://id.loc.gov/authorities/sh2003008355#concept&gt;, &lt;http://id.loc.gov/authorities/sh2005001919#concept&gt;, &lt;http://id.loc.gov/authorities/sh2006001276#concept&gt;, &lt;http://id.loc.gov/authorities/sh2006002547#concept&gt;, &lt;http://id.loc.gov/authorities/sh2006005143#concept&gt;, &lt;http://id.loc.gov/authorities/sh2007007463#concept&gt;, &lt;http://id.loc.gov/authorities/sh2009008123#concept&gt;;
    skos:related &lt;http://id.loc.gov/authorities/sh85076810#concept&gt;, &lt;http://id.loc.gov/authorities/sh85090222#concept&gt;, &lt;http://id.loc.gov/authorities/sh90000612#concept&gt; .
</pre>

<p>Here we can see the topic of <a href="http://id.loc.gov/authorities/sh85014203">Biology</a> as it relates to other concepts in the Library of Congress Subject Headings, as well as a similar concept in <a href="ttp://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119716335">Biologie générale</a> from <a href="http://rameau.bnf.fr/">RAMEAU</a>, which are subject headings from the <a href="http://www.bnf.fr/">Bibliothèque nationale de France</a>.</p>

<pre>
&lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119440835&gt;
    a skos:Concept ;
    skos:altLabel "Biologie générale"@fr ;
    skos:broader &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119716335&gt; ;
    skos:closeMatch &lt;http://d-nb.info/gnd/4006851-1&gt;, &lt;http://id.loc.gov/authorities/sh85014203#concept&gt; ;
    skos:inScheme &lt;http://stitch.cs.vu.nl/vocabularies/rameau/autorites_matieres&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/noms_communs&gt; ;
    skos:narrower &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11931061x&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11931064z&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119310659&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11933905d&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119348479&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11940847j&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119422283&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119440599&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11944082t&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11946836b&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11953082s&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11958000t&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119586710&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11962028z&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119658909&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11978175d&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11978521k&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11978651s&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11979066d&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11979284w&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11985187w&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119867466&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb11988172k&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb119886708&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb120174264&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb12100722v&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb121238944&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb121441898&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb121519084&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb123305379&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb13162665q&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb13319250k&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb13622707v&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb144016698&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb150318912&gt;, &lt;http://stitch.cs.vu.nl/vocabularies/rameau/ark:/12148/cb150557648&gt; ;
    skos:note "Domaine : 570"@fr ;
    skos:prefLabel "Biologie"@fr, "FRBNF119440833"@x-notation ;
</pre>

<p>So at this point maybe you'll admit that it's kind of cool to wander around in the data like this. But if you haven't drunk the <a href="http://linkeddata.org/">Kool-Aid</a> recently (unlikely if you've read this far) you might be wondering: what's the point? Who cares?</p>

<p>I think you should care about this example because it shows:</p>

<ol>
<li>how an existing organization can leverage its pre-existing identifiers on the Web to enable data publishing (Linked Data)</li>
<li>how important it is for publishers to consider who they link to in their data, and how they do it</li>
<li>how essential the RDF data model is for using the Web to join up these pools (or some may call them silos) of data</li>
</ol>

<p>The raw Turtle RDF above may have made your eyes glaze over, so its worth restating that this new DOI service allows those with DOIs in their databases to use the machinery of the Web to aggregate and join up data from 4 different organizations: CrossRef, Data Incubator, Library of Congress, and the Bibliothèque nationale de France:</p>

<p><img src="http://inkdroid.org/images/doi.jpg" /></p>

<p>And it's not just the traditional scholarly publishing community that will potentially benefit from this new Linked Data. As I discovered last August when <a href="http://inkdroid.org/2010/08/25/top-hosts-referenced-in-wikipedia-part-2/">routing around</a> in the external links dumps from <a href="http://en.wikipedia.org">English Wikipedia</a> there were 323,805 links from Wikipedia Articles to dx.doi.org--for example the article for <a href="http://en.wikipedia.org/wiki/Molecular_structure_of_Nucleic_Acids">Molecular Structure of Nucleic Acids</a> has a citation that includes an external link to the DOI URL included above.</p>

<p>CrossRef's new Linked Data service could allow someone to write a bot to crawl and verify the citations on Wikipedia. Or perhaps there could be a template on Wikipedia that would allow an editor to add a citation to an article by simply using the DOI, which would then fill in the other bits of article metadata needed for display. There are lots of possibilities.</p>

<p>As I commented over on the <a href="http://www.crossref.org/CrossTech/2011/04/content_negotiation_for_crossr.html">CrossTech blog</a> (not approved yet), it would be handy if the service was able to parse and act on non-simple Accept headers during content negotiation, since it's fairly common for RDF tools like jena, rdflib, arc, redland to send Accept headers with q-values in them. It might actually be nice to see support for some simple JSON views, that might be handy for people that get scared off RDF easily. But those are some minor quibbles in comparison to the outstanding work that CrossRef have done in getting this service going. Hopefully we'll see more publishing organizations like <a href="http://datacite.org/">DataCite</a> helping build this data publishing community more as well.</p>

<p><em>Update: if this topic interests you, and you want to read more about it, definitely check out <a href="http://twitter.com/#!/olyerickson">John Erickson</a>'s blog post <a href="http://bitwacker.wordpress.com/2010/02/04/dois-uris-and-cool-resolution/">DOIs, URIs and Cool Resolution</a></em>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3176</wp:post_id>
		<wp:post_date>2011-04-25 13:25:07</wp:post_date>
		<wp:post_date_gmt>2011-04-25 20:25:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>dois-as-linked-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="doi"><![CDATA[doi]]></category>
		<category domain="post_tag" nicename="identifiers"><![CDATA[identifiers]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="scholarship"><![CDATA[scholarship]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{i:0;s:5:"84429";i:1;s:5:"84450";i:2;s:5:"84461";i:3;s:5:"84499";i:4;s:5:"84518";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_33669c229470f2adc4b19b9a01871f3b</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_003319082dfe2e06386a82b07a160e65</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84429</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>68.50.223.110</wp:comment_author_IP>
			<wp:comment_date>2011-04-25 18:22:18</wp:comment_date>
			<wp:comment_date_gmt>2011-04-26 01:22:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[So what I'm thinking though, is that realistically I'm not going to be able to write software to actually USE this unless I know what vocabularies to expect. 

It's already odd to me that entirely different vocabularies are being used in the Atom version (PRISM), (mostly DC -- but DC used in particular ways; there's no reason for software to expect an ISSN in dcterms:isPartOf unless it knows to, is there? And why are they using their own local ISSN URIs?), and xml-rdf (I don't think this is even using the same vocab as the turtle, is it?)

Am I mis-reading, or is each serialization using entirely different set of mixed and matched vocabularies?

And it looks like while you can content-negotiate with dx.doi.org, what actual vocabularies you get back in a given content-negotiated format is completely up in the air, up to the registrar, could be different for every DOI or change at any time. 

It strikes me that knowing that something is "atom" or "turtle" or "rdf+xml" is NOT enough to write software that can consume it and do something with it.  Those really end up being more 'serialization' formats than actual useful metadata formats -- whether software is going to be able to do something with it is all about the vocabularies (expressed as namespaces in atom). 

So, while a human can look at these to 'follow their nose'... I'd really love my software (such as Umlaut) to be able to make use of it too, but unless DOI does a bit of standardization here and documents what vocabularies one can expect... it's not clear to me how to do that. A promissing first step, but it seems to me a second step is needed to make it more than a toy/proof of concept. No?

(And yeah, I posted a comment to this effect on their blog too. I'm thinking that our comments are lost to the aether never to be approved).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303780938.1945";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303816335.3734";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84450</wp:comment_id>
			<wp:comment_author><![CDATA[Andy Powell]]></wp:comment_author>
			<wp:comment_author_email>andy.powell@eduserv.org.uk</wp:comment_author_email>
			<wp:comment_author_url>http://andypowe11.net/</wp:comment_author_url>
			<wp:comment_author_IP>188.92.143.3</wp:comment_author_IP>
			<wp:comment_date>2011-04-26 01:56:31</wp:comment_date>
			<wp:comment_date_gmt>2011-04-26 08:56:31</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Minor(ish) comment.  The

 "Nature Publishing Group" ;

in the first Turtle example, should really be

  ;

I think. Dunno if you can pass this back to people at Crossref?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>74</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303808191.1955";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:10:"andypowe11";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84461</wp:comment_id>
			<wp:comment_author><![CDATA[Andy Powell]]></wp:comment_author>
			<wp:comment_author_email>andy.powell@eduserv.org.uk</wp:comment_author_email>
			<wp:comment_author_url>http://andypowe11.net/</wp:comment_author_url>
			<wp:comment_author_IP>188.92.143.3</wp:comment_author_IP>
			<wp:comment_date>2011-04-26 04:00:52</wp:comment_date>
			<wp:comment_date_gmt>2011-04-26 11:00:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Gah, sorry... garbled comment of mine above. Trying again...

The

&lt;http://purl.org/dc/terms/publisher&gt; "Nature Publishing Group" ;

in the first Turtle example, should really be

&lt;http://purl.org/dc/terms/publisher&gt; &lt;http://periodicals.dataincubator.org/organization/nature-publishing-group&gt; ;

I think. Dunno if you can pass this back to people at Crossref?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>74</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303816343.3875";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303815652.6874";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:10:"andypowe11";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84466</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2011-04-26 06:08:44</wp:comment_date>
			<wp:comment_date_gmt>2011-04-26 13:08:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@jonathan I agree that it would be nice to see a page that describes the new CrossRef Linked Data service, and which documents the RDF vocabularies that are used. I did double-check that the application/rdf+xml and text/turtle serializations are saying the same thing (a sorted ntriples view + diff is handy for this btw).

Personally, I wouldn't worry too much about the vocabulary changing significantly. At this point Dublin Core is kind of the lingua-franca of metadata on the Web--at least in the Linked Data space. It is a bit worrisome that the PRISM vocabulary doesn't seem to be defined at the URIs that they are using, so if you are looking to be cautious I'd probably shy away from burning any of those into you software.

Another thing to think about: is the potentially variable use of RDF vocabulary really anything new? For example an API provider could change the structure of the JSON being delivered at any time. It's arguable that JSON tends to be a lot simpler, so there's less brittleness. Also JSON driven APIs tend to be documented and versioned more explicitly.

Your comments got me thinking about how important the documentation around these data APIs is. Apart from providing much needed information about the reason why the API is there, and how to use it, they provide context which really forms a foundation for trust around the service, which in turn emboldens people to start to depend on the service in other services like the Umlaut.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303823324.5983";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84499</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>68.50.223.110</wp:comment_author_IP>
			<wp:comment_date>2011-04-26 17:31:16</wp:comment_date>
			<wp:comment_date_gmt>2011-04-27 00:31:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ah, Ed, but I've GOT to use the PRISM elements to do what I want to do, just DC is not nearly sufficient for my use cases. 

It's true that people providing APIs that change all the time without notice or documentation is nothing new -- but we know those as 'crappy APIs'.  

I think some people are misunderstanding the practical utility of RDF, thinking "Oh, see, it's in RDF, that automatically makes it machine useable." But without documentation and a commit to standardization of the vocabularies in certain ways, I can't write code to use it -- at least not with any way to predict what proportion of identifiers (DOIs in this case) it will actually work with (do they all have PRISM or just some? Who knows), or any way to predict if it will keep working in the future. 

As near as I can tell from reading their announcement, exactly what metadata to provide is _completely_ up to the "registration agency", such as CrossRef or DataCite.  An individual registration agency don't have to provide the same vocabularies (or even the same top-level serialization formats) for every identifier; they don't need to keep it the same over time; they don't need to be consistent amongst themselves (CrossRef vs DataCite), and they don't need to TELL us what they're planning on providing or how often it will change. 

From my perspective, that's completely unworkable for me to invest software development resources against it. It needs the next step, which is some documentation/standardization of some kind. And I think DOI needs to take that next step and require that of the registration agencies. And I think they need to hear that from people like us, or rather people like you who understand this stuff way better than me, everyone just tells me I'm missing the true power (of having no idea what data to expect, heh).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303864276.1519";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84502</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2011-04-26 19:06:14</wp:comment_date>
			<wp:comment_date_gmt>2011-04-27 02:06:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@jonathan I think you are reading between the lines with the CrossRef linked data offerings quite a bit...which is understandable given the amount of writing that has been done about it. I would see what you can use in the data, and try to trust that it won't change much before assuming that it will. If I'm going to read a bit between your lines I would hazard a guess that this might be the first time you've thought about trying to consume some RDF in an application...and it's normal (and wise) to be a bit nervous about that...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303869974.8724";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84514</wp:comment_id>
			<wp:comment_author><![CDATA[New Linked Data for Libraries - semanticweb.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://semanticweb.com/new-linked-data-for-libraries_b19505</wp:comment_author_url>
			<wp:comment_author_IP>38.126.102.100</wp:comment_author_IP>
			<wp:comment_date>2011-04-27 08:01:32</wp:comment_date>
			<wp:comment_date_gmt>2011-04-27 15:01:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] new blog post discusses some “big news for folks interested in Library Linked Data: CrossRef has made the [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303916493.3796";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303953691.4484";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84518</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>68.50.223.110</wp:comment_author_IP>
			<wp:comment_date>2011-04-27 15:58:01</wp:comment_date>
			<wp:comment_date_gmt>2011-04-27 22:58:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed, where do you think I'm reading between the lines? I'm confused. 

They seem to be saying that the registration agencies themselves will be responsible for those alternate representaitons, not DOI central. No?"It also means that, as registration agency members (CrossRef publishers, for instance) start providing more complete and richer representations of their content, we can simply redirect content-negotiated requests directly to them."

And they make no mention of any standardization of vocabularies or serializations.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1303945081.586";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84519</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2011-04-27 18:26:51</wp:comment_date>
			<wp:comment_date_gmt>2011-04-28 01:26:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@jonathan yeah I see your point ... they could change the service to redirect to representations hosted by publishers in the future. For some reason I don't see that happening particularly soon. It would definitely make the situation more complicated if that happened without some vocabulary consolidation. But I don't think it's necessarily worthwhile to worry too much about a hypothetical situation right now.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1303954011.4059";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84526</wp:comment_id>
			<wp:comment_author><![CDATA[The DOI, DataCite and Linked Data: Made for each other! &laquo; Bitwacker Associates]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bitwacker.wordpress.com/2010/01/19/the-doi-datacite-and-linked-data-made-for-each-other/</wp:comment_author_url>
			<wp:comment_author_IP>66.135.48.233</wp:comment_author_IP>
			<wp:comment_date>2011-04-28 08:03:05</wp:comment_date>
			<wp:comment_date_gmt>2011-04-28 15:03:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Ed Summers provides a great explanation of the significance of this in his recent INKDROID post, DOIs as Linked Date. Possibly related posts: (automatically generated)Long Tails and &ldquo;Scaling Down&rdquo; Linked [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304002985.8526";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304385618.2206";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84527</wp:comment_id>
			<wp:comment_author><![CDATA[DOIs, URIs and Cool Resolution &laquo; Bitwacker Associates]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bitwacker.wordpress.com/2010/02/04/dois-uris-and-cool-resolution/</wp:comment_author_url>
			<wp:comment_author_IP>72.233.69.25</wp:comment_author_IP>
			<wp:comment_date>2011-04-28 08:08:23</wp:comment_date>
			<wp:comment_date_gmt>2011-04-28 15:08:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Update 3 (28 Apr 2011): Great news! This week CrossRef, the IDF and CNRI announced the completion of their implementation of Content Negotiation for CrossRef DOIs. As that post describes, it is an implementaton of &#8220;Option D&#8221; in last year&#8217;s CrossTech post, DOIs and Linked Data: Some Concrete Proposals. Ed Summers provides a great explanation of the significance of this in his recent INKDROID post, DOIs as Linked Date. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304003303.0503";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304385618.1778";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84535</wp:comment_id>
			<wp:comment_author><![CDATA[Chris Mavergames: Random thoughts on Drupal, RDF, Open Calais and linked data &#8211; Exquisite Web Creations]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.bestcybersolutions.com/exquisite/blog/2011/04/30/chris-mavergames-random-thoughts-on-drupal-rdf-open-calais-and-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>174.120.188.130</wp:comment_author_IP>
			<wp:comment_date>2011-04-30 01:00:32</wp:comment_date>
			<wp:comment_date_gmt>2011-04-30 08:00:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Okay, I&#8217;ll stop quoting from his post. You really need to read the whole post. It&#8217;s great. Go here [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1304150432.122";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304385618.1361";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84542</wp:comment_id>
			<wp:comment_author><![CDATA[Infobib &raquo; DOIs as Linked Data]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://infobib.de/blog/2011/05/02/dois-as-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>78.46.44.44</wp:comment_author_IP>
			<wp:comment_date>2011-05-02 08:18:36</wp:comment_date>
			<wp:comment_date_gmt>2011-05-02 15:18:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Zusammenfassung des Themas DOIs as Linked Data von Ed Summers, als Reaktion auf die Ankündigung von Crossref, DOIs in dieser Form verfügbar zu [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304349516.6187";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304385618.0487";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84543</wp:comment_id>
			<wp:comment_author><![CDATA[inkdroid › DOIs as Linked Data | Virtually a Librarian]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://infofairy.wordpress.com/2011/05/02/inkdroid-%e2%80%ba-dois-as-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>76.74.248.186</wp:comment_author_IP>
			<wp:comment_date>2011-05-02 13:25:11</wp:comment_date>
			<wp:comment_date_gmt>2011-05-02 20:25:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid › DOIs as Linked Data. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304367911.1267";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304385617.9385";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84606</wp:comment_id>
			<wp:comment_author><![CDATA[Building a semantic notebook &#8211; Carl Boettiger]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.carlboettiger.info/archives/1626</wp:comment_author_url>
			<wp:comment_author_IP>173.236.144.178</wp:comment_author_IP>
			<wp:comment_date>2011-05-08 18:40:20</wp:comment_date>
			<wp:comment_date_gmt>2011-05-09 01:40:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] In my notebook, the citations are included by DOI whenever possible, using the KCite plugin.  DOI&#8217;s are linked data.  Further information can be added to any link using the [2] [3] plugin, which in addition to [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304989710.1598";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304905220.4979";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85422</wp:comment_id>
			<wp:comment_author><![CDATA[Altmetric for Scopus app launched! - Weekly Twitter Activity 2012-06-15 | Michael Habib | Nudging Serendipity]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://mchabib.com/2012/06/15/weekly-twitter-activity-2012-06-15/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-06 15:06:08</wp:comment_date>
			<wp:comment_date_gmt>2012-07-06 22:06:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Research objects get DOI:s @figshare http://t.co/n3u09GVd and DOI:s as #LinkedData @CrossRefNews http://t.co/HcGLSsdV 2012-06-14RT @altmetrics12: Check out the program of our workshop at @websci12: 2 keynotes, 5 [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1341612368.5301";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1341613234.2241";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>a bit about scruffiness</title>
		<link>http://inkdroid.org/2011/05/03/a-bit-about-scruffiness/</link>
		<pubDate>Tue, 03 May 2011 07:06:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3268</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="float: left; font-size: smaller; text-align: center; background-color: #eeeeee; margin-right: 10px;"><a href="http://en.wikipedia.org/wiki/Terry_Winograd"><img src="http://inkdroid.org/images/winograd.jpg" style="width: 300px;"/></a>
<a href="http://en.wikipedia.org/wiki/Terry_Winograd">Terry Winograd</a> at CHI 2006 (by <a href="http://en.wikipedia.org/wiki/File:WinogradSkeleton.jpg">boltron</a>)
</span>

I just finished reading <a href="http://books.google.com/books?id=2sRC8vcDYNEC">Understanding Computers and Cognition</a> by <a href="http://en.wikipedia.org/wiki/Terry_Winograd">Terry Winograd</a> and <a href="http://en.wikipedia.org/wiki/Fernando_Flores">Fernando Flores</a> and have to jot down some quick notes & quotes before I jump in and start reading it again ... yeah, it's <em>that</em> good. 

Having gone on <a href="http://en.wikipedia.org/wiki/Richard_Rorty">Rorty</a> and <a href="http://en.wikipedia.org/wiki/Wittgenstein">Wittgenstein</a> kicks recently, I was really happy to find this book while browsing the <a href="http://en.wikipedia.org/wiki/Neats_vs._scruffies">Neats vs Scruffies</a> Wikipedia article a few months ago. It seems to combine this somewhat odd interest I have in <a href="http://en.wikipedia.org/wiki/Pragmatism">pragmatism</a> and writing software. While it was first published in 1986, it's still very relevant today, especially in light of what the more Semantic Web heavy <a href="http://linkeddata.org">Linked Data</a> crowd are trying to do with <a href="http://www.w3.org/2004/OWL/">ontologies</a> and <a href="http://www.w3.org/2005/rules/wiki/RIF_Working_Group">rules</a>. Plus it's written in clear and accessible language, which is perfect for the arm-chair compsci/philosopher-type ...  so it's ideal for a dilettante like me.

While only 207 pages long, the breadth of the book is kind of astounding. The philosophy of <a href="http://en.wikipedia.org/wiki/Martin_Heidegger">Heidegger</a> figures prominently...in particular his ideas about <em>throwness</em>, <em>breakdowns</em> and <em>readiness to hand</em> which emphasize the importance of <em>concernful activity</em> over rationalist, representations of knowledge. 

<blockquote>
Heidegger insists that it is meaningless to talk about the existence of objects and their properties in the absence of concernful activity, with its potential for breaking down.
</blockquote>

The work of the biologist <a href="http://en.wikipedia.org/wiki/Humberto_Maturana">Humberto Maturana</a> forms the second part of the theoretical foundation of the book. The authors draw upon Maturana's ideas about <em>structural coupling</em> to emphasize the point that:

<blockquote>
The most successful designs are not those that try to fully model the domain in which they operate, but those that are 'in alignment' with the fundamental structure of that domain, and that allow for modification and evolution to generate new structure coupling.
</blockquote>

And the third leg in the chair is <a href="http://en.wikipedia.org/wiki/John_Searle">John Searle's</a> notion of <em>speech acts</em> which emphasizes the role of commitment and action, or the social role of language in dealing with meaning. 

<blockquote>
Words correspond to our intuition about "reality" because our purposes in using them are closely aligned with our physical existence in a world and our <strong>actions</strong> within it. But the coincidence is the result of our use of language within a tradition ... our structure coupling within a consensual domain. Language and cognition are fundamentally social ... our ability to think and to give meaning to language is rooted in our participation in a society and a tradition.
</blockquote>

<span style="text-alignment: center; background-color: #eeeeee; float: right; font-size: smaller; margin-left: 10px;"><a href="http://en.wikipedia.org/wiki/Fernando_Flores"><img src="http://inkdroid.org/images/flores.jpg" style=" width: 260px;"/></a><br /><a href="http://en.wikipedia.org/wiki/Fernando_Flores">Fernando Flores</a> (by <a href="http://en.wikipedia.org/wiki/File:Fernando_Flores.jpg">Sebastián Piñera</a>)
</span>

So the really wonderful thing that this book does here is take this theoretical framework (Heidegger, Maturana & Searle) and apply it to the design of computer software. As the preface makes clear, the authors largely wrote this book to dismantle popular (at the time) notions that computers would "think" like humans. While much of this seems anachronistic today, we still see similar thinking in some of the ways that the Semantic Web is described, where machines will <em>understand </em> the semantics of data, using ontologies that model the "real world". 

There is still a fair bit of talk about getting the ontologies <em>just right</em> so that they model the world properly, and then running rule driven inference engines over the instance data, to "learn" more things. But what is often missing is a firm idea of <em>what</em> actual tools will use this new data. How will these tools be used by people acting in a particular domain? Like <a href="http://blogs.ecs.soton.ac.uk/webteam/2010/09/02/the-modeler/">The Modeler</a>, practitioners in the Linked Data and Semantic Web community often jump to modeling a domain, and trying to get it to match "reality" before understanding what the field of activity we want to support is...what we are trying to have the computer help us do ... what new conversations we want the computer to enable <em>with other people</em>.

<blockquote>
In creating tools we are designing new conversations and connections. When a change is made, the most significant innovation is the modification of the conversation structure, not the mechanical means by which the conversation is carried out. In making such changes we alter the overall pattern of conversation, introducing new possibilities or better anticipating breakdowns in the previously existing ones ... When we are aware of the real impact of design we can more consciously design conversation structures that work.
</blockquote>

It's important to note here that these are conversations between people, who are acting in some domain, and using computers as tools. It's the social activity that grounds the computer software, and not some correspondence that the software shares with reality or truth. I guess this is a subtle point, and I'm not doing a terrific job of elucidating it here, but if your interest is piqued definitely pick up a copy of the book. Over the past 5 years I've been lucky to work with several people who intuitively understand how important the social setting and alignment are to successful software development--but it's nice to have the theoretical tools as ballast when the weather gets rough.

Another really surprising part of the book (given that it was written in 1986) is the foreshadowing of the <a href="http://en.wikipedia.org/wiki/Agile_software_development">agile school of programming</a>:

<blockquote>
... the development of any computer-based system will have to proceed in a cycle from design to experience and back again. It is impossible to anticipate all of the relevant breakdowns and their domains. They emerge gradually in practice. System development methodologies need to take this as a fundamental condition of generating the relevant domains, and to facilitate it through techniques such as building prototypes early in the design process and applying them in situations as close as possible to those in which they will eventually be used.
</blockquote>

Compare that with the notion of <a href="http://en.wikipedia.org/wiki/Iterative_and_incremental_development">iterative development</a> that's now prevalent in software development circles. I guess it shouldn't be that surprising since the roots of extend back <a href="http://c2.com/cgi/wiki/wiki?HistoryOfIterative">quite a ways</a>. But still, it was pretty eerie seeing how on target Winograd and Flores could be still, particularly in the field of computing which has changed so rapidly in the last 25 years.

<em>update: Kendall Clark has an interesting <a href="http://weblog.clarkparsia.com/2011/05/04/how-to-create-business-value-with-semantic-tech/">post</a> that addresses some of the concerns about semantic web technologies.</em>
<em>update: Ryan Shaw <a href="http://twitter.com/#!/rybesh/status/65822533097766912">recommended</a> some more reading material in this vein.</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3268</wp:post_id>
		<wp:post_date>2011-05-03 00:06:06</wp:post_date>
		<wp:post_date_gmt>2011-05-03 07:06:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-bit-about-scruffiness</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="agile"><![CDATA[agile]]></category>
		<category domain="post_tag" nicename="ai"><![CDATA[ai]]></category>
		<category domain="post_tag" nicename="biology"><![CDATA[biology]]></category>
		<category domain="post_tag" nicename="linkeddata"><![CDATA[linkeddata]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="pragmatism"><![CDATA[pragmatism]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"84563";i:1;s:5:"84686";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84563</wp:comment_id>
			<wp:comment_author><![CDATA[sgillies.net/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://sgillies.net/</wp:comment_author_url>
			<wp:comment_author_IP>66.35.39.33</wp:comment_author_IP>
			<wp:comment_date>2011-05-03 07:13:13</wp:comment_date>
			<wp:comment_date_gmt>2011-05-03 14:13:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the recommendation, Ed. I think I see a lot of ready-to-hand in HTTP, REST, and the web. I'm going to pick up this book and see if that notion holds up.

Speaking of scruffy work, check out the title page in Google Books :)

http://books.google.com/books?id=2sRC8vcDYNEC&amp;pg=PR3&amp;source=gbs_selected_pages&amp;cad=3#v=onepage&amp;q&amp;f=false]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>326</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304431993.5492";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:12:"sgillies.net";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1304433576.093";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84564</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2011-05-03 07:48:04</wp:comment_date>
			<wp:comment_date_gmt>2011-05-03 14:48:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@sgillies looks like that page was ready-to-hand eh? :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1304434084.8304";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84686</wp:comment_id>
			<wp:comment_author><![CDATA[Alistair Miles]]></wp:comment_author>
			<wp:comment_author_email>londonbonsaipurple@yahoo.co.uk</wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/alimanfoo#df40d</wp:comment_author_url>
			<wp:comment_author_IP>129.67.46.49</wp:comment_author_IP>
			<wp:comment_date>2011-05-16 03:58:56</wp:comment_date>
			<wp:comment_date_gmt>2011-05-16 10:58:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[As far as I'm concerned, OWL is just another programming language :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>385</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1305815124.8325";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1305543536.4454";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:8:"Alistair";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84811</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>69.243.24.29</wp:comment_author_IP>
			<wp:comment_date>2011-05-24 14:13:10</wp:comment_date>
			<wp:comment_date_gmt>2011-05-24 21:13:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@alistair I like that re: OWL ... but it makes me wonder what Hello World would look like :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1306271590.5489";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>the dpla as a generative platform</title>
		<link>http://inkdroid.org/2011/05/25/the-dpla-as-a-generative-platform/</link>
		<pubDate>Wed, 25 May 2011 07:04:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3381</guid>
		<description></description>
		<content:encoded><![CDATA[Last week I had the opportunity to attend a meeting of the <a href="http://cyber.law.harvard.edu/research/dpla/">Digital Public Library of America</a> (DPLA) in Amsterdam. Several people have asked me why an American project was meeting in Amsterdam. In large part it was an opportunity for the DPLA to reach out to, and learn from European projects such as <a href="http://europeana.eu/">Europeana</a>, <a href="http://lod2.eu/">LOD2</a> and <a href="">Wikimedia Germany</a>--or as the <a href="http://inkdroid.org/data/dpla-amsterdam.pdf">agenda</a> describes:

<blockquote>
The purpose of the May 16 and 17 expert working group meeting, convened with generous support from the Open Society Foundations, is to begin to identify the characteristics of a technical infrastructure for the proposed DPLA. This infrastructure ideally will be interoperable with international efforts underway, support global learning, and act as a <strong>generative platform</strong> for undefined future uses. This workshop will examine interoperability of discovery, use, and deep research in existing global digital library infrastructure to ensure that the DPLA adopts best practices in these areas.  It will also serve to share information and foster exchange among peers, to look for opportunities for closer collaboration or harmonization of existing efforts, and to surface topics for deeper inquiry as we examine the role linked data might play in a DPLA.
</blockquote>

Prior to the meeting I read the <a href="http://cyber.law.harvard.edu/dpla/Concept_Note">DPLA Concept Note</a>, watched the <a href="https://cyber.law.harvard.edu/lists/arc/dpla-discussion">discussion list</a> and <a href="http://cyber.law.harvard.edu/dpla/Main_Page">wiki activity</a> -- but the DPLA still seemed somewhat hard to grasp to me. The thing I learned at the meeting in Amsterdam is that this nebulousness is by design--not by accident. The DPLA steering committee aren't really pushing a particular solution that they have in mind. In fact, there doesn't seem to be a clear consensus about what problem they are trying to solve. Instead the steering committee seem to be making a concerted effort to keep an open, beginners-mind about what a Digital Public Library of America <em>might</em> be. They are trying to create conversations around several broad topic areas or <em>work-streams</em>: content and scope, financial/business models, governance, legal issues, and technical aspects. The recent meeting in Amsterdam focused on the <em>technical aspects</em> work-stream--in particular, best practices for data interoperability on the Web. The thought being that perhaps the DPLA could exist in some kind of distributed relationship with existing digital library efforts in the United States--and possibly abroad. Keeping an open mind in situations like this takes quite a bit of effort. There is often an irresistable urge to jump to particular use cases, scenarios or technical solutions, for fear of seeming ill informed or rudderless. I think the DPLA should be commended for creating conversations at this formative stage, instead of solutions in search of a problem.

I hadn't noticed the phrase "generative platform" in the meeting announcement above until I began this blog post...but in hindsight it seems very evocative of the potential of the DPLA. At their best, digital libraries currently put content on the Web, so that researchers can discover it via search engines like Google, Bing, Baidu, etc. Researchers discover a pool of digital library content while performing a query in a search engine. Once they've landed in the digital library webapp they can wander outwards to related resources, and perhaps do a more nuanced search within the scoped context of the collection. But in practice this doesn't happen all that often. I imagine many institutions digitize content that actually never makes it onto the Web at all. And when it does make it onto the Web it is often <a href="http://en.wikipedia.org/wiki/Deep_Web">deep-web</a> content hiding behind a web form, un-discoverable by crawlers. Or worse, the content might be actively made invisible by using a <a href="http://en.wikipedia.org/wiki/Robots.txt">robots.txt</a> to prevent search engines from crawling it. Sadly this is often done for performance reasons, not out of any real desire to keep the content from being found--because all too often library webapps are not designed to support crawling.

<iframe src="https://docs.google.com/present/embed?id=dv89m3d_512fd83d9c5" frameborder="0" width="410" height="342" style="float: left; margin-right: 10px;"></iframe>

I was invited to talk very briefly (10 minutes) about Linked Data at the Amsterdam meeting. I think most everyone recognizes that a successful DPLA would exist in a space where there has been years of digitization efforts in the US, with big projects like the <a href="http://hathitrust.org">HathiTrust</a> and countless others going on. I wanted to talk about how the Web could be used to integrate these collections. Rather than digging into a particular Linked Data solution to the problem of synchronization, I thought I would try to highlight how libraries could learn to do web1.0 a bit better. In particular I wanted to showcase how Google Scholar <a href="http://googlewebmastercentral.blogspot.com/2008/04/retiring-support-for-oai-pmh-in.html">abandoned</a> OAI-PMH (a traditional library standard for integrating collections) in favor of using <a href="http://www.sitemaps.org/">sitemaps</a> and metadata embedded in HTML. I wanted to show how thoughtful use of sitemaps, a sensible robots.txt, and perhaps some Atom to publish updates, and deletes a bit more methodically can offer just the same functionality as OAI-PMH, but in a way that is <em>aligned</em> with the Web, and the services that are built on top of it. Digital library initiatives often go off and create their own specialized way of looking at the Web, and ignore broader trends. The nature of grant funding, and research papers often serve as an incentive for this behavior. I've heard rumors that there is even some NISO working group being formed to look into standardizing some sort of feed based approach to metadata harvesting. Personally I think it's probably more important for us to use some of the standards and patterns that are already available instead of trying to define another one.

So you could say I pulled a bit of a bait and switch: instead of talking about Linked Data I really ended up talking about Search Engine Optimization. I didn't mention RDF or SPARQL once. If anyone noticed they didn't seem to mind too much. 
 
I learned a lot of very useful information during the presentations--too much to really note here. But there was one conversation that really stands out after a week has passed. 

<a href="http://www.cs.tufts.edu/people/faculty/bio/crane">Greg Crane</a> of the <a href="http://www.perseus.tufts.edu/">Perseus Digital Library</a> spoke about about Deep Research, and how students and researchers participate in the creation of online knowledge. At one point Greg had a slide that contained a map of the entire world, and spoke about how the scope of the DPLA can't really be confined to the United States alone--since American society is largely made up of immigrant communities (some by choice, some not) the scope of the DPLA is in fact the <em>entire world</em>. I couldn't help but think how completely audacious it was to say that the Digital Public Library of <em>America</em> would somehow encompass the world -- similar to how brick and mortar library and museum collections can often mirror the imperialistic interests of the states that they belong to.

<p><span style="float: right; font-size: smaller; text-align: center; background-color: #eeeeee; margin-right: 10px;"><a href="http://www.flickr.com/photos/danbri/4030764915/"><img src="http://inkdroid.org/images/share-what-we-know.jpg" style="width: 325px;"/></a><br />Original WWW graphic by Robert Cailliau</span></p>

So I was relieved when <a href="http://www.ibi.hu-berlin.de/institut/mitarbA-Z/professoren/gradmann">Stefan Gradmann</a> asked how Greg thought the DPLA would fit in with projects like Europeana, which are already aggregating content from Europe. I can't exactly recall Greg's response <em>(update: Greg filled in some of the blanks via <a href="https://cyber.law.harvard.edu/lists/arc/dpla-discussion/2011-05/msg00087.html">email</a>)</em>, but this prompted <a href="http://danbri.org">Dan Brickley</a> to point out that in fact it's pretty hard to draw lines around Europe too ... and more importantly the Web is a space that can unite these projects. At this point <a href="http://en.wikipedia.org/wiki/Joshua_Greenberg">Josh Greenberg</a> jumped in and suggested that perhaps some thoughtful <em>linking</em> between sites like a DPLA and Europeana could help bring them together. This all probably happened in the span of 3 or 4 minutes, but the exchange really crystallized for me that the cultural heritage community could do a whole lot better at deep linking with each other. Josh's suggestion is particularly good, because researchers could see and act on contextual links. It wouldn't be something hidden in a data layer that nobody ever looks at. But to do this sort of linking right we would need to share our data better with each other, and it would most likely need to be Linked Data -- machine readable data with URLs at its core. I guess it's a no-brainer that for it to succeed the DPLA needs to be aligned with the ultimate generative platform of our era: the World Wide Web. Name things with URLs, create typed links between them, and other people's stuff.

Another thing that struck me was how <a href="http://europeana.eu">Europeana</a> really gets the linking part. Europeana is essentially a portal site, or directory of digital objects for more than 15 million items provided by hundreds of providers across Europe. You can find these objects in Europeana, but if you drill down far enough you eventually find yourself on the original site that made the object available. I agree with folks that think that perhaps the user experience of the site would be improved if the user never left Europeana to view the digital object in full. This would necessarily require harvesting a richer version of the digital object, which would be more difficult, but not impossible. There would also be an opportunity to serve as a second copy for the object, which is potentially very desirable to originating institutions for preservation purposes...lots of copies keeps stuff safe.

But even in this hypothetical scenario where the object is available in full on Europeana, I think it would still be important to link out to the originating institution that digitized the object. Linking makes the provenance of the item explicit, which will continue to be important to researchers on the Web. But perhaps more importantly it gives institutions a reason to participate in the project as a whole. Participants will see increased awareness and use of their web properties, as users wander over from Europeana. Perhaps they could even link back to Europeana, which ought to increase Europeana's <a href="http://www.betaversion.org/~stefano/linotype/news/304/">density</a> in the graph of the web, which also should boost its relevancy ranking in search engines like Google.

Another good lesson of Europeana is that it's not just about libraries, but also includes many archives, museums and galleries. One of my personal disappointments about the Amsterdam meeting was that <a href="http://twitter.com/#!/presroi">Mathias Schindler</a> of  Wikimedia-Germany had to pull out at the last minute. I've never met him, but Mathias has had a lot to do with trying to bring the Wikipedia and Library communities together. Efforts to promote the use of Wikipedia as a platform in the Galleries, Libraries, Archives and Museums (GLAM) sector are <a href="http://en.wikipedia.org/wiki/Wikipedia:GLAM">intensifying</a>. The pivotal role that Wikipedia has had in the Linked Data community in the form of <a href="http://dbpedia.org">dbpedia.org</a> is also very significant. Earlier this year there was a meeting of various Wikipedia, dbpedia and Freebase folks at a <a href="http://meta.wikimedia.org/wiki/Data_summit_2011">Data Summit</a>, where people talked about the potential for an inner hub for the various languages wikipedias to share inter-wiki links, and extracted structured metadata. I haven't heard whether this is actually leading anywhere currently, but at the very least its a recognition that Wikipedia is itself turning into a key part of information infrastructure on the web.

So I've rambled on a bit at this point. Thanks for reading this far. My take-away from the Amsterdam meeting was that the DPLA needs to think about how it wants to align itself with the Web, and work with its grain ... not against it. This is easier said than done. The DPLA needs to think about incentives that would give existing digital library projects practical reasons to want to be involved. This also is easier said than done. And hopefully these incentives won't just involve getting grant money. Keeping an open mind, taking a <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">REST</a> here and there, and continuing to have these very useful conversations (and <a href="http://cyber.law.harvard.edu/newsroom/Digital_Public_Library_America_Beta_Sprint">contests</a>) should help shape the DPLA as a generative platform.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3381</wp:post_id>
		<wp:post_date>2011-05-25 00:04:52</wp:post_date>
		<wp:post_date_gmt>2011-05-25 07:04:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-dpla-as-a-generative-platform</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="amsterdam"><![CDATA[amsterdam]]></category>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="dpla"><![CDATA[dpla]]></category>
		<category domain="post_tag" nicename="europeana"><![CDATA[europeana]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="museums"><![CDATA[museums]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>84961</wp:comment_id>
			<wp:comment_author><![CDATA[The DPLA as a generative platform &#171; Evolving Libraries]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.evolvinglibraries.com/the-dpla-as-a-generative-platform-86</wp:comment_author_url>
			<wp:comment_author_IP>69.163.237.0</wp:comment_author_IP>
			<wp:comment_date>2011-06-03 03:55:59</wp:comment_date>
			<wp:comment_date_gmt>2011-06-03 10:55:59</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] DPLA as a generative platform  As usual, someone else said it better than I could.  Ovet at Inkdroid, Ed Summers nicely states what I was getting at in my comments at The PLA Blog: Keeping an open [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1307098559.8373";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1307366571.9806";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85187</wp:comment_id>
			<wp:comment_author><![CDATA[How a good open-source blogging editor could win the DPLA friends&#8212;and help it be more &ldquo;self-generative&rdquo; | LibraryCity]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://librarycity.org/?p=2552</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-10-25 16:07:06</wp:comment_date>
			<wp:comment_date_gmt>2011-10-25 23:07:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] bowed to the gods of interactivity and also called for the Harvard-hosted library initiative to be “self generative.” I agree. Let open content lead to yet more content, whether of local, national, or global interest. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1319584026.2132";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1319961134.7798";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84915</wp:comment_id>
			<wp:comment_author><![CDATA[Beyond OAI | Inherent Vice]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.inherentvice.net/?p=383</wp:comment_author_url>
			<wp:comment_author_IP>72.32.209.132</wp:comment_author_IP>
			<wp:comment_date>2011-05-27 16:00:01</wp:comment_date>
			<wp:comment_date_gmt>2011-05-27 23:00:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Should we abandon OAI in favor of more web-friendly approaches? (See @edsu Digital Public Library as a Generative Platform) [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1306537201.7422";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1307366575.2714";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>84959</wp:comment_id>
			<wp:comment_author><![CDATA[Digital Natives Want It Now! | 21st Century Library Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://21stcenturylibrary.wordpress.com/2011/05/31/digital-natives-want-it-now/</wp:comment_author_url>
			<wp:comment_author_IP>69.170.134.101</wp:comment_author_IP>
			<wp:comment_date>2011-05-31 07:53:29</wp:comment_date>
			<wp:comment_date_gmt>2011-05-31 14:53:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] youth. According to Ed Summers’ Blog (he’s a computer code writer at the Library of Congress) INKDROID, the agenda of the recent meeting of DPLA in Amsterdam (not as incongruous as it sounds) was for [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1306853609.1104";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1307366573.3959";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>meeting notes and a manifesto</title>
		<link>http://inkdroid.org/2011/07/14/stanford-linked-data-meeting-notes-an-incidental-manifesto/</link>
		<pubDate>Thu, 14 Jul 2011 14:49:36 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3489</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A few weeks ago I was in sunny (mostly) Palo Alto, California at a Linked Data Workshop hosted by Stanford University, and funded by <a href="http://www.clir.org/news/pressrelease/11mellonpr2.html" title="CLIR">Council on Library and Information Resources</a>. It was an invite-only event, with very little footprint on the public Web, and an attendee list that was distributed via email with "Confidential PLEASE do not re-distribute" across the top. So it feels more than a little bit foolhardy to be writing about it here, even in a limited way. But I was going to the meeting as an employee of the Library of Congress, so I feel a bit responsible for making some kind of public notes/comments about what happened. I suspect this might impact future invitations to similar events, but I can live with that :-)</p>

<p>A week is a long time to talk about anything...and Linked Data is certainly no exception. The conversation was buoyed by the fact that it was a pretty small group of around 30 folks from a wide variety of institutions including: <a href="http://stanford.edu/">Stanford University</a>, <a href="http://www.bibalex.org/">Bibliotheca Alexandrina</a>, <a href="http://www.bnf.fr">Bibliothèque nationale de France</a>, <a href="http://www.aalto.fi/">Aalto University</a>, <a href="http://www.emory.edu">Emory University</a>, <a href="http://www.google.com">Google</a>, <a href="http://www.nii.ac.jp/">National Institute of Informatics</a>, <a href="http://www.virginia.edu">University of Virginia</a>, <a href="http://www.umich.edu">University of Michigan</a>, <a href="http://www.d-nb.de/">Deutschen Nationalbibliothek</a>, <a href="http://www.bl.uk">British Library</a>, <a href="http://www.kb.dk/">Det Kongelige Bibliotek</a>, <a href="http://www.cdlib.org/">California Digital Library</a> and <a href="http://seme4.com">Seme4</a>. So the focus was definitely on cultural heritage institutions, and specifically what Linked Data can offer them. There was a good mix of people in attendance: some who were relatively new to Linked Data and RDF, to others who had been involved in the space before the term Linked Data, RDF or the Web existed...and there were people like me who were somewhere in between.</p>

<p>A few of us took collaborative <a href="http://piratepad.net/YJLUIp164y">notes</a> in PiratePad, which sort of tapered off as the week progressed and more discussion happened. After some initial lighting-talk-style presentations from attendees on Linked Data projects they were involved in, we spent most of the rest of the week breaking up into 4 groups, to discuss various issues, and then joining back up with the larger group for discussion. If things go as planned you can expect to see a report from Stanford that covers the opportunities and challenges that Linked Data offers the cultural heritage sector, which were raised during these sessions. I think it'll be a nice compliment to the report that the W3C Library Linked Data Incubator Group is preparing, which recently became available as a <a href="http://www.w3.org/2005/Incubator/lld/wiki/DraftReportWithTransclusion">draft</a>.</p>

<p>One thing that has stuck with me a few weeks later, is the continued need in the cultural heritage Linked Data sector for reconciliation services, that help people connect up their resources with appropriate resources that other folks have published. If you work for a large organization, there is often even a need for reconciliation services <em>within</em> the enterprise. For example the British Library reported that it has some 300 distinct data systems within the organization, that sometimes need to be connected together. Linking is the essential ingredient, the <em>sine qua non</em> of Linked Data. Linking is what makes Linked Data and the RDF data model different. It helps you express the work you may have done in joining up your data with other people's data. It's the 4th design pattern in Tim Berners-Lee's Linked Data Design Issues:</p>

<blockquote>
Include links to other URIs. so that they can discover more things.
</blockquote>

<p>But, expressing the links is the easy part...creating them where they do not currently exist is harder.</p>

<p>Fortunately, <a href="http://www.seme4.com/who-we-are/profile/hugh-glaser/">Hugh Glaser</a> was on hand to talk about the role that <a href="http://sameas.org">sameas.org</a> plays in the Linked Data space, and how the <a href="http://www.rkbexplorer.com/">RKBexplorer</a> managed to reconcile authors names across institutional repositories. He also has described some work with the British Museuem linking up two different data silos about museum objects, to provide a unified web views for those objects. Hugh, if you are reading this, can you comment with a link to this work you did, and how it surfaces in British Museum website?</p>

<p>Similarly <a href="http://www.betaversion.org/~stefano/">Stefano Mazzocchi</a> talked about how Google's tools like <a href="http://www.freebase.com/docs/suggest">Freebase Suggest</a> and their <a href="http://ids.freebaseapps.com/">WebID</a> app can make it easy to integrate Freebase's identity system into your applications. If you are building a cataloging tool, take a serious look at what using something like Freebase Suggest (a jquery plugin) can offer your application. In addition, as part of the <a href="http://code.google.com/p/google-refine/">Google Refine</a> data cleanup tool, Google has created an API for data <a href="http://code.google.com/p/google-refine/wiki/ReconciliationServiceApi">reconciliation services</a>, which other service providers could supply. Stefano indicated that Google was considering releasing the code behind this reconciliation service, and stressed that it is useful for the community to make more of these reconciliation services available, to help others link their data with other peoples data. It seems obvious I guess, but I was interested to hear that Google themselves are encouraging the use of Freebase IDs to join up data within their enterprise.</p>

<p>Almost a year ago <a href="http://www.ldodds.com/">Leigh Dodds</a> created <a href="http://www.ldodds.com/blog/2010/08/gridworks-reconciliation-api-implementation/">a similar API layer</a> for data that is stored in the Talis Platform. Now that the <a href="http://www.bl.uk/bibliographic/datafree.html">British National Bibliography</a> is being made available in a Talis Store, it might be possible to use Leigh's code to put a reconciliation service on top of that data. Caveat being that not all the BNB is currently available there. By the way, hats off to the British Library for iteratively making that data available, and getting feedback early, instead of waiting for it all to be "done"...which of course they never will be, if they are successful at integrating Linked Data into their existing data work flows.</p>

<p>If you squint right, I think it's also possible to look at the  <a href="http://www.oclc.org/developer/documentation/virtual-international-authority-file-viaf/request-types#autosuggest">VIAF AutoSuggest</a> service as a type reconciliation service. It would be useful to have a similar service over the <a href="http://web.archive.org/web/20110720034058/http://id.loc.gov:80/authorities">Library of Congress Subject Headings</a> at id.loc.gov. Having similar APIs for these services could be a handy thing as we begin to build new descriptive cataloging tools that take advantage of these pools of data. But I don't think it's necessarily essential, as the APIs could be orchestrated in a more ad hoc, web2.0 mashup style. I imagine I'm not alone in thinking we're now at the stage when we can start building new cataloging tools that take advantage of these data services. Along those lines <a href="http://twitter.com/rlfrick">Rachel Frick</a> had an excellent idea to try to enhance collection building applications like <a href="http://omeka.org/">Omeka</a> and <a href="http://archivesspace.org/">Archives Space</a> to take advantage of reconciliation services under the covers. Adding a bit of suggest-like functionality to these tools could smooth out the description of resources that libraries, museums and archives are putting online. I think the <a href="http://omeka.org/forums/topic/lcsh-plugin">Omeka-LCSH plugin</a> is a good example of steps in this direction.</p>

<p>One other thing that stuck with me from the workshop is that the new (dare I say buzzwordy) focus on Library Linked Data is somewhat frightening to library data professionals. There is a lot of new terminology, and issues to work out (as the Stanford report will hopefully highlight). Some of this scariness is bound up with the <a href="http://en.wikipedia.org/wiki/Resource_Description_and_Access">Resource Description and Access</a> sea change that is underway. But crufty as they are, data systems built around MARC have served the library community well over the past 30 years. Some of the motivations for Linked Data are specifically for Linked <em>Open</em> Data, where the linking isn't as important as the <em>openness</em>. The LOD-LAM summit captured some of this spirit in the <a href="http://lod-lam.net/summit/2011/06/06/proposed-a-4-star-classification-scheme-for-linked-open-cultural-metadata/">4 Star Classification Scheme for Linked Open Cultural Metadata</a>, which focuses on licensing issues. There was a strong undercurrent at the Stanford meeting about licensing issues. The group recognized that explicit licensing is important, but it was intentionally kept out of scope of most of the discussion. Still I think you can expect to see some of the heavy hitters from this group exert some influence in this arena to help bring clarity to licensing issues around our data. I think that some of the ideas of opening up the data, and disrupting existing business workflows around the data can seem quite scary to those who have (against a lot of odds) gotten them working. I'm thinking of the various cooperative cataloging efforts that allow work to get done in libraries today.</p>

<p>Truth be told, I may have inspired some of the "fear" around Linked Data by suggesting that the Stanford group work on a manifesto to rally around, much like what the <a href="http://agilemanifesto.org/">Agile Manifesto</a> did for the Agile software development movement. I don't think we had come to enough consensus to really get a manifesto together, but on the last day the sub-group I was in came up with a straw man (near the bottom of the <a href="http://piratepad.net/YJLUIp164y">piratepad notes</a>) to throw darts at. Later on (on my own) I kind of wordsmithed them into a briefer list. I'll conclude this blog post by including the "manifesto" here not as some official manifesto of the workshop (it certainly is not), but more as a personal manifesto, that I'd like to think has been guiding some of the work I have been involved in at the Library of Congress over the past few years:</p>

<div style="background-color: #eee; padding: 10px 20px 10px 20px;">
<h1>Manifesto for Linked Libraries</h1>

<p>We are uncovering better ways of publishing, sharing and using information by doing it and helping others do it. Through this work we have come to value:

<ul>
<li><b>Publishing data on the Web for discovery</b> over preserving it in dark archives.</li>
<li><b>Continuous improvement of data</b> over waiting to publish perfect data.</li>
<li><b>Semantically structured data</b> over flat unstructured data.</li>
<li><b>Collaboration</b> over working alone.</li>
<li><b>Web standards</b> over domain-specific standards.</li>
<li><b>Use of open, commonly understood licenses</b> over closed, local licenses.</li>
</ul>

That is, while there is value in the items on the right, we value the items on the left more.
</p>
</div>

<p>The manifesto is also on a <a href="http://bit.ly/sldw-mf">publicly editable Google Doc</a>; so if you feel the need to add or comment please have a go. I was looking for an alternative to "Linked Libraries" since it was not inclusive of archives and museums ... but I didn't spend much time obsessing on it. One of the selfish motivations for publishing the manifesto here was to capture it a particular point in time where I was happy with it :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3489</wp:post_id>
		<wp:post_date>2011-07-14 07:49:36</wp:post_date>
		<wp:post_date_gmt>2011-07-14 14:49:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>stanford-linked-data-meeting-notes-an-incidental-manifesto</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="post_tag" nicename="museums"><![CDATA[museums]]></category>
		<category domain="post_tag" nicename="stanford"><![CDATA[stanford]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="workshops"><![CDATA[workshops]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85062";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85066</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-07-22 02:22:10</wp:comment_date>
			<wp:comment_date_gmt>2011-07-22 09:22:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks Jonathan. I think RDF is an OK technology as it stands. The data model is present in a lot of places, and I think the fact that it has a data model with different serializations is itself a good model for the library community.

But you are right, I tried to keep the language in the manifesto neutral about particular technologies and standards--except for "the Web" of course, which most of us don't need to argue about much anymore. I'm glad you like it by the way--thanks for writing to let me know.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1311326533.2406";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85062</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-07-19 16:38:34</wp:comment_date>
			<wp:comment_date_gmt>2011-07-19 23:38:34</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I have to admit that I only get more and more suspicious of RDF, "linked data" and "semantic web" the more I learn/see/occasioally-use it. 

However, I STILL heartily endorse every single point in your manifesto, the technology/standard-neutral language of which is presumably intentional. The manifesto hits the nail on the head.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1311118717.289";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>GoodReads microdata</title>
		<link>http://inkdroid.org/2011/08/02/goodreads-microdata/</link>
		<pubDate>Tue, 02 Aug 2011 19:30:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3561</guid>
		<description></description>
		<content:encoded><![CDATA[I'm not sure how long it has been there, but I just happened to notice that <a href="http://goodreads.com">GoodReads</a> (the popular social networking site for book lovers to share what they are reading and have read) has implemented <a href="http://dev.w3.org/html5/md/">HTML5 Microdata</a> to make metadata for books available in the HTML of their pages. GoodReads has chosen to use the the <a href="http://schema.org/Book">Book</a> type from <a href="http://schema.org/">schema.org</a> vocabulary, most likely because the big three search engines (Google, Bing and Yahoo) <a href="http://googlewebmastercentral.blogspot.com/2011/06/introducing-schemaorg-search-engines.html">announced</a> that they will use the metadata to enhance their search results. So web publishers are motivated to publish metadata in their pages, not because it's the "right" thing to do, but because they want to drive traffic to their websites.

If you are new to HTML5 Microdata, schema.org and what it means for books, check out Eric Hellman's recent post <a href="http://go-to-hellman.blogspot.com/2011/07/spoonfeeding-library-data-to-search.html">Spoonfeeding Library Data to Search Engines</a>. And if you are generally curious about HTML5 Microdata, the <a href="http://diveintohtml5.org/extensibility.html">chapter</a> in Mark Pilgrim's <a href="http://diveintohtml5.org/">Dive into HTML5</a> is really quite good.

But Microdata and schema.org are not just good for the search engines, they are actually good for the Web ecosystem, and for hackers like you and me. For example, go to the page for <a itemprop="image" href="http://www.goodreads.com/book/show/24220.Alice_s_Adventures_in_Wonderland_and_Through_the_Looking_Glass">Alice in Wonderland</a>:

<a href="http://www.goodreads.com/book/show/24220.Alice_s_Adventures_in_Wonderland_and_Through_the_Looking_Glass"><img src="http://inkdroid.org/images/goodreads-alice.png"/></a>

If you view source on the page, and search for <em>itemtype</em> or <em>itemprop</em> you'll see the extra Microdata markup. The latest <a href="http://dev.w3.org/html5/md/">HTML5 specification</a> has a <a href="http://dev.w3.org/html5/md/#json">section</a> on how to serialize microdata as JSON, and the processing model is straightforward enough for me to write a <a href="https://github.com/edsu/microdata/blob/master/microdata.py">parser</a> on top of html5lib in less than 200 lines of Python. So this means you can:

<pre lang="python">
import urllib
import microdata

url = "http://www.goodreads.com/book/show/24220.Alice_s_Adventures_in_Wonderland_and_Through_the_Looking_Glass"
items = microdata.get_items(urllib.urlopen(url))

print items[0].json()
</pre>

and you'll see:

<pre lang="javascript">
{
  "numberOfPages": [
    "400"
  ],
  "isbn": [
    "9780141439761"
  ],
  "name": [
    "Alice's Adventures in Wonderland and Through the Looking-Glass"
  ],
  "author": [
    {
      "url": [
        "http://www.goodreads.com/author/show/8164.Lewis_Carroll",
        "http://www.goodreads.com/author/show/495248.Hugh_Haughton",
        "http://www.goodreads.com/author/show/180439.John_Tenniel"
      ],
      "type": "http://schema.org/Person"
    }
  ],
  "image": [
    "/book/photo/24220.Alice_s_Adventures_in_Wonderland_and_Through_the_Looking_Glass"
  ],
  "inLanguage": [
    "English"
  ],
  "ratingValue": [
    "4.03"
  ],
  "ratingCount": [
    "64,628 ratings"
  ],
  "bookFormatType": [
    "Paperback"
  ],
  "type": "http://schema.org/Book"
}
</pre>

If you have spent a bit of time writing screenscrapers in the past, this should make your jaw drop a bit. What's more they've also added Microdata to the <a href="http://www.goodreads.com/search?query=world+wide+web">search results page</a>, so you can see metadata for all the books in the results, for example using <a href="http://www.google.com/webmasters/tools/richsnippets?url=http%3A%2F%2Fwww.goodreads.com%2Fsearch%3Fquery%3Dhtml5&view=">Google's Rich Snippets Testing Tool</a>.

Funnily enough, while I was writing this blog post, over in the <a href="irc://irc.freenode.net/code4lib">#code4lib</a> IRC chat room <a href="https://twitter.com/_cb_">Chris Beer</a> brought up the fact that some <a href="http://projectblacklight.org/">Blacklight</a> developers were concerned that &lt;link rel="unapi-server"&gt; wasn't valid HTML5. Chris was wondering if anyone was interested in trying to register "unapi-server" with the WHATWG... 

&nbsp;&nbsp;&nbsp;&nbsp;&amp;crickets;

Issues of "valid" HTML5 aside, this discussion highlighted for me just how far the world of metadata on the Web has advanced since a small group of library hackers worked on <a href="http://unapi.info">unAPI</a>. The use of HTML5 Microdata and schema.org by Google, Bing and Yahoo, and the use of <a href="https://developers.facebook.com/docs/opengraph/">RDFa</a> by Facebook are great examples of some mainstream solutions to what some of us were trying to achieve with unAPI. Seeing sites like GoodReads implement Microdata, and announcements like <a href="http://my.opera.com/desktopteam/blog/2011/07/27/latency-microdata-qresync">Opera support for Microdata</a> are good reminders that the library software development community is best served by paying attention to mainstream solutions, as they become available, even if they eclipse homegrown stopgap solutions.

It is somewhat problematic that Facebook has aligned with RDFa and the Open Graph Protocol, and Google, Bing and Yahoo have aligned with HTML5 and schema.org. GoodReads has implemented both incidentally. I heard a rumor that Facebook was invited to the schema.org table and declined. I have no idea if that is actually true. I also have heard a rumor that Ian Hickson of Google wrote up the Microdata spec in a weekend because he hates RDFa. I don't know it that's actually true either. The company and personality rivalries aside, if you are having trouble deciding which one to more fully support, try writing a program to parse RDFa and Microdata. It will probably help clarify some things...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3561</wp:post_id>
		<wp:post_date>2011-08-02 12:30:04</wp:post_date>
		<wp:post_date_gmt>2011-08-02 19:30:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>goodreads-microdata</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="facebook"><![CDATA[facebook]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="html5"><![CDATA[html5]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="microdata"><![CDATA[microdata]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="post_tag" nicename="unapi"><![CDATA[unapi]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="yahoo"><![CDATA[yahoo]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85071";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85089</wp:comment_id>
			<wp:comment_author><![CDATA[The Artolater &raquo; Friday Links]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.theartolater.com/?p=1065</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-02 04:47:17</wp:comment_date>
			<wp:comment_date_gmt>2011-09-02 11:47:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] * Interesting GoodReads microdata. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1315680278.0883";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1314964038.1456";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85073</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-08-03 17:47:21</wp:comment_date>
			<wp:comment_date_gmt>2011-08-04 00:47:21</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@bibwild sorry, it really is an exercise for the reader :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1312418843.9138";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85071</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-08-02 19:34:45</wp:comment_date>
			<wp:comment_date_gmt>2011-08-03 02:34:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[" try writing a program to parse RDFa and Microdata. It will probably help clarify some things…"

Willing to enlighten us who don't have time to do the exersize?  I'm guessing you're suggesting parsing microdata is a lot easier than parsing RDFa?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1312338885.3455";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Spotify, Rdio and Albums of the Year</title>
		<link>http://inkdroid.org/2011/08/06/spotify-rdio-and-albums-of-the-year/</link>
		<pubDate>Sun, 07 Aug 2011 05:21:09 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3603</guid>
		<description></description>
		<content:encoded><![CDATA[I've recently started listening to streamed music a bit more on <a href="http://rdio.com">Rdio</a> right around when <a href="http://spotify.com">Spotify</a> launched in the US. I noticed that some albums that I might want to listen to weren't available for streaming on Rdio, so I got it into my head that I'd like to compare the coverage of Rdio and Spotify--but I wasn't sure what albums to check. Earlier this week I remembered that since 2007 <a href="http://twitter.com/invisiblecomma">Alf Eaton</a> has put together <a href="http://aoty.hubmed.org">Albums of the Year (AOTY)</a> lists, where he has compiled the top albums of the year from a variety of sources. I think Alf's tastes tend a bit toward independent music, which suits me fine because that's the kind of stuff I listen to. So ...

The AOTY HTML is eminently readable (thanks Alf), so I wrote a <a href="https://github.com/edsu/aotycmp/blob/master/aoty.py">scraper</a> to pull down the albums and store it as <a href="https://raw.github.com/edsu/aotycmp/master/aoty.json">a JSON file</a>. With this in hand I was able to use the Rdio and Spotify web services to look up the albums, and record whether it was found, and whether it was streamable in the United States, which I saved off as <a href="https://raw.github.com/edsu/aotycmp/master/aoty-cmp.json">another JSON file</a>. So, of the 7,406 unique albums on AOTY, <strike>60% of them were available in Spotify, and 46% in Rdio</strike> 32% of them were available on Spotify, and 46% on Rdio (the strikeout is because of a bug that Alf spotted in the Spotify lookup code). I put the data in a <a href="http://www.google.com/fusiontables/DataSource?dsrcid=1256513">public Fusion Table</a> if you want to look at the results. If you notice anomalies please let me know. And speaking of anomalies...

<h2>Caveat Lector!</h2>

I was kind of surprised that <a href="http://en.wikipedia.org/wiki/Teen_Dream">Teen Dream</a> by <a href="http://en.wikipedia.org/wiki/Beach_House">Beach House</a> (which was mentioned on 27 AOTY lists in 2010) wasn't showing up as being streamable on Spotify. So I asked on <a href="http://twitter.com/#!/edsu/status/99966705723379712">Twitter</a> and <a href="https://plus.google.com/100036494993214603355/posts/PCG4ms88xDr">Google+</a> if people in the US saw it as streamable. The results were kind of surprising. People from Michigan, Illinois, Texas, New York and the District of Columbia confirmed what the Web Service told me, that the album was not streamable. But then there were people in Massachusetts and California who reported it as streamable. What's more, premium membership didn't seem to affect the availability: the Massachusetts subscriber had a free account, and the Californian had a premium account, and both could stream it. So take the numbers above with a boulder sized grain of salt. It's not clear what's going on.

The spotify <a href="http://developer.spotify.com/en/metadata-api/search/album/">search API</a> does not require authentication, and they have nice results that include all territories where the content is available. Rdio's <a href="http://developer.rdio.com/docs/read/rest/Methods#catalog">search API</a> does require authentication, which <a href="http://groups.google.com/group/rdio-api/msg/c37b6b1b742d8aac">apparently</a> is used to tie your account to a geographic region, which in turn effects what whether the API will say the album is streamable or not.

So anyway, it was interesting to play around with the APIs a bit. It didn't seem like the service agreements for the various APIs prevented this sort of exploration. I like the fact that Rdio is web based (go Django), and doesn't require a proprietary client to use. But it looks like the coverage in Spotify is better. I'm not sure I will make any changes. If anyone has any information about whether streamability of content can vary within the United States I would be interested to hear it. This rights stuff is <em>hard</em>. Given the complexity of managing the rights to this content I'm kind of amazed that Rdio and Spotify exist at all...and I'm very glad that they do.

<em>Update: it turns out that the folks who saw Teen Dream as available had it in their personal collection (Spotify is smart like that), which is why Spotify said it was available to them. So, no crazy state-by-state rights issues need to be entertained :-)</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3603</wp:post_id>
		<wp:post_date>2011-08-06 22:21:09</wp:post_date>
		<wp:post_date_gmt>2011-08-07 05:21:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>spotify-rdio-and-albums-of-the-year</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<category domain="post_tag" nicename="music"><![CDATA[music]]></category>
		<category domain="post_tag" nicename="rdio"><![CDATA[rdio]]></category>
		<category domain="post_tag" nicename="spotify"><![CDATA[spotify]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web-services"><![CDATA[web services]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85280</wp:comment_id>
			<wp:comment_author><![CDATA[Spotify vs. Rdio: Who Has The Exclusives? |]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://rhazi.wordpress.com/2011/12/14/spotify-vs-rdio-who-has-the-exclusives/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-12-14 10:12:18</wp:comment_date>
			<wp:comment_date_gmt>2011-12-14 17:12:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] more? Ed Summers did his own fascinating deep-dive into Spotify and Rdio uses top album lists from Alf Eaton&#8217;s Album of the Year list [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1323917831.2145";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86867</wp:comment_id>
			<wp:comment_author><![CDATA[Spotify vs. Rdio: Who Has The Exclusives? | Business | Wired]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://mstag.wired.com/2011/12/spotify-vs-rdio/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-03-05 07:10:09</wp:comment_date>
			<wp:comment_date_gmt>2014-03-05 14:10:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] more? Ed Summers did his own fascinating deep-dive into Spotify and Rdio uses top album lists from Alf Eaton&rsquo;s Album of the Year list [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394037211.4843189716339111328125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394028610.014072895050048828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>DbHd</title>
		<link>http://inkdroid.org/2011/08/09/dbhd/</link>
		<pubDate>Wed, 10 Aug 2011 02:02:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3633</guid>
		<description></description>
		<content:encoded><![CDATA[<iframe width="560" height="349" src="http://www.youtube.com/embed/5OWhcrjxP-E" frameborder="0" allowfullscreen style="float: left; margin-right: 15px; margin-bottom: 10px;"></iframe>

Via <a href="http://www.w3.org/People/Ivan/">Ivan Herman</a> I found out about this 1 year old, <em>excellent</em> presentation from <a href="http://en.wikipedia.org/wiki/Hans_Rosling">Hans Rosling</a> at the World Bank about the importance of sharing our data.

DbHd is certainly a problem. But it strikes me that, paradoxically, it's the love and care that people put into their datasets, that makes them so valuable to share. 

If nobody was hugging their data, then nobody would care about setting it free on the Web.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3633</wp:post_id>
		<wp:post_date>2011-08-09 19:02:04</wp:post_date>
		<wp:post_date_gmt>2011-08-10 02:02:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>dbhd</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="data"><![CDATA[data]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>an ode to node</title>
		<link>http://inkdroid.org/2011/11/07/an-ode-to-node/</link>
		<pubDate>Mon, 07 Nov 2011 12:40:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3665</guid>
		<description></description>
		<content:encoded><![CDATA[When I made my first edit to Wikipedia a few years ago I can remember watching the <a href="http://en.wikipedia.org/wiki/Special:RecentChanges">recent changes</a> page to see my contribution pop up. I was shocked to see just how quickly my edit was swept up in the torrent of edits that are going on all the time. I think everyone who googles for topical information is familiar with the experience of having Wikipedia articles routinely appear near the top of their search results. In hindsight it should've been obvious, but the level of participation in the curation of content at Wikipedia struck me as significant...and somehow different. It was wonderful to see living evidence of so many people caring to collaboratively document our world.

<h2>The Obsession</h2>

I work as a software developer in the cultural heritage sector, and often find myself building editing environments for users to collaboratively create and edit content. These systems typically get used here and there; but they in no way compare to the sheer volume of edit activity that Wikipedia sees from around the world, every single day. I guess I'd read about crowdsourcing, but had never been provided with a window into it like this before. My wife encourages her 5th grade students to think critically about Wikipedia as an information source. One way she has done this in the past was by having them author an article for <a href="http://en.wikipedia.org/wiki/Lowell_School_(Washington,_DC)">their school</a>, which didn't have an article previously. I wanted to help her and her students see how they were part of a large community of Wikipedia editors; and to give them a tactile sense of the amount of people who are actively engaged in making Wikipedia better.

A few months later <a href="http://www.twitter.com/gkob">Georgi Kobilarov</a> let me know about the <a href="http://meta.wikimedia.org/wiki/IRC/Channels#Recent_changes">many IRC channels</a> where various bits of metadata about recent changes in Wikipedia are announced. Georgi told me about a bot that the BBC run to track changes to Wikipedia, so that relevant article content can be pulled back to the BBC. I guess a light bulb turned on. Could I use these channels to show people how much Wikipedia is actively curated, without requiring them to reload the recent changes page, connect to some cryptic IRC channels, or dig around in some (wonderfully) detailed <a href="http://stats.wikimedia.org/">statistics</a>. More importantly, could it be done in a playful way?

<h2>The Apps</h2>

Some more time passed and I came across some new tools (more about these below) that made it easy to throw together a Web visualization of the Wikipedia update stream. The tools proved to be so much fun that I ended up making two.

<a href="http://wikistream.inkdroid.org"><img src="http://inkdroid.org/images/wikistream.png" style="border: thin gray solid;"/></a>

<a href="http://wikistream.inkdroid.org">wikistream</a> displays the edits to 38 language wikipedias as a river of flowing text. The content moves by so quickly that I had to add a pause button (the letter p) in order to test things like clicking on the update to see the change that was made. The little icons to the left indicate whether the edit was made by a registered Wikipedia user, an anonymous user, or a bot (there are lots of them). After getting some good feedback on the <a href="http://www.gossamer-threads.com/lists/wiki/wikitech/238445">wikitech-l</a> discussion list I added some knobs to limit updates to specific languages and types of user, or size of the edit. I also added a periodically updating background image based on uploads to the <a href="http://commons.wikimedia.org/wiki/Main_Page">Wikimedia Commons</a>. 

<a href="http://wikipulse.herokuapp.com"><img width="300" src="http://inkdroid.org/images/wikipulse.png" style="border: none; float: left; margin-right: 10px;"/></a>

The second visualization app is called <a href="http://wikipulse.herokuapp.com">wikipulse</a>. <a href="http://nitens.org/taraborelli/home">Dario Taraborelli</a> of the <a href="http://wikimediafoundation.org/">Wikimedia Foundation</a> emailed me with the idea to use the same update data stream I used in <a href="http://wikistream.inkdroid.org">wikistream</a> to fuel a higher level view of the edit activity using the <a href="http://code.google.com/apis/chart/interactive/docs/gallery/gauge.html">gauge widget</a> in Google's Chart API. To the left is one of these gauges which displays the edits per minute on 36 wikipedia properties. If you visit <a href="http://wikipulse.herokuapp.com">wikipulse</a> you will also see individual gauges for each language wikipedia. It's a bit overkill seeing all the gauges on the screen, but it's also kind of fun to see them update automatically every second relative to each other, based on the live edit activity.

<h2>The Tools</h2>

<div style="width: 205px; float: right">
<a href="http://nodejs.org"><img src="https://github.com/edsu/wikistream/raw/master/public/images/node.png" width="200"/></a>
<a href="http://redis.io"><img src="https://github.com/edsu/wikistream/raw/master/public/images/redis.png" width="200"/></a>
</div>

For both of these apps I needed to log into the wikimedia IRC server, listen on ~30 different channels, push all the updates through some code that helped visualize the data in some way, and then get this data out to the browser. I had heard good things about <a href="http://nodejs.org">node</a> for high concurrency network programming from several people. I ran across a node library called <a href="http://socket.io">socket.io</a> that reported to make it easy to stream updates from the server to the client, in a browser independent way, using a variety of <a href="http://socket.io/#browser-support">transport protocols</a>. Instinctively it felt like the <a href="http://en.wikipedia.org/wiki/Publish/subscribe">pub/sub</a> model would also be handy for connecting up the IRC updates with the webapp. I had been wanting to play around with the pub/sub features in <a href="http://redis.io">redis</a> for some time, and since there is a nice <a href="https://github.com/mranney/node_redis">redis library</a> for node I decided to give it a try.

Like many web developers I am used to writing JavaScript for the browser. Tools like <a href="http://jquery.com">jQuery</a> and <a href="http://documentcloud.github.com/underscore/">underscore.js</a> successfully raised the bar to the point that I'm able to write JavaScript and still look myself in the mirror in the morning. But I was still a bit skeptical about JavaScript running on the server side. The thing I didn't count on was how well node's event driven model, the library support (socket.io, redis, express), and the functional programming style fit the domain of making the Wikipedia update stream available on the Web.

For example here's is the code to connect to the ~30 IRC chatrooms stored in the <code>channels</code> variable, and send all the messages to a function <code>processMessage</code>:

<pre lang="javascript" line="1">
var client = new irc({server: 'irc.wikimedia.org', nick: config.ircNick});

client.connect(function () {
  client.join(channels);
  client.on('privmsg', processMessage);
});
</pre>

The <code>processMessage</code> function then parses the IRC message into a JavaScript dictionary and publishes it to a 'wikipedia' channel in redis:

<pre lang="javascript" line="1">
function processMessage (msg) {
  m = parse_msg(msg.params);
  redis.publish('wikipedia', m);
}
</pre>

Then over in my wikistream web application I set up socket.io so that when a browser goes to my webapp it negotiates for the best way to get updates from the server. Once a connection is established the server subscribes to the <code>wikipedia</code> channel and sends any updates it receives out to the browser. When the browser disconnects, the connection to redis is closed.

<pre lang="javascript" line="1">
var io = sio.listen(app);

io.sockets.on('connection', function(socket) {
  var updates = redis.createClient();
  updates.subscribe('wikipedia');
  updates.on("message", function (channel, message) {
    socket.send(message);
  });
  socket.on('disconnect', function() {
    updates.quit();
  });
});
</pre>

Each update is represented as a JavaScript dictionary, which socket.io and node's redis client transparently serialize and deserialize. In order to understand the socket.io protocol a bit more, I wrote a little <a href="https://github.com/edsu/wikistream/blob/master/stream.py">python script</a> that connects to wikistream.inkdroid.org, negotiates for the xhr-polling transport, and prints out the stream JSON to the console. It's a demonstration of how a socket.io instance like wikistream can be used as an API for creating a firehose like service. Although I guess the example might've been a bit cleaner to negotiate for a websocket instead.

<pre lang="javascript" line="1">
{
  'anonymous': False,
  'comment': '/* Anatomy */  changed statement that orbit was the eye to saying that the orbit was the eye socket for accuracy',
  'delta': 7,
  'flag': '',
  'namespace': 'article',
  'newPage': False,
  'page': 'Optic nerve',
  'pageUrl': 'http://en.wikipedia.org/wiki/Optic_nerve',
  'robot': False,
  'unpatrolled': False,
  'url': 'http://en.wikipedia.org/w/index.php?diff=449570600&oldid=447889877',
  'user': 'Moearly',
  'userUrl': 'http://en.wikipedia.org/wiki/User:Moearly',
  'wikipedia': '#en.wikipedia',
  'wikipediaLong': 'English Wikipedia',
  'wikipediaShort': 'en',
  'wikipediaUrl': 'http://en.wikipedia.org'
}
</pre>

This felt so easy, it really made me re-evaluate everything I thought I knew about JavaScript. Plus it all became worth it when <a href="http://en.wikipedia.org/wiki/Ward_Cunningham">Ward Cunningham</a> (the creator of the Wiki idea) wrote on the <a href="http://www.mail-archive.com/wiki-research-l@lists.wikimedia.org/msg01267.html">wiki-research list</a>:

<blockquote>
I've written this app several times using technology from text-to-speech to quartz-composer. I have to tip my hat to Ed for doing a better job than I ever did and doing it in a way that he makes look effortless. Kudos to Ed for sharing both the page and the software that produces it. You made my morning.
</blockquote>

Ward is a personal hero of mine, so making his morning pretty much made my professional career. 

I guess this is all a long way of saying what many of you probably already know...the tooling around JavaScript (and especially <a href="http://nodejs.org">node</a>) has changed so much, that it really does offer a radically new programming environment, that is worth checking out, especially for network programming. The event driven model that is baked into node, and the fact that v8 runs blisteringly fast, make it possible to write apps that do a whole lot in one low memory process. This is handy when deploying an app to an EC2 mini instance or <a href="http://heroku.com">Heroku</a>, which is where <a href="http://wikipulse.herokuapp.com">wikipulse</a> is running...<em>for free</em>.

Of course it helped that my wife and kids got a kick out of <a href="http://wikistream.inkdroid.org">wikistream</a> and <a href="http://wikipulse.herokuapp.com">wikipulse</a>. I suspect that they think I'm a bit obsessed with Wikipedia, but that's ok ... because I kinda am.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3665</wp:post_id>
		<wp:post_date>2011-11-07 05:40:52</wp:post_date>
		<wp:post_date_gmt>2011-11-07 12:40:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>an-ode-to-node</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="node"><![CDATA[node]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85226</wp:comment_id>
			<wp:comment_author><![CDATA[kapilt]]></wp:comment_author>
			<wp:comment_author_email>kapilt@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://twitter.com/kapilvt</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-11-22 03:33:20</wp:comment_date>
			<wp:comment_date_gmt>2011-11-22 10:33:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[That's pretty awesome ed! I just did my first wikipedia hackathon last month, its a great group of folks.

Just for giggles, python's eventlet/gevent could give you the same scalability/concurrency and libraries without that callback aftertaste (also lua, but the lib support is more esoteric).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>421</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1321958000.8025";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:6:"kapilt";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1321960055.213";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85227</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-11-22 04:17:16</wp:comment_date>
			<wp:comment_date_gmt>2011-11-22 11:17:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey Kapil. The DC Wikipedia group are great. I haven't been able to attend much recently because the meetups are often on the weekend when it is harder for me to "get away". It's pretty awesome that <a href="http://wikimania2012.wikimedia.org/wiki/Main_Page" rel="nofollow">WikiMania</a> is going to happen here in DC next year!

I'll definitely take a look at <a href="http://eventlet.net/" rel="nofollow">eventlet</a>. The callback pattern does take some getting used to, and is often hard to read. But actually the thing that drove me to node.js for <a href="http://wikistream.inkdroid.org" rel="nofollow">wikistream</a> was <a href="http://socket.io/" rel="nofollow">SocketIO</a>. I see that there is a Python <a href="http://pypi.python.org/pypi/SocketTornad.IO" rel="nofollow">version</a> available for use on top of Tornado, but I haven't tried it.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1321960637.168";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>the digital repository marketplace</title>
		<link>http://inkdroid.org/2011/09/19/the-digital-repository-marketplace/</link>
		<pubDate>Mon, 19 Sep 2011 14:23:12 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3690</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The University of Southern California recently <a href="http://repository.usc.edu/">announced</a> its Digital Repository (USCDR) which is a joint venture between the <a href="http://dornsife.usc.edu/vhi/">Shoah Foundation Institute</a> and the <a href="http://www.usc.edu/">University of Southern California</a>. The site is quite an impressive brochure that describes the various services that their digital preservation system provides. But a few things struck me as odd. I was definitely pleased to see a prominent description of <a href="http://repository.usc.edu/web-access/">access services</a> centered on the Web:</p>

<blockquote>
The USCDR can provide global access to digital collections through an expertly managed, cloud-computing environment. With its own content distribution network (CDN), the repository can make a digital collection available around the world, securely, rapidly, and reliably. <strong>The USCDR’s CDN is an efficient, high-performance alternative to leading commercial content distribution networks.</strong> The USCDR’s network consists of a system of disk arrays that are strategically located around the world. Each site allows customers to upload materials and provides users with high-speed access to the collection. The network supports efficient content downloads and real-time, on-demand streaming. The repository can also arrange content delivery through commercial CDNs that specialize in video and rich media.
</blockquote>

<p>But from this description it seems clear that the USCDR is creating their own content delivery network, despite the fact that there is already a <a href="http://en.wikipedia.org/wiki/Content_delivery_network">good marketplace</a> for these services. I would have thought it would be more efficient for the USCDR to provide plugins for the various CDNs rather than go through the effort (and cost) of building out one themselves. Digital repositories are just a drop in the ocean of Web publishers that need fast and cheap delivery networks for their content. Does the USCDR really think they are going to be able to compete and innovate in this  marketplace? I'd also be kind of curious to see what public websites there are right now that are built on top of the USCDR.</p>

<p>Secondly, in the section on <a href="http://web.archive.org/web/20111112060505/http://repository.usc.edu:80/cataloging/">Cataloging</a> this segment jumped out at me:</p>

<blockquote>
The USC Digital Repository (USCDR) offers cost-effective cataloging services for large digital collections by applying a sophisticated system that tags groups of related items, making them easier to find and retrieve. The repository can convert archives of all types to indexed, searchable digital collections. The repository team then creates and manages searchable indices that are customized to reflect the particular nature of a collection.

The USCDR’s cataloging system employs <strong>patented software created by the USC Shoah Foundation Institute (SFI)</strong> that lets the customers define the basic elements of their collections, as well as the relationships among those elements. The repository’s control standards for metadata verify that users obtain consistent and accurate search results. The repository also supports the use of any standard thesaurus or classification system, as well as the use of customized systems for special collections.</blockquote>

<p>I'm certainly not a patent expert, but doesn't it seem ill advised to build a digital preservation system around a patented technology? Sure, most of our running systems use possibly thousands of patented technologies, but ordinarily we are insulated from them by standards like <a href="http://en.wikipedia.org/wiki/POSIX">POSIX</a>, <a href="http://en.wikipedia.org/wiki/HTTP">HTTP</a>, or <a href="http://en.wikipedia.org/wiki/Internet_Protocol_Suite">TCP/IP</a> that allow us to swap out various technologies for other ones. If the particular technique to cataloging built into the USCDR is protected by a patent for 20 years, won't that limit the dissemination of the technique into other digital preservation systems, and ultimately undermine the ability of people to move their content in and out of digital preservation systems as they become available--what <a href="http://www.alexandria.ucsb.edu/~gjanee/">Greg Janée</a> calls <a href="http://www.ijdc.net/index.php/ijdc/article/view/102/77">relay supporting archives</a>. I guess without more details of the patented technology it's hard to say, but I would be worried about it.</p>

<p>After working in this repository space for a few years I guess I've become pretty jaded about turnkey digital repository systems that say they do it all. Not that it's impossible, but it always seems like a risky leap for an organization to take. I guess I'm also a software developer, which adds quite a bit of bias. But on the other hand it's great to see a repository systems that are beginning to address the basic concerns raised by the <a href="http://brtf.sdsc.edu/">Blue Ribbon Task Force on Sustainable Digital Preservation and Access</a>, which identified the need for building sustainable models for digital preservation. The <a href="http://www.cdlib.org/">California Digital Library</a> is doing something similar with its <a href="http://www.cdlib.org/services/uc3/merritt/">UC3 Merritt</a> system, which offers fee based curation services to the University of California (which USC is not part of).</p>

<p>Incidentally the service costs of USCDR and Merritt are quite difficult to compare. Merritt's <a href="http://www.cdlib.org/uc3/docs/Merritt-cost-calculator-v3.xlsx">Excel Calculator</a> says their cost is $1040 per TB per year (which is pretty straightforward, but doesn't seem to account for the degree to which the data is accessed). The USCDR <a href="http://web.archive.org/web/20111211211652/http://repository.usc.edu:80/pricing/">is listed</a> as $70/TB per month for Disk-based File-Server Access, and $1000/TB for 20 years for Preservation Services. That would seem indicate the raw storage is a bit less than Merritt at $840.00 per TB per year. But what the preservation services are, and how the 20 year cost would be applied over a growing collection of content seems unclear to me. Perhaps I'm misinterpreting <em>disk-based file-server access</em>, which might actually refer to terabytes of data sent outside their USCDR CDN. In that case the $70/TB measures up quite nicely with a <a href="http://calculator.s3.amazonaws.com/calc5.html">recent quote</a> from Amazon S3 at $120.51 per terabyte transferred out per month. But again, does USCDR really think it can compete in the cloud storage space?</p>

<p>Based on the current pricing models, where there are no access driven costs, the USCDR and Merritt might find a lot of clients outside of the traditional digital repository ecosystem (I'm thinking online marketing or pornography) that have images they would like to serve at high volume for no cost other than the disk storage. That was my bad idea of a joke, if you couldn't tell. But seriously I sometimes worry that digital repository systems are oriented around the functionality of a dark archive, where lots of data goes in, and not much data comes back out for access.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3690</wp:post_id>
		<wp:post_date>2011-09-19 07:23:12</wp:post_date>
		<wp:post_date_gmt>2011-09-19 14:23:12</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-digital-repository-marketplace</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="access"><![CDATA[access]]></category>
		<category domain="post_tag" nicename="california"><![CDATA[california]]></category>
		<category domain="post_tag" nicename="cdl"><![CDATA[cdl]]></category>
		<category domain="post_tag" nicename="cdn"><![CDATA[cdn]]></category>
		<category domain="post_tag" nicename="markets"><![CDATA[markets]]></category>
		<category domain="post_tag" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="usc"><![CDATA[usc]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>stepping backwards</title>
		<link>http://inkdroid.org/2011/09/22/stepping-backwards/</link>
		<pubDate>Fri, 23 Sep 2011 03:42:46 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3723</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://bibwild.wordpress.com/about/">Jonathan Rochkind</a> recently wrote a good <a href="http://bibwild.wordpress.com/2011/09/22/alternate-format-microdata/">blog post</a> about using <a href="http://dev.w3.org/html5/md/">HTML5 Microdata</a> to help citation managers like <a href="http://www.mendeley.com/">Mendeley</a> and <a href="http://www.zotero.org/">Zotero</a> discover citation metadata that is available in formats such as <a href="http://en.wikipedia.org/wiki/RIS_(file_format)">RIS</a>. It's an excellent and detailed complement to Eric Hellman's <a href="http://go-to-hellman.blogspot.com/2011/07/spoonfeeding-library-data-to-search.html">piece</a> on the same subject.</p>

<p>I contributed to the <a href="http://unapi.info">unAPI</a> effort 5 years ago, which aimed to fix the same problem: making citation metadata available to browsers. I wrote the <a href="http://validator.unapi.info/">unAPI validator</a> which helped implementors confirm they were doing things right, <a href="http://www.ariadne.ac.uk/issue48/chudnov-et-al/">articles</a> were written, and we saw implementations in software such as the opensource integrated library system <a href="http://open-ils.org/">Evergreen</a> and the popular citation manager <a href="http://zotero.org">Zotero</a>, which at one point looked first for unAPI metadata in pages...perhaps it still does.</p>

<p>As Jonathan points out, there are some issues with unAPI, such as <a href="http://microformats.org/wiki/abbr-design-pattern#Accessibility_issues">accessibility problems</a> around Microformats in general, which unAPI was partly modeled on. <a href="http://dev.w3.org/html5/md/">HTML5 Microdata</a> and <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a> weren't around when we were working on unAPI, so I think Jonathan is right that it definitely makes sense to think about using these technologies nowadays instead of unAPI when making structured metadata available in HTML. I personally think the same thing goes for <a href="http://en.wikipedia.org/wiki/COinS">COinS</a> where OpenURL key value pairs are used to express the metadata. Companies like Google, Microsoft, Yahoo and Facebook actively scrape HTML5 Microdata and RDFa, and there are vocabularies for describing books and articles. And because these technologies are deployed wider than the small niche that libraries occupy, they fit the Web better.</p>

<p>But there is a fair bit of turmoil in the structured-data-on-the-Web space. Today's <a href="https://www.facebook.com/f8?sk=app_283743208319386">F8</a> product announcements seemed to indicate that Facebook is <a href="https://developers.facebook.com/docs/beta/">deepening its use</a> of the <a href="https://developers.facebook.com/docs/opengraph/">OpenGraphProtocol</a>, which is their rebranding of RDFa. We're seeing the <a href="http://www.iptc.org">International Press Telecommunications Council</a> standardizing <a href="http://dev.iptc.org/rNews">rNews</a> as an RDFa vocabulary for expressing online news metadata. And meanwhile Google, Microsoft and Yahoo are continuing to work on schema.org Microdata vocabularies. The recent <a href="http://semanticweb.com/schema-org-workshop-a-path-forward_b23387">Schema.org Workshop</a> seems to anticipate significant changes in that space in the near future, particularly regarding the output of the W3C <a href="http://www.w3.org/2001/sw/interest/webschema.html">Web Schema</a> and <a href="http://www.w3.org/wiki/Html-data-tf">HTML Data</a> task forces.</p>

<p>At <a href="http://lod-lam.net/summit/2011/09/16/525/">LODLAM-DC</a> we had a good conversation about RDFa, Microdata, Microformats and JSON publishing options for the cultural heritage sector. Perhaps I was just projecting, but it seemed like there was a fair bit of uncertainty about which to use. At the end of the day it seems like making your decisions based on things you want to enable is a good way forward. Are you trying to get your content to show up nicely on Facebook or Google--or both?</p>

<p>...or are you trying to do something else, like advertise some RIS citation metadata that is related to an HTML page so a citation manager can pick it up?</p>

<p>Even before the pixels had dried on the first version of the unAPI spec I was left with the nagging feeling that it had missed the point. I felt like we hadn't really used the mechanics of the Web that were already there, and had sort of inadvertently succumbed to how standards development would be lampooned later by XKCD:</p>

<div style="text-align: center;"><a href="http://xkcd.com/927/"><img src="http://imgs.xkcd.com/comics/standards.png"/></a></div>

<p>Specifically, I felt like we could have documented an even simpler pattern, namely using a &lt;link&gt; or &lt;a&gt; elements in conjunction with the <em>rel</em> and <em>type</em> parameters. So if you have a search result that is available as RIS, why not add this to your &lt;head&gt; element:</p>

<pre lang="html">
<link rel="alternate" type="application/x-research-info-systems" href="/search?q=cartoons&format=ris" />
</pre>

<p>My IRC conversation with Jonathan about his blog post was rolling around in my head when this Kurt Vonnegut quote went by in my Twitter stream:</p>

<div style="text-align: center"><a href="https://twitter.com/Kurt_Vonnegut/status/116889502353588226"><img src="http://inkdroid.org/images/step-backward.png" style="border: thin gray solid;"/></a></div>

<p>It seemed oddly appropriate given the uncertainty in the structured-data-on-the-web marketplace, and some missteps with unAPI. If all we want to do is replace unAPI with something easier and more web-friendly, then why not fall back on basic functionality that has been in HTML for years?</p>

<p>If you want to make structured metadata available directly in HTML, sure HTML5 Microdata and RDFa are important technologies to use. But if all you want to do is link to an external metadata file I personally think the scholarly community would be better served by a simpler and less controversial approach.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3723</wp:post_id>
		<wp:post_date>2011-09-22 20:42:46</wp:post_date>
		<wp:post_date_gmt>2011-09-23 03:42:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>stepping-backwards</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="citations"><![CDATA[citations]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="html5"><![CDATA[html5]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="microdata"><![CDATA[microdata]]></category>
		<category domain="post_tag" nicename="microformats"><![CDATA[microformats]]></category>
		<category domain="post_tag" nicename="rdfa"><![CDATA[rdfa]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:5:{i:0;s:5:"85118";i:1;s:5:"85119";i:2;s:5:"85120";i:3;s:5:"85121";i:4;s:5:"85124";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85124</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-27 21:16:11</wp:comment_date>
			<wp:comment_date_gmt>2011-09-28 04:16:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I started to write a comment, but as is my habit, it ended up turning into a lengthy essay of it's own here: 

http://bibwild.wordpress.com/2011/09/28/more-on-microdata-vs-rel-attribute/]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1317183371.6794";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85122</wp:comment_id>
			<wp:comment_author><![CDATA[More on microdata vs &#8216;rel&#8217; attribute | Bibliographic Wilderness]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/2011/09/28/more-on-microdata-vs-rel-attribute/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-27 21:15:42</wp:comment_date>
			<wp:comment_date_gmt>2011-09-28 04:15:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Ed Summers for referencing my post suggesting a microdata solution to &#8216;alternate format&#8217; advertisement for [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1317183343.4318";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1317211250.617";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85121</wp:comment_id>
			<wp:comment_author><![CDATA[Chris Adams]]></wp:comment_author>
			<wp:comment_author_email>chris@improbable.org</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawnjYt4eA4hzgDgYRfMpqMFitgKISVvzTc8</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-23 08:09:06</wp:comment_date>
			<wp:comment_date_gmt>2011-09-23 15:09:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[To expand Jakob's comment, I always recommend trying to imagine being a freelance or internal webdev: why would I go to my client and suggest they pay me to add this feature? I can make that case for SEO, for Google/Facebook/Twitter sharing, etc. For complex XML metadata cathedrals it's a lot harder to make the case that we should spend an enormous amount of time implementing a cumbersome spec in the hope that it will prove useful in the future.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>354</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1316790548.2634";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawnjYt4eA4hzgDgYRfMpqMF";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85120</wp:comment_id>
			<wp:comment_author><![CDATA[jakoblog.de/]]></wp:comment_author>
			<wp:comment_author_email>jakob@nichtich.de</wp:comment_author_email>
			<wp:comment_author_url>http://jakoblog.de/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-23 06:37:05</wp:comment_date>
			<wp:comment_date_gmt>2011-09-23 13:37:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[You wrote: "At the end of the day it seems like making your decisions based on things you want to enable is a good way forward. Are you trying to get your content to show up nicely on Facebook or Google–or both? …or are you trying to do something else, like advertise some RIS citation metadata that is related to an HTML page so a citation manager can pick it up?" That's the point. Most discussions about which technologies to use, miss a clear goal. In the end most standards are just defined by concrete applications: we want to make data accessible to to Mendeley and Zotero, Google, Facebook etc. - so we must give it in forms they want, no matter how ill-designed these forms are.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>337</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1316785027.7173";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:11:"jakoblog.de";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85119</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>christophergutteridge@googlemail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawm0m65jY9yTh8OhKSSTpRjD1IAlOJI7awA</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-23 01:38:35</wp:comment_date>
			<wp:comment_date_gmt>2011-09-23 08:38:35</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[(obviously schema.org has the rather killer SEO as it's ROI.)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>406</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1316767115.9903";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawm0m65jY9yTh8OhKSSTpRj";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1316775058.9711";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85118</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>christophergutteridge@googlemail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawm0m65jY9yTh8OhKSSTpRjD1IAlOJI7awA</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-09-23 01:34:29</wp:comment_date>
			<wp:comment_date_gmt>2011-09-23 08:34:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I think it's really important to give people a reason to create data, and an immediate ROI for doing so...

eg. javascript &amp; PHP libraries which augment your site, validators, services which consume your data and do something neat with it which you can try out right away.

Unless the company gets something in return, why waste the time and bandwidth on this stuff the academics waffle on about but doesn't make you money? (we need quick easy-to-understand answers to that question)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>406</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1316775059.0887";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1316766870.4929";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawm0m65jY9yTh8OhKSSTpRj";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85338</wp:comment_id>
			<wp:comment_author><![CDATA[The Code4Lib Journal – HTML5 Microdata and Schema.org | Programmer Solution]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.programsolution.info/the-code4lib-journal-html5-microdata-and-schema-org.html</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-02-03 09:49:50</wp:comment_date>
			<wp:comment_date_gmt>2012-02-03 16:49:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] is to link your HTML presentation to an alternate representation of your data. Here’s a simple example of making RIS formatted citation data [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1328287790.7082";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1328290359.8443";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>day of digital archives psa</title>
		<link>http://inkdroid.org/2011/10/05/day-of-digital-archives-psa/</link>
		<pubDate>Thu, 06 Oct 2011 06:27:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3776</guid>
		<description></description>
		<content:encoded><![CDATA[Today is <a href="http://dayofdigitalarchives.blogspot.com/">Day of Digital Archives</a> day and I had this semi-thoughtful post written up about <a href="http://en.wikipedia.org/wiki/BagIt">BagIt</a> and how it's a brain dead simple format to use to package up your files so that you'll know if you still have them 5 minutes, 5 hours, 5 days, 5 years, maybe even 5 decades from now--if the notion of directories and files persists that long. 

But I deleted that...you're welcome...

I was also going to write about how in a fit of web <a href="http://en.wikipedia.org/wiki/Performance_art">performance art</a> <a href="http://en.wikipedia.org/wiki/Mark_Pilgrim_(software_developer)">Mark Pilgrim</a> recently deleted his online presence, including various extremely useful opensource tools, and several popular online books, only to see them re-materialize on the Web at new locations.

But I deleted most of that too...you're welcome again!

Here's a public service announcement instead. If you happen to use Franco Lazzarino's <a href="https://github.com/tipr/bagit">Ruby BagIt Library</a> to create bags that contains largish files (> 500MB), you might have accidentally created bad SHA1 manifests. I added a test, and fixed the bug with help from <a href="http://twitter.com/anarchivist">Mark Matienzo</a> and <a href="https://twitter.com/mbklein">Michael Klein</a>, and sent a <a href="https://github.com/tipr/bagit/pull/4">pull request</a>. It hasn't been applied yet, so here's to hoping it will.

At $mpow we've been getting terabytes of data from this social media company that has been bagging their data using this Ruby library. Many of the files are multi-gigabytes gzip compressed. And many of the bags now have bad SHA1 manifests. The social media company wasn't sure what the problem was, and told us just to ignore the SHA1 manifests. Which is easy enough to do.

It seems like no matter how simple the spec, it's easy to create bugs. If you create bags, throw <code>Bag-Software-Agent</code> into your bag-info.txt...you never know who might find it useful.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3776</wp:post_id>
		<wp:post_date>2011-10-05 23:27:04</wp:post_date>
		<wp:post_date_gmt>2011-10-06 06:27:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>day-of-digital-archives-psa</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bagit"><![CDATA[bagit]]></category>
		<category domain="post_tag" nicename="github"><![CDATA[github]]></category>
		<category domain="category" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="post_tag" nicename="opensource"><![CDATA[opensource]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="ruby"><![CDATA[ruby]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[youre-welcome]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[day-of-digital-archives]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{i:0;s:5:"85137";i:1;s:5:"85138";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85137</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>escowles@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawnRoh7CeQbGMWiVdLwVBme63nx9rWs4fac</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-10-06 05:51:27</wp:comment_date>
			<wp:comment_date_gmt>2011-10-06 12:51:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[thank you.  i'd seen your exchange about the different versions of the ruby bagit library, and hadn't gotten the message that i needed to add a Bag-Software-Agent field in the bags i'm generating.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>412</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1317905487.4354";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawnRoh7CeQbGMWiVdLwVBme";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1317958833.3542";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85138</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-10-06 10:52:05</wp:comment_date>
			<wp:comment_date_gmt>2011-10-06 17:52:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Oh man, "just ignore it" makes the SHA1 hash useless, which, since you don't know on which files it should be ignored and which it shouldn't, makes it unreliable on your whole corpus, which kind of seriously reduces the preservation capacity of your entire archive. 

It's sort of like ignoring security warnings in your browser. If you ignore them, then you don't have security anymore. Which may be a trade-off you're willing to take as an individual user when having security is way too annoying and all you're going to potentially harm is your own workstation. 

But a preservation archive where you can't tell if your data has been corrupted or not?  I'm not sure why anyone saying "ignore it" is using BagIt in the first place then!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1317923526.9513";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85139</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-10-06 11:52:35</wp:comment_date>
			<wp:comment_date_gmt>2011-10-06 18:52:35</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yes, I should've added that I plan on deleting the invalid manifests. We still have manifest-md5.txt files that are legit, and will suffice I think. Also, having a record of the files that are in the bag is still minimally useful I suppose--even if their fixity is incorrect. ]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1317927156.0882";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>thanks Wikipedia</title>
		<link>http://inkdroid.org/2011/11/15/thanks-wikipedia/</link>
		<pubDate>Tue, 15 Nov 2011 16:52:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3821</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://wikimediafoundation.org/wiki/Support_Wikipedia/en"><img border="0" alt="Support Wikipedia" src="//upload.wikimedia.org/wikipedia/commons/4/4b/Fundraising_2009-square-treasure-en.png" style="float: left; margin-right: 10px; border: none;"/></a>
The field of software development, the Web and libraries is changing so fast that there is no way to know everything I need to know to do my job well. Wikipedia continues to be an essential resource for learning about technologies, algorithms, people, and history related to my work. It's hard to imagine what the world would be like without it. <a href="http://wikimediafoundation.org/wiki/Support_Wikipedia/en">Thanks</a> for another year of awesome Wikipedia! The check is in the mail; well OK it's actually coming from PayPal ... you know the drill.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3821</wp:post_id>
		<wp:post_date>2011-11-15 09:52:23</wp:post_date>
		<wp:post_date_gmt>2011-11-15 16:52:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>thanks-wikipedia</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="donations"><![CDATA[donations]]></category>
		<category domain="post_tag" nicename="non-profits"><![CDATA[non-profits]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikimedia"><![CDATA[wikimedia]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[no]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Visualizing FRBR Worksets</title>
		<link>http://inkdroid.org/2011/11/12/visualizing-frbr-worksets/</link>
		<pubDate>Sun, 13 Nov 2011 03:55:51 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=3916</guid>
		<description></description>
		<content:encoded><![CDATA[<img title="More on this Venn diagram down below" src="https://chart.googleapis.com/chart?chs=125x75&cht=v&chd=t:70.0,87.5,22.5,23,8,9,8&chco=77FF77,7777FF,FF7777" style="float: left;"/>
The model behind the <a href="http://en.wikipedia.org/wiki/Functional_Requirements_for_Bibliographic_Records">Functional Requirements for Bibliographic Records</a> (FRBR) was <a href="http://www.ifla.org/en/publications/functional-requirements-for-bibliographic-records">published</a> over 10 years ago, and has been simmering in library land ever since. Bit by bit, FRBR has been finding its way into library systems and software, sometimes in a slightly modified form. But it has been slow going because FRBR offers a more nuanced view of bibliographic data than what is available in our legacy MARC data. So the FRBR relationships <em>we want</em> largely have to be teased out of the data <em>we have</em>.

<div style="float: right; margin-left: 10px;">
<br />
<img src="http://inkdroid.org/images/neuromancer1.jpg" width="100" />
<br />
<br />
<img src="http://inkdroid.org/images/neuromancer2.jpg" width="100p" />
</div>

One of the primary things that FRBR offers is the notion of a Work that groups together Expressions and Manifestations. For example, <a href="http://openlibrary.org/authors/OL26283A/William_F._Gibson">William Gibson</a> wrote a book <a href="http://openlibrary.org/works/OL27258W/Neuromancer">Neuromancer</a>, which has been translated into many languages, and is available from multiple publishers. Collectors are sometimes interested in specific editions of a book, say a first edition printing; but readers are often interested in any edition of a work, because they don't particularly care what's on the cover, or what pagination or typeface is used. FRBR provides a conceptual model for working with books in this way. For the software developer FRBR also holds out the promise of a normalized view of book data, where some things, such as the author and subject of the book can be expressed in one place (as attributes of the Work) rather than repeated for all the Expressions and Manifestations.

If you are a bibliographic data aficionado, you are probably already familiar with FRBR-ization Web services like <a href="http://www.worldcat.org/affiliate/webservices/xisbn/app.jsp">xISBN</a> and <a href="http://www.librarything.com/wiki/index.php/LibraryThing_APIs">ThingISBN</a> that make it possible to determine other related editions, or the workset, for a given ISBN. So to look up the 1995 Ace Books printing of Neuromancer (0441569595) at xISBN you can GET a URL like <a href="http://xisbn.worldcat.org/webservices/xid/isbn/0441569595?method=getEditions&amp;format=xml">http://xisbn.worldcat.org/webservices/xid/isbn/0441569595?method=getEditions&amp;format=xml</a> and get back some XML like:

<pre language="xml" style="height: 200px;">
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;rsp xmlns="http://worldcat.org/xid/isbn/" stat="ok"&gt;
  &lt;isbn&gt;0441569595&lt;/isbn&gt;
  &lt;isbn&gt;0441569579&lt;/isbn&gt;
  &lt;isbn&gt;0441012035&lt;/isbn&gt;
  &lt;isbn&gt;0006480411&lt;/isbn&gt;
  &lt;isbn&gt;1570420599&lt;/isbn&gt;
  &lt;isbn&gt;0007119585&lt;/isbn&gt;
  &lt;isbn&gt;0736638369&lt;/isbn&gt;
  &lt;isbn&gt;0441569587&lt;/isbn&gt;
  &lt;isbn&gt;1570421560&lt;/isbn&gt;
  &lt;isbn&gt;9029042478&lt;/isbn&gt;
  &lt;isbn&gt;229000619X&lt;/isbn&gt;
  &lt;isbn&gt;415010672X&lt;/isbn&gt;
  &lt;isbn&gt;0307969940&lt;/isbn&gt;
  &lt;isbn&gt;0441569560&lt;/isbn&gt;
  &lt;isbn&gt;569700124X&lt;/isbn&gt;
  &lt;isbn&gt;5792101205&lt;/isbn&gt;
  &lt;isbn&gt;2707115622&lt;/isbn&gt;
  &lt;isbn&gt;7542818732&lt;/isbn&gt;
  &lt;isbn&gt;229030820X&lt;/isbn&gt;
  &lt;isbn&gt;2744139157&lt;/isbn&gt;
  &lt;isbn&gt;0932096417&lt;/isbn&gt;
  &lt;isbn&gt;3453313895&lt;/isbn&gt;
  &lt;isbn&gt;1616577843&lt;/isbn&gt;
  &lt;isbn&gt;9607002504&lt;/isbn&gt;
  &lt;isbn&gt;8445072897&lt;/isbn&gt;
  &lt;isbn&gt;0002252325&lt;/isbn&gt;
  &lt;isbn&gt;8842907464&lt;/isbn&gt;
  &lt;isbn&gt;9029049367&lt;/isbn&gt;
  &lt;isbn&gt;8445075950&lt;/isbn&gt;
  &lt;isbn&gt;9029050748&lt;/isbn&gt;
  &lt;isbn&gt;8071930482&lt;/isbn&gt;
  &lt;isbn&gt;0586066454&lt;/isbn&gt;
  &lt;isbn&gt;7542824139&lt;/isbn&gt;
  &lt;isbn&gt;9119027818&lt;/isbn&gt;
  &lt;isbn&gt;8085601273&lt;/isbn&gt;
  &lt;isbn&gt;0441000681&lt;/isbn&gt;
  &lt;isbn&gt;8445070843&lt;/isbn&gt;
  &lt;isbn&gt;8385784012&lt;/isbn&gt;
  &lt;isbn&gt;8982738851&lt;/isbn&gt;
  &lt;isbn&gt;3893111387&lt;/isbn&gt;
  &lt;isbn&gt;807193318X&lt;/isbn&gt;
  &lt;isbn&gt;5170198892&lt;/isbn&gt;
  &lt;isbn&gt;8371500432&lt;/isbn&gt;
  &lt;isbn&gt;8467426373&lt;/isbn&gt;
  &lt;isbn&gt;0441007465&lt;/isbn&gt;
  &lt;isbn&gt;057503470X&lt;/isbn&gt;
  &lt;isbn&gt;8585887907&lt;/isbn&gt;
  &lt;isbn&gt;3893111379&lt;/isbn&gt;
  &lt;isbn&gt;911300347X&lt;/isbn&gt;
  &lt;isbn&gt;8422672596&lt;/isbn&gt;
  &lt;isbn&gt;9118721826&lt;/isbn&gt;
  &lt;isbn&gt;3453056655&lt;/isbn&gt;
  &lt;isbn&gt;3807703098&lt;/isbn&gt;
  &lt;isbn&gt;8390021439&lt;/isbn&gt;
  &lt;isbn&gt;8203203329&lt;/isbn&gt;
  &lt;isbn&gt;8789586735&lt;/isbn&gt;
  &lt;isbn&gt;8485752414&lt;/isbn&gt;
  &lt;isbn&gt;9612310203&lt;/isbn&gt;
  &lt;isbn&gt;8445074059&lt;/isbn&gt;
  &lt;isbn&gt;8445076620&lt;/isbn&gt;
  &lt;isbn&gt;8974271419&lt;/isbn&gt;
  &lt;isbn&gt;3453403851&lt;/isbn&gt;
  &lt;isbn&gt;9510172049&lt;/isbn&gt;
  &lt;isbn&gt;8758804110&lt;/isbn&gt;
  &lt;isbn&gt;9510193062&lt;/isbn&gt;
  &lt;isbn&gt;2277223255&lt;/isbn&gt;
  &lt;isbn&gt;9637632050&lt;/isbn&gt;
  &lt;isbn&gt;9755760326&lt;/isbn&gt;
  &lt;isbn&gt;3898132595&lt;/isbn&gt;
  &lt;isbn&gt;8790136292&lt;/isbn&gt;
  &lt;isbn&gt;8804516445&lt;/isbn&gt;
  &lt;isbn&gt;8842910686&lt;/isbn&gt;
&lt;/rsp&gt;
</pre>

<a href="http://librarything.com">LibraryThing</a> has a similar <a href="http://www.librarything.com/wiki/index.php/LibraryThing_APIs">API</a> call which allows you to splice the ISBN into a URL like so <a href="http://www.librarything.com/api/thingISBN/0441569595">http://www.librarything.com/api/thingISBN/0441569595</a>, and get:

<pre style="height: 200px;">
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;idlist&gt;
  &lt;isbn&gt;0441569595&lt;/isbn&gt;
  &lt;isbn&gt;0441012035&lt;/isbn&gt;
  &lt;isbn&gt;0006480411&lt;/isbn&gt;
  &lt;isbn&gt;0586066454&lt;/isbn&gt;
  &lt;isbn&gt;0441007465&lt;/isbn&gt;
  &lt;isbn&gt;0441000681&lt;/isbn&gt;
  &lt;isbn&gt;8585887907&lt;/isbn&gt;
  &lt;isbn&gt;0002252325&lt;/isbn&gt;
  &lt;isbn&gt;0441569560&lt;/isbn&gt;
  &lt;isbn&gt;3453056655&lt;/isbn&gt;
  &lt;isbn&gt;0441569579&lt;/isbn&gt;
  &lt;isbn&gt;0932096417&lt;/isbn&gt;
  &lt;isbn&gt;0441569587&lt;/isbn&gt;
  &lt;isbn&gt;057503470X&lt;/isbn&gt;
  &lt;isbn&gt;229030820X&lt;/isbn&gt;
  &lt;isbn&gt;8445070843&lt;/isbn&gt;
  &lt;isbn&gt;2277223255&lt;/isbn&gt;
  &lt;isbn&gt;3453313895&lt;/isbn&gt;
  &lt;isbn&gt;8804516445&lt;/isbn&gt;
  &lt;isbn&gt;9510193062&lt;/isbn&gt;
  &lt;isbn&gt;0007119585&lt;/isbn&gt;
  &lt;isbn&gt;8445075950&lt;/isbn&gt;
  &lt;isbn&gt;9119027818&lt;/isbn&gt;
  &lt;isbn&gt;9510172049&lt;/isbn&gt;
  &lt;isbn&gt;8842907464&lt;/isbn&gt;
  &lt;isbn&gt;1570420599&lt;/isbn&gt;
  &lt;isbn&gt;9637632050&lt;/isbn&gt;
  &lt;isbn&gt;9029042478&lt;/isbn&gt;
  &lt;isbn&gt;415010672X&lt;/isbn&gt;
  &lt;isbn&gt;9634970982&lt;/isbn&gt;
  &lt;isbn&gt;8085601273&lt;/isbn&gt;
  &lt;isbn&gt;0613922514&lt;/isbn&gt;
  &lt;isbn&gt;2707115622&lt;/isbn&gt;
  &lt;isbn&gt;8445074059&lt;/isbn&gt;
  &lt;isbn&gt;8842913529&lt;/isbn&gt;
  &lt;isbn&gt;1569564116&lt;/isbn&gt;
  &lt;isbn&gt;9118721826&lt;/isbn&gt;
  &lt;isbn&gt;8842910686&lt;/isbn&gt;
  &lt;isbn&gt;3898132595&lt;/isbn&gt;
  &lt;isbn&gt;1570421560&lt;/isbn&gt;
  &lt;isbn&gt;229000619X&lt;/isbn&gt;
  &lt;isbn&gt;3893111387&lt;/isbn&gt;
  &lt;isbn&gt;8071930482&lt;/isbn&gt;
  &lt;isbn&gt;2744139157&lt;/isbn&gt;
  &lt;isbn&gt;8445072897&lt;/isbn&gt;
  &lt;isbn&gt;8371500432&lt;/isbn&gt;
  &lt;isbn&gt;8576570491&lt;/isbn&gt;
  &lt;isbn&gt;8789586735&lt;/isbn&gt;
  &lt;isbn&gt;9639238023&lt;/isbn&gt;
  &lt;isbn&gt;3453074203&lt;/isbn&gt;
  &lt;isbn&gt;3893111379&lt;/isbn&gt;
  &lt;isbn&gt;0307969940&lt;/isbn&gt;
  &lt;isbn&gt;8203203329&lt;/isbn&gt;
  &lt;isbn&gt;8842906808&lt;/isbn&gt;
  &lt;isbn&gt;9752103677&lt;/isbn&gt;
  &lt;isbn&gt;0736638369&lt;/isbn&gt;
  &lt;isbn&gt;8324577750&lt;/isbn&gt;
  &lt;isbn&gt;8790136292&lt;/isbn&gt;
  &lt;isbn&gt;8778803438&lt;/isbn&gt;
  &lt;isbn&gt;807193318X&lt;/isbn&gt;
&lt;/idlist&gt;
</pre>

I don't actually know the mechanics of ThingISBN and xISBN in detail, but it's my understanding that xISBN uses an algorithm to unify works, whereas LibraryThing relies on people to connect things up. 

A newer player in this space is the <a href="http://openlibrary.org/dev/docs/restful_api#content">OpenLibrary API</a>. Instead of providing an ISBN -> ISBNs function, OpenLibrary make the editions for a given Work available using a URL like  <a href="http://openlibrary.org/works/OL27258W/editions.json?limit=50&amp;offset=0">http://openlibrary.org/works//works/OL27258W/editions.json?limit=50&amp;offset=0</a>. This requires you to know the OpenLibrary Work identifier (e.g. OL27258W). Fortunately you can look up their Work identifier using another REST call 
using the ISBN: <a href="http://openlibrary.org/api/books?bibkeys=ISBN:0441569595&amp;jscmd=details&amp;format=json">http://openlibrary.org/api/books?bibkeys=ISBN:0441569595&amp;jscmd=details&amp;format=json</a>. The OpenLibrary response includes a lot more information than the LibraryThing or xISBN results, which is way you are required to page through the results with the API, rather than getting all the results back at once:

<pre language="javascript" style="height: 400px;">
{
  "size": 19, 
  "links": {
    "self": "/works/OL27258W/editions.json?limit=50&offset=0", 
    "work": "/works/OL27258W"
  }, 
  "entries": [
    {
      "number_of_pages": 322, 
      "subtitle": "roman", 
      "series": [
        "Cyberspace trilogien", 
        "Gibsons Cyberspace trilogi -- 1"
      ], 
      "latest_revision": 3, 
      "edition_name": "2. udg./1. opl.", 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:1618994437:773"
      ], 
      "title": "Neuromantiker", 
      "work_titles": [
        "Neuromancer."
      ], 
      "languages": [
        {
          "key": "/languages/dan"
        }
      ], 
      "publish_country": "dk ", 
      "by_statement": "William Gibson ; p\u00e5 dansk ved Arne Herl\u00f8v Petersen.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 3, 
      "publishers": [
        "Per Kof"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-18T09:06:00.229423"
      }, 
      "key": "/books/OL17987798M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "Copenhagen"
      ], 
      "pagination": "322 p.", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-10-08T22:54:50.763681"
      }, 
      "notes": {
        "type": "/type/text", 
        "value": "Translation of: Neuromancer."
      }, 
      "identifiers": {
        "librarything": [
          "609"
        ]
      }, 
      "isbn_10": [
        "8790136292"
      ], 
      "publish_date": "1995", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "14771"
        ]
      }, 
      "subject_place": [
        "Japan"
      ], 
      "lc_classifications": [
        "PR9199.3.G514 N4x 1986"
      ], 
      "latest_revision": 4, 
      "edition_name": "1st Phantasia Press ed.", 
      "genres": [
        "Fiction."
      ], 
      "source_records": [
        "marc:marc_records_scriblio_net/part20.dat:107059645:825"
      ], 
      "title": "Neuromancer", 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "subjects": [
        "Computer hackers -- Fiction", 
        "Business intelligence -- Fiction", 
        "Information superhighway -- Fiction", 
        "Nervous system -- Wounds and injuries -- Fiction", 
        "Conspiracies -- Fiction", 
        "Japan -- Fiction"
      ], 
      "publish_country": "miu", 
      "by_statement": "William Gibson.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 4, 
      "publishers": [
        "Phantasia Press"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-07-31T08:19:43.878905"
      }, 
      "key": "/books/OL2154100M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "West Bloomfield, Mich"
      ], 
      "pagination": "vi, 231 p. ;", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-04-01T03:28:50.625462"
      }, 
      "lccn": [
        "88672297"
      ], 
      "number_of_pages": 231, 
      "isbn_10": [
        "0932096417"
      ], 
      "publish_date": "1986", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "826097"
        ]
      }, 
      "latest_revision": 4, 
      "source_records": [
        "marc:talis_openlibrary_contribution/talis-openlibrary-contribution.mrc:1128979384:559"
      ], 
      "title": "Neuromancer", 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "publish_country": "xxk", 
      "by_statement": "William Gibson.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 4, 
      "publishers": [
        "Voyager"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-19T09:28:46.010665"
      }, 
      "key": "/books/OL22822383M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "London"
      ], 
      "pagination": "317p. ;", 
      "created": {
        "type": "/type/datetime", 
        "value": "2009-01-04T10:04:32.718474"
      }, 
      "dewey_decimal_class": [
        "813.54"
      ], 
      "notes": {
        "type": "/type/text", 
        "value": "Originally published: [London]: Gollancz; 1984."
      }, 
      "number_of_pages": 317, 
      "isbn_10": [
        "0006480411"
      ], 
      "publish_date": "1995", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "number_of_pages": 316, 
      "latest_revision": 4, 
      "edition_name": "1a ed. en bolsillo.", 
      "source_records": [
        "marc:SanFranPL10/SanFranPL10.out:61656066:1111"
      ], 
      "title": "Neuromante", 
      "work_titles": [
        "Neuromancer."
      ], 
      "languages": [
        {
          "key": "/languages/spa"
        }
      ], 
      "subjects": [
        "Ciencia-ficci\u00f3n"
      ], 
      "publish_country": "sp ", 
      "by_statement": "William Gibson ; [traducci\u00f3n de Jos\u00e9 Arconada Rodr\u00edguez y Javier Ferreira Ramos].", 
      "oclc_numbers": [
        "50083763"
      ], 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 4, 
      "publishers": [
        "Minotauro"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-19T10:44:18.483562"
      }, 
      "key": "/books/OL23054075M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "Barcelona"
      ], 
      "pagination": "316 p. ;", 
      "created": {
        "type": "/type/datetime", 
        "value": "2009-02-18T07:02:41.481991"
      }, 
      "notes": {
        "type": "/type/text", 
        "value": "Translation of: Neuromancer.\n\nPremio Hugo.\n\nPremio Nebula.\n\nPremio Philip K. Dick."
      }, 
      "identifiers": {
        "librarything": [
          "609"
        ]
      }, 
      "isbn_10": [
        "8445072897"
      ], 
      "publish_date": "1997", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "1163291"
        ]
      }, 
      "latest_revision": 4, 
      "source_records": [
        "marc:talis_openlibrary_contribution/talis-openlibrary-contribution.mrc:1449506617:614"
      ], 
      "title": "Neuromancer", 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "publish_country": "enk", 
      "by_statement": "William Gibson.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 4, 
      "publishers": [
        "HarperCollins"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-19T09:38:30.187012"
      }, 
      "key": "/books/OL22849249M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "London"
      ], 
      "pagination": "277p.", 
      "created": {
        "type": "/type/datetime", 
        "value": "2009-01-07T20:05:13.391858"
      }, 
      "dewey_decimal_class": [
        "813.54"
      ], 
      "notes": {
        "type": "/type/text", 
        "value": "Originally published in Great Britain by Gollancz, 1984."
      }, 
      "number_of_pages": 277, 
      "isbn_10": [
        "0002252325"
      ], 
      "publish_date": "1994", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "publishers": [
        "Harper Collins"
      ], 
      "pagination": "317p. ;", 
      "source_records": [
        "marc:talis_openlibrary_contribution/talis-openlibrary-contribution.mrc:2979159053:556"
      ], 
      "title": "Neuromancer", 
      "dewey_decimal_class": [
        "813/.54"
      ], 
      "notes": {
        "type": "/type/text", 
        "value": "Originally published, London , Gollancz, 1984."
      }, 
      "number_of_pages": 317, 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-10-25T02:27:53.587823"
      }, 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-10-15T15:26:45.512262"
      }, 
      "latest_revision": 3, 
      "publish_country": "xxk", 
      "key": "/books/OL19969875M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_date": "1993", 
      "publish_places": [
        "London"
      ], 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ], 
      "type": {
        "key": "/type/edition"
      }, 
      "by_statement": "William Gibson.", 
      "revision": 3
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "2292560"
        ]
      }, 
      "subtitle": "Science Fiction Roman", 
      "series": [
        "Heyne science fiction & fantasy -- Bd. 06/4400"
      ], 
      "latest_revision": 4, 
      "edition_name": "3. Aufl.", 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:716896905:827"
      ], 
      "title": "Neuromancer", 
      "work_titles": [
        "Neuromancer."
      ], 
      "languages": [
        {
          "key": "/languages/ger"
        }
      ], 
      "publish_country": "gw ", 
      "by_statement": "William Gibson ; Deutsche \u00dcbersetzund von Reinhard Heinz.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 4, 
      "publishers": [
        "W. Heyne"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-18T03:53:57.235299"
      }, 
      "key": "/books/OL16064340M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "M\u00fcnchen"
      ], 
      "pagination": "363 p. :", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-09-22T02:36:53.194997"
      }, 
      "notes": {
        "type": "/type/text", 
        "value": "\"Deutsche Erstver\u00f6ffentlichung.\"\n\nTranslation of: Neuromancer."
      }, 
      "number_of_pages": 363, 
      "isbn_10": [
        "3453313895"
      ], 
      "publish_date": "1989", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "number_of_pages": 371, 
      "subject_place": [
        "Japan"
      ], 
      "covers": [
        284192
      ], 
      "lc_classifications": [
        "PS3557.I2264 N48 2004"
      ], 
      "latest_revision": 6, 
      "edition_name": "20th anniversary ed.", 
      "genres": [
        "Fiction."
      ], 
      "source_records": [
        "marc:marc_records_scriblio_net/part15.dat:26112823:924", 
        "marc:marc_loc_updates/v35.i20.records.utf8:16403653:1145"
      ], 
      "title": "Neuromancer", 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "subjects": [
        "Computer hackers -- Fiction", 
        "Business intelligence -- Fiction", 
        "Information superhighway -- Fiction", 
        "Nervous system -- Wounds and injuries -- Fiction", 
        "Conspiracies -- Fiction", 
        "Japan -- Fiction"
      ], 
      "publish_country": "nyu", 
      "by_statement": "William Gibson ; with a new introduction by the author ; with an afterword by Jack Womack.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 6, 
      "publishers": [
        "Ace Books"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-07-31T14:51:42.931650"
      }, 
      "key": "/books/OL3305354M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "New York"
      ], 
      "pagination": "xi, 371 p. ;", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-04-01T03:28:50.625462"
      }, 
      "dewey_decimal_class": [
        "813/.54"
      ], 
      "identifiers": {
        "goodreads": [
          "14770"
        ], 
        "librarything": [
          "609"
        ]
      }, 
      "lccn": [
        "2004048718"
      ], 
      "isbn_10": [
        "0441012035"
      ], 
      "publish_date": "2004", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "888628"
        ]
      }, 
      "subject_place": [
        "Japan"
      ], 
      "covers": [
        283860
      ], 
      "lc_classifications": [
        "PS3557.I2264 N48 2000"
      ], 
      "latest_revision": 5, 
      "edition_name": "Ace trade ed.", 
      "genres": [
        "Fiction."
      ], 
      "source_records": [
        "marc:marc_records_scriblio_net/part13.dat:153635745:885"
      ], 
      "title": "Neuromancer", 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "subjects": [
        "Computer hackers -- Fiction", 
        "Business intelligence -- Fiction", 
        "Information superhighway -- Fiction", 
        "Nervous system -- Wounds and injuries -- Fiction", 
        "Conspiracies -- Fiction", 
        "Japan -- Fiction"
      ], 
      "publish_country": "nyu", 
      "series": [
        "Ace science fiction"
      ], 
      "by_statement": "William Gibson ; with an afterword by Jack Womack.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 5, 
      "publishers": [
        "Ace Books"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-03T20:25:35.114363"
      }, 
      "key": "/books/OL3963678M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "New York"
      ], 
      "pagination": "276 p. ;", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-04-01T03:28:50.625462"
      }, 
      "dewey_decimal_class": [
        "813/.54"
      ], 
      "number_of_pages": 276, 
      "lccn": [
        "2001268016"
      ], 
      "isbn_10": [
        "0441007465"
      ], 
      "publish_date": "2000", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "122395"
        ]
      }, 
      "latest_revision": 4, 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:219836701:673"
      ], 
      "title": "Neuromancien", 
      "work_titles": [
        "Neuromancer."
      ], 
      "languages": [
        {
          "key": "/languages/fre"
        }
      ], 
      "publish_country": "fr ", 
      "by_statement": "William Gibson ; traduit de l'am\u00e9ricain par Jean Bonnefoy.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 4, 
      "publishers": [
        "\u00c9ditions J'ai lu"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-18T21:33:39.583788"
      }, 
      "key": "/books/OL21395048M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "Paris"
      ], 
      "pagination": "318 p.", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-11-02T11:15:35.318748"
      }, 
      "notes": {
        "type": "/type/text", 
        "value": "Translation of: Neuromancer."
      }, 
      "number_of_pages": 318, 
      "isbn_10": [
        "2277223255"
      ], 
      "publish_date": "1988", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "subtitle": "en sp\u00e6ndingsroman", 
      "latest_revision": 3, 
      "contributions": [
        "Mortensen, Hans Palle"
      ], 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:848159064:705"
      ], 
      "title": "Neuromantiker", 
      "work_titles": [
        "Neuromancer."
      ], 
      "languages": [
        {
          "key": "/languages/dan"
        }
      ], 
      "publish_country": "de ", 
      "by_statement": "William Gibson ; p\u00e5 dansk ved Hans Palle Mortensen.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 3, 
      "publishers": [
        "Vega"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-10-15T15:26:45.512262"
      }, 
      "key": "/books/OL16541408M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "[K\u00f8obenhavn]"
      ], 
      "pagination": "329 p.", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-09-24T15:45:30.569311"
      }, 
      "notes": {
        "type": "/type/text", 
        "value": "Translation of: Neuromancer."
      }, 
      "number_of_pages": 329, 
      "isbn_10": [
        "8758804110"
      ], 
      "publish_date": "1989", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "1163292"
        ]
      }, 
      "lc_classifications": [
        "PS3513.I2824"
      ], 
      "latest_revision": 4, 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:2715222992:644"
      ], 
      "title": "Neuromancer", 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "publish_country": "enk", 
      "by_statement": "by William Gibson.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 4, 
      "publishers": [
        "Gollancz"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-18T12:15:40.027146"
      }, 
      "key": "/books/OL19160947M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "London"
      ], 
      "pagination": "251 p. ;", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-10-21T06:38:01.937259"
      }, 
      "dewey_decimal_class": [
        "823/.914"
      ], 
      "number_of_pages": 251, 
      "isbn_10": [
        "057503470X"
      ], 
      "publish_date": "1984", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "number_of_pages": 273, 
      "latest_revision": 6, 
      "contributions": [
        "Cuijpers, Peter"
      ], 
      "edition_name": "1. druk.", 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:848175223:692"
      ], 
      "title": "Zenumagi\u00ebr", 
      "work_titles": [
        "Neuromancer."
      ], 
      "languages": [
        {
          "key": "/languages/dut"
        }
      ], 
      "publish_country": "ne ", 
      "by_statement": "William Gibson ; vertaling Peter Cuijpers.", 
      "oclc_numbers": [
        "64599048"
      ], 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 6, 
      "publishers": [
        "Meulenhoff"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2011-04-28T07:26:35.438655"
      }, 
      "key": "/books/OL16541422M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "Amsterdam"
      ], 
      "pagination": "273 p.", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-09-24T15:45:41.892954"
      }, 
      "notes": {
        "type": "/type/text", 
        "value": "Translation of: Neuromancer."
      }, 
      "identifiers": {
        "librarything": [
          "609"
        ]
      }, 
      "isbn_10": [
        "9029042478"
      ], 
      "publish_date": "1989", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "number_of_pages": 295, 
      "latest_revision": 6, 
      "contributions": [
        "Eggen, Torgrim, 1958-"
      ], 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:4064625723:790"
      ], 
      "title": "Nevromantiker", 
      "work_titles": [
        "Neuromancer."
      ], 
      "languages": [
        {
          "key": "/languages/nor"
        }
      ], 
      "publish_country": "no ", 
      "by_statement": "William Gibson ; oversatt av og med etterord av Torgrim Eggen.", 
      "oclc_numbers": [
        "224937105"
      ], 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 6, 
      "publishers": [
        "Aschehoug"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2011-04-25T21:45:39.581918"
      }, 
      "key": "/books/OL19726291M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "Oslo"
      ], 
      "pagination": "295 p.", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-10-23T17:52:44.936450"
      }, 
      "notes": {
        "type": "/type/text", 
        "value": "Translation of: Neuromancer."
      }, 
      "identifiers": {
        "librarything": [
          "609"
        ]
      }, 
      "isbn_10": [
        "8203203329"
      ], 
      "publish_date": "1999", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "publishers": [
        "Editrice Nord"
      ], 
      "pagination": "iii, 260 p.", 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:1419376120:645"
      ], 
      "title": "Neuromante", 
      "work_titles": [
        "Neuromancer."
      ], 
      "series": [
        "Cosmo -- 80"
      ], 
      "notes": {
        "type": "/type/text", 
        "value": "Translation of: Neuromancer."
      }, 
      "number_of_pages": 260, 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-09-28T17:38:21.398006"
      }, 
      "languages": [
        {
          "key": "/languages/ita"
        }
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-10-15T15:26:45.512262"
      }, 
      "latest_revision": 3, 
      "publish_country": "it ", 
      "key": "/books/OL17407456M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_date": "1986", 
      "publish_places": [
        "Milano"
      ], 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ], 
      "type": {
        "key": "/type/edition"
      }, 
      "by_statement": "William Gibson.", 
      "revision": 3
    }, 
    {
      "number_of_pages": 271, 
      "subject_place": [
        "Japan"
      ], 
      "covers": [
        284574
      ], 
      "lc_classifications": [
        "PS3557.I2264 N48 1984"
      ], 
      "latest_revision": 11, 
      "ocaid": "neuromancer00gibs", 
      "genres": [
        "Fiction."
      ], 
      "source_records": [
        "marc:marc_records_scriblio_net/part22.dat:84028207:784", 
        "marc:CollingswoodLibraryMarcDump10-27-2008/Collingswood.out:7879172:1418", 
        "marc:marc_cca/b10621386.out:20298617:552", 
        "ia:neuromancer00gibs"
      ], 
      "title": "Neuromancer", 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "subjects": [
        "Computer hackers -- Fiction", 
        "Business intelligence -- Fiction", 
        "Information superhighway -- Fiction", 
        "Nervous system -- Wounds and injuries -- Fiction", 
        "Conspiracies -- Fiction", 
        "Japan -- Fiction"
      ], 
      "publish_country": "nyu", 
      "by_statement": "William Gibson.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 11, 
      "publishers": [
        "Ace Books"
      ], 
      "ia_box_id": [
        "IA111402"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2011-08-12T04:31:24.064755"
      }, 
      "key": "/books/OL1627167M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "New York"
      ], 
      "pagination": "271 p. ;", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-04-01T03:28:50.625462"
      }, 
      "dewey_decimal_class": [
        "813/.54"
      ], 
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "22328"
        ]
      }, 
      "lccn": [
        "91174394"
      ], 
      "isbn_10": [
        "0441569595"
      ], 
      "publish_date": "1984", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "313982"
        ]
      }, 
      "subject_place": [
        "Japan"
      ], 
      "covers": [
        283491
      ], 
      "lc_classifications": [
        "PS3557.I2264 N48 1994"
      ], 
      "latest_revision": 5, 
      "edition_name": "1st Ace hardcover ed.", 
      "genres": [
        "Fiction."
      ], 
      "source_records": [
        "marc:marc_records_scriblio_net/part24.dat:178109658:845"
      ], 
      "title": "Neuromancer", 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "subjects": [
        "Computer hackers -- Fiction", 
        "Business intelligence -- Fiction", 
        "Information superhighway -- Fiction", 
        "Nervous system -- Wounds and injuries -- Fiction", 
        "Conspiracies -- Fiction", 
        "Japan -- Fiction"
      ], 
      "publish_country": "nyu", 
      "by_statement": "William Gibson.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 5, 
      "publishers": [
        "Ace Books"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-07-31T01:58:09.386680"
      }, 
      "key": "/books/OL1234381M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "New York"
      ], 
      "pagination": "278 p. ;", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-04-01T03:28:50.625462"
      }, 
      "dewey_decimal_class": [
        "813/.54"
      ], 
      "number_of_pages": 278, 
      "lccn": [
        "94237181"
      ], 
      "isbn_10": [
        "0441000681"
      ], 
      "publish_date": "1994", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "122395"
        ]
      }, 
      "latest_revision": 4, 
      "source_records": [
        "marc:talis_openlibrary_contribution/talis-openlibrary-contribution.mrc:718603618:565"
      ], 
      "title": "Neuromancien", 
      "work_titles": [
        "Neuromancer."
      ], 
      "languages": [
        {
          "key": "/languages/fre"
        }
      ], 
      "publish_country": "fr ", 
      "by_statement": "traduit de l'ame\u0301ricain par Jean Bonnefoy.", 
      "type": {
        "key": "/type/edition"
      }, 
      "revision": 4, 
      "publishers": [
        "J'ai Lu"
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-18T23:31:09.118145"
      }, 
      "key": "/books/OL21795410M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ], 
      "publish_places": [
        "[Paris]"
      ], 
      "pagination": "319p.", 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-11-04T00:30:01.234536"
      }, 
      "notes": {
        "type": "/type/text", 
        "value": "Translation of: Neuromancer."
      }, 
      "number_of_pages": 319, 
      "isbn_10": [
        "2277223255"
      ], 
      "publish_date": "1985", 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ]
    }, 
    {
      "publishers": [
        "Voyager"
      ], 
      "pagination": "317 p.", 
      "identifiers": {
        "librarything": [
          "609"
        ], 
        "goodreads": [
          "953070"
        ]
      }, 
      "revision": 4, 
      "source_records": [
        "marc:marc_university_of_toronto/uoft.marc:3986224271:616"
      ], 
      "title": "Neuromancer", 
      "isbn_10": [
        "0586066454"
      ], 
      "number_of_pages": 317, 
      "created": {
        "type": "/type/datetime", 
        "value": "2008-10-30T08:07:12.492696"
      }, 
      "languages": [
        {
          "key": "/languages/eng"
        }
      ], 
      "last_modified": {
        "type": "/type/datetime", 
        "value": "2010-08-18T17:29:49.077199"
      }, 
      "latest_revision": 4, 
      "edition_name": "Pbk. ed.", 
      "key": "/books/OL20872554M", 
      "authors": [
        {
          "key": "/authors/OL26283A"
        }
      ],
      "publish_date": "2000", 
      "publish_places": [
        "London"
      ], 
      "works": [
        {
          "key": "/works/OL27258W"
        }
      ], 
      "type": {
        "key": "/type/edition"
      }, 
      "by_statement": "William Gibson.", 
      "publish_country": "enk"
    }
  ]
}
</pre>

<img src="https://chart.googleapis.com/chart?chs=300x200&cht=v&chd=t:86.7469879518,72.2891566265,19.2771084337,49,16,15,15&chco=77FF77,7777FF,FF7777&chdl=xISBN|ThingISBN|OpenLibrary&chtt=Neuromancer (0441569595)" style="float: left; margin-right: 10px"/>Because of some work I've been doing helping out at <a href="http://gluejar.com">Gluejar</a> I became curious about the coverage of these three FRBR workset APIs. What sort of overlap is there between them? I wrote a little script <a href="https://github.com/edsu/worksvenn/blob/master/worksvenn.py">worksvenn.py</a> that takes one or more ISBNs as input, looks them up in the OpenLibrary, LibraryThing and OCLC APIs, and then outputs the resulting data with a Venn diagram using the <a href="http://code.google.com/apis/chart/image/docs/gallery/venn_charts.html">Google Chart API</a>.

It's interesting to see that each service has unique results. You can see these when you run <code>worksvenn.py</code> on the command line:

<pre>
Workset Results:

oclc: 7542818732,2707115622,7542824139,9029042478,8085601273,0441012035,
0441569579,229000619X,3893111387,2744139157,9607002504,8071930482,
9637632050,8585887907,8485752414,8758804110,8445076620,9118721826,
8203203329,0441569587,8804516445,8422672596,8789586735,0932096417,
3893111379,1570420599,8445072897,5792101205,9755760326,569700124X,
9510172049,0441007465,0736638369,9510193062,8390021439,911300347X,
8445075950,0002252325,0441569595,0441000681,5170198892,3807703098,
0007119585,415010672X,807193318X,3453056655,8974271419,8842910686,
9029050748,3898132595,3453313895,057503470X,1616577843,0307969940,
8385784012,2277223255,0006480411,9029049367,0586066454,1570421560,
8371500432,229030820X,8842907464,0441569560,9119027818,8445070843,
8467426373,9612310203,8790136292,8982738851,3453403851,8445074059

librarything: 0441569595,2707115622,0441000681,9634970982,9118721826,
9029042478,8085601273,3453056655,0006480411,8842906808,0441569579,
229000619X,415010672X,3893111387,0441012035,9639238023,3453074203,
9510193062,9637632050,8585887907,8842910686,0441007465,3898132595,
8203203329,1569564116,8371500432,3453313895,0736638369,057503470X,
8789586735,0932096417,9752103677,8445075950,8778803438,2277223255,
8576570491,8804516445,0613922514,0586066454,1570421560,3893111379,
229030820X,807193318X,8071930482,8842913529,0441569560,9119027818,
8445070843,0007119585,9510172049,2744139157,8324577750,8790136292,
0307969940,0441569587,8842907464,1570420599,8445072897,8445074059,
0002252325

openlibrary: 8758804110,0441569595,8203203329,3453313895,057503470X,
0932096417,9029042478,2277223255,0441000681,0006480411,0441012035,
0586066454,0002252325,8445072897,0441007465,8790136292

Differences:

oclc \ librarything: 7542818732,7542824139,5170198892,569700124X,
8974271419,9607002504,8485752414,9029050748,8758804110,8445076620,
8422672596,9612310203,1616577843,8385784012,9029049367,3453403851,
5792101205,3807703098,9755760326,8467426373,8982738851,8390021439,
911300347X

oclc \ openlibrary: 7542818732,2707115622,9118721826,5170198892,
0007119585,8085601273,8445070843,3453056655,0441569579,229000619X,
415010672X,3893111387,2744139157,8467426373,8974271419,9607002504,
8071930482,9637632050,8585887907,8485752414,8371500432,9029050748,
3898132595,8445076620,7542824139,0441569587,8982738851,8804516445,
8422672596,8789586735,9612310203,1616577843,0307969940,8385784012,
8842907464,9029049367,8842910686,1570421560,3893111379,229030820X,
807193318X,911300347X,0441569560,5792101205,9119027818,3807703098,
9755760326,569700124X,9510172049,8445074059,0736638369,9510193062,
8390021439,1570420599,8445075950,3453403851

librarything \ oclc:  8842906808,9634970982,8842913529,9639238023,
9752103677,1569564116,8778803438,8576570491,8324577750,0613922514,
3453074203

librarything \ openlibrary:  2707115622,9634970982,9118721826,
8085601273,3453056655,8842906808,0441569579,229000619X,415010672X,
3893111387,2744139157,9639238023,9510193062,9637632050,807193318X,
8585887907,8842910686,3898132595,8324577750,3893111379,8804516445,
1570420599,8789586735,9752103677,8778803438,8576570491,0613922514,
1570421560,8371500432,229030820X,3453074203,8071930482,8842913529,
0441569560,9119027818,8445070843,0007119585,9510172049,1569564116,
0736638369,0307969940,0441569587,8842907464,8445075950,8445074059

openlibrary \ oclc:  

openlibrary \ librarything:  8758804110
</pre>

This suggests that the workset data in these services actually reinforce each other, and a lot could be gained by sharing. For comparison here are the diagrams for a few more books:

<div style="text-align: center">
<img src="https://chart.googleapis.com/chart?chs=275x200&cht=v&chd=t:72.2044728435,54.6325878594,30.6709265176,109,62,57,48&chco=77FF77,7777FF,FF7777&chdl=xISBN|ThingISBN|OpenLibrary&chts=000000,10&chtt=Ulysses (0824043758)" style=""/><img src="https://chart.googleapis.com/chart?chs=275x200&cht=v&chd=t:48.1132075472,74.5283018868,45.2830188679,27,19,36,10&chco=77FF77,7777FF,FF7777&chdl=xISBN|ThingISBN|OpenLibrary&chts=000000,10&chtt=Handmaid's Tale (1853811742)" />
<img src="https://chart.googleapis.com/chart?chs=275x200&cht=v&chd=t:70.0,87.5,22.5,23,8,9,8&chco=77FF77,7777FF,FF7777&chdl=xISBN|ThingISBN|OpenLibrary&chts=000000,10&chtt=Gravity's Rainbow (0140106618)" style=""/><img src="https://chart.googleapis.com/chart?chs=275x200&cht=v&chd=t:10.1694915254,96.6101694915,16.9491525424,5,3,9,3&chco=77FF77,7777FF,FF7777&chdl=xISBN|ThingISBN|OpenLibrary&chts=000000,10&chtt=White Teeth (0375501851)" />
</div>

<img src="https://chart.googleapis.com/chart?chs=300x200&cht=v&chd=t:68.2382133995,70.306038048,16.8734491315,479,158,176,143&chco=77FF77,7777FF,FF7777&chdl=xISBN|ThingISBN|OpenLibrary" style="float: right"/> As I mentioned earlier, you can pass <code>worksvenn.py</code> a list of ISBNs and it will pool them all together. At Gluejar we have a list of 53 books that are examples of potential books for ungluing. so I ran these through and came up with this diagram.

Although looking on a piecemeal basis can be interesting, it would be fun to see a Venn diagram given a larger pool of seed ISBNs. Perhaps <code>worksvenn.py</code> will give you some ideas. If it does please let me know!

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3916</wp:post_id>
		<wp:post_date>2011-11-12 20:55:51</wp:post_date>
		<wp:post_date_gmt>2011-11-13 03:55:51</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>visualizing-frbr-worksets</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="frbr"><![CDATA[frbr]]></category>
		<category domain="post_tag" nicename="librarything"><![CDATA[librarything]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="post_tag" nicename="openlibrary"><![CDATA[openlibrary]]></category>
		<category domain="post_tag" nicename="visualization"><![CDATA[visualization]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85223</wp:comment_id>
			<wp:comment_author><![CDATA[Visualizing FRBR Worksets (inkdroid.org) &laquo; e-what?]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web.archive.org/web/20111117154018/http://www.e-what.net:80/958</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2011-11-13 04:24:44</wp:comment_date>
			<wp:comment_date_gmt>2011-11-13 11:24:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>[...] Visualizing FRBR Worksets http://inkdroid.org/2011/11/12/visualizing-frbr-worksets/ [...]</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1321183484.487";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1321185702.3915";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86755</wp:comment_id>
			<wp:comment_author><![CDATA[Perspective and Doing Good Work]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://ns4lib.com/lead-prototype/2012/01/11/perspective-and-doing-good-work/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-01-11 15:33:14</wp:comment_date>
			<wp:comment_date_gmt>2014-01-11 22:33:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] he appears to have some sort of working relationship with Library of Congress librarian/programmer Ed Summers as [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1389901605.9788448810577392578125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87327</wp:comment_id>
			<wp:comment_author><![CDATA[In the Library with the Lead Pipe &raquo; Perspective and Doing Good Work]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.inthelibrarywiththeleadpipe.org/2012/perspective-and-doing-good-work/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-04-22 14:01:13</wp:comment_date>
			<wp:comment_date_gmt>2015-04-22 21:01:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] he appears to have some sort of working relationship with Library of Congress librarian/programmer Ed Summers as [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1429740417.92037200927734375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1429736473.4376709461212158203125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>fascinating, hypnotic, inspirational, appalling, irrelevant</title>
		<link>http://inkdroid.org/2011/12/06/fascinating-hypnotic-inspirational-appalling-irrelevant/</link>
		<pubDate>Wed, 07 Dec 2011 06:36:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4050</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://inkdroid.org/images/wikistream-tweeters.png"><img src="http://inkdroid.org/images/wikistream-tweeters-small.png"/ style="float: left; margin: 10px; width:50%;"/></a></p>

<p>Thanks to everyone that noticed the <a href="http://wikistream.inkdroid.org">Wikistream</a> coverage in the <a href="http://thenextweb.com/apps/2011/12/04/not-impressed-with-wikipedia-watch-all-of-its-activity-in-real-time/">NextWeb article</a> and elsewhere. If you happen to have tweeted about Wikistream in the last 2 days you should see your avatar to the left. Click on it to make it bigger. I'm in there somewhere too :-)</p>

<p>Before Sunday the site hadn't seen more than 180 unique visitors per day, and on Monday it saw almost 30,000. The site is kind of <a href="http://inkdroid.org/2011/11/07/an-ode-to-node/">different</a> since it streams all the Wikipedia updates to the browser as JSON, where it is then displayed. I had some nail-biting moments as I watched <a href="http://nodejs.org">Node</a> frequently streaming up to 300 concurrent connections. It was a wild ride for my little <a href="http://www.linode.com/">Linode VPS</a> with 512MB of RAM, where <a href="http://inkdroid.org/journal">Wordpress</a> and a <a href="http://linkypedia.info">Django</a> website were also running...but it seemed to weather the storm OK. Mostly I think I could have used more RAM during peak usage when Node and Redis were wanting enough memory to cause the system to swap. Thanks to <a href="https://twitter.com/g5f">Gabe</a> and <a href="http://twitter.com/acdha">Chris</a> for helping me get the cache headers set right in <a href="http://expressjs.com/">Express</a>.</p>

<p>I thought briefly about upgrading to a larger Linode instance, putting the app on EC2, or maybe asking Wikimedia if they wanted to host it. But Wikistream is really more a piece of performance art than it is a useful website. I'm expecting people that have looked at Wikistream once will have seen how much Wikipedia is actively edited, and not feel compelled to look at it again. After a few days I expect the usage to plummet and it can go back to running comfortably on my little Linode VPS to serve as a live prop in presentations about Wikipedia, crowd-sourcing, Web culture, etc.</p>

<p>One of my favorite mentions of Wikistream came from Nat Torkington's <a href="http://radar.oreilly.com/2011/12/four-short-links-5-december-20.html">Four Short Links</a> on O'Reilly Radar. Nat described Wikistream as</p>

<blockquote>
 fascinating and hypnotic and inspirational and appalling and irrelevant all at once
</blockquote>

<p>I took this as high-praise of course. I could only get the last two days out of Twitter's <a href="https://dev.twitter.com/docs/api/1/get/search">search API</a>, which misses the day that the NextWeb article appeared, followed by it getting picked up on <a href="http://news.ycombinator.com/item?id=3311612/">Hacker News</a>. But it was 226 tweets, and provided for a fun little data set to look at. I wrote a little script to look for URLs in the tweets, unshorten them and come up with a list of web pages that mentioned Wikistream in the past few days. One thing that was really interesting to me was the predominance of non-English websites. Here's a list of some of them if you are interested.</p>

<ul>
<li><a href="http://br.wwwhatsnew.com/2011/12/a-atividade-de-wikipedia%E2%80%A6-em-tempo-real/">A atividade de Wikipedia… em tempo real</a></li>
<li><a href="http://erreurs404.net/blog/Erreurs404.net-Partageons-plus-que-des-erreurs/wikipedia-voir-les-changements-en-temps-reel.html?utm_source=twitterfeed&utm_medium=twitter">Wikipedia // Voir les changements en temps réel | Erreurs404.net - Partageons plus que des erreurs | Erreurs404.net</a></li>
<li><a href="http://fr.appy-geek.com/Web/ArticleWeb3.aspx?regionid=2&articleid=1662476">Appy Geek</a></li>
<li><a href="http://web.archive.org/web/20111209034027/http://huscftc.wordpress.com:80/2011/12/06/wikistream-ou-le-flux-de-wikipedia-en-temps-reel/">Wikistream ou le flux de Wikipédia en temps réel</a></li>
<li><a href="http://networkedblogs.com/r7ZkL">Wikistream, El stream de Wikipedia en tiempo real</a></li>
<li><a href="http://news.geek.com.br/posts/18442-site-monitora-as-atividades-na-wikipedia-em-tempo-real?utm_source=twitterfeed&utm_medium=twitter">Geek | Site monitora as atividades na Wikipedia em tempo real</a></li>
<li><a href="http://redlight.blog.over-blog.com/article-wikistream-pour-constater-des-maj-d-articles-de-wikipedia-91357280.html">Wikistream, pour constater des MàJ d’articles de Wikipedia - Geekerie</a></li>
<li><a href="http://revdev.mx/index.php/Internet/wikistream-asi-se-ve-la-edicion-de-wikipedia-en-tiempo-real.html?utm_source=feedburner&utm_medium=twitter&utm_campaign=Feed%3A+revdev%2FGLeV+%28Blog+de+Revdev%29">Wikistream: Así se vé la edición de Wikipedia en tiempo real | Internet</a></li>
<li><a href="http://seetio.com/blog/2011/12/06/wikistream-la-wikipedia-en-tiempo-real/">Wikistream | La Wikipedia en tiempo real</a></li>
<li><a href="http://tecnoark.com/wikistream-visualiza-las-actualizaciones-wikipedia-en-tiempo-real/12280/">Wikistream, visualiza las actualizaciones Wikipedia en tiempo real</a></li>
<li><a href="http://tecnologia.ig.com.br/site-monitora-as-atividades-na-wikipedia-em-tempo-real/n1597398578011.html?utm_source=twitterfeed&utm_medium=twitter">Site monitora as atividades na Wikipedia em tempo real - Tecnologia - iG</a></li>
<li><a href="http://thenextweb.com/apps/2011/12/04/not-impressed-with-wikipedia-watch-all-of-its-activity-in-real-time/">Watch Wikipedia Activity Stream In Real-Time</a></li>
<li><a href="http://wikilovesbieb.nl/?p=421">Wikistream: wijzigingen op Wikipedia volgen in real-time &laquo;  Wiki loves bieb</a></li>
<li><a href="http://www.actualitte.com/actualite/patrimoine-education/education-international/wikistream-pour-suivre-wikipedia-a-la-lettre-en-temps-reel-30307.htm">Wikistream, pour suivre Wikipedia à la lettre, en temps réel</a></li>
<li><a href="http://www.gadgets.es/wikistream-visualiza-actualizaciones-de-la-wikipedia-en-tiempo-real/?utm_source=feedburner&utm_medium=twitter&utm_campaign=Feed%3A+GadgetsesTecnologia+%28Gadgets.es+Tecnologia%29">Wikistream visualiza actualizaciones de la Wikipedia en tiempo real</a></li>
<li><a href="http://www.geek.com/articles/geek-cetera/wikistream-shows-off-the-unbelieveable-amount-of-work-that-goes-into-wikipedia-2011126/">Wikistream shows off the unbelieveable amount of work that goes into Wikipedia | Geek.com</a></li>
<li><a href="http://www.generation-nt.com/wikistream-temps-reel-modifications-wikipedia-actualite-1511461.html">Wikistream : l'activité Wikipédia en temps réel</a></li>
<li><a href="http://www.gizmodo.fr/2011/12/06/wikistream-ou-le-flux-de-wikipedia-en-temps-reel.html">Wikistream ou le flux de Wikipédia en temps réel | Gizmodo</a></li>
<li><a href="http://www.ideepercomputeredinternet.com/2011/12/wikistream-per-visualizzare-in-tempo.html?utm_source=twitterfeed&utm_medium=twitter&utm_campaign=Feed%3A+ComputerChePassione+%28computer+che+passione%29">Wikistream per visualizzare in tempo reale l&#39;attività di Wikipedia.</a></li>
<li><a href="http://www.journaldugeek.com/2011/12/05/wikistream/">Wikistream, pour constater des MàJ d&#8217;articles de Wikipedia</a></li>
<li><a href="http://web.archive.org/web/20120512074246/http://www.letelegraphe-k5.eu/article-le-flux-wikipedia-en-direct-vertige-des-tendances-sur-wikistream-91346825.html">Le flux Wikipedia en direct ... vertige des tendances sur Wikistream ! - Le Telegraphe - k5</a></li>
<li><a href="http://www.microsiervos.com/archivo/internet/velocidad-wikipedia.html">Editoriales, igualad esto: la velocidad de la Wikipedia | Microsiervos  (Internet)</a></li>
<li><a href="http://web.archive.org/web/20120109032518/http://www.monmag.com:80/2011/12/06/wikistream-le-flux-de-wikipedia-en-temps-reel/">  Wikistream : le flux de Wikipédia en temps réel</a></li>
<li><a href="http://www.omicrono.com/2011/12/los-cambios-de-la-wikipedia-en-tiempo-real-con-wikistream/">Los cambios de la Wikipedia en tiempo real con Wikistream &laquo;  Omicrono</a></li>
<li><a href="http://www.pizcos.net/2011/12/wikistream-el-stream-de-wikipedia-en.html">Wikistream, El stream de Wikipedia en tiempo real | Pizcos.net</a></li>
<li><a href="http://www.scoop.it/t/veille-culture-numerique/p/776757733/wikistream-ou-le-flux-de-wikipedia-en-temps-reel">Wikistream ou le flux de Wikip&eacute;dia en temps r&eacute;el | Veille &amp; Culture num&eacute;rique | Scoop.it</a></li>
<li><a href="http://www.tecnologiabit.com/wikistream-visualiza-actualizaciones-de-la-wikipedia-en-tiempo-real/">Wikistream visualiza actualizaciones de la Wikipedia en tiempo real</a></li>
<li><a href="http://www.ubergizmo.com/2011/12/wikistream/">Wikistream shows you Wikipedia real-time activity | Ubergizmo</a></li>
<li><a href="https://plus.google.com/108266160060580572072/posts/C1fXHUzvBvM">Aleksandar Petrovi? - Google+ - U ?lanku koji linkujem je opisan sajt wikistream na…</a></li>
</ul>

<p>OK, I'm finished with the narcissistic navel gazing for a bit. But seriously, thanks for the attention. I've never experienced anything like that before. Wow.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4050</wp:post_id>
		<wp:post_date>2011-12-06 23:36:00</wp:post_date>
		<wp:post_date_gmt>2011-12-07 06:36:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>fascinating-hypnotic-inspirational-appalling-irrelevant</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="linode"><![CDATA[linode]]></category>
		<category domain="post_tag" nicename="news"><![CDATA[news]]></category>
		<category domain="post_tag" nicename="node"><![CDATA[node]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>polling and pushing with the Times Newswire API</title>
		<link>http://inkdroid.org/2011/12/21/polling-and-pushing-with-the-times-newswire-api/</link>
		<pubDate>Wed, 21 Dec 2011 15:53:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4125</guid>
		<description></description>
		<content:encoded><![CDATA[I've been continuing to play around with <a href="http://nodejs.org">Node.js</a> some. Not because it's the <a href="http://mrjoes.github.com/2011/12/15/sockjs-bench.html">only game in town</a>, but mainly because of the  <a href="http://dannorth.net/2011/12/19/the-rise-and-rise-of-javascript/"> renaissance</a> that's going in the JavaScript community, which is kind of fun and slightly addictive. Ok, I guess that makes me a fan-boy, whatever...

So the latest in my experiments is <a href="http://nytimestream.herokuapp.com">nytimestream</a>, which is a visualization (ok, it's just a list) of <a href="http://nytimes.com">New York Times</a> headlines using the <a href="http://developer.nytimes.com/docs/times_newswire_api">Times Newswire API</a>. When I saw <a href="http://blog.thescoop.org/">Derek Willis</a> recently put some work into a <a href="https://github.com/NYTimes/times_wire">Ruby library</a> for the API I got to thinking what it might be like to use <a href="http://nodejs.org">Node.js</a> and <a href="http://socket.io">Socket.IO</a> to provide a push stream of updates. It didn't take too long. I actually highly doubt anyone is going to use nytimestream much. So you might be wondering why I bothered to create it at all. I guess it was kind more of an academic exercise than anything to reinforce some things that Node.js has been teaching me.

Normally if you wanted a web page to dynamically update based on events elsewhere you'd have some code running in the browser routinely poll a webservice for updates. In this scenario our clients (c1, c2 and c3) poll the Times Newswire directly:

<img src="http://inkdroid.org/images/nytimestreampoll.png"/>

But what happens if lots of people start using your application? Yup, you get lots of requests going to the web service...which may not be a good thing, particularly if you are limited to a certain number of requests per day.

So a logical next step is to create a proxy for the webservice, which will reduce hits on the Times Newswire API.

<img src="http://inkdroid.org/images/nytimestreamws.png"/>

But still, the client code needs to poll for updates. This can result in the proxy web service needing to field lots of requests as the number of clients increases. You can poll less, but that will diminish the real time nature of your app. If you are interested in having the real time updates in your app in the first place this probably won't seem like a great solution.

So what if you could have the proxy web service <em>push</em> updates to the clients when it discovers an update?

<img src="http://inkdroid.org/images/nytimestreamnodejs.png"/>

This is basically what an event-driven webservice application allows you to do (labelled NodeJS in the diagram above). Node's <a href="http://socket.io">Socket.IO</a> provides a really nice abstraction around streaming updates in the browser. If you view source on <a href="http://nytimestream.herokuapp.com">nytimestream</a> you'll see a bit of code like this:

<pre lang="javascript">
var socket = io.connect();
socket.on('connect', function() {
  socket.on('story', function(story) {
    addStory(story);
    removeOld();
    fadeList();
  });
});
</pre>

<code>story</code> is a JavaScript object that comes directly from the proxy webservice as a chunk of JSON. I've got the app running on Heroku, which <a href="http://devcenter.heroku.com/articles/using-socket-io-with-node-js-on-heroku">currently</a> recommends Socket.IO be configured to only do <a href="http://en.wikipedia.org/wiki/Push_technology#Long_polling">long polling</a> (xhr-polling). Socket.IO actually supports a <a href="http://socket.io/#browser-support">bunch</a> of other transports suitable for streaming, including web sockets. xhr-polling  basically means the browser keeps a connection open to the server until an update comes down, after which it quickly reconnects to wait for the next update. This is still preferable to constant polling, especially for the NYTimes API which often sees 15 minutes or more go by without an update. Keeping connections open like this can be expensive in more typical web stacks where each connection translates into a thread or process. But this is what Node's <a href="http://www.stumbleupon.com/su/AtWYKW/www.ibm.com/developerworks/opensource/library/os-nodejs/">non-blocking IO</a> programming environment fixes up for you.

Just because I could, I added a little easter egg view in nytimestream, which allows you to see new stories come across the wire as JSON when nytimestream discovers them. It's similar to Twitter's <a href="https://dev.twitter.com/docs/streaming-api">stream API</a> in that you can call it with curl. It's different in that, well, there's hardly the same amount of updates. Try it out with:

<pre>
curl http://nytimestream.herokuapp.com/stream/
</pre>

The occasional newlines are there to prevent the connection from timing out.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4125</wp:post_id>
		<wp:post_date>2011-12-21 08:53:23</wp:post_date>
		<wp:post_date_gmt>2011-12-21 15:53:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>polling-and-pushing-with-the-times-newswire-api</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="nodejs"><![CDATA[nodejs]]></category>
		<category domain="post_tag" nicename="nytimes"><![CDATA[nytimes]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="webservices"><![CDATA[webservices]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>genealogy of a typo</title>
		<link>http://inkdroid.org/2011/12/25/genealogy-of-a-typo/</link>
		<pubDate>Mon, 26 Dec 2011 02:12:36 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4161</guid>
		<description></description>
		<content:encoded><![CDATA[I got a <a href="http://www.amazon.com/gp/product/B005890G8O/">Kindle Touch</a> today for Christmas--thanks Kesa! Admittedly I'm pretty late to this party. As I made ready to purchase my first ebook I hopped over to my GoodReads <a href="http://www.goodreads.com/review/list/5899086?shelf=to-read">to-read</a> list, to pick something out. I scanned  the list quickly, and my eye came to rest on <a href="http://lenz.unl.edu/">Stephen Ramsey</a>'s recent book <a href="http://www.goodreads.com/book/show/11296511-reading-machines">Reading Machines</a>. But I got hung up on something irrelevant: the subtitle was <em>Toward <span style="color: red;">and</span> Algorithmic Criticism</em> instead of <em>Toward <span style="color: red;">an</span> Algorithmic Criticism</em>, the latter of which is clearly correct based on the cover image.

<a href="http://www.goodreads.com/book/show/11296511-reading-machines"><img style="border: thin solid gray;" src="http://inkdroid.org/images/reading-machines/reading-machines-goodreads.png"/></a>

Having recently looked at API services for book data I got curious about how the title appeared on other popular web properties, such as <a href="http://www.amazon.com/Reading-Machines-Algorithmic-Criticism-Humanities/dp/0252078209">Amazon</a>:

<a href="http://www.amazon.com/Reading-Machines-Algorithmic-Criticism-Humanities/dp/0252078209"><img style="border: thin solid gray;" src="http://inkdroid.org/images/reading-machines/reading-machines-amazon.png"/></a>

<a href="http://books.google.com/books?id=14KPI0ORQigC">GoogleBooks</a>:

<a href="http://books.google.com/books?id=14KPI0ORQigC"><img style="border: thin solid gray;" src="http://inkdroid.org/images/reading-machines/reading-machines-googlebooks.png"/></a>

<a href="http://www.barnesandnoble.com/w/reading-machines-stephen-ramsay/1100565003">Barnes & Noble</a>: 

<a href="http://www.barnesandnoble.com/w/reading-machines-stephen-ramsay/1100565003"><img style="border: thin solid gray;" src="http://inkdroid.org/images/reading-machines/reading-machines-bn.png"/></a>

and <a href="http://www.librarything.com/work/11420163">LibraryThing</a>

<a href="http://www.librarything.com/work/11420163"><img style="border: thin solid gray;" src="http://inkdroid.org/images/reading-machines/reading-machines-librarything.png"/></a>

I wasn't terribly surprised not to find it on <a href="http://openlibrary.org">OpenLibrary</a>. But it does seem interesting that the exact same typo is present on all these book websites as well, while the title appears correct on the <a href="http://www.press.uillinois.edu/books/catalog/75tms2pw9780252036415.html">publisher's website</a>:

<a href="http://www.press.uillinois.edu/books/catalog/75tms2pw9780252036415.html"><img style="border: thin solid gray;" src="http://inkdroid.org/images/reading-machines/reading-machines-ui.png"/></a>

and at <a href="http://www.worldcat.org/title/reading-machines-toward-an-algorithmic-criticism/oclc/708761605">OCLC</a>:

<a href="http://www.worldcat.org/title/reading-machines-toward-an-algorithmic-criticism/oclc/708761605"><img style="border: thin solid gray;" src="http://inkdroid.org/images/reading-machines/reading-machines-worldcat.png"/></a>

It's hard to tell for sure, but my guess is that Amazon, Barnes & Noble, and GoogleBooks got the error from <a href="http://www.bowkerlink.com/corrections/common/titleexpress.asp">Bowker Link</a> (the Books in Print data service), and that LibraryThing then picked up the data from Amazon, and similarly GoodReads picked up the data from GoogleBooks. LibraryThing can pull data from a variety of sources, including Amazon; and I'm not entirely sure where GoodReads gets their data from, but it seems likely that it comes from the GoogleBooks API given other tie-ins with Google.

If you know more about the lineage of data in these services I would be interested to hear it. Specifically if you have a subscription to BowkerLink it would be great if you could check the title. It would be nice to live in a world where these sorts of data provenance issues were easier to read.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4161</wp:post_id>
		<wp:post_date>2011-12-25 19:12:36</wp:post_date>
		<wp:post_date_gmt>2011-12-26 02:12:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>genealogy-of-a-typo</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="amazon"><![CDATA[amazon]]></category>
		<category domain="post_tag" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="bowker"><![CDATA[bowker]]></category>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="goodreads"><![CDATA[goodreads]]></category>
		<category domain="post_tag" nicename="googlebooks"><![CDATA[googlebooks]]></category>
		<category domain="post_tag" nicename="librarything"><![CDATA[librarything]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85333</wp:comment_id>
			<wp:comment_author><![CDATA[dakvid]]></wp:comment_author>
			<wp:comment_author_email>david@frig.gen.nz</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-01-03 20:51:52</wp:comment_date>
			<wp:comment_date_gmt>2012-01-04 03:51:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[LibraryThing can get data from hundreds of sources, including manual
entry - but in this case both of the people (there are only two users with the book at the moment) imported from Amazon:
http://www.librarything.com/work/11420163/details/81245530
http://www.librarything.com/work/11420163/details/74768154

However you'll find that the typo has disappeared from the work page
(as with the series title in brackets) because I've set the Canonical
Title field in the Common Knowledge section:
http://www.librarything.com/work/11420163]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>427</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1325649112.8146";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:6:"dakvid";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1325649435.4405";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>2011 Musics</title>
		<link>http://inkdroid.org/2012/01/04/2011-musics/</link>
		<pubDate>Wed, 04 Jan 2012 21:34:43 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4196</guid>
		<description></description>
		<content:encoded><![CDATA[2011 was the year of streaming music for me--specifically using <a href="http://www.rdio.com/#/people/esummers/">Rdio</a>. Being able to follow what friends and folks I admire are listening to, easily listen along, and then build my own online collection in the cloud was a revelation. Being able to easily do it from my desktop at home, or at work, or on my mobile device for $5/month was just astounding. The world ain't perfect, but this is damn near close. 

Anyhow, here's some of my favorite music from 2011, in no particular order ... a lot of which I probably wouldn't have listened to if it wasn't streamable on the Web. <em>You might have to wait a few seconds while the YouTube clips load.</em>

<iframe width="560" height="315" src="http://www.youtube.com/embed/ggeLJeWnLaI" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/2BKUjnyf8uY" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/37pqMoW4z5s" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/i2vvAck6z5c" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/nc__3nsfxwA" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/4HWcViTXdYc" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/QzHWTbdj-FI" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/sgkTtQRPLHw" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/rmjMFPSLXI4" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/XpOH2lWFEl8" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/dX3k_QDnzHE" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/8J8n9R8rnB8" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/63KB-EJKdyI" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/WbN0nX61rIs" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/-Sg7YkPnEYw" frameborder="0" allowfullscreen></iframe>

<iframe width="560" height="315" src="http://www.youtube.com/embed/-TKap4S-pJ0" frameborder="0" allowfullscreen></iframe>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4196</wp:post_id>
		<wp:post_date>2012-01-04 14:34:43</wp:post_date>
		<wp:post_date_gmt>2012-01-04 21:34:43</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>2011-musics</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_summary</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>btcnew_comment_counts</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85334</wp:comment_id>
			<wp:comment_author><![CDATA[My 2011 in Music &laquo; Feral Librarian]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://chrisbourg.wordpress.com/2012/01/05/my-2011-in-music/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-01-05 10:33:11</wp:comment_date>
			<wp:comment_date_gmt>2012-01-05 17:33:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] by Ed Sommers post on 2011 Musics, here is my summary of new (to me) music in [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1325784791.2207";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1325816541.3623";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>NoDB</title>
		<link>http://inkdroid.org/2012/02/21/nodb/</link>
		<pubDate>Tue, 21 Feb 2012 08:23:40 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4253</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Last week <a href="https://twitter.com/wittylama">Liam Wyatt</a> emailed me asking if I could add <a href="http://www.nma.gov.au/">The National Museum of Australia</a> to <a href="http://linkypedia.info">Linkypedia</a>, which tracks external links from Wikipedia articles to specific websites. Specifically Liam was interested in seeing the list of articles that reference the National Museum, sorted by how much they are viewed at Wikipedia. This presented me with two problems:</p>

<ol>
<li>I turned Linkypedia off a few months ago, since the site hadn't been seeing much traffic, and I have not yet figured out how to keep the site going on the paltry Linode VPS I'm using for other things like this blog.</li>
<li>I hadn't incorporated Wikipedia page view statistics into Linkypedia, because I didn't know they were <a href="http://dumps.wikimedia.org/other/pagecounts-raw/">available</a>, and even if I had I didn't have Liam's idea of using them in this way.</li>
</ol>

<h1>1 was easily rectified since I still had the database lying around and had just disabled the <a href="http://linkypedia.info">Linkypedia</a> Apache vhost. I brought it up, and added www.nma.gov.au to the list of hosts to monitor. #2 was surprisingly easy to solve as well, with a somewhat hastily written <a href="https://github.com/edsu/linkypedia/blob/master/linkypedia/web/management/commands/stats.py">Django management command</a> that pulls down the hourly stats and rifles through them looking for links to sites that Linkypedia manages. Incidentally the Python <a href="http://requests.readthedocs.org">requests</a> library makes efficiently iterating through large amounts of gzip compressed text at a URL very simple indeed.</h1>

<p>After the management command had run for a few days and the stats for 2012 had been added to the Linkypedia database, I was able to see that the top 10 most accessed Wikipedia articles that linked to the National Museum were: Mick Jagger, Cricket, Kangaroo, Victorian Era, Ned Kelly, James Cook, Thylacine, Indigenous Australians and Playing Card and Emu ... if you are curious <a href="http://linkypedia.inkdroid.org/websites/43/pages/">the full list</a> is available with counts, as well as similar lists for the <a href="http://linkypedia.inkdroid.org/websites/6/pages/">Museum of Modern Art</a>, the <a href="http://linkypedia.inkdroid.org/websites/34/pages/">Biodiversity Heritage Library</a>, <a href="http://linkypedia.inkdroid.org/websites/17/pages/">Wikileaks</a>, <a href="http://linkypedia.inkdroid.org/websites/42/pages/">Thomas</a> and <a href="http://linkypedia.inkdroid.org/">other sites</a> which Linkypedia also monitors somewhat sporadically.</p>

<p>So this was interesting...but it's not actually what I set out to write about today.</p>

<p>While I had seen aggregate reports of Wikipedia page view data, prior to Liam's email I didn't know that <a href="http://dumps.wikimedia.org/other/pagecounts-raw/">hourly dumps</a> of page view statistics existed. I recently did a <a href="http://inkdroid.org/2011/11/07/an-ode-to-node/">series of experiments</a> with realtime Wikipedia data, so naturally I wondered what might be do-able with the page-view stats. The data is gzip compressed and space delimited, which made it a perfect fit for noodling around in the Unix shell with curl, cut, sort, etc. Before much time passed I had a <a href="https://github.com/edsu/wikitrends/blob/master/fetch.sh">bash script</a> that could run from cron every hour and dump out the top 1000 accessed English Wikipedia pages as a JSON file:</p>

<pre lang="bash">
#!/bin/bash

# fetch.sh is a shell script to run from cron that will download the latest 
# hour's page view statistics and write out the top 1000 to a JSON file
# you will probably want to run it sufficiently after the top of the hour
# so that the file is likely to be there, e.g.
#
# 30 * * * * cd /home/ed/Projects/wikitrends/; ./fetch.sh

# first, get some date info

year=`date -u +%Y`
month=`date -u +%m`
day=`date -u +%d`
hour=`date -u +%H`

# this is the expected URL for the pagecount dump file

url="http://dumps.wikimedia.org/other/pagecounts-raw/$year/$year-$month/pagecounts-$year$month$day-${hour}0000.gz" 

# sometimes the filenames have a timestamp of 1 second instead of 0 
# so if 0000.gz isn't there try using 0001.gz instead

curl -f -s -I $url > /dev/null
retval=$?
if [ $retval -ne 0 ]; then
    url="http://dumps.wikimedia.org/other/pagecounts-raw/$year/$year-$month/pagecounts-$year$month$day-${hour}0001.gz" 
fi

# create a directory and filename for the JSON

json_dir="data/$year/$month/$day"
json_file="$json_dir/$hour.json"
mkdir -p $json_dir

# fetch the data and write out the stats!

curl --silent $url | \
    gunzip -c | \
    egrep '^en ' | \
    perl -npe '@cols=split/ /; print "$cols[2] $cols[1]\n";' | \
    sort -rn | \
    head -n 1000 | \
    ./jsonify.py > $json_file 
</pre>

<p>And a short time later after that I had some pretty simple HTML and JavaScript that could use the JSON to display the top 25 accessed Wikipedia articles from the last hour.</p>

<p><a href="http://inkdroid.org/wikitrends/"><img src="http://inkdroid.org/images/wikitrends.png"/></a></p>

<p>Which I put it up on GitHub as <a href="http://github.com/edsu/wikitrends">Wikitrends</a>. It was kind of surprising at first to see how prevalent mainstream media (mostly television) figures into the statistics. Perhaps it was a function of me working on the code at night when "normal" people are typically watching TV and looking up stuff related to the shows they are looking at. I did notice some odd things pop up in the list occasionally and found myself googling to see if there was recent news on the topic.</p>

<p>To help provide some context I added flyovers so that when you hover over the article title you will see the summary of the Wikipedia article. Behind the scenes the JavaScript looks up the article using the <a href="http://en.wikipedia.org/w/api.php">Wikipedia API</a> and extracts the summary. This got me thinking that it could also be useful to include some links to canned searches at Google (last hour), Twitter and Facebook to provide context for the spike that the Wikipedia article is seeing. Perhaps it would be more interesting to see this information flow by somehow on the side...</p>

<p>A nice side effect of this minimalist (hereby dubbed NoDB...take that NoSQL!) approach to developing the Wikitrends app is that I have uniquely named, URL addressable JSON files for the top 1000 accessed English Wikipedia articles every hour at <a href="http://inkdroid.org/wikitrends/data">http://inkdroid.org/wikitrends/data</a>. Even better, the JSON files even get <a href="https://github.com/edsu/wikitrends/tree/master/data">archived at GitHub</a>. Now don't take this seriously, it's late (or early) and I'm really just making a really lame joke. I'm sure having a database would be useful for trend analysis and whatnot. But for my immediate purposes it wasn't really needed.</p>

<p>So, Wikitrends is another Wikiepdia stats curiosity app. At the very least I got a chance to use JavaScript a bit more seriously by working with <a href="http://documentcloud.github.com/underscore/">Underscore</a> and the very slick <a href="https://github.com/caolan/async">async</a> library. Perhaps there are some ways you can think of to make Wikitrends more useful or interesting. If so please let me know.</p>

<p>And, since I haven't said it enough before: thank you Wikimedia for taking such a pragmatic approach to <a href="http://dumps.wikimedia.org/">making your data available</a>. It is an inspiration.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4253</wp:post_id>
		<wp:post_date>2012-02-21 01:23:40</wp:post_date>
		<wp:post_date_gmt>2012-02-21 08:23:40</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>nodb</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="facebook"><![CDATA[facebook]]></category>
		<category domain="post_tag" nicename="github"><![CDATA[github]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="json"><![CDATA[json]]></category>
		<category domain="post_tag" nicename="search"><![CDATA[search]]></category>
		<category domain="post_tag" nicename="statistics"><![CDATA[statistics]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85347</wp:comment_id>
			<wp:comment_author><![CDATA[National Museum of Australia Situation Report | Witty&#8217;s Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.wittylama.com/2012/03/national-museum-of-australia-situation-report/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-03-01 17:38:20</wp:comment_date>
			<wp:comment_date_gmt>2012-03-02 00:38:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] relaunching and modifying his Linkypedia tool specifically to help me with this report (he has also written that story up in his own blog). Thanks also to Sydney Wikipedian Whiteghost.ink and NMA in-house Wikipedian Shamto for assisting [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1330648700.2413";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1330653679.845";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Curating Curation</title>
		<link>http://inkdroid.org/2012/03/14/curating-curation/</link>
		<pubDate>Wed, 14 Mar 2012 14:26:17 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4306</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>This post was composed over at <a href="http://storify.com/edsu/meta-curation/">Storify</a> and exported here.</em></p>

<script src="http://storify.com/edsu/meta-curation.js?border=false&amp;header=false&amp;sharing=false&amp;more=false"></script>

<div style="display: none;" class="sfy-html"><div id="meta-curation" class="s-story noborder"><ol class="s-elements"><li id="4f60a3ea0695cf5b198bbcee" class="s-element s-element-text"><div class="s-element-content s-text">Because of stuff I've been&nbsp;doing at work lately, and some recent&nbsp;conversations at code4lib in Seattle I've been&nbsp;getting more and more interested in archival description and the Web.&nbsp;When I first ran across Storify&nbsp;it&nbsp;seemed like it might provide some useful user interface ideas that could be used in archival description. I've been thinking how web content such as Wikipedia, authority records, etc could be easily referenced while composing descriptive text about a collection. And once this content has been referenced how can it be baked in so that the content is usable in the future?<div><br />Recently I stumbled upon (pun intended) a topic to try out Storify:&nbsp;the emerging conversation going on&nbsp;in Twitter and in blogs&nbsp;about Web&nbsp;curation.&nbsp;I know how meta right?&nbsp;As a software developer working in the cultural heritage sector my interest in curation has already been piqued for some time. But until just now&nbsp;I was completely&nbsp;oblivious to the emerging debate about new mechanics for expressing attribution on the Web. I&nbsp;actually ran across it because this tweet from Matt Langer flitted across my TweetDeck:</div></div></li><li id="4f605c444f3c51d720cbea40" class="s-element s-element-quote"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-quote s-element-content"><div class="s-quote-open">&#147;</div><div class="s-quote-content"><div class="s-quote-text">Stop Calling it Curation <a href=' http://tmblr.co/ZpI0byHtVz5d' target='_blank' rel='external'> tmblr.co/ZpI0byHtVz5d</a></div></div><div class="s-attribution"><div class="s-source s-twitter"><a href="http://twitter.com" target="_blank"><div class="s-source-icon"></div></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/mattlanger" target="_blank" rel="mattlanger" class="s-author-name">Matt Langer</a><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/mattlanger" target="_blank"><img src="http://a1.twimg.com/profile_images/1764049118/uhhh_normal.jpeg" alt="mattlanger" class="s-author-avatar"/></a></div><div class="s-posted"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/mattlanger/status/179254598652014593" target="_blank" class="s-posted"><div data-timestamp="2012-03-12T17:16:41.000Z" class="timestamp">Mon, Mar 12 2012 13:16:41</div></a></div><div class="s-element-actions"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/intent/tweet?in_reply_to=179254598652014593&amp;related=storify&amp;via=storify&amp;url=permalink" target="_blank" title="reply" event="twitter-reply" value="@mattlanger" class="twitter-newwindow twitter-reply">Reply</a><a tweet_id="179254598652014593" target="_blank" username="mattlanger" title="retweet" event="twitter-retweet" text="Stop Calling it Curation http://tmblr.co/ZpI0byHtVz5d" class="twitter-newwindow twitter-retweet">Retweet</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f605c444f3c51d720cbea42" class="s-element s-element-link"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-link s-element-content"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://gizmodo.com/5892582/stop-calling-it-curation" target="_blank" class="s-link-a">Stop Calling It Curation</a><img src="http://img.gawkerassets.com/img/17g8idrf2vj6gjpg/fb.jpg" class="s-link-thumbnail"/><div class="s-link-desc">1 day ago ... Stop Calling It Curation Imagine, if you will, a world in which Richard Seaver or   Robert Gottlieb had stomped their feet ...</div><div class="s-attribution"><div class="s-source s-gizmodo"><a href="http://gizmodo.com" target="_blank"><img src="http://g.etfv.co/http://gizmodo.com" style="max-width: 16px" border="0"/></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://gizmodo.com" target="_blank" class="s-author-name">Gizmodo</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60a516f9072ad620f8b2d1" class="s-element s-element-text"><div class="s-element-content s-text">The tweet led me over to his blog post on Gizmodo, which rankled my anti-authoritarian sensibilities a bit. This statement in particular got the blood pumping:<div><br /><div><i>"Curation"&nbsp;is an act performed by people with PhDs in art history; the business in which we're all engaged when we're tossing links around on the Internet is simple "sharing".</i></div><div><br /></div><div>But getting into an argument about the semantics of "curation" doesn't seem particularly appealing or useful. One of the reasons why I think&nbsp;"curation" works for <a target="_blank" style="" href="http://curatecamp.org/">Curate Camp</a>, &nbsp;<a target="_blank" style="" href="http://www.ijdc.net/index.php/ijdc">International Journal of Digital Curation</a>&nbsp;and elsewhere is that it has somewhat loose semantics, which allows useful collaboration and conversation to spring up around it. Saying you need a PhD to do curation makes me mad, probably because I don't have one. Maybe it was a joke.&nbsp;Anyhow, moving on.&nbsp;</div><div><br /></div><div>Speaking of semantics Langer goes on to say:<div><div><br /><i>But we should not delude ourselves for a moment into bestowing any special significance on this, because when we do this thing that so many of us like to call "curation" we're not providing any sort of ontology or semantic continuity beyond that of our own whimsy or taste or desire.</i></div><div><i><br /></i></div><div>I think Langer under-estimates how&nbsp;<a target="_blank" style="" href="http://www.cs.rpi.edu/~hendler/LittleSemanticsWeb.html">a little semantics can go a long way</a>. Exhibit A:&nbsp;<a target="_blank" style="" href="http://en.wikipedia.org/wiki/PageRank">PageRank</a>. Exhibit B:&nbsp;<a target="_blank" style="" href="http://microformats.org/wiki/Main_Page">Microformats</a>. I have to thank Langer's piece for drawing me into the discussion more. I'll chalk it up to another sign&nbsp;that <a target="_blank" style="" href="http://scobleizer.com/2012/03/05/have-arrington-and-conway-screwed-up-big-time-with-their-investment-in-highlight/">every good consumer technology has haters.</a></div><div><div><br /></div><div><br /></div></div></div></div></div></div></li><li id="4f605e02f9072ad620e64835" class="s-element s-element-link"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-link s-element-content"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://curatorscode.org/" target="_blank" class="s-link-a">Curator&#39;s Code</a><img src="http://curatorscode.org/images/parallax_sketch/hole.png" class="s-link-thumbnail"/><div class="s-link-desc">Keep the rabbit hole of the Internet open by honoring discovery.</div><div class="s-attribution"><div class="s-source s-curatorscode"><a href="http://curatorscode.org" target="_blank"><img src="http://g.etfv.co/http://curatorscode.org" style="max-width: 16px" border="0"/></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://curatorscode.org" target="_blank" class="s-author-name">Curatorscode</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60a5344f3c51d720de34e2" class="s-element s-element-text"><div class="s-element-content s-text">So it was time to actually look at the Curator's Code itself. The instructions are pretty short and brief: use "via" and "HT" or their unicode equivalents&nbsp;&#5413; and&nbsp;&#8620; respectively. I've already been sub-consciously using "via" for some time now, so a little bit of discussion about seems like a good idea.<br /></div></li><li id="4f605c444f3c51d720cbea44" class="s-element s-element-link"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-link s-element-content"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://www.brainpickings.org/index.php/2012/03/09/curators-code/" target="_blank" class="s-link-a">Introducing The Curator&#39;s Code: A Standard for Honoring Attribution ...</a><img src="http://www.brainpickings.org/wp-content/uploads/2012/03/cchome.png" class="s-link-thumbnail"/><div class="s-link-desc">4 days ago ... As both a consumer and curator of information, I spend a great deal of time   thinking about the architecture of knowledge...</div><div class="s-attribution"><div class="s-source s-brainpickings"><a href="http://www.brainpickings.org" target="_blank"><img src="http://g.etfv.co/http://www.brainpickings.org" style="max-width: 16px" border="0"/></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://www.brainpickings.org" target="_blank" class="s-author-name">Brainpickings</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60a58e64b046b33749f821" class="s-element s-element-text"><div class="s-element-content s-text">Maria Popova has a more detailed description of the rationale behind the&nbsp;use of&nbsp;unicode characters. Strangely the discussion didn't mention what I thought was going to be the primary reason for them: brevity. There have been<a target="_blank" href="http://news.ycombinator.com/item?id=435113"> similar efforts</a> to use special unicode characters on Twitter (where real estate is scarce) before. There are already bookmarklets for easily creating links that use the correct unicode glyphs. But this led me to another post:</div></li><li id="4f605c444f3c51d720cbea4a" class="s-element s-element-link"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-link s-element-content"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://gigaom.com/2012/03/13/its-not-curation-or-aggregation-its-just-how-the-internet-works/" target="_blank" class="s-link-a">It&#39;s not curation or aggregation, it&#39;s just how the Internet works ...</a><img src="http://gigaom2.files.wordpress.com/2010/09/1431384410_db38f8a58f_z.png?w=604" class="s-link-thumbnail"/><div class="s-link-desc">It&#39;s not curation or aggregation, it&#39;s just how the Internet works. By Mathew Ingram   Mar. 13, 2012, 12:43pm PT 2 Comments &#183; Twe...</div><div class="s-attribution"><div class="s-source s-gigaom"><a href="http://gigaom.com" target="_blank"><img src="http://g.etfv.co/http://gigaom.com" style="max-width: 16px" border="0"/></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://gigaom.com" target="_blank" class="s-author-name">Gigaom</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60a7c88018017f1ce28669" class="s-element s-element-text"><div class="s-element-content s-text">Ingram's essential point is that if the past is any guide the Web will route around efforts to control the way citations are made, and more importantly that:<div><br /><i>... we already have a tool for providing credit to the original source &#8212; it&#8217;s called the hyperlink.&nbsp;</i><br /></div><div><i><br /></i></div><div>Which I strongly agree with. That being said we have seen some&nbsp;pretty wide deployment and use of&nbsp;mechanisms like <a target="_blank" href="http://microformats.org/wiki/rel-license">rel=license</a> microformat for expressing the license for a piece of content. Of course typed links between resources is nothing new. It has been the much muddied central message of the Semantic Web movement.&nbsp;</div><div><br /></div></div></li><li id="4f605c444f3c51d720cbea4b" class="s-element s-element-quote"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-quote s-element-content"><div class="s-quote-open">&#147;</div><div class="s-quote-content"><div class="s-quote-text">hope the #curation community supports upcoming #w3c spec on provenance - signal attribution, quotation and source <a href=' http://www.w3.org/blog/SW/2011/10/23/5-simple-provenance-statements/' target='_blank' rel='external'> w3.org/blog/SW/2011/10/23/...</a></div></div><div class="s-attribution"><div class="s-source s-twitter"><a href="http://twitter.com" target="_blank"><div class="s-source-icon"></div></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/pgroth" target="_blank" rel="pgroth" class="s-author-name">Paul Groth</a><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/pgroth" target="_blank"><img src="http://a0.twimg.com/profile_images/317722128/n699857188_298910_8649_normal.jpg" alt="pgroth" class="s-author-avatar"/></a></div><div class="s-posted"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/pgroth/status/179115770377285632" target="_blank" class="s-posted"><div data-timestamp="2012-03-12T08:05:02.000Z" class="timestamp">Mon, Mar 12 2012 04:05:02</div></a></div><div class="s-element-actions"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/intent/tweet?in_reply_to=179115770377285632&amp;related=storify&amp;via=storify&amp;url=permalink" target="_blank" title="reply" event="twitter-reply" value="@pgroth" class="twitter-newwindow twitter-reply">Reply</a><a tweet_id="179115770377285632" target="_blank" username="pgroth" title="retweet" event="twitter-retweet" text="hope the #curation community supports upcoming #w3c spec on provenance - signal attribution, quotation and source http://www.w3.org/blog/SW/2011/10/23/5-simple-provenance-statements/" class="twitter-newwindow twitter-retweet">Retweet</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60a89a4f3c51d720df9520" class="s-element s-element-text"><div class="s-element-content s-text">Groth mentions some serious work that has been going on at the W3C for expressing provenance on the Web. The challenge here I think is to have something with the simplicity of a microformat for expressing these semantics. Perhaps some additions to the <a target="_blank" href="http://www.iana.org/assignments/link-relations/link-relations.xml">Link Relations Registry</a>&nbsp;that would let HTML authors use rel="via" or whatever...This seems a bit more sensible to me than expecting people to all use the same obscure unicode characters at any rate.</div></li><li id="4f60a8f40695cf5a198aa766" class="s-element s-element-text"><div class="s-element-content s-text">I guess the back story here is that a lot of this discussion is the result of discussions going on at SXSW.</div></li><li id="4f605c444f3c51d720cbea46" class="s-element s-element-quote"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-quote s-element-content"><div class="s-quote-open">&#147;</div><div class="s-quote-content"><div class="s-quote-text">SXSW Sketch Reportage Bulletin 06: Curate or be curated. <a href=' http://blog.fueledbycoffee.com/tagged/sxswcurate' target='_blank' rel='external'> blog.fueledbycoffee.com/ta...</a> #SXSW #sketchnotes cc: @brainpicker @david2n @maxlinsky</div></div><div class="s-attribution"><div class="s-source s-twitter"><a href="http://twitter.com" target="_blank"><div class="s-source-icon"></div></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/craightonberman" target="_blank" rel="craightonberman" class="s-author-name">Craighton Berman</a><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/craightonberman" target="_blank"><img src="http://a0.twimg.com/profile_images/1244515617/craig_headshot_normal.jpg" alt="craightonberman" class="s-author-avatar"/></a></div><div class="s-posted"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/craightonberman/status/178627298231205888" target="_blank" class="s-posted"><div data-timestamp="2012-03-10T23:44:01.000Z" class="timestamp">Sat, Mar 10 2012 18:44:01</div></a></div><div class="s-element-actions"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://twitter.com/intent/tweet?in_reply_to=178627298231205888&amp;related=storify&amp;via=storify&amp;url=permalink" target="_blank" title="reply" event="twitter-reply" value="@craightonberman" class="twitter-newwindow twitter-reply">Reply</a><a tweet_id="178627298231205888" target="_blank" username="craightonberman" title="retweet" event="twitter-retweet" text="SXSW Sketch Reportage Bulletin 06: Curate or be curated. http://blog.fueledbycoffee.com/tagged/sxswcurate #SXSW #sketchnotes cc: @brainpicker @david2n @maxlinsky" class="twitter-newwindow twitter-retweet">Retweet</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f605c444f3c51d720cbea47" class="s-element s-element-image"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-element-content s-image"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://img.scoop.it/Y6yG8NGJk8vOJNmWdm2voTl72eJkfbmt4t8yenImKBXEejxNn4ZJNZ2ss5Ku7Cxt" target="_blank" class="s-image-content"><img src="http://img.scoop.it/Y6yG8NGJk8vOJNmWdm2voTl72eJkfbmt4t8yenImKBXEejxNn4ZJNZ2ss5Ku7Cxt"/></a><div class="s-attribution"><div class="s-source s-scoop"><a href="http://img.scoop.it" target="_blank"><img src="http://g.etfv.co/http://img.scoop.it" style="max-width: 16px" border="0"/></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://img.scoop.it" target="_blank" class="s-author-name">Scoop</a></div><div class="s-posted"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://img.scoop.it/Y6yG8NGJk8vOJNmWdm2voTl72eJkfbmt4t8yenImKBXEejxNn4ZJNZ2ss5Ku7Cxt" target="_blank" class="s-posted"><div data-timestamp="2012-03-14T08:34:04.000Z" class="timestamp">Wed, Mar 14 2012 04:34:04</div></a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60a91264b046b3374b012d" class="s-element s-element-text"><div class="s-element-content s-text">And has been amplified by venerable institutions like the the New York Times:</div></li><li id="4f605c444f3c51d720cbea49" class="s-element s-element-link"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-link s-element-content"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://www.nytimes.com/2012/03/12/business/media/guidelines-proposed-for-content-aggregation-online.html?pagewanted=2&amp;src=twrhp" target="_blank" class="s-link-a">Guidelines Proposed for Content Aggregation Online - NYTimes.com</a><div class="s-link-desc">2 days ago ... &#8220;What makes the Internet magical to me is that it is a place of radical discovery,&#8221;   said Ms. Popova, who describes herse...</div><div class="s-attribution"><div class="s-source s-nytimes"><a href="http://www.nytimes.com" target="_blank"><img src="http://g.etfv.co/http://www.nytimes.com" style="max-width: 16px" border="0"/></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://www.nytimes.com" target="_blank" class="s-author-name">Nytimes</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60a94e50bf325919a07acd" class="s-element s-element-text"><div class="s-element-content s-text">and The Atlantic:</div></li><li id="4f605c444f3c51d720cbea4e" class="s-element s-element-link"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-link s-element-content"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://www.theatlantic.com/technology/archive/2012/03/the-curators-guide-to-the-galaxy/254294/" target="_blank" class="s-link-a">The Curator&#39;s Guide to the Galaxy - Megan Garber - Technology ...</a><img src="http://cdn.theatlantic.com/static/mt/assets/science/curatorscode.png" class="s-link-thumbnail"/><div class="s-link-desc">2 days ago ... How to steal other people&#39;s ideas (without being a jerk about it)</div><div class="s-attribution"><div class="s-source s-theatlantic"><a href="http://www.theatlantic.com" target="_blank"><img src="http://g.etfv.co/http://www.theatlantic.com" style="max-width: 16px" border="0"/></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://www.theatlantic.com" target="_blank" class="s-author-name">Theatlantic</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60a930dc36ed7d1ce4be52" class="s-element s-element-text"><div class="s-element-content s-text">And of course there is the requisite (and much welcomed) humorous take on the whole thing:</div></li><li id="4f605c444f3c51d720cbea4c" class="s-element s-element-link"><div class="s-element-share"><div class="s-element-share-label"><i></i><span class="label">Share</span></div></div><div class="s-link s-element-content"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://www.observer.com/2012/03/nine-additional-symbols-for-the-curators-code/" target="_blank" class="s-link-a">Nine Additional Symbols for the Curator&#39;s Code | The New York ...</a><img src="http://web.archive.org/web/20120509190849/http://www.observer.com/files/2012/03/20120312_CARR_graphic-articleInline.jpg" class="s-link-thumbnail"/><div class="s-link-desc">This weekend&#39;s biggest Internet news involves The Curator&#39;s Code, a new system   &#8220;for honoring the creative and intellectual labo...</div><div class="s-attribution"><div class="s-source s-observer"><a href="http://www.observer.com" target="_blank"><img src="http://g.etfv.co/http://www.observer.com" style="max-width: 16px" border="0"/></a><!--.s-source-name= source.name--></div><div class="s-author"><a href="http://stats.storify.com/record/click?sid=4f605c4150bf3259198ef9d2&amp;redirect=http://www.observer.com" target="_blank" class="s-author-name">Observer</a></div><div class="s-clear"></div></div></div><div class="s-clear"></div></li><li id="4f60aa1f64b046b3374bdc72" class="s-element s-element-text"><div class="s-element-content s-text">I for one hope the topic of curation and the Web continues. And in other news, Storify is kind of fun. I'm going to test out the export feature after I hit publish.</div></li></ol></div></div>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4306</wp:post_id>
		<wp:post_date>2012-03-14 07:26:17</wp:post_date>
		<wp:post_date_gmt>2012-03-14 14:26:17</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>curating-curation</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85349</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-03-17 03:40:36</wp:comment_date>
			<wp:comment_date_gmt>2012-03-17 10:40:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi David, thanks for the comment, and for the link to what you wrote about this topic a long while ago, in <em>web time</em>.

I'm certainly not an expert on journalism, but to me editing is taking some discrete object (a text, an image, etc) and changing it in some way to (hopefully) make it better. I think as creators we are often 'editing' our own work. But typically the 'editor' role is distinct from the creator, and 'editing' becomes more of a collaborative relationship that both the author and the editor enter into prior to making the object public.

Curation seems to be an activity that is more concerned with situating public objects in different contexts in order to understand them better. So there often isn't a single object under consideration, but a collection of them. Because of its background in preservation I think most curation activities would be deemed a failure if the object was actually altered in any way. Also the curator is typically a few steps removed from the creator, so it is less of a collaborative activity...but that's not always the case I suppose.

I agree with your post that the medium of the Web makes a lot of the traditional activities carried about by journalists and librarians begin to blur a lot. It's fun to think how these roles fit into the new medium, and how the tools that support the activities can be transformed. I'm also in complete agreement about how important pragmatics are to the activities of editing and curation! Thanks again for taking the time to comment here.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1331980837.1403";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85348</wp:comment_id>
			<wp:comment_author><![CDATA[David Talley]]></wp:comment_author>
			<wp:comment_author_email>dtalley@preciserecall.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.preciserecall.com/lis</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-03-16 16:48:12</wp:comment_date>
			<wp:comment_date_gmt>2012-03-16 23:48:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice collection of links you've curated here! I've wondered for a while why it's called 'curating' though, when it looks to me more like the familiar old editor role. That's never required any fancy degree, goodness knows -- it's traditionally based more on practical experience and the sort of mind that connects facts. My preference for this interpretation may be an artifact of my own editorial background, though. Anyhow, if you'll indulge a moderately ranty review of one discussion in this area, I'll offer this:

http://www.preciserecall.com/lis/index.php/16]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>431</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1331941692.4414";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:12:"DavidWTalley";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1331979499.1648";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>cc0 and git for data</title>
		<link>http://inkdroid.org/2012/03/27/cc0-and-git-for-data/</link>
		<pubDate>Tue, 27 Mar 2012 17:48:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4314</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In case you missed it the <a href="http://cooperhewitt.org">Cooper-Hewitt National Design Museum</a> at the <a href="http://www.si.edu">Smithsonian Institution</a> made a pretty important <a href="http://labs.cooperhewitt.org/2012/releasing-collection-github/">announcement</a> almost a month ago that they have released their <a href="http://www.cooperhewitt.org/collections/data">collection metadata</a> on <a href="https://github.com/cooperhewitt/collection">GitHub</a> using the <a href="http://creativecommons.org/about/cc0">CC0</a> Creative Commons license. The choice to use GitHub is interesting (more about that below) but the big news is the choice to license the data with CC0. John Wilbanks wrote a nice <a href="http://del-fi.org/post/19787775081/us-government-and-cc0
">piece</a> about why the use of CC0 is important. Rather than paraphrase I'll just quote his main point:</p>

<blockquote>
... the fact that the Smithsonian has gone to CC0 is actually a great step. It means that data owners inside the USG have the latitude to use tools that put USG works into a legal status outside the US that is interoperable with their public domain status inside the US, and that’s an unalloyed Good Thing in my view. 
</blockquote>

<p>While I helped prototype and bring the first version of <a href="http://id.loc.gov">id.loc.gov</a> online the licensing of the data was a persistent question that I heard from people who wanted to use the data. The <a href="http://id.loc.gov/about/">about</a> page at id.loc.gov current says:</p>

<blockquote>
The Library of Congress has prepared this vocabulary terminology system and is making it available as a public domain data set. While it has attempted to minimize inaccuracies and defects in the data and services furnished, THE LIBRARY OF CONGRESS IS PROVIDING THIS DATA AND THESE SERVICES "AS IS" AND DISCLAIMS ANY AND ALL WARRANTIES, WHETHER EXPRESS OR IMPLIED.
</blockquote>

<p>But as John detailed in his post, this isn't really enough for folks outside the US. I think even for parties inside the US a CC0 license would add more confidence to using the data set in different contexts, and would help align the Library of Congress more generally with the Web. Last Friday <a href="https://twitter.com/erimille">Eric Miller</a> of Zepheira spoke about Linked Data at the Library of Congress (eventually the talk should be made <a href="http://www.loc.gov/today/cyberlc/results.php?mode=s&cat=45">available</a>). Near the end of his talk he focused on things that need to be worked on, and I was glad to hear him stress that work needed to be done on licensing. The issue is really <a href="http://cloudofdata.com/2009/10/licensing-of-linked-data/">nothing new</a>, and it really transcends the Linked Data space. I'm not saying it's easy, but I agree with everyone who is saying it is important to focus on...and it's great to see the advances that others in the federal government are making.</p>

<h2>Git and GitHub</h2>

<p>The other fun part of the Smithsonian announcement was the use of GitHub as a platform for publishing the data. To do this Cooper-Hewitt established an <a href="https://github.com/cooperhewitt/collection">organizational account</a> on GitHub, which you might think is easy, but is actually no small achievement by itself for an organization in the US federal government. With the account in hand  the <a href="http://github.com/cooperhewitt/collection">collection</a> project was created and the collection metadata was released as two CSV files (media.csv and objects.csv) by <a href="http://labs.cooperhewitt.org/author/micah/">Micah Walter</a>. The repository was then <a href="https://github.com/straup/collection">forked</a> by <a href="http://www.aaronland.info/">Aaron Straup Cope</a>. Aaron added some Python scripts for converting the CSV files into record based JSON files. In the comments to the Cooper-Hewitt Labs <a href="http://labs.cooperhewitt.org/2012/releasing-collection-github/">blog post</a> Aaron <a href="http://labs.cooperhewitt.org/2012/releasing-collection-github/#comment-449050855">commented</a> on why he chose to break up the CSV into JSON. The beautiful thing about using Git and GitHub this way for data is that you have a history view like this:</p>

<p><a href="https://github.com/cooperhewitt/collection/commits/master/"><img src="http://inkdroid.org/images/cooperhewitt-github-history.png" style="border: thin gray solid;"/></a></p>

<p>For digital preservation folks this view of <em>what</em> changed, <em>when</em>, and by <em>who</em> is extremely important for establishing <a href="http://en.wikipedia.org/wiki/Provenance">provenance</a>. The fact that you get this for free by using the opensource <a href="http://git-scm.com/">Git</a> version control system, and pushing your repository to GitHub is very compelling.</p>

<p>Over the past couple of years there has been quite a bit of informal discussion in the digital curation community about using Git for versioning data. Just a couple weeks before the Smithsonian announcement Stephanie Collett and Martin Haye from the <a href="http://www.cdlib.org/">California Digital Library</a> reported on the use of Git and Mercurial to version data at <a href="http://code4lib.org/conference/2012/collett">Code4lib 2012</a>.</p>

<p>But as Alf Eaton <a href="https://twitter.com/invisiblecomma/status/183140416848281600">observed</a>:</p>

<p><a href="https://twitter.com/invisiblecomma/status/183140416848281600"><img src="http://inkdroid.org/images/cooperhewitt-alf.png" style="border: thin gray solid;"/></a></p>

<p>In this case we're talking 205,137 files. If you doubt Alf, try cloning the repository. Here's what I see:</p>

<pre style="border: thin solid gray;">
ed@rorty:~$ time git clone https://github.com/cooperhewitt/collection.git cooperhewitt-collection
Cloning into cooperhewitt-collection...
remote: Counting objects: 230004, done.
remote: Compressing objects: 100% (19507/19507), done.
remote: Total 230004 (delta 102489), reused 223775 (delta 96260)
Receiving objects: 100% (230004/230004), 27.84 MiB | 3.96 MiB/s, done.
Resolving deltas: 100% (102489/102489), done.

real    8m49.408s
user    0m16.477s
sys 0m17.073s
</pre>

<p>Yes, that was close to 9 minutes to clone the repository, during which my workstation was pretty much unusable. I suspect that the majority of the time was spent in I/O but more research would be required to know for sure. There are also <a href="http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git">challenges</a> to using Git for large binary files, which are very common in the digital preservation space. Git and GitHub were designed for versioning code. As any experienced programmer will <a href="http://blog.cleartrip.com/2007/7/7/lisp-is-sin-and-all-data-is-code/">tell you</a>: at a certain level of abstraction all code is data, and all data is code. So it's not illogical to expect Git and GitHub to be used this way.</p>

<p>But practically speaking code and data can be laid out on disk quite differently, and the tools for managing code aren't necessarily optimized for managing data out of the box. There was also an interesting <a href="http://sunlightlabs.com/blog/2010/we-dont-need-a-github-for-data/">discussion</a> two years ago over on the Sunlight Foundation blog questioning the merits of using Git and GitHub for managing data.  Be that as it may, I don't think the digital preservation community can question the importance of tracking where data came from, and the transformations it has undergone. However, the granularity at which to record these details is still an open issue. Are some notes written in English in a README like file enough, or do they need to be machine actionable? Does <a href="http://www.loc.gov/standards/premis/">PREMIS</a> provide some guidance here? Or maybe <a href="http://en.wikipedia.org/wiki/Open_Archival_Information_System">OAIS</a>? Perhaps there's no firm rule here, and there are only pragmatic answers? I guess I should feel embarrassed that I don't already know the answers to these question...I imagine I'm not the only one.</p>

<p>By a strange coincidence at roughly the same time I happened to notice in a  <a href="https://twitter.com/eosadler/status/183676338908041216">tweet</a> from <a href="http://www.ibiblio.org/bess/?page_id=2">Bess Sadler</a> that Stanford University recently published <a href="http://lib.stanford.edu/files/Digital%20Object%20Storage%20and%20Versioning.pdf">Digital Object Storage and Versioning in the Stanford Digital Repository</a>. Maybe there is something to be learned in the findings in there. I'll let you know when I have read it. I suspect Alf and Bess are right that there are other tools like <a href="http://code.google.com/p/boar/">boar</a>, extensions to Git such as <a href="http://git-annex.branchable.com/">git-annex</a>, or new services like <a href="http://figshare.com/">figshare</a> that will have an important role to play in versioning data.</p>

<h2>Meanwhile</h2>

<p>The concerns about GitHub and Git for versioning data aside, the Cooper-Hewitt collection is inspiring, because it is emblematic of trying to use mainstream tools for digital preservation work. It also seems to emphasize the important (and under-recognized) role of access in doing digital preservation at all. I actually only discovered the announcement after getting into a conversation with <a href="http://www.sebchan.com/">Seb Chan</a> who <a href="https://twitter.com/sebchan/status/183028334177959936">responded</a> to me when I grumptweeted about how the <a href="http://findingaids.loc.gov">archival finding aids application</a> at the Library of Congress has a <a href="http://findingaids.loc.gov/robots.txt">robots.txt</a> file that prevents anyone from crawling and indexing it:</p>

<p><a href="https://twitter.com/sebchan/status/183028334177959936"><img src="http://inkdroid.org/images/sebchan-tweet.png" style="border: thin gray solid;"/></a></p>

<p>The distressing thing is that so much work has gone into describing these archival collections, and getting them on the Web...but two lines in a robots.txt file mean they are invisible to anyone searching for related material in Google, Bing, Yahoo, etc. The date on the robots.txt from 2003 made it look like perhaps the exclusion was from some previous version of the software. I decided to be "that guy" and get in touch with some of the people who helped put the content online, so there may be a chance of letting crawlers in.</p>

<p>I happened to know that the <a href="http://www.loc.gov/ead/">Encoded Archival Description</a> XML files for the finding aids are available online. These XML files are the source data for the HTML view that you see in your browser. I've been meaning to try out <a href="http://nodejs.org">nodejs</a> and <a href="http://jquery.com">jQuery</a> for <strike>scraping</strike> harvesting for some time, so I put together a short <a href="https://github.com/edsu/lc-findingaids/blob/master/harvest.js">program</a> that pulls down the XML, which I pushed up to GitHub as <a href="http://github.com/edsu/lc-findingaids">lc-findingaids</a>. There aren't that many XML files to make performance any kind of a problem. The main thing I discovered is that node + jquery using <a href="https://github.com/tmpvar/jsdom">jsdom</a> is a really nice environment for scraping. Here's an example of using jsdom to print out the title for the Library of Congress homepage:</p>

<pre lang="javascript">
var jsdom = require('jsdom');

jsdom.env('http://www.loc.gov', ['http://code.jquery.com/jquery-1.5.min.js'], function (errors, window) {
  console.log(window.$('title').text());
});
</pre>

<p>The beauty of using jsdom and jquery here is that it works on the HTML that is most commonly found on the Web: broken HTML or <a href="http://en.wikipedia.org/wiki/Tag_soup">tag soup</a>. Yes, there are other tools for this, but if you already using jQuery to interact with the DOM it's particularly friendly. Yes, I guess I ignored the robots.txt file, but I was gentle and only retrieved a page at a time, and the server responded quite happily...which bodes well for removing the robots exclusion. or at least relaxing it.</p>

<p>As the <a href="https://github.com/edsu/lc-findingaids/blob/master/README.md">README.md</a> states, these XML files are in the public domain...which may or may not help you. If you are outside the US you might decide not to test the legal waters. If you are in the US maybe you will feel like it's ok to use them to build some web app that helps visualize the data in some new way, like what the <a href="http://socialarchive.iath.virginia.edu/">SNAC</a> folks have done. But can you make your app available to the world on the World Wide Web? I've been slow to realize the importance of this issue to GLAM institutions. Here's to hoping we can get some clarity on the licensing issues in the near future, and for more success stories like Cooper-Hewitt's.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4314</wp:post_id>
		<wp:post_date>2012-03-27 10:48:41</wp:post_date>
		<wp:post_date_gmt>2012-03-27 17:48:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>cc0-and-git-for-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="cc0"><![CDATA[cc0]]></category>
		<category domain="post_tag" nicename="creative-commons"><![CDATA[creative commons]]></category>
		<category domain="post_tag" nicename="ead"><![CDATA[ead]]></category>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="category" nicename="licensing"><![CDATA[licensing]]></category>
		<category domain="post_tag" nicename="loc"><![CDATA[loc]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="nodejs"><![CDATA[nodejs]]></category>
		<category domain="post_tag" nicename="smithsonian"><![CDATA[smithsonian]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85371";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85371</wp:comment_id>
			<wp:comment_author><![CDATA[Ross]]></wp:comment_author>
			<wp:comment_author_email>rossfsinger@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawlBhPsOb5GN8bfRnBAf9sp8p3TO9o2CYyA</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-01 08:01:39</wp:comment_date>
			<wp:comment_date_gmt>2012-05-01 15:01:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I used github to simplify the management of the linked MARC codes RDF:

https://github.com/rsinger/LinkedMARCCodes

since it seemed like an easier way for people to contribute data than some homegrown editor, etc.  Admittedly, the format is unorthodox (they're turtle fragments with shared prefix 'headers'), but I think storing data in git is both new (and thus everybody's experimenting) and subject to compromises to bridge the gaps between the SCM mental model and the native dataformats.

That said, I haven't gotten a single pull request or fork of the data (although I never really did much advertisement (I always meant to blog about it, but...): not sure if that's a critique of my approach, the value of the data or my marketing.

Still, for smaller datasets, this seems an invaluable channel for both publishing the data and providing a mechanism for contributing edits, although I definitely would question its scalability (I probably wouldn't, for example, publish the OpenLibrary's data this way).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>345</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335884511.5708";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawlBhPsOb5GN8bfRnBAf9sp";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85353</wp:comment_id>
			<wp:comment_author><![CDATA[Do you git it?: Open educational resources/practices meets software version control #ukoer &#8211; MASHe]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://mashe.hawksey.info/2012/03/do-you-git-it-open-educational-resourcespractices-meets-software-version-control/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-03-28 06:53:40</wp:comment_date>
			<wp:comment_date_gmt>2012-03-28 13:53:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] so we can hear what’s become of our baby!”[Sticking very loosely with art I see via Ed Summers cc0 and git for data post that:]the Cooper-Hewitt National Design Museum at the Smithsonian Institution made a pretty [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1332942820.3454";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1333884935.5271";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>way, way back</title>
		<link>http://inkdroid.org/2012/05/03/way-way-back/</link>
		<pubDate>Thu, 03 May 2012 16:04:09 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4387</guid>
		<description></description>
		<content:encoded><![CDATA[<p>For some experimental work I've been talking about with <a href="http://twitter.com/nullhandle">Nicholas Taylor</a> (his idea, which he or I will write about later if it pans out) I've gotten interested in programmatic ways of seeing when a URL is available in a <a href="http://en.wikipedia.org/wiki/Web_archiving">web archive</a>. Of course there is the <a href="http://archive.org">Internet Archive</a>'s collection; but what isn't as widely known perhaps is that web archiving is going on around the world at a smaller scale, often using similar software, and often under the auspices of the <a href="http://web.archive.org/web/20120831153724/http://netpreserve.org:80/about/index.php">International Internet Preservation Consortium</a>.</p>

<p>Nicholas pointed me at some <a href="http://code.google.com/p/memento-server/source/browse/trunk/proxies/
">work</a> around Memento, which provides a proxy-like API in front of some web archives. If you aren't already familiar with it, <a href="http://www.mementoweb.org/guide/quick-intro/">Memento</a> is some machinery, or a REST API for deterimining when a given URL is available in a Web Archive--which is pretty useful. Of course, like many standardization efforts it relies on people actually implementing it. For Web Architecture folks, the core idea in Memento is pretty simple; but I think its core simplicity may be obscured from software developers who need to fully digest the <a href="http://www.mementoweb.org/guide/rfc/ID/">spec</a> in order to say they "do" Memento.</p>

<p>Meanwhile a lot of web archives have used the <a href="http://en.wikipedia.org/wiki/Wayback_Machine">Wayback Machine</a> from the <a href="http://archive.org">Internet Archive</a> to provide a human interface to the archived web content. While looking at the <a href="http://code.google.com/p/memento-server/source/browse/#svn%2Ftrunk%2Fproxies">memento-server code</a> I was surprised to learn that the Wayback can also return structured data about what URLs have been archived. For example, you can see what content the Internet Archive has for the New York Times homepage by visiting:</p>

<pre>
<a href="http://wayback.archive.org/web/xmlquery?url=http://www.nytimes.com">http://wayback.archive.org/web/xmlquery?url=http://www.nytimes.com</a>
</pre>

<p>which returns a chunk of XML like:</p>

<pre lang="xml">
< ?xml version="1.0" encoding="UTF-8"?>
<wayback>
  <request>
    <startdate>19960101000000</startdate>
    <numreturned>4425</numreturned>
    <type>urlquery</type>
    <enddate>20120503151837</enddate>
    <numresults>4425</numresults>
    <firstreturned>0</firstreturned>
    <url>nytimes.com/</url>
    <resultsrequested>40000</resultsrequested>
    <resultstype>resultstypecapture</resultstype>
  </request>
  <results>
    <result>
      <compressedoffset>68043717</compressedoffset>
      <mimetype>text/html</mimetype>
      <file>IA-001766.arc.gz</file>
      <redirecturl>-</redirecturl>
      <urlkey>nytimes.com/</urlkey>
      <digest>GY3</digest>
      <httpresponsecode>200</httpresponsecode>
      <url>http://www.nytimes.com:80/</url>
      <capturedate>19961112181513</capturedate>
    </result>
    <result>
      <compressedoffset>8107</compressedoffset>
      <mimetype>text/html</mimetype>
      <file>BK-000007.arc.gz</file>
      <redirecturl>-</redirecturl>
      <urlkey>nytimes.com/</urlkey>
      <digest>GY3</digest>
      <httpresponsecode>200</httpresponsecode>
      <url>http://www.nytimes.com:80/</url>
      <capturedate>19961121230155</capturedate>
    </result>
    ...
  </results>
</wayback>
</pre>

<p>Sort of similarly you can see what the British Library's <a href="http://www.webarchive.org.uk/ukwa/">Web Archive</a> has for the BBC homepage by visiting:</p>

<pre>
<a href="http://www.webarchive.org.uk/wayback/archive/*/http://www.bbc.co.uk">http://www.webarchive.org.uk/wayback/archive/*/http://www.bbc.co.uk/</a>
</pre>

<p>Where you will see:</p>

<pre lang="xml">
< ?xml version="1.0" encoding="UTF-8"?>
<wayback>
  <request>
    <startdate>19910806145620</startdate>
    <numreturned>201</numreturned>
    <type>urlquery</type>
    <enddate>20120503152750</enddate>
    <numresults>201</numresults>
    <firstreturned>0</firstreturned>
    <url>bbc.co.uk/</url>
    <resultsrequested>10000</resultsrequested>
    <resultstype>resultstypecapture</resultstype>
  </request>
  <results>
    <result>
      <compressedoffset>75367408</compressedoffset>
      <mimetype>text/html</mimetype>
      <file>BL-196764-0.warc.gz</file>
      <redirecturl>-</redirecturl>
      <urlkey>bbc.co.uk/</urlkey>
      <digest>sha512:b155b8dd868c17748405b7a8d2ee3606efea1319ee237507055f258189c0f620c38d2c159fc4e02211c1ff6d265f45e17ae7eb18f94a5494ab024175fe6f79c3</digest>
      <httpresponsecode>200</httpresponsecode>
      <url>http://www.bbc.co.uk/</url>
      <capturedate>20080410162445</capturedate>
    </result>
    <result>
      <compressedoffset>92484146</compressedoffset>
      <mimetype>text/html</mimetype>
      <file>BL-7307314-46.warc.gz</file>
      <redirecturl>-</redirecturl>
      <urlkey>bbc.co.uk/</urlkey>
      <digest>sha512:6e37c62b3aa7b60cccc50d430bc7429ecf0d2662bca5562b61ba0bc1027c824a2f7526c747bfca52db46dba5a2ae9c9d96d013e588b2ae5d78188ea4436c571f</digest>
      <httpresponsecode>200</httpresponsecode>
      <url>http://www.bbc.co.uk/</url>
      <capturedate>20080527231330</capturedate>
    </result>
    ...
  </results>
</wayback>
</pre>

<p>It turns out British Library are <a href="http://britishlibrary.typepad.co.uk/webarchive/2012/01/techtalk-wayback-hdfs.html">using</a> this structured data to access data from Hadoop where their web archives live on HDFS as <a href="http://www.digitalpreservation.gov/formats/fdd/fdd000236.shtml">WARC</a> files--which is pretty slick. Actually WARCs on spinning disk is pretty awesome by itself, no matter how you are doing it.</p>

<p>Unfortunately I wasn't able to make it to the International Internet Preservation Consortium <a href="http://web.archive.org/web/20120703040141/http://netpreserve.org:80/events/2012ga.php">meeting</a> going on right now at the Library of Congress. I'm at home heating bottles, changing diapers, and dozing off in comfy chairs with a boppy around my waist. If I was there I think I would be asking:</p>

<ol>
<li>Is there a list of Wayback Machine endpoints that are on the Web? There are multiple ones at the California Digital Library, the Library of Congress, and elsewhere I bet.</li>
<li>How many of them are configured to make this XML data available? Can it easily be turned on for those that don't have it?</li>
<li>Rather than requiring people to implement a new standard to improve interoperability, could we document the XML format that Wayback can already emit, and share the endpoints? This way web archives that don't run Heretrix and Wayback could also share what content they have collected in the same community.</li>
</ol>

<p>This isn't to say that Memento isn't a good idea (I think it is). I just think there might be some quick wins to be had by documenting and raising awareness about things that are already working away quietly behind the scenes. Perhaps the list of Wayback endpoints could be added to the <a href="http://en.wikipedia.org/wiki/Wayback_Machine">Wikipedia page</a>?</p>

<p>Ok, enough for now. I have a bottle to heat up :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4387</wp:post_id>
		<wp:post_date>2012-05-03 09:04:09</wp:post_date>
		<wp:post_date_gmt>2012-05-03 16:04:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>way-way-back</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="internet-archive"><![CDATA[internet archive]]></category>
		<category domain="post_tag" nicename="memento"><![CDATA[memento]]></category>
		<category domain="post_tag" nicename="wayback"><![CDATA[wayback]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:4:{i:0;s:5:"85377";i:1;s:5:"85378";i:2;s:5:"85380";i:3;s:5:"85383";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85380</wp:comment_id>
			<wp:comment_author><![CDATA[Gordon Mohr]]></wp:comment_author>
			<wp:comment_author_email>gojomo-inkdroid@xavvy.com</wp:comment_author_email>
			<wp:comment_author_url>http://memesteading.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-05 15:05:24</wp:comment_date>
			<wp:comment_date_gmt>2012-05-05 22:05:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi!

The behavior of a Wayback installation is controlled via the (Spring 2.5 XML) configuration that sets up its 'access points' and 'renderers' from among the available choices. If you start from the included example 'wayback.xml'...

https://github.com/internetarchive/wayback-machine/blob/master/wayback/wayback-webapp/src/main/webapp/WEB-INF/wayback.xml#L183

...that "org.archive.wayback.query.Renderer" should offer the sort of XML results described above. That is, unless you've done something else to remove/change the expected/default XML-related JSPs, as mentioned at...

http://archive-access.sourceforge.net/projects/wayback/administrator_manual.html#Query_UI

...and reviewable in the source as the last two files in the subdirectory...

https://github.com/internetarchive/wayback-machine/blob/master/wayback/wayback-webapp/src/main/webapp/WEB-INF/query/

So if you're working from a recent 'wayback' and haven't intentionally (or inadvertently) disabled this, or perhaps filtered out access (by URL patterns?) in some intervening layer, this style of access should be available from your Wayback install. 

- Gordon (oftentimes-contributor to web archive.org)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>444</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336255524.2336";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:16:"memesteading.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336259567.9313";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85384</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-08 05:14:01</wp:comment_date>
			<wp:comment_date_gmt>2012-05-08 12:14:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Some of this discussion has moved over to a <a href="https://sourceforge.net/mailarchive/message.php?msg_id=29228547" rel="nofollow">thread</a> on the Wayback discussion list.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336479253.4351";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85385</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-08 05:29:58</wp:comment_date>
			<wp:comment_date_gmt>2012-05-08 12:29:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@atomotic wrote on Twitter:

<blockquote>
on wayback and xml, it was not difficult. i should have read carefully the manual. <a href="http://gist.github.com/2634523" rel="nofollow">gist.github.com/2634523</a> ciao ;)
</blockquote>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336480198.6542";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85383</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>raffaele.messuti@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawlwAIsdKcHJGYmWov34EWuNLE4MsGNoUtY</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-07 05:22:12</wp:comment_date>
			<wp:comment_date_gmt>2012-05-07 12:22:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[thank you Gordon,
but is still not so clear, could you point me at a working examples?

after add/modify the bean org.archive.wayback.query.Renderer, where i define the route of this new renderer? web.xml ?
sorry, but java is an obscure beast to me.
ciao

/raffaele]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>85380</wp:comment_parent>
			<wp:comment_user_id>443</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336393344.6787";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawlwAIsdKcHJGYmWov34EWu";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85378</wp:comment_id>
			<wp:comment_author><![CDATA[eric]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://epugh.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-04 16:23:48</wp:comment_date>
			<wp:comment_date_gmt>2012-05-04 23:23:48</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The Archive is really focused on collecting the data, and preserving it.  Much less focused on the dissemination of that information!  They do make the various datasets available to folks doing research, but they aren't interested in collecting data just to make it easier for the rest of the world to mine it for any old purpose.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>339</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336173828.7863";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:18:"epugh.myopenid.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336249723.8649";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85379</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-05 13:34:30</wp:comment_date>
			<wp:comment_date_gmt>2012-05-05 20:34:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@atomotic thanks for the info regarding the setup of that XML response. It's surprising that there's no information about it. Perhaps I'll ask someone at the British Library if they found a knob to turn, or did it custom themselves.

@eric I'm not really sure I agree with your distinguishing between preservation and dissemination. It seems like a pretty rudimentary task to be able to say if a given URL is in the archive, and they currently do this. Are you working for the Internet Archive and are able to speak on their behalf?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336250071.0259";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85377</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>raffaele.messuti@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawlwAIsdKcHJGYmWov34EWuNLE4MsGNoUtY</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-03 10:49:02</wp:comment_date>
			<wp:comment_date_gmt>2012-05-03 17:49:02</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[for question (2) i searched wayback configuration files with no success,  and i asked developer list too a while ago (no response).
we've a private wayback archive in Italy at Biblioteca Centrale di Firenze, we should be happy to learn how to enable this api and eventually contribute to memento

http://sourceforge.net/mailarchive/forum.php?forum_name=archive-access-discuss&amp;max_rows=25&amp;style=ultimate&amp;viewmonth=201202

ciao
/raffaele (@atomotic)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>443</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336067342.4827";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawlwAIsdKcHJGYmWov34EWu";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336070707.9338";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85481</wp:comment_id>
			<wp:comment_author><![CDATA[anjackson]]></wp:comment_author>
			<wp:comment_author_email>anj@anjackson.net</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-11-22 09:34:45</wp:comment_date>
			<wp:comment_date_gmt>2012-11-22 16:34:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Although we've been looking at serving from HDFS for a while, we're only just now moving this into production (http://britishlibrary.typepad.co.uk/webarchive/2012/11/upgrading-the-wayback-machine.html). 

Please note that this will also change the way we expose the XML API, making things more consistent with other Wayback deployments. Specifically, the API calls will look like this once we are live:

www.webarchive.org.uk/wayback/archive/xmlquery.jsp?url=http://www.bl.uk/]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>458</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1353602093.3258";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:9:"anjackson";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>on not linking</title>
		<link>http://inkdroid.org/2012/04/11/on-not-linking/</link>
		<pubDate>Wed, 11 Apr 2012 20:46:44 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4390</guid>
		<description></description>
		<content:encoded><![CDATA[<p>NPR Morning Edition recently ran an <a href="http://www.npr.org/2012/04/09/150068298/simple-tweets-of-fate-teju-coles-condensed-news">interview</a> with <a href="http://en.wikipedia.org/wiki/Teju_Cole">Teju Cole</a> about his most recent project called <a href="http://www.tejucole.com/other-words/small-fates/">Small Fates</a>. Cole is the recipient of the 2012 Hemingway Foundation/PEN Award for his novel <a href="http://www.nytimes.com/2011/02/27/books/review/Syjuco-t.html?pagewanted=all">Open City</a>. Small Fates is a series of poetic snippets from Nigerian newspapers, which Cole has been posting on <a href="https://twitter.com//tejucole">Twitter</a>. It turns out Small Fates draws on a tradition of compressed news stories known as <em>fait divers</em>. The interview is a really nice description of the poetic use of this material to explore time and place. In some ways it reminds me a little of the <a href="http://en.wikipedia.org/wiki/Cut-up_technique">cut-up technique</a> that <a href="http://en.wikipedia.org/wiki/William_S._Burroughs">William S. Bouroughs</a> popularized; albeit in a more lyrical, less dadaist form.</p>

<p>At about the 3 minute mark in the interview Cole mentions that he has recently been using content from historic New York newspapers using <a href="http://chroniclingamerica.loc.gov">Chronicling America</a>. For example:</p>

<p><img src="http://inkdroid.org/images/teju-tweet.png" style="text-align: middle; border: thin solid gray;"/></p>

<p>Chronicling America is a software project I work on. Of course we were all <em>really</em> excited to hear Cole mention us on NPR. One thing that we were wondering is whether he could include shortened URLs to the newspaper page in Chronicling America in his tweets. Obviously this would be a clever publicity vehicle for the <a href="http://neh.gov">NEH</a> funded <a href="http://www.loc.gov/ndnp/">National Digital Newspaper Program</a>. It would also allow the Small Fates reader to follow the tweet to the source material, if they wanted more context.</p>

<p>Through the magic of Facebook, Twitter, good old email and Teju's generosity I got in touch with him to see if he would be willing to include some shortened Chronicling America URLs in his tweets. His response indicated that he had clearly already thought about linking, but had decided not to. His reasons for not linking struck me as really interesting, and he agreed to let me quote them here:</p>

<blockquote>
I can't include links directly in my tweets for three reasons. 

The first is aesthetic: I like the way the tweets look as clean sentences. One wouldn't wish to hyperlink a poem.

The second is artistic: I want people to stay here, not go off somewhere else and crosscheck the story. Why go through all the trouble of compression if they're just going to go off and read more about it? What's omitted from a story is, to me, an important part of a writer's storytelling strategy.

And the third is practical: though I seldom use up all 140 characters, rarely do I have enough room left for a url string, even a shortened one.
</blockquote>

<p>I really loved this artistic (and pragmatic) rationale for <em>not</em> linking, and thought you might too.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4390</wp:post_id>
		<wp:post_date>2012-04-11 13:46:44</wp:post_date>
		<wp:post_date_gmt>2012-04-11 20:46:44</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-not-linking</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="poetry"><![CDATA[poetry]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="post_tag" nicename="urls"><![CDATA[urls]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="writing"><![CDATA[writing]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85363</wp:comment_id>
			<wp:comment_author><![CDATA[fredericknoronha]]></wp:comment_author>
			<wp:comment_author_email>fredericknoronha1@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://photosfromgoa.notlong.com</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-17 11:39:41</wp:comment_date>
			<wp:comment_date_gmt>2012-04-17 18:39:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[As a writer, would Teju Cole be happy about anyone citing him without sourcing or acknowledging? For reasons "aesthetic, artistic or practical"? If giving credit is important (and it is) we should do it... articulate excuses apart.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>438</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334687982.1994";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:16:"fredericknoronha";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334688021.1664";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85362</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-17 11:08:19</wp:comment_date>
			<wp:comment_date_gmt>2012-04-17 18:08:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@gavrojames simple, he's not a journalist ... he's using newspaper material in his art.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334686100.1744";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85361</wp:comment_id>
			<wp:comment_author><![CDATA[gavrojames@gmail.com]]></wp:comment_author>
			<wp:comment_author_email>gavrojames@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-17 10:45:45</wp:comment_date>
			<wp:comment_date_gmt>2012-04-17 17:45:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm having a hard time understanding your acceptance of this. His artistic justification shows a careless lack of integrity.......any other respected journalist who tweets has figured out how to link to their sources. What gives?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>437</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334684746.4811";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:20:"gavrojames@gmail.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334685783.3282";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85360</wp:comment_id>
			<wp:comment_author><![CDATA[Sunday Reading &laquo; zunguzungu]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://zunguzungu.wordpress.com/2012/04/15/sunday-reading-44/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-15 06:28:09</wp:comment_date>
			<wp:comment_date_gmt>2012-04-15 13:28:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] On Not Linking [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334496490.0734";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334498375.0392";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85358</wp:comment_id>
			<wp:comment_author><![CDATA[Teju Cole Fait Drivers &laquo; Chamblee54]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://chamblee54.wordpress.com/2012/04/12/teju-cole-fait-drivers/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-12 21:09:01</wp:comment_date>
			<wp:comment_date_gmt>2012-04-13 04:09:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] died Sunday by mistake.   In today&#8217;s twitter feed, there was this:  A link on not linking: http://inkdroid.org/2012/04/11/on-not-linking/. In the story, there was a link to an interview on NPR. We learn that Mr. Cole finds many of his [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334290142.3561";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1334378844.0975";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85783</wp:comment_id>
			<wp:comment_author><![CDATA[Brief But Inappropriate | Chamblee54]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://chamblee54.wordpress.com/2013/04/07/brief-but-inappropriate/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-04-07 10:21:22</wp:comment_date>
			<wp:comment_date_gmt>2013-04-07 17:21:22</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] died Sunday by mistake.   In today&#8217;s twitter feed, there was this:  A link on not linking: http://inkdroid.org/2012/04/11/on-not-linking/. In the story, there was a link to an interview on NPR. We learn that Mr. Cole finds many of his [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1365355283.0948650836944580078125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1365355564.516271114349365234375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Lessons of JSON </title>
		<link>http://inkdroid.org/2012/04/30/lessons-of-json/</link>
		<pubDate>Mon, 30 Apr 2012 11:22:11 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4438</guid>
		<description></description>
		<content:encoded><![CDATA[A recent (and short) <a href="http://www.computer.org/portal/web/computingnow/computing-conversations/-/blogs/discovering-javascript-object-notation-with-douglas-crockford?_33_redirect=%2Fportal%2Fweb%2Fcomputingnow%2Fcomputing-conversations">IEEE Computing Conversations interview</a> with <a href="http://en.wikipedia.org/wiki/Douglas_Crockford">Douglas Crockford</a> about the development of <a href="http://en.wikipedia.org/wiki/JSON">JavaScript Object Notation (JSON)</a> offers some profound, and sometimes counter-intuitive, insights into standards development on the Web.

<iframe width="560" height="315" src="http://www.youtube.com/embed/kc8BAR7SHJI" frameborder="0" allowfullscreen></iframe>

<h2>Pave the Cowpaths</h2>

<blockquote>
I don't claim to have invented it, because it already existed in nature. I just saw it, recognized the value of it, gave it a name, and a description, and showed its benefits. I don't claim to be the only person to have discovered it.
</blockquote>

Crockford is likeably humble about the origins of JSON. Rather than claiming he <em>invented</em> JSON he instead says he <em>discovered</em> it--almost as if he was a naturalist on an expedition in some uncharted territory. Looking at the Web as an ecosystem with forms of life in it might seem more like a stretched metaphor or sci-fi plot; but I think it's a useful and pragmatic stance. The Web is a complex space, and efforts to forcibly make it move in particular directions often fail, even when big organizations and trans-national corporations are behind them. Grafting, aligning and cross-fertilizing technologies, while respecting the communities that they grow from will probably feel more chaotic, but it's likely to yield better results in the long run.

<h2>Necessity is the Mother of Invention</h2>

Crockford needed JSON when building an application where a client written in JavaScript needed to communicate with a server written in Java. He wanted something simple that would let him solve a real need he had right in front of him. He didn't want the client to have to compose a query for the server, have the server perform the query against a database, and return something to the client that in turn needed to be queried again. He wanted something where the data serialization matched the data structures available to both programming language environments. He wanted something that made his life easier. Since Crockford was actually using JSON in his work it has usability at its core.

<h2>Less is More</h2>

Crockford tried very hard to strip unnecessary stuff from JSON so it stood a better chance of being language independent. When confronted with push back about JSON not being a "standard" Crockford registered <a href="http://json.org">json.org</a>, put up a specification that documented the data format, and declared it as a standard. He didn't expend a lot of energy in top-down mode, trying to convince industry and government agencies that JSON was the way to go. Software developers discovered it, and started using it in their applications because it made their life easier too. Some people complained that the X in AJAX stood for XML, and therefore JSON should not be used. But this dogmatism faded into the background over time as the benefits of JSON were recognized.

JSON is not versioned, and has no mechanism for revision. JSON cannot change. This probably sounds like heresy to many folks involved in standardization. It is more radical than the WHATWG's <a href="http://blog.whatwg.org/html-is-the-new-html5">decision</a> to remove the version number from HTML5. It may only be possible because Crockford focused so much on keeping the JSON specification so small and tight. Crockford is realistic about JSON not solving all data representation problems, and speculates that we will see use cases that JSON does not help solve. But when this happens something new will be created instead of extending JSON. This relieves the burden of backwards compatibility that can often drag projects down into a quagmire of complexity. Software designed to work with JSON will always work with whatever valid JSON is found in the wild.

<h2>Anyhow</h2>

Don't listen to me, go watch the <a href="http://www.youtube.com/watch?v=kc8BAR7SHJI">interview</a>!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4438</wp:post_id>
		<wp:post_date>2012-04-30 04:22:11</wp:post_date>
		<wp:post_date_gmt>2012-04-30 11:22:11</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lessons-of-json</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85409";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85374</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-01 13:22:32</wp:comment_date>
			<wp:comment_date_gmt>2012-05-01 20:22:32</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm old enough to remember yes. I'm also getting by OK with JSON even with the glaring defects you mention. I don't personally have an axe to grind about XML. XML seems pretty good for representing hierarchically structured documents. XML does have a lot of machinery, but my exposure to Semantic Web technologies have made them look pretty simple in comparison. I'm not sure if that's damning with faint praise...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335903753.7999";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85409</wp:comment_id>
			<wp:comment_author><![CDATA[florin.jurcovici]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://openid-provider.appspot.com/florin.jurcovici</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-06-05 05:56:51</wp:comment_date>
			<wp:comment_date_gmt>2012-06-05 12:56:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The general evolution of web apps seems to show that dealing with JSON's glaring omissions is easier than dealing with XML's heavyweight correctness and completeness.

I haven't seen XML being consistently used as a transfer format anywhere else except in enterprise apps. Such apps use XML not because it's the better choice (which it probably isn't), but because XML is regarded as the "serious" transfer format by consultants - you often risk to be considered unprofessional if you come up with reasonable technical propositions in the enterprise world.

From personal experience, JSON is always the better choice when your data structure is relatively simple, and gets difficult to both read and control when your data structure is complex - but XML isn't a solution for complexity either.

Also, efforts like JSON-WSDL add the same convenience and heavyweight, enterprisey mechanism to JSON which are available for XML, only, without making the format itself similarly verbose and cluttered up with markup which just gets in the way.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>448</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1338901011.6151";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:16:"florin.jurcovici";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1338931783.1163";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85376</wp:comment_id>
			<wp:comment_author><![CDATA[alex@alexgrande.com]]></wp:comment_author>
			<wp:comment_author_email>alex@alexgrande.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-03 09:09:11</wp:comment_date>
			<wp:comment_date_gmt>2012-05-03 16:09:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[One of my favorite points in this article is calling out the natural world. If you create something natural and necessary, you don't need massive marketing dollars to convince people to use it.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>442</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1336061351.7846";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:19:"alex@alexgrande.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1336070742.638";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85369</wp:comment_id>
			<wp:comment_author><![CDATA[Lessons of JSON - Monday By Noon]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://mondaybynoon.com/20120430/lessons-of-json/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-30 10:16:30</wp:comment_date>
			<wp:comment_date_gmt>2012-04-30 17:16:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Lessons of JSON | inkdroid. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335806190.7141";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335815794.6231";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85368</wp:comment_id>
			<wp:comment_author><![CDATA[Ross Patterson]]></wp:comment_author>
			<wp:comment_author_email>Ross.Patterson@GMail.Com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-04-30 08:42:16</wp:comment_date>
			<wp:comment_date_gmt>2012-04-30 15:42:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Crockford is a very smart guy, with very deep experience.  Truly a bright light of the second wave in computing, an important figure now that almost all of the first wave are retired and many are dead.

But he makes the same mistake every creator/discover of something new and lightweight does: He assumes the heavyweight alternative has no good reason for its weight.  Fast-forward 10 years, and JSON has some glaring defects that were noted early on and ignored.  Perhaps the mot obvious is the lack of a JSON specification for representing dates and times - native datatypes in both of the languages Crockford initially used with JSON, and a traditional point of difficulty in most programming systems.

JSON is simple, and XML is hard, because JSON sidesteps the hard things that XML supports.  This is always how shiny, new things get started.  If you're old enough, you may remember that XML was initially touted as a simple, minimal system for data interchange.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>439</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1335800536.862";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:14:"Ross Patterson";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1335801749.6363";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>diving into VIAF</title>
		<link>http://inkdroid.org/2012/05/15/diving-into-viaf/</link>
		<pubDate>Tue, 15 May 2012 17:57:59 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4497</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Last week saw a big (well big for library data nerds) <a href="http://www.infodocket.com/2012/05/04/oclc-research-makes-virtual-international-authority-file-viaf-dataset-publicly-available/">announcement</a> from OCLC that they are making the <a href="http://viaf.org/viaf/data/ ">data</a> for the <a href="http://viaf.org">Virtual International Authority File (VIAF)</a> available for download under the terms of the <a href="http://opendatacommons.org/licenses/by/">Open Data Commons Attribution (ODC-BY)</a> license. If you're not already familiar with VIAF here's a brief description from <a href="http://www.oclc.org/research/activities/viaf/">OCLC Research</a>:</p>

<blockquote>
Most large libraries maintain lists of names for people, corporations, conferences, and geographic places, as well as lists to control works and other entities. These lists, or authority files, have been developed and maintained in distinctive ways by individual library communities around the world. The differences in how to approach this work become evident as library data from many communities is combined in shared catalogs such as OCLC's WorldCat.

VIAF's goal is to make library authority files less expensive to maintain and more generally useful to the library domain and beyond. To achieve this, VIAF seeks to include authoritative names from many libraries into a global service that is available via the Web. By linking disparate names for the same person or organization, VIAF provides a convenient means for a wider community of libraries and other agencies to repurpose bibliographic data produced by libraries serving different language communities

More specifically, the VIAF service: links national and regional-level authority records, creating clusters of related records and expands the concept of universal bibliographic control by:

<ul>
<li>allowing national and regional variations in authorized form to coexist</li>
<li>supporting needs for variations in preferred language, script and spelling</li>
<li>playing a role in the emerging Semantic Web</li>
</ul>

</blockquote>

<p>If you went and looked at the OCLC Research page you'll notice that last month the VIAF project <a href="http://www.oclc.org/news/releases/2012/201224.htm">moved to OCLC</a>. This is evidence of a growing commitment on OCLC's part to make VIAF part of the library information landscape. It currently includes data about people, places and organizations from 22 different national libraries and other organizations.</p>

<p>Already there has been <a href="http://bibwild.wordpress.com/2012/04/09/re-usable-linked-big-data-for-real/">some</a> <a href="http://kcoyle.blogspot.com/2012/04/viaf-gets-serious.html">great</a> <a href="http://gavialib.com/2012/04/discovery-layers-and-metadata/">writing</a> about what the release of VIAF data means for the cultural heritage sector. In particular Thom Hickey's <a href="http://outgoing.typepad.com/outgoing/">Outgoing</a> is a trove of information about the project, which provides a behind-the-scense look at the various <a href="http://www.oclc.org/developer/documentation/virtual-international-authority-file-viaf/request-types">services</a> it offers.</p>

<p>Rather than paraphrase what others have said already I thought I would download some of the <a href="http://viaf.org/viaf/data/ ">data</a> and report on what it looks like. Specifically I'm interested in the RDF data (as opposed to the custom XML, and MARC variants) since I believe it to have the most explicit structure and relations. The shared semantics in the RDF vocabularies that are used also make it the most interesting from a Linked Data perspective.</p>

<h2>Diving In</h2>

<p>The primary data structure of interest in the data dumps that OCLC has made available is what they call the <em>cluster</em>. A cluster is essentially a hub-and-spoke model with a resource for the person, place or organization in the middle that is attached via the spokes to conceptual resources at the participating VIAF institutions. As an example here is an illustration of the VIAF cluster for the Canadian archivist <a href="http://viaf.org/viaf/14894854/">Hugh Taylor</a></p>

<p><a href="http://inkdroid.org/images/hugh-taylor-viaf.png"><img style="width: 700px;" src="http://inkdroid.org/images/hugh-taylor-viaf.png"/></a></p>

<p>Here you can see a FOAF Person resource (yellow) in the middle that is linked to from SKOS Concepts (blue) for Bibliothèque nationale de France, The Libraries and Archives of Canada, Deutschen Nationalbibliothek,  BIBSYS (Norway) and the Library of Congress. Each of the SKOS Concepts have their own preferred label, which you can see varies across institution. This high level view obscures quite a bit of data, which is probably best viewed in Turtle if you want to see it:</p>

<pre style="height: 300px;">
&lt;http://viaf.org/viaf/14894854&gt;
    rdaGr2:dateOfBirth "1920-01-22" ;
    rdaGr2:dateOfDeath "2005-09-11" ;
    a rdaEnt:Person, foaf:Person ;
    owl:sameAs &lt;http://d-nb.info/gnd/109337093&gt; ;
    foaf:name "Taylor, Hugh A.", "Taylor, Hugh A. (Hugh Alexander), 1920-", "Taylor, Hugh Alexander 1920-2005" .

&lt;http://viaf.org/viaf/sourceID/BIBSYS%7Cx90575046#skos:Concept&gt;
    a skos:Concept ;
    skos:inScheme &lt;http://viaf.org/authorityScheme/BIBSYS&gt; ;
    skos:prefLabel "Taylor, Hugh A." ;
    foaf:focus &lt;http://viaf.org/viaf/14894854&gt; .

&lt;http://viaf.org/viaf/sourceID/BNF%7C12688277#skos:Concept&gt;
    a skos:Concept ;
    skos:inScheme &lt;http://viaf.org/authorityScheme/BNF&gt; ;
    skos:prefLabel "Taylor, Hugh Alexander 1920-2005" ;
    foaf:focus &lt;http://viaf.org/viaf/14894854&gt; .

&lt;http://viaf.org/viaf/sourceID/DNB%7C109337093#skos:Concept&gt;
    a skos:Concept ;
    skos:inScheme &lt;http://viaf.org/authorityScheme/DNB&gt; ;
    skos:prefLabel "Taylor, Hugh A." ;
    foaf:focus &lt;http://viaf.org/viaf/14894854&gt; .

&lt;http://viaf.org/viaf/sourceID/LAC%7C0013G3497#skos:Concept&gt;
    a skos:Concept ;
    skos:inScheme &lt;http://viaf.org/authorityScheme/LAC&gt; ;
    skos:prefLabel "Taylor, Hugh A. (Hugh Alexander), 1920-" ;
    foaf:focus &lt;http://viaf.org/viaf/14894854&gt; .

&lt;http://viaf.org/viaf/sourceID/LC%7Cn++82148845#skos:Concept&gt;
    a skos:Concept ;
    skos:exactMatch &lt;http://id.loc.gov/authorities/names/n82148845&gt; ;
    skos:inScheme &lt;http://viaf.org/authorityScheme/LC&gt; ;
    skos:prefLabel "Taylor, Hugh A." ;
    foaf:focus &lt;http://viaf.org/viaf/14894854&gt; .

</pre>

<h2>The Numbers</h2>

<p>The RDF Cluster Dataset <a rel="no-follow" href="http://viaf.org/viaf/data/viaf-20120422-clusters.xml.gz">http://viaf.org/viaf/data/viaf-20120422-clusters.xml.gz</a> is 2.1G gzip compressed RDF data. Rather than it being one complete RDF/XML file, each line has a complete RDF/XML document on it, which represents a single cluster. All in all there are 20,379,541 clusters in the file.</p>

<p>I quickly hacked together a <a href="https://github.com/RDFLib/rdflib">rdflib</a> filter that reads the uncompressed line-oriented RDF/XML and writes the RDF as <a href="http://en.wikipedia.org/wiki/N-Triples">ntriples</a>:</p>

<pre lang="python">
import sys

import rdflib

for line in sys.stdin:
    g = rdflib.Graph()
    g.parse(data=line)
    print g.serialize(format='nt').encode('utf-8'),
</pre>

<p>This took 4 days to run on my (admittedly old) laptop. If you are interested in seeing the ntriples let me know and I can see about making it available somewhere. It is 2.8G gzip compressed. An ntriples dump might be a useful version of the RDF data for OCLC to make available, since it would be easier to load into triplestores, and otherwise muck around with (more on that below) than the line oriented RDF/XML. I don't know much about the backend that drives VIAF (has anyone seen it written up?)...but I would understand if someone said it was too expensive to generate, and was intentionally left as an exercise for the downloader.</p>

<p>Given its line-oriented nature, ntriples is very handy for doing analysis from the Unix command line with <a href="http://blog.datagraph.org/2010/03/grepping-ntriples">cut, sort, uniq, etc</a>. From the ntriples file I learned that the VIAF RDF dump is made up of 377,194,224 assertions or RDF triples. Here's the breakdown on the types of resources present in the data:</p>

<table>
<tr><th>Resource Type</th><th style="text-align: right;">Number of Resources</th></tr>
<tr><td>skos:Concept</td><td style="text-align: right;">26,745,286</td></tr>
<tr><td>foaf:Document</td><td style="text-align: right;">20,379,541</td></tr>
<tr><td>foaf:Person</td><td style="text-align: right;">15,043,112</td></tr>
<tr><td>rda:Person</td><td style="text-align: right;">15,043,112</td></tr>
<tr><td>foaf:Organization</td><td style="text-align: right;">3,722,318</td></tr>
<tr><td>foaf:CorporateBody</td><td style="text-align: right;">3,722,318</td></tr>
<tr><td>dbpedia:Place</td><td style="text-align: right;">195,472</td></tr>
</table>

<p>Here's a breakdown of predicates (RDF properties) that are used:</p>

<table>
<tr><th>RDF Property</th><th style="text-align: right;">Number of Assertions</th></tr>
<tr><td>rdf:type</td><td style="text-align: right;">84,851,159</td></tr>
<tr><td>foaf:focus</td><td style="text-align: right;">45,510,716</td></tr>
<tr><td>foaf:name</td><td style="text-align: right;">44,729,247</td></tr>
<tr><td>rdfs:comment</td><td style="text-align: right;">41,253,178</td></tr>
<tr><td>owl:sameAs</td><td style="text-align: right;">32,741,138</td></tr>
<tr><td>skos:prefLabel</td><td style="text-align: right;">26,745,286</td></tr>
<tr><td>skos:inScheme</td><td style="text-align: right;">26,745,286</td></tr>
<tr><td>foaf:primaryTopic</td><td style="text-align: right;">20,379,541</td></tr>
<tr><td>void:inDataset</td><td style="text-align: right;">20,379,541</td></tr>
<tr><td>skos:altLabel</td><td style="text-align: right;">16,702,081</td></tr>
<tr><td>skos:exactMatch</td><td style="text-align: right;">8,487,197</td></tr>
<tr><td>rda:dateOfBirth</td><td style="text-align: right;">5,215,150</td></tr>
<tr><td>rda:dateOfDeath</td><td style="text-align: right;">1,364,355</td></tr>
<tr><td>owl:differentFrom</td><td style="text-align: right;">1,045,172</td></tr>
<tr><td>rdfs:seeAlso</td><td style="text-align: right;">1,045,172</td></tr>
</table>

<p>I'm expecting these statistics to be useful in helping target some future work I want to do with the VIAF RDF dataset (to explore what an idiomatic JSON representation for the dataset would be, shhh). In addition to the RDF, OCLC also makes a dump of <a href="http://viaf.org/viaf/data/viaf-20120422-links.txt.gz">link data</a> available. It is a smaller file (239M gzip compressed) of tab delimited data, which looks like:</p>

<pre>
...
http://viaf.org/viaf/10014828   SELIBR:219751
http://viaf.org/viaf/10014828   SUDOC:052584895
http://viaf.org/viaf/10014828   NKC:xx0015094
http://viaf.org/viaf/10014828   BIBSYS:x98003783
http://viaf.org/viaf/10014828   LC:24893
http://viaf.org/viaf/10014828   NUKAT:vtls000425208
http://viaf.org/viaf/10014828   BNE:XX917469
http://viaf.org/viaf/10014828   DNB:121888096
http://viaf.org/viaf/10014828   BNF:http://catalogue.bnf.fr/ark:/12148/cb13566121c
http://viaf.org/viaf/10014828   http://en.wikipedia.org/wiki/Liza_Marklund
...
</pre>

<p>There are 27,046,631 links in total. With a little more Unix commandline-fu I was able to get some stats on the number of links by institution:</p>

<table>
<tr><th>Institution</th><th style="text-align: right">Number of Links</th></tr>
<tr><td><a href="http://id.loc.gov">LC NACO (United States)</a></td><td style="text-align: right">8,325,352</td></tr>
<tr><td><a href="http://dnb.de">Deutschen Nationalbibliothek (Germany)</a></td><td style="text-align: right">7,732,546</td></tr>
<tr><td><a href="http://www.sudoc.abes.fr/">SUDOC (France)</a></td><td style="text-align: right">2,031,452</td></tr>
<tr><td><a href="http://web.archive.org/web/20130426113122/http://bibsys.no/norsk/">BIBSYS (Norway)</a></td><td style="text-align: right">1,822,681</td></tr>
<tr><td><a href="http://www.bnf.fr">Bibliothèque nationale de France</a></td><td style="text-align: right">1,643,068</td></tr>
<tr><td><a href="http://www.nla.gov.au/">National Library of Australia</a></td><td style="text-align: right">977,141</td></tr>
<tr><td><a href="http://centrum.nukat.edu.pl/">NUKAT Center (Poland)</a></td><td style="text-align: right">894,981</td></tr>
<tr><td><a href="http://www.collectionscanada.gc.ca/">Libraries and Archives of Canada</a></td><td style="text-align: right">674,088</td></tr>
<tr><td><a href="http://www.nkp.cz/">National Library of the Czech Republic</a></td><td style="text-align: right">598,848</td></tr>
<tr><td><a href="http://www.bne.es/">Biblioteca Nacional de España</a></td><td style="text-align: right">519,511</td></tr>
<tr><td><a href="http://web.nli.org.il/sites/NLI">National Library of Israel</a></td><td style="text-align: right">327,455</td></tr>
<tr><td><a href="http://www.bnportugal.pt/">Biblioteca Nacional de Portugal</a></td><td style="text-align: right">321,064</td></tr>
<tr><td><a href="http://en.wikipedia.org">English Wikipedia</a></td><td style="text-align: right">301,345</td></tr>
<tr><td><a href="http://www.vaticanlibrary.va/">Vatican Library</a></td><td style="text-align: right">247,574</td></tr>
<tr><td><a href="http://www.getty.edu/research/conducting_research/vocabularies/">Getty Union List of Artist Names</a></td><td style="text-align: right">202,711</td></tr>
<tr><td><a href="http://www.kb.se/">National Library of Sweden</a></td><td style="text-align: right">161,845</td></tr>
<tr><td><a href="http://www.rero.ch/">RERO (Switzerland)</a></td><td style="text-align: right">119,366</td></tr>
<tr><td><a href="http://www.sbn.it/opacsbn/opac/iccu/informazioni.jsp">Istituto Centrale per il Catalogo Unico (Italy)</a></td><td style="text-align: right">45,208</td></tr>
<tr><td><a href="http://www.nb.admin.ch/slb/">Swiss National Library</a></td><td style="text-align: right">33,866</td></tr>
<tr><td><a href="http://www.oszk.hu/">National Széchényi Library (Hungary)</a></td><td style="text-align: right">33,727</td>
</tr><tr><td><a href="http://www.bibalex.org/">Bibliotheca Alexandrina (Egypt)</a></td><td style="text-align: right">26,877</td></tr>
<tr><td><a href="http://www.bibnet.be/portaal/Bibnet/Open-Vlacc">Flemish Public Libraries</a></td><td style="text-align: right">4,819</td></tr>
<tr><td><a href="http://www.rsl.ru/">Russian State Library</a></td><td style="text-align: right">997</td></tr>
<tr><td><a href="http://viaf.org/hosted/xa/">Extended VIAF Authority</a></td><td style="text-align: right">109</td></tr>
</table>

<p>The 301,345 links to Wikipedia are really great to see. It might be a fun project to see how many of these links are actually present in Wikipedia, and if they can be automatically added with a bot if they are missing. I think it's useful to have the HTTP identifier in the link dump file, as is the case for the BNF identifiers. I'm not sure why the DNB, Sweden, and LC URLs aren't expressed URLs as well.</p>

<p>One other parting observation (I'm sure I'll blog more about this) is that it would be nice if more of the data that you see in the HTML presentation were available in the RDF dumps. Specifically, it would be useful to have the Wikipedia links expressed in the RDF data, as well as linked works (uniform titles).</p>

<p>Anyway, a big thanks to OCLC for making the VIAF dataset available! It really feels like a major sea change in the cultural heritage data ecosystem.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4497</wp:post_id>
		<wp:post_date>2012-05-15 10:57:59</wp:post_date>
		<wp:post_date_gmt>2012-05-15 17:57:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>diving-into-viaf</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="authority-records"><![CDATA[authority records]]></category>
		<category domain="category" nicename="data"><![CDATA[data]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="unix"><![CDATA[unix]]></category>
		<category domain="post_tag" nicename="viaf"><![CDATA[viaf]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85394";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85403</wp:comment_id>
			<wp:comment_author><![CDATA[Thom]]></wp:comment_author>
			<wp:comment_author_email>hickey@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-17 09:35:40</wp:comment_date>
			<wp:comment_date_gmt>2012-05-17 16:35:40</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Right, caught that too late.  The links file uses Wikipedia links and the RDF DBPedia.  Maybe not the best idea, but there for 'historical' reasons.

--Th]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>80</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337272541.7147";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:13:"ThomasBHickey";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85404</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-17 09:39:04</wp:comment_date>
			<wp:comment_date_gmt>2012-05-17 16:39:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice, thanks for the clarification Thom.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337272745.0037";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85401</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-17 08:00:41</wp:comment_date>
			<wp:comment_date_gmt>2012-05-17 15:00:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thom, unless I'm doing something profoundly wrong it doesn't look like there are any dbpedia URLs in the links dump:

<pre>
% curl http://viaf.org/viaf/data/viaf-20120422-links.txt.gz | zcat - |grep dbpedia
</pre>

The only non-library links I could find were to en.wikipedia.org.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337266841.6721";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85398</wp:comment_id>
			<wp:comment_author><![CDATA[Thom]]></wp:comment_author>
			<wp:comment_author_email>hickey@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-17 01:21:56</wp:comment_date>
			<wp:comment_date_gmt>2012-05-17 08:21:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The source IDs in the links file typically come from the 035 fields.  BnF is actually putting URIs in their 035's.
 
You should see links to DBPedia in the RDF and in the links file.

Posted from #elag2012 !

--Th]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>80</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337242916.2684";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:13:"ThomasBHickey";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85399</wp:comment_id>
			<wp:comment_author><![CDATA[Thom]]></wp:comment_author>
			<wp:comment_author_email>hickey@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-17 02:10:37</wp:comment_date>
			<wp:comment_date_gmt>2012-05-17 09:10:37</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Just noticed that Chris was trying to parse the native XML representation, not the RDF XML.  Same clusters, different views in different files.  The native XML will parse, but just as XML, not XML-RDF.

-Th]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>80</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1337245838.179";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:13:"ThomasBHickey";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85397</wp:comment_id>
			<wp:comment_author><![CDATA[Thom]]></wp:comment_author>
			<wp:comment_author_email>hickey@oclc.org</wp:comment_author_email>
			<wp:comment_author_url>http://</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-17 01:03:57</wp:comment_date>
			<wp:comment_date_gmt>2012-05-17 08:03:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[We're open to adding more to the RDF view.  Not quite as enthusiastic about yet-another-view of the data, but we could do it.  Takes us about 20-30 minutes to do a transformation, plus the time to pull it off the cluster and move it to the public location.

--Th]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>80</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337241843.8105";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:13:"ThomasBHickey";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85394</wp:comment_id>
			<wp:comment_author><![CDATA[Chris Adams]]></wp:comment_author>
			<wp:comment_author_email>chris@improbable.org</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawnjYt4eA4hzgDgYRfMpqMFitgKISVvzTc8</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-15 11:50:51</wp:comment_date>
			<wp:comment_date_gmt>2012-05-15 18:50:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I was curious whether PyPy would handle this faster but it looks like something changed which makes RDFlib choke on http://viaf.org/viaf/data/viaf-20120422-clusters.xml.gz. With a modified version of your script (https://gist.github.com/2704123) every line fails to parse ("Invalid property attribute URI: http://www.w3.org/1999/02/22-rdf-syntax-ns#about")]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>354</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337107852.3848";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawnjYt4eA4hzgDgYRfMpqMF";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337109104.3486";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85395</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-15 12:14:20</wp:comment_date>
			<wp:comment_date_gmt>2012-05-15 19:14:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Sorry it didn't work for you Chris. It works fine for me using python 2.7.3 and rdflib v3.2. Let me know if you figure out what the problem is with your environment. For anyone else trying from home, in case it wasn't obvious you need to uncompress the data before you send it along to the script, e.g.

<pre>
zcat viaf-20120422-clusters-rdf.xml.gz | ./nt.py 
</pre>]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337109260.5165";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85396</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-15 12:28:00</wp:comment_date>
			<wp:comment_date_gmt>2012-05-15 19:28:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Out of curiosity I tried running the script to create ntriples under PyPy v1.8.0:

<pre>
(viaf-pypy)ed@curry:~/Datasets$ time zcat viaf-20120422-clusters-rdf.xml.gz | head -n1000 | pypy nt.py > y

real	0m13.565s
user	0m13.101s
sys	0m0.204s
</pre>

and compared it to Python v2.7.3:

<pre>
(viaf)ed@curry:~/Datasets$ time zcat viaf-20120422-clusters-rdf.xml.gz | head -n1000 | python nt.py > x 

real	0m12.408s
user	0m12.077s
sys	0m0.120s
</pre>

Strangely, PyPy seems a bit slower, at least for the first 1000 lines...]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1337110080.261";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85400</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-05-17 07:20:44</wp:comment_date>
			<wp:comment_date_gmt>2012-05-17 14:20:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thom: 

Wow, that's impressive: 30 minutes to create the a dump of this data. Have you written about the backend architecture recently? I remember seeing that OCLC was beginning to use Hadoop for some things. I understand the reluctance to create another RDF format, but arguably an ntriples dump is probably a bit more useful than the line oriented RDF/XML documents, at least for the cloister of folks who work with RDF data.

Thanks also for catching what Chris was doing wrong, I'll let him know (we work together).  I also probably have some wrong numbers for links to Wikipedia, since I lumped together all the http protocol links. I'll rerun the stats for the links, and update my post :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1337264447.7895";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Wikimania Justification</title>
		<link>http://inkdroid.org/2012/06/11/wikimania-justification/</link>
		<pubDate>Mon, 11 Jun 2012 18:31:55 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4597</guid>
		<description></description>
		<content:encoded><![CDATA[Due to fiscal constraints we (understandably) have to write justifications for travel requests at $work, to make it clear how the conference/meeting fits in with the goals of the institution. I am planning on going to Wikimania for this first time this year, which is happening a short metro ride away at George Washington University. The cost for the full event is $50, which is an amazing value, and makes it a bit of a no-brainer on the cost-benefit scale. But I still need to justify it, mainly because of the time away from work. If you work in a cultural heritage organization and ever find yourself wanting to go to Wikimania maybe the justification I wrote will be of interest. I imagine you could easily substitute in your own organization's Web publishing projects appropriately ...

<blockquote>
The <a href="http://wikimania2012.wikimedia.org/wiki/Main_Page">Wikimania</a> conference is the annual conference supporting the Wikipedia community. It is attended by thousands of people from around the world, and is the premier event for discussions and research about the continued development of Wikipedia--and it is being held in Washington, DC this year. Wikipedia comprises 22 million articles, in 285 languages, and it has become the largest and most popular general reference work on the Internet, ranking sixth globally among all websites. 

Wikimania is of particular interest to cultural heritage institutions, and specifically the Library of Congress, because of the important role that collections like <a href="http://memory.loc.gov">American Memory</a>, <a href="http://chroniclingamerica.loc.gov">Chronicling America</a>, the <a href="http://www.loc.gov/pictures/">Prints and Photographs Online Catalog</a> and the <a href="http://www.wdl.org">World Digital Library</a> (among others) have for Wikipedia editors. Primary resource material on the Web is extremely important to editors for verifiability of article content--so much so that the Wikipedia community is specifically conducting outreach with its <a href="http://en.wikipedia.org/wiki/Wikipedia:GLAM">Galleries, Libraries, Archives and Museums (GLAM) project</a>. Several of the our peer institutions are involved in the GLAM effort, including: the National Archives, the Smithsonian and OCLC. Wikipedia remains one of the top referrers of web traffic to the Library of Congress web properties. LC's multi-decade effort to put its unique collections online for the American people naturally aligns it with the mission of Wikipedia, and Wikimania is an excellent place to learn more about this collaboration that is going on with cultural heritage organizations.

I will be <a href="http://wikimania2012.wikimedia.org/wiki/Submissions/Wikistream ">presenting</a> on the value of open access to underlying datasets when conducting a real-time visualization of Wikipedia edits. There is a <a href="http://wikimania2012.wikimedia.org/wiki/Schedule">track of presentations</a> for the cultural heritage community which I plan on attending. There is also a workshop on the <a href="http://meta.wikimedia.org/wiki/Wikidata">Wikidata</a> project, which has particular relevance for LC's historic involvement in subject and name authority control files. In addition there is a <a href="http://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Loves_Libraries">Wikipedia Loves Libraries</a> workshop being sponsored by OCLC to explore the ways in which libraries and Wikipedia can support each other in enriching discoverability and access to research material.
</blockquote>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4597</wp:post_id>
		<wp:post_date>2012-06-11 11:31:55</wp:post_date>
		<wp:post_date_gmt>2012-06-11 18:31:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wikimania-justification</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="conferences"><![CDATA[conferences]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="travel"><![CDATA[travel]]></category>
		<category domain="post_tag" nicename="wikimania"><![CDATA[wikimania]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<category domain="post_tag" nicename="work"><![CDATA[work]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ouroboros</title>
		<link>http://inkdroid.org/2012/06/13/ouroboros/</link>
		<pubDate>Thu, 14 Jun 2012 03:51:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4611</guid>
		<description></description>
		<content:encoded><![CDATA[https://twitter.com/edsu/status/213115800700198914]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4611</wp:post_id>
		<wp:post_date>2012-06-13 20:51:03</wp:post_date>
		<wp:post_date_gmt>2012-06-14 03:51:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>ouroboros</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="post_tag" nicename="wordpress"><![CDATA[wordpress]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_9ab2fa90c96b12e4b2407a0885146d82</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>a bit of Storify making it over into Wordpress v3.4: Twitter Embeds <a href="http://t.co/kGB1NCpC" title="http://en.support.wordpress.com/twitter/twitter-embeds/">en.support.wordpress.com/twitter/twitte…</a></p>&mdash; Ed Summers (@edsu) <a href="https://twitter.com/edsu/status/213115800700198914">June 14, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_02a6cd5753506616cc0d662bb35ae9fc</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="500"><p>a bit of Storify making it over into Wordpress v3.4: Twitter Embeds <a href="http://t.co/kGB1NCpC" title="http://en.support.wordpress.com/twitter/twitter-embeds/">en.support.wordpress.com/twitter/twitte…</a></p>&mdash; Ed Summers (@edsu) <a href="https://twitter.com/edsu/status/213115800700198914">June 14, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_b145384698e544daede06f63381e8114</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>a bit of Storify making it over into Wordpress v3.4: Twitter Embeds <a href="http://t.co/kGB1NCpC" title="http://en.support.wordpress.com/twitter/twitter-embeds/">en.support.wordpress.com/twitter/twitte…</a></p>&mdash; Ed Summers (@edsu) <a href="https://twitter.com/edsu/status/213115800700198914">June 14, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_124b70678f5abdef4ebd64a42e5e3edf</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>a bit of Storify making it over into Wordpress v3.4: Twitter Embeds <a href="http://t.co/kGB1NCpC">http://t.co/kGB1NCpC</a></p>&mdash; Ed Summers (@edsu) <a href="https://twitter.com/edsu/statuses/213115800700198914">June 14, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On Discovery</title>
		<link>http://inkdroid.org/2012/06/19/on-discovery/</link>
		<pubDate>Tue, 19 Jun 2012 20:47:48 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4621</guid>
		<description></description>
		<content:encoded><![CDATA[<p>There's an interesting <a href="http://www.theatlantic.com/technology/archive/12/06/nota-bene-if-you-discover-something-in-an-archive-its-not-a-discovery/258538/">story</a> over at The Atlantic which discusses the important role that cataloging and archival description play in historical research. The example is a recently discovered report to the Surgeon General from <a href="http://en.wikipedia.org/wiki/Charles_Leale">Charles Leale</a> about his treatment of Abraham Lincoln after he was shot. A few weeks ago a researcher named Helen Papaioannou discovered the report while combing a collection of correspondence to the Surgeon General looking for materials related to Lincoln for a project at the <a href="http://en.wikipedia.org/wiki/Abraham_Lincoln_Presidential_Library_and_Museum">Abraham Lincoln Presidential Library and Museum</a>. The Atlantic piece boldly declares in its title:</p>

<blockquote>
If You 'Discover' Something in an Archive, It's Not a Discovery.
</blockquote>

<p>Then it goes on to heap accolades on the silent archivists toiling away for centuries, that made the report possible to find. I've done my fare share of cataloging, and put in enough time working with EAD finding aids to enjoy the pat on the back. But something about the piece struck me as odd, and it took a bit of reading of the <a href="http://web.archive.org/web/20120611200531/http://www.papersofabrahamlincoln.org:80/NewsReleases/Is%20there%20a%20surgeon%20in%20the%20house.pdf">announcement</a> of the discovery, and listening to a <a href="http://www.npr.org/2012/06/06/154456416/new-document-sheds-light-on-lincolns-last-hours">NPR interview</a> with Papaioannou to put my finger on it.</p>

<blockquote>
It's very possible, of course, with the volume of material that archives hold, for a particular professional to not know exactly what the repository holds. This is because archivists catalogue not at "item level," a description of every piece of paper, which would take millennia, but at "collection level," a description of the shape of the collection, who owned it, and what kinds of things it contains. With the volume of materials, some collections may be undescribed or even described wrongly. <strong>But if anyone thought that a report to the Surgeon General from a physician who saw Lincoln post-assassination existed, they might have looked through these correspondence files -- which is exactly what the researcher, Helen Papaioannou, did.</strong> The exciting part about the Leale report is not that it was rescued from a "dusty archives" (an abhorrent turn of phrase!) but that since it's now catalogued, everyone who wants to find it can.
</blockquote>

<p>Papaioannou's own <a href="http://www.npr.org/2012/06/06/154456416/new-document-sheds-light-on-lincolns-last-hours">account</a> is a bit more nuanced though:</p>

<blockquote>
Well, the record group I was currently searching was the records of the Office of the Surgeon General. And I was looking through his letters received, and I was in the L's. And I was going through 1865, so I - since Lincoln died in 1865. I was almost finished with L and there it was, sitting right in the middle of a box.
</blockquote>

<p>This account makes it sound more like she was combing various record groups looking for correspondence <em>from</em> Lincoln, and accidentally ran across a letter from Leale, that was filed nearby...and she happened to notice that it was about Lincoln, and subsequently that the documents existence was not known. So Papaioannou didn't suspect that the report to the Surgeon General existed, and go searching for it. She was instead examining various record groups for <em>any</em> correspondence from Lincoln, and was alert enough to notice something as she was moving through the collection. And most importantly she recognized that the document was not known to the historical community: the all important context, that is not completely knowable by any individual cataloger or archivist. At least that's how I'm reading it.</p>

<p>Saying that there is no discovery in libraries and archives, because all the discovery has been pre-coordinated by librarians and archivists is putting the case for the work we do too strongly. It doesn't give enough credit to the acts of discovery and creativity that library users like Papaioannou perform, and which our institutions depend on. I'm not an expert, but it seems to me that the lines that divide the historian and the archivist are more or less semi-permeable, especially since what is historic research gets archived itself, and archivists end up doing their own flavor of historical research when documenting the provenance of a collection. If we care about the future of libraries and archives we need to not only pat ourselves on the back for the work we do, but we need to recognize and appreciate the real work that goes on inside our buildings and on our websites.</p>

<p>And yes it's great that the letter is now cataloged for re-discovery. But even better (for me) was that I was able to read the Atlantic piece, do some searches, and then go and listen to an interview with Papaioannou, and read the <a href="http://web.archive.org/web/20120611200531/http://www.papersofabrahamlincoln.org:80/NewsReleases/Is%20there%20a%20surgeon%20in%20the%20house.pdf">announcement</a> from the Lincoln Library which includes a transcription of the actual letter.</p>

<p>...and then go and <a href="http://en.wikipedia.org/w/index.php?title=Charles_Leale&diff=498387203&oldid=498224563">update</a> the Wikipedia entry for Charles Leale to include information about the (very real) discovery of the letter.</p>

<p>Hopefully it won't get reverted :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4621</wp:post_id>
		<wp:post_date>2012-06-19 13:47:48</wp:post_date>
		<wp:post_date_gmt>2012-06-19 20:47:48</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-discovery</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="abraham-lincoln"><![CDATA[abraham lincoln]]></category>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="cataloging"><![CDATA[cataloging]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[history]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85413</wp:comment_id>
			<wp:comment_author><![CDATA[pogonippy]]></wp:comment_author>
			<wp:comment_author_email>pogonippy@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-06-19 15:15:48</wp:comment_date>
			<wp:comment_date_gmt>2012-06-19 22:15:48</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice analysis! Discovery of information in the growing tidal wave of available sources is a community effort, plain and simple, with a whole lot of serendipity mixed in. You've captured that really well. This Leale report discovery is a great example of the importance of names (not that the researcher knew she was looking for Leale) and context (she did know that there might be something worth sniffing out in the 1865 Surgeon General's records) that will be helpful in the teaching I do with undergrads using archival materials.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>451</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340144148.7166";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:9:"pogonippy";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340156852.2802";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85414</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-06-19 21:16:50</wp:comment_date>
			<wp:comment_date_gmt>2012-06-20 04:16:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks Bill. I'm glad that you got something out of the post. Now that you mention it, I like the use of names in this example as well. Thanks for pointing it out!]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>85413</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340165810.7249";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85415</wp:comment_id>
			<wp:comment_author><![CDATA[On Discovery &laquo; Walker Sampson]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://wsampson.wordpress.com/2012/06/20/on-discovery/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-06-20 07:46:09</wp:comment_date>
			<wp:comment_date_gmt>2012-06-20 14:46:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] inkdroid.org/     Published: June 20, 2012 Filed Under: archives [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340203570.0156";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340204240.8843";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85417</wp:comment_id>
			<wp:comment_author><![CDATA[Archives as Discovery Zones | Trevor Owens]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.trevorowens.org/2012/06/archives-as-discovery-zones/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-06-22 08:29:57</wp:comment_date>
			<wp:comment_date_gmt>2012-06-22 15:29:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] I wholeheartedly agree, with the later statement.  To this, Ed Summers responds &#8220;Saying that there is no discovery in libraries and archives, because all the discovery has been pre-...&#8221; Which I also agree with, archives are are places to explore and discover and celebrating [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340378997.1423";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1340380378.722";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>viaf ntriples</title>
		<link>http://inkdroid.org/2012/06/24/viaf-ntriples/</link>
		<pubDate>Mon, 25 Jun 2012 02:05:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4650</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I had a few requests for the Virtual International Authority File ntriples file I <a href="http://inkdroid.org/2012/05/15/diving-into-viaf/">wrote about</a> earlier. Having the various flavors of <a href="http://viaf.org/viaf/data/">VIAF data</a> available is great, but if an RDF dump is going to be made available I think <a href="http://en.wikipedia.org/wiki/N-Triples">ntriples</a> kinda makes more sense than line oriented rdf/xml. I say this only because most RDF libraries and tools have support for bulk loading ntriples, and none (to my knowledge) support loading line oriented rdf/xml files.</p>

<p>I've made the 1.9G bzipped ntriples file available on Amazon S3 if you are interested in having at it:</p>

<blockquote>
  <p><a rel="nofollow" href="http://viaf-ntriples.s3.amazonaws.com/viaf-20120524-clusters-rdf.nt.bz2">http://viaf-ntriples.s3.amazonaws.com/viaf-20120524-clusters-rdf.nt.bz2</a> 
  Incidentally you can torrent it as well, which would help spread the download of the file (and would spare me some cost on s3) by pointing your BitTorrent client at:</p>
  
  <p><a rel="nofollow" href="http://viaf-ntriples.s3.amazonaws.com/viaf-20120524-clusters-rdf.nt.bz2?torrent">http://viaf-ntriples.s3.amazonaws.com/viaf-20120524-clusters-rdf.nt.bz2?torrent</a> 
  As with the original VIAF dataset, this ntriples VIAF download contains information from VIAF (Virtual International Authority File) which is made available under the <a href="http://opendatacommons.org/licenses/by/1.0/">ODC Attribution License (ODC-By)</a>. Similarly, I am making the ntriples VIAF download available using the <a rel="license" href="http://opendatacommons.org/category/odc-by/">ODC-By License</a> as well, because I think I have to given the of <a href="http://opendatacommons.org/licenses/by/1.0/#4-0-conditions-of-use">viral nature</a> of ODC-By. At least that's my unprofessional (I am not a lawyer) reading of the license. I'm not really complaining either, I'm all for openness going viral :-)</p>
</blockquote>

<hr />

<p>On a side note, I upgraded my laptop after the 4 days it took to initially create the ntriples file. In the process I accidentally deleted the ntriples file when I reformatted my hard drive. So the second time around I spent some time seeing if I could generate it quicker on <a href="http://aws.amazon.com/elasticmapreduce/">Elastic MapReduce</a> by splitting the file across multiple workers that would generate the ntriples from the rdf/xml and merge it back together. The conversion of the rdf/xml to ntriples using rdflib was largely CPU bound on my laptop, so I figured <a href="http://web.archive.org/web/20120829035941/http://hadoop.apache.org:80/common/docs/r0.15.2/streaming.html">Hadoop Streaming</a> would help me run my little Python script on as many workers nodes as I needed.</p>

<p>EMR made setting up and running a job quite easy, but I ran into my own ignorance pretty quickly. It turns out that Hadoop was not able to split the gzipped VIAF data, which meant data was only ever sent to one worker, no matter how many I ran. I then ran across some advice to use LZO compression, which is <a href="https://forums.aws.amazon.com/thread.jspa?messageID=347820&amp;#347820">supposedly splittable</a> on EMR, but after lots of experimentation I couldn't get it to split either. I thought about uncompressing the original gzipped file on S3, but felt kind of depressed about doing that for some reason.</p>

<p>I time-boxed only a few days to try to get EMR working, so I backpedaled to rewriting the conversion script with Python's <a href="http://docs.python.org/library/multiprocessing.html">multiprocessing</a> library. I thought multiprocessing would let me take advantage of a multi-core EC2 machine. But oddly the conversion ran slower using multiprocessing's <a href="http://docs.python.org/library/multiprocessing.html#using-a-pool-of-workers">Pool</a> than it did as a single process. I chalked this up to the overhead of pickling large strings of rdf/xml and ntriples to send them around during inter-process-communication...but I didn't investigate further.</p>

<p>So, in the end, I just re-ran the script for 4 days, but this time up at EC2 so that I could use my laptop during that time. &sigh;</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4650</wp:post_id>
		<wp:post_date>2012-06-24 19:05:06</wp:post_date>
		<wp:post_date_gmt>2012-06-25 02:05:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>viaf-ntriples</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="amazon"><![CDATA[amazon]]></category>
		<category domain="post_tag" nicename="elastic-mapreduce"><![CDATA[elastic mapreduce]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="lzo"><![CDATA[lzo]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="viaf"><![CDATA[viaf]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85419";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85419</wp:comment_id>
			<wp:comment_author><![CDATA[christopher wanko]]></wp:comment_author>
			<wp:comment_author_email>remo_williams@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url>https://me.yahoo.com/a/RZIPFwBw1I6ZdzkEP8Ba1jF2U0NSLCJZtys-#c3bec</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-06-25 03:02:51</wp:comment_date>
			<wp:comment_date_gmt>2012-06-25 10:02:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This may seem naive, but couldn't you have piped the decompression into Hadoop Streaming, and just fed it the data that way?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>452</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340618571.7238";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:11:"christopher";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340618860.6963";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85420</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-06-25 03:13:01</wp:comment_date>
			<wp:comment_date_gmt>2012-06-25 10:13:01</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Chris, good question. Hadoop Streaming, at least on Elastic MapReduce, wants to have the input file on Amazon S3. I thought about decompressing the file on S3 but was kind of put off that I couldn't get the LZO compressed file to split properly. I'm a Hadoop newbie, so definitely don't take any of this as gospel--not that you would :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1340619181.9017";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>straw</title>
		<link>http://inkdroid.org/2012/07/06/straw/</link>
		<pubDate>Fri, 06 Jul 2012 15:02:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4670</guid>
		<description></description>
		<content:encoded><![CDATA[By now I imagine you've heard the <a href="http://www.oclc.org/news/releases/2012/201238.htm">announcement</a> that OCLC has started to make <a href="http://www.worldcat.org">WorldCat</a> bibliographic data available as openly licensed Linked Data. The availability of <a href="http://www.whatwg.org/specs/web-apps/current-work/multipage/microdata.html">microdata</a> and <a href="http://www.w3.org/TR/rdfa-primer/">RDFa</a> metadata in WorldCat pages coupled with the <a href="http://opendatacommons.org/licenses/by/">ODC-BY</a> license and the availability of <a href="http://www.worldcat.org/robots.txt">sitemaps</a> for crawlers is a huge win for the library community. Similar announcements about <a href="http://ddc.typepad.com/025431/2012/06/ddc-23-released-as-linked-data-at-deweyinfo.html">Dewey Decimal Classification</a> and the <a href="http://outgoing.typepad.com/outgoing/2010/05/viafs-new-linked-data.html">Virtual International Authority File</a> are further evidence that there is a big paradigm shift going on at OCLC.

A few weeks ago <a href="http://dataliberate.com/">Richard Wallis</a> (formerly of Talis, and now at OCLC) asked me to take a look at the strawman library microdata vocabulary that OCLC put together for the WorldCat release: <a href="http://purl.org/library">http://purl.org/library</a>. Richard stressed that the library vocabulary was a prototype to focus and gather interest from the cultural heritage sector outside of OCLC, and the metadata community in general. Combined with the prototype microdata at <a href="http://worldcat.org">WorldCat</a> I think it represents an excellent first step. At this point I should <a href="http://inkdroid.org/about/">re-iterate</a> that these remarks about schema.org are mine and not those of my employer. 

The vocabulary is actually currently expressed in <a href="http://www.w3.org/TR/owl-features/">OWL</a>, and visiting that URL will redirect you to an application that lets you read the OWL file as documentation. Rather than write up a few paragraphs and send my comments to Richard in email, I figured I would jot them down here, in case anyone else has feedback.

Examining the classes that the library vocabulary defines tells the majority of the story. They are broken down into 

<ul>
<li>ArchiveMaterial</li>
<li>Carrier</li>
<li>Computer File</li>
<li>Game</li>
<li>Image</li>
<li>Interactive Multimedia</li>
<li>Kit</li>
<li>Musical Score</li>
<li>Newspaper</li>
<li>Periodical</li>
<li>Thesis</li>
<li>Toy</li>
<li>Video</li>
<li>VideoGame</li>
<li>Visual Material</li>
<li>Web Site</li>
</ul>

These classes should seem familiar to catalogers who have worked in MARC since there is a lot of similarity with the types of data that are encoded into the <a href="http://www.loc.gov/marc/bibliographic/bd008.html">008</a> field. However some are missing such as maps, dictionaries, encyclopedias, etc. It's kind of amusing that Book isn't mentioned. I'm not sure what the rationale was for selecting these classes, perhaps some sort of ranking based on use in WorldCat? Examining the OWL shows that OCLC has made an effort to express mappings between the library vocabulary and schema.org:

<table>
<tr><th>library</th><th>schema.org</th></tr>
<tr><td>http://purl.org/library/ArchiveMaterial</td> <td>http://schema.org/CreativeWork/ArchiveMaterial</td></tr>
<tr><td>http://purl.org/library/ComputerFile</td> <td>http://schema.org/CreativeWork/ComputerFile</td></tr>
<tr><td>http://purl.org/library/Game</td> <td>http://schema.org/CreativeWork/Game</td></tr>
<tr><td>http://purl.org/library/Image</td> <td>http://schema.org/CreativeWork/Image</td></tr>
<tr><td>http://purl.org/library/InteractiveMultimedia</td> <td>http://schema.org/CreativeWork/InteractiveMultimedia</td></tr>
<tr><td>http://purl.org/library/Kit</td> <td>http://schema.org/CreativeWork/Kit</td></tr>
<tr><td>http://purl.org/library/MusicalScore</td> <td>http://schema.org/CreativeWork/MusicalScore</td></tr>
<tr><td>http://purl.org/library/Newspaper</td> <td>http://schema.org/CreativeWork/Newspaper</td></tr>
<tr><td>http://purl.org/library/Periodical</td> <td>http://schema.org/CreativeWork/Periodical</td></tr>
<tr><td>http://purl.org/library/Thesis</td> <td>http://schema.org/CreativeWork/Book/Thesis</td></tr>
<tr><td>http://purl.org/library/Toy</td> <td>http://schema.org/CreativeWork/Toy</td></tr>
<tr><td>http://purl.org/library/Video</td> <td>http://schema.org/CreativeWork/Video</td></tr>
<tr><td>http://purl.org/library/VideoGame</td> <td>http://schema.org/CreativeWork/VideoGame</td></tr>
<tr><td>http://purl.org/library/VisualMaterial</td> <td>http://schema.org/CreativeWork/VisualMaterial</td></tr>
<tr><td>http://purl.org/library/WebSite</td> <td>http://schema.org/CreativeWork/WebSite</td></tr>
</table>

However these schema.org URLs do not resolve, and are not actually present as specifications of schema.org's <a href="http://schema.org/CreativeWork">Creative Work</a>. Perhaps the presence of these mappings in the library vocabulary is evidence of a desire to create these classes at schema.org. But then there are cases like <code>library:Image</code> which seem to bear a lot resemblance to schema.org's <a href="http://schema.org/ImageObject">ImageObject</a>.

Examining the OWL also yields a set of <code>library:Carrier</code> instances.

<ul>
<li>BlurayDisk</li>
<li>CassetteTape</li>
<li>CD</li>
<li>DVD</li>
<li>FilmReel</li>
<li>LP</li>
<li>Microform</li>
<li>VHSTape</li>
<li>Volume</li>
<li>WWW</li>
</ul>

Again, there are more carriers than this in the MARC world. Why these were selected is a bit of a mystery. What <code>library:WWW</code> has to do with <code>library:Website</code> (if anything) isn't clear, etc.

So even in this prototype library vocabulary there is a lot to examine and unpack. I imagine some phone calls or face to face meetings would be required to get at what went into their production.

Be that as it may, I think it could prove more useful to look at the WorldCat microdata and see what library vocabulary was used. For example here is the microdata extracted from the WorldCat page for Tim Berners-Lee's <a href="http://www.worldcat.org/title/weaving-the-web-the-original-design-and-ultimate-destiny-of-the-world-wide-web-by-its-inventor/oclc/41238513">Weaving the Web</a> expressed as JSON:

<pre lang="javascript">
{
  "type": "http://schema.org/Book", 
  "properties": {
    "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
      "http://schema.org/Book"
    ], 
    "http://purl.org/library/placeOfPublication": [
      {
        "type": "http://schema.org/Place", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://schema.org/Place"
          ], 
          "http://schema.org/name": [
            "San Francisco :"
          ]
        }
      }
    ], 
    "http://schema.org/bookEdition": [
      "1st ed."
    ], 
    "http://schema.org/publisher": [
      {
        "type": "http://schema.org/Organization", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://schema.org/Organization"
          ], 
          "http://schema.org/name": [
            "HarperSanFrancisco"
          ]
        }
      }
    ], 
    "http://schema.org/genre": [
      "History"
    ], 
    "http://schema.org/name": [
      "Weaving the Web : the original design and ultimate destiny of the World Wide Web by its inventor"
    ], 
    "http://schema.org/numberOfPages": [
      "226"
    ], 
    "http://purl.org/library/holdingsCount": [
      "2096"
    ], 
    "http://schema.org/about": [
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://schema.org/name": [
            "Erfindung."
          ]
        }
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://schema.org/name": [
            "WWW."
          ]
        }
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://schema.org/name": [
            "prospective informatique."
          ]
        }
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://www.w3.org/2004/02/skos/core#inScheme": [
            "http://dewey.info/scheme/e21/"
          ]
        }, 
        "id": "http://dewey.info/class/025/e21/"
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://www.loc.gov/mads/rdf/v1#isIdentifiedByAuthority": [
            "http://id.loc.gov/authorities/subjects/sh95000541"
          ], 
          "http://schema.org/name": [
            "World wide web."
          ]
        }
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://www.loc.gov/mads/rdf/v1#isIdentifiedByAuthority": [
            "http://id.loc.gov/authorities/subjects/sh95000541"
          ], 
          "http://schema.org/name": [
            "World Wide Web."
          ]
        }
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://www.loc.gov/mads/rdf/v1#isIdentifiedByAuthority": [
            "http://id.loc.gov/authorities/subjects/sh95000541"
          ], 
          "http://schema.org/name": [
            "World Wide Web--History."
          ]
        }
      }, 
      {
        "type": "http://schema.org/Person", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://schema.org/Person"
          ], 
          "http://www.loc.gov/mads/rdf/v1#isIdentifiedByAuthority": [
            "http://id.loc.gov/authorities/names/no99010609"
          ], 
          "http://schema.org/name": [
            "Berners-Lee, Tim."
          ]
        }, 
        "id": "http://viaf.org/viaf/85312226"
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://schema.org/name": [
            "Web--Histoire."
          ]
        }
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://schema.org/name": [
            "World Wide Web"
          ]
        }, 
        "id": "http://id.worldcat.org/fast/1181326"
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://schema.org/name": [
            "historique informatique."
          ]
        }
      }, 
      {
        "type": "http://www.w3.org/2004/02/skos/core#Concept", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://www.w3.org/2004/02/skos/core#Concept"
          ], 
          "http://schema.org/name": [
            "Web--Histoire."
          ]
        }
      }, 
      {
        "type": "http://schema.org/Person", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://schema.org/Person"
          ], 
          "http://schema.org/name": [
            "Berners-Lee, Tim"
          ]
        }
      }
    ], 
    "http://schema.org/description": [
      "Enquire within upon everything -- Tangles, links, and webs -- info.cern.ch -- Protocols: simple rules for global systems -- Going global -- Browsing -- Changes -- Consortium -- Competition and consensus -- Web of people -- Privacy -- Mind to mind -- Machines and the Web -- Weaving the Web."
    ], 
    "http://purl.org/library/oclcnum": [
      "41238513"
    ], 
    "http://schema.org/copyrightYear": [
      "1999"
    ], 
    "http://schema.org/contributor": [
      {
        "type": "http://schema.org/Person", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://schema.org/Person"
          ], 
          "http://www.loc.gov/mads/rdf/v1#isIdentifiedByAuthority": [
            "http://id.loc.gov/authorities/names/n97003262"
          ], 
          "http://schema.org/name": [
            "Fischetti, Mark."
          ]
        }, 
        "id": "http://viaf.org/viaf/874883"
      }
    ], 
    "http://schema.org/isbn": [
      "9780062515872", 
      "006251587X", 
      "0062515861", 
      "9780062515865"
    ], 
    "http://schema.org/inLanguage": [
      "en"
    ], 
    "http://schema.org/reviews": [
      {
        "type": "http://schema.org/Review", 
        "properties": {
          "http://schema.org/reviewBody": [
            "Tim Berners-Lee, the inventor of the World Wide Web, has been hailed by Time magazine as one of the 100 greatest minds of this century. His creation has already changed the way people do business, entertain themselves, exchange ideas, and socialize with one another.\" \"Berners-Lee offers insights to help readers understand the true nature of the Web, enabling them to use it to their fullest advantage. He shares his views on such critical issues as censorship, privacy, the increasing power of software companies in the online world, and the need to find the ideal balance between the commercial and social forces on the Web. His criticism of the Web's current state makes clear that there is still much work to be done. Finally, Berners-Lee presents his own plan for the Web's future, one that calls for the active support and participation of programmers, computer manufacturers, and social organizations to make it happen.\"--Jacket."
          ], 
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://schema.org/Review"
          ], 
          "http://schema.org/itemReviewed": [
            "http://www.worldcat.org/oclc/41238513"
          ]
        }
      }
    ], 
    "http://schema.org/author": [
      {
        "type": "http://schema.org/Person", 
        "properties": {
          "http://www.w3.org/1999/02/22-rdf-syntax-ns#type": [
            "http://schema.org/Person"
          ], 
          "http://www.loc.gov/mads/rdf/v1#isIdentifiedByAuthority": [
            "http://id.loc.gov/authorities/names/no99010609"
          ], 
          "http://schema.org/name": [
            "Berners-Lee, Tim."
          ]
        }, 
        "id": "http://viaf.org/viaf/85312226"
      }
    ]
  }, 
  "id": "http://www.worldcat.org/oclc/41238513"
}
</pre>

Yes, that's a lot of data. But interestingly only three library vocabulary elements were used:

<ul>
<li>placeOfPublication</li>
<li>holdingsCount</li>
<li>oclcnum</li>
</ul>

One could argue that rather than creating <code>library:placeOfPublication</code> they could use  <code>schema:publisher</code> with a nested <a href="http://schema.org/Organization">Organization</a> item having a <code>schema:location</code>. Similarly <code>library:oclcnum</code> could've been expressed using <code>itemid</code> with a value of <code>info:oclc/41238513</code> using the info-uri namespace that OCLC maintain the <a href="http://info-uri.info/">registry</a> for. This leaves <code>library:holdingsCount</code>, which does seem to be missing from schema.org but also begs the question of whose holdings?

As Tom Gruber famously <a href="http://www.aiai.ed.ac.uk/~jessicac/psfiles/SIG-SEMIS/issue3-tom-gruber.pdf">said</a>:

<blockquote>
Every ontology is a treaty – a social agreement – among people with some common motive in sharing.
</blockquote>

So the question for me is what is the library vocabulary trying to do, and for who? Is it trying to make it easy to share MARC data as microdata on the Web? Is it trying to communicate something to search engines so that they can have enhanced displays? Who are the people that want to share and consume this data? I think having rough consensus about the answers to these questions is really important before diving into modeling exercises...even prototypes. And when the modeling begins I think it's really important to follow the lead of the WorldCat developers in using the bits of schema.org vocabulary they could, and beginning to mint vocabulary terms for things that are missing. I don't think it's going to be fruitful to start from the position of modeling the bibliographic universe completely. I'd rather see real implementations (both publishers and consumers) drive the discovery of what is missing or awkward in schema.org, and how can it be fixed. Ideally, schema.org implementors like <a href="http://goodreads.com">GoodReads</a> would be at the table, along with members of the academic community like <a href="http://jronallo.github.com/">Jason Ronallo</a>, <a href="http://bibwild.wordpress.com/">Jonathan Rochkind</a> and <a href="http://edchamberlain.wordpress.com/">Ed Chamberlain</a> (among others) who care about these issues. In addition <a href="http://loc.gov">my employer</a> is <a href="http://www.loc.gov/marc/transition/news/modeling-052212.html">actively engaged</a> in an effort to rethink bibliographic data on the Web. It seems imperative that these efforts at schema.org and Zepheira's work be combined somehow--especially since OCLC and Zepheira are hardly strangers.

I was of course flattered to be asked my opinion about the library vocabulary. I hope that my remarks haven't accidentally set this strawman vocabulary on fire, because I think the work that OCLC has begun in this area is incredibly important. My experience watching the designers of <a href="http://en.wikipedia.org/wiki/SKOS">SKOS</a> has made me mindful of minimizing <a href="http://en.wikipedia.org/wiki/Ontological_commitment">ontological commitments</a> when designing a vocabulary, and wary of trying to exhaustively model a domain. In some ways I guess I'm a bit of a schema.org skeptic given its encyclopedic coverage. schema.org should take a page from the HTML 5 book and stay hyper-focused on letting implementations drive standardization. A bit of <a href="http://en.wikipedia.org/wiki/Seymour_Lubetzky">Seymour Lubetzky's</a> attention to simplification and user friendliness would be welcome as well.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4670</wp:post_id>
		<wp:post_date>2012-07-06 08:02:57</wp:post_date>
		<wp:post_date_gmt>2012-07-06 15:02:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>straw</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="microdata"><![CDATA[microdata]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<category domain="post_tag" nicename="schema-org"><![CDATA[schema.org]]></category>
		<category domain="post_tag" nicename="standards"><![CDATA[standards]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{i:0;s:5:"85423";i:1;s:5:"85424";i:2;s:5:"85467";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85423</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-08 09:40:50</wp:comment_date>
			<wp:comment_date_gmt>2012-07-08 16:40:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I don't understand the point of publicizing 'mappings' to fictitious/made-up schema.org vocabulary identifiers. It seems like it will confuse people into thinking these are legit, when they're not. To make matters more confusing, _some_ of those schema.org URI's _do_ exist (are documented to exist on schema.org documentation, although none of them resolve, there's no actual requirement that URI's used as identifiers resolve), and others don't. 

Part of the point of using http URI's as identifiers, is that you can create new identifiers in a DNS domain that you 'control' somehow, but not in someone elses!  

I can't figure out what they were thinking, this seems like a poor choice. Imagine if someone else publicized 'mappings' to DC, where they just made up dcterms URI's and called that a mapping, the DC folks wouldn't like that very much, no?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1341765657.5313";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85424</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-08 09:46:15</wp:comment_date>
			<wp:comment_date_gmt>2012-07-08 16:46:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[PS: Contrary to some popular belief, you don't need to use URI's beginning http://schema.org with microdata -- you can use any URI's at all.  The foundation of URI-based linked data is that if you need an identifier that doesn't exist, you mint one yourself (in a domain _you_ control) -- they can just use their purl.org/library URI's, just fine. 

There is obvious value in providing a mapping to _legit_ schema.org/ URI's, so applications that understand schema.org/ URIs can understand elements in your vocabulary that map too, by the data publisher publishing with the mapping, or by someone else applying the mapping. 

I see _no_ value in mapping to _fictitious_ schema.org/ URI's.  If you expect people to modify their appications to understand your made-up schema.org URIs -- why wouldn't they just modify to understand your legit purl.org/library URIs instead?  A URI is just an identifier -- but by trying an unauthorized 'extension' of schema.org, they are possibly introducing conflicts with future legit authorized extensions of schema.org.  You don't get to go adding elements to someone else's vocabulary using URI's beginning with a DNS hostname someone else controls.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1341765976.2081";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85425</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-08 12:53:12</wp:comment_date>
			<wp:comment_date_gmt>2012-07-08 19:53:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Jonathan, I totally agree about the mappings. I feel like I am missing some part of the story, or perhaps I interpreted things incorrectly. One thing to bear in mind is that OCLC's library vocabulary really is just a strawman to generate discussion, not some finished thing. While it is definitely true that microdata allows people to create their own itemtypes and itemprops,  I think OCLC (and others) anticipate that having what libraries, archives and museums need in schema.org will encourage interoperability, and could ultimately make the metadata more useful in search engines like Google. When enhancing search results with metadata harvested from the Web they will have to be selective in the types of schemas they pay attention to and use.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1341777197.3563";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85426</wp:comment_id>
			<wp:comment_author><![CDATA[Richard Wallis]]></wp:comment_author>
			<wp:comment_author_email>richard.wallis@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-08 15:58:10</wp:comment_date>
			<wp:comment_date_gmt>2012-07-08 22:58:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks Ed for you detailed reply - not only much better than a few paragraph in an email, but also a conversation starter. 

For those that had not caught the announcement, your link goes to a strange place, the correct place is <a href="http://www.oclc.org/news/releases/2012/201238.htm" rel="nofollow">here</a>.  Also for my take on why this announcement is significant in many ways, check out <a href="http://dataliberate.com/2012/06/oclc-worldcat-linked-data-release-significant-in-many-ways/" rel="nofollow">my post on the background</a>.

Some of the decisions taken when approaching this, may clarify the thinking behind some of the issues you raise.

The starting point, for this experimental release of WordCat data, was Schema.org.  How far would we get only using the Schema.org vocabulary, what 'library' extensions, if any, would be needed to describe things sufficiently for general consumption by the major search engines.  

You were amused that the potential extension to schema.org does not include a Book class.  As Schema.org already has a <a href="http://schema.org/Book" rel="nofollow">Book</a> class, there is obviously no need to add one.

You highlight possible areas where we are suggesting extensions that may not be needed, such as library:image which may be covered by schema:ImageObject.  I am sure there are others, which we need to think about.

You ask what the purpose of this library ontology.  

Several groups with particular focuses on data on the web, have proposed extensions to schema.org.  For example the <a href="http://www.iptc.org" rel="nofollow">IPTC</a>'s recommendation for <a href="http://iptc.cms.apa.at/site/Home/Media_Releases/schema.org_adopts_IPTC's_rNews_for_news_markup" rel="nofollow">rNews</a> was accepted and now schema.org has better coverage for news media data.  Others such as eCommerce, via <a href="http://www.slideshare.net/mhepp/extending-schemaorg-with-goodrelations-and-wwwproductontologyorg" rel="nofollow">GoodRelations</a>, and <a href="http://www.slideshare.net/mhepp/extending-schemaorg-with-goodrelations-and-wwwproductontologyorg" rel="nofollow">health and medicine</a>, have also been adopted.  This is the pattern with schema.org - a group that understand an area proposes an extension, which if adopted, will broaden the scope and usefulness of this broad vocabulary.

So, the purpose of this library ontology is to start to address that from the point of view of libraries.  To start a conversation around how we [the library interested community] could help the schema.org vocabulary to be more useful for describing the kind of resources that libraries hold, license, or describe.

You also ask who [in this case] the consumers of this data are and who might want to share it in this way.  The consumers are the search engines or 'anyone else on the web', hence the generic nature of the schema.org vocabulary and the high-level of the library extension.  This is not how you would share the full richness contained today in a Marc record, for library to library communication.  As to who would want to share it in this way - anyone with bibliographic information or items that the want to be found more easily.  That obviously includes libraries.

Schema.org is not the only show in town when looking for a way to share descriptions of resources libraries hold, but with it's already significant take up on the wider web (<a href="http://dataliberate.com/2012/06/schema-org-consensus-at-semtechbiz/" rel="nofollow">around 7% of crawled pages</a>) it is something we can not ignore.  I believe we should embrace it as part of the general move towards linked data in the library community.

If you or anyone else what to join in the conversation around this, a great place to start would be to drop me an email at data@oclc.org.

Thanks again, Richard.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>453</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1341788290.2385";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:14:"Richard Wallis";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1341827737.1398";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85427</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-09 03:00:21</wp:comment_date>
			<wp:comment_date_gmt>2012-07-09 10:00:21</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the link correction Richard, and some clarification. I hope that this discussion can take place on <a href="http://lists.w3.org/Archives/Public/public-vocabs/" rel="nofollow">public-vocabs</a> or some other discussion list rather than your personal email box. Cheers :-)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1341828022.3929";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85467</wp:comment_id>
			<wp:comment_author><![CDATA[profiles.google.com/11420235179…]]></wp:comment_author>
			<wp:comment_author_email>dethtron5000@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://profiles.google.com/114202351797438253625</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-10-01 08:49:54</wp:comment_date>
			<wp:comment_date_gmt>2012-10-01 15:49:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[There is a draft schema.org proposal for periodicals (and comics - the proposal came from us at Marvel Entertainment and we have some experience in that domain) here:
http://www.w3.org/wiki/WebSchemas/PeriodicalsComics. The proposal is currently in candidate status.  Hopefully this can inform the mappings you posted earlier.

We are active on the public-vocabs discussion list if you wish to discuss or I'm happy to talk directly.

-peter]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>461</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1349106594.6991";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:41:"profiles.google.com-114202351797438253625";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1349111316.1725";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86078</wp:comment_id>
			<wp:comment_author><![CDATA[Is Your Library Using Schema.org? | Eduhacker]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.eduhacker.net/libraries/library-schema-dot-org.html</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-06-10 13:47:21</wp:comment_date>
			<wp:comment_date_gmt>2013-06-10 20:47:21</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Straw | Indroid &#8211; Ed Summers [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1370897241.48219203948974609375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1371130802.339333057403564453125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>and then the web happened</title>
		<link>http://inkdroid.org/2012/07/11/wikimania/</link>
		<pubDate>Wed, 11 Jul 2012 20:35:27 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4745</guid>
		<description></description>
		<content:encoded><![CDATA[Here is the text of my talk I'm giving at Wikimania today, and the <a href="http://bit.ly/wikimania-wikistream">slides</a>.

Let me begin by saying thank you to the conference organizers for accepting my talk proposal. I am excited to be here at my first WikiMania conference, and hope that it will be the first of many. Similar to millions of other people around the world, I use Wikipedia every day at work and at home. In the last three years I've transitioned from being a consumer to a producer, by making modest edits to articles about libraries, archives, and occasionally music. I had heard horror stories of people having their work reverted and deleted, so I was careful to cite material in my edits. I was pleasantly surprised when editors swooped in not to delete my work, but to improve it. So, I also want to say thanks to all of you for creating such an improbably open and alive community. I know there is room for improvement, but it's a pretty amazing thing you all have built.

And really, this is all my talk about Wikistream is about. Wikistream was born out of a desire to share just how amazing the Wikipedia community is, with people who didn't know it already. I know, I'm preaching to the choir. I also know that I'm speaking in the Technology and Infrastructure track, and I promise to get to some details about how Wikistream works. But really, there's nothing that radically new in Wikistream--and perhaps what I'm going to say would be more appropriate for the GLAM track, or a performance art track, if there was one. If you are a multi-tasker and want to listen to me with one ear, please pull up http://wikistream.inkdroid.org in your browser, and try to make sense of it as I talk. Lets see what breaks first, the application or the wi-fi connection--hopefully neither.

<h2>Wikipedia and the BBC</h2>

A couple years ago I was attending the dev8d conference in London and dropped into a <a href="http://www.meetup.com/Web-Of-Data/events/12317420/">2nd Linked Data Meetup</a> that happened to be going on nearby. Part of the program included presentations from Tom Scott, Silver Oliver and Georgi Kobilarov about some work they did at the BBC. They demo'd two web applications, the BBC Wildlife Finder and BBC Music, that used Wikipedia as a content management platform <sup><a href="#cite1">1</a>, <a href="#cite2">2</a></sup>.

<a href="http://www.bbc.co.uk/nature/life/Blue_Whale"><img src="http://inkdroid.org/images/wikistream-talk/1.png"/></a>
<a href="http://www.bbc.co.uk/nature/life/Blue_Whale"><img src="http://inkdroid.org/images/wikistream-talk/2.png"/></a>
<a href="http://en.wikipedia.org/wiki/Blue_Whale"><img src="http://inkdroid.org/images/wikistream-talk/3.png"/></a>
<a href="http://www.bbc.co.uk/music/artists/3c8ff4c9-b5b9-4e78-ad9d-5157c323de24"><img src="http://inkdroid.org/images/wikistream-talk/4.png"/></a>
<a href="http://www.bbc.co.uk/music/artists/3c8ff4c9-b5b9-4e78-ad9d-5157c323de24"><img src="http://inkdroid.org/images/wikistream-talk/5.png"/></a>
<a href="http://en.wikipedia.org/wiki/Wye_Oak_(band)"><img src="http://inkdroid.org/images/wikistream-talk/6.png"/></a>

If I'm remembering right it was Tom who demonstrated how an edit to a Wikipedia article resulted in the content being immediately updated at the BBC. It seemed like magic. More than that it struck me as mind-blowingly radical for an organization like the BBC to tap into the Wikipedia platform and community like this.

After a few questions I learned from Georgi that part of the magic of this update mechanism was a bot that the BBC created which sits in the #en.wikipedia IRC chatroom, where edits are announced <sup><a href="#cite4">4</a></sup>. I logged into the chatroom and was astonished by the number of edits flying by:

<iframe width="420" height="315" src="http://www.youtube.com/embed/tP89Rxttl7o" frameborder="0" allowfullscreen></iframe>

And remember this was just the English language Wikipedia channel. There are more than <a href="http://meta.wikimedia.org/wiki/IRC/Channels#Raw_feeds">730</a> other Wikimedia related channels where updates are announced. The BBC's use of Wikipedia really resonated with me, but to explain why I need to back up a little bit more.

<h2>Crowdsourcing in the Library</h2>

I work as a software developer at the Library of Congress. In developing web applications there I often end up using data about books, people and topics that have been curated for hundreds of years, and which began to be made available in electronic form in the early 1970s. The library community has had a longstanding obsession with collaboration, or (dare I say) crowdsourcing, to maintain its information about the bibliographic universe. Librarians would most likely call it cooperative cataloging instead of crowdsourcing, but the idea is roughly the same. 

As early as 1850, Charles Jewett proposed that the Smithsonian be established as the national library of the United States, which would (among other things) collect the catalogs of libraries all around the country <a href="#cite3"><sup>2</sup></a>. The Smithsonian wasn't as sure as Jewett, so it wasn't until the 1890s that we saw his ideas take hold when the Library of Congress assumed the role of the national library, and home to the Copyright Office. To this day, copyright registration results in a copy of a registered book being deposited at the Library of Congress. In 1901 the Library of Congress established its printed card service which made its catalog cards available to libraries around the United States and the world.

<a href="http://www.loc.gov/pictures/resource/cph.3c18630/"><img src="http://inkdroid.org/images/wikistream-talk/6.jpg"/></a>

This meant that a book could be cataloged once by one of the growing army of catalogers at the Library of Congress, instead of the same work being done over and over by all the libraries all over the country. But the real innovation happened in 1971 when Fred Kilgour's dream of an online shared cataloging database was powered up at OCLC. This allowed a book to be cataloged by any library, and instantly shared with other libraries around the country. It was at this point that the cataloging became truly cooperative, because catalogers could be anywhere, at any member institution, and weren't required to be in an office at the Library of Congress.

<iframe width="420" height="315" src="http://www.youtube.com/embed/StWinA_Juns" frameborder="0" allowfullscreen></iframe>

This worked for a bit, but then the Web happened. As the Web began to spread in the mid to late 1990s the library community got it into their head that they would catalog it, with efforts like the Cooperative Online Resource Catalog. But the Web was growing too fast, there just weren't enough catalogers who cared, and the tools weren't up to the task, so the project died. 

So when I saw Tom, Silver and Georgi present on the use of Wikipedia as a curated content platform at the BBC, and saw how active the community was I had a bit of a light bulb moment. It wasn't a if-you-can't-beat-em-join-em moment in which libraries and other cultural heritage organizations (like the BBC) fade into the background and become irrelevant, but one in which Wikipedia helps libraries do their job better...and maybe libraries can help make Wikipedia better. It just so happened that this was right as the <a href="http://en.wikipedia.org/wiki/Wikipedia:GLAM">Galleries, Libraries, Archives and Museums (GLAM)</a> effort was kicking off at Wikipedia. I really wanted to be able to help show librarians and others not likely to drop into an IRC chat how active the Wikipedia community was, and that's how Wikistream came to be.

<iframe width="420" height="315" src="http://www.youtube.com/embed/qiukWH-LUxY" frameborder="0" allowfullscreen></iframe>

<h2>How</h2>

So now that you understand the <em>why</em> of Wikistream I'll tell you briefly about the <em>how</em>. When I released Wikistream I got this really nice email from Ward Cunningham, who is a personal hero of mine, and I imagine a lot of you too:

<pre lang="text">
To: wiki-research-l@lists.wikimedia.org
From: Ward Cunningham &lt;ward@c2.com&gt;
Subject: Re: wikistream: displays wikipedia updates in realtime
Date: Jun 16, 2011 7:43:11 am

I've written this app several times using technology from text-to-speech 
to quartz-composer. I have to tip my hat to Ed for doing a better job 
than I ever did and doing it in a way that he makes look effortless. 
Kudos to Ed for sharing both the page and the software that produces 
it. You made my morning. -- Ward
</pre>

Sure enough, my idea wasn't really new at all. But at least I was in good company. I was lucky to stumble across the idea for Wikistream when a Google search for streaming to the browser pulled up <a href="http://socket.io">SocketIO</a>. If you haven't seen it before SocketIO is a JavaScript library that allows you to easily stream data to the browser without needing to care about the transport mechanisms that the browser supports: WebSocket, Adobe FlashSocket, AJAX long polling, AJAX multipart-streaming, Forever iframe, JSONP Polling. It autodetects the capabilities of the browser and the server, and gives you a simple callback API for publishing and consuming events. For example here is the code that runs in your browser to connect to the server and start getting updates:

<pre lang="javascript">
$(document).ready(function() {
  var socket = io.connect();
  socket.on('message', function(msg) {
    addUpdate(msg);
  });
});
</pre>

There's a bit more to it, like loading the SocketIO library, and the details of adding the information about the change stored in the <code>msg</code> JavaScript object (more on that below) to the DOM, but SocketIO makes the hard part of streaming data from the server to the client easy. 

Of course you need a server to send the updates, and that's where things get a bit more interesting. SocketIO is designed to run in a <a href="http://nodejs.org">NodeJS</a> environment with the <a href="http://expressjs.com/">Express</a> web framework. Once you have your webapp set up, you can add SocketIO to it:

<pre lang="javascript">
var express = require("express");
var sio = require("socket.io");

var app = express.createServer();
// set up standard app routes/views
var io = sio.listen(app);
</pre>

Then the last bit is to do the work of listening to the IRC chatrooms and pushing the updates out to the clients that want to be updated. To make this a bit easier I created a reusable library called <a href="http://github.com/edsu/wikichanges/">wikichanges</a> that abstracts away the business of connecting to the IRC channels and parsing the status updates into a JavaScript object, and lets you pass in a callback function that will be given updates as they occur.

<pre lang="javascript">
var wikichanges = require('wikichanges');

var w = wikichanges.WikiChanges();
w.listen(function(msg) {
  io.sockets.emit('message', msg);
});
</pre>

This results in updates being delivered as JSON objects to the client code we started with, where each update looks something like: 

<pre lang="javascript">
{ 
  channel: '#en.wikipedia',
  wikipedia: 'English Wikipedia',
  page: 'Persuasion (novel)',
  pageUrl: 'http://en.wikipedia.org/wiki/Persuasion_(novel)',
  url: 'http://en.wikipedia.org/w/index.php?diff=498770193&oldid=497895763',
  delta: -13,
  comment: '/* Main characters */',
  wikipediaUrl: 'http://en.wikipedia.org',
  user: '108.49.244.224',
  userUrl: 'http://en.wikipedia.org/wiki/User:108.49.244.224',
  unpatrolled: false,
  newPage: false,
  robot: false,
  anonymous: true,
  namespace: 'Article'
  flag: '',
}
</pre>

As I already mentioned I extracted the interesting bit of connecting to the IRC chatrooms, and parsing the IRC colored text into a JavaScript object as a NodeJS library called <a href="http://github.com/edsu/wikichanges">wikichanges</a>. Working with the stream of edits is surprisingly addictive, and I found myself wanting to create some other similar applications:

<ul>
<li><a href="http://wikipulse.herokuapp.com">wikipulse</a> which displays the rate of change of wikipedias as a set of accelerator displays</li>
<li><a href="http://wikitweets.herokuapp.com/">wikitweets</a>: a visualization of how Wikipedia is cited on Twitter</li>
<li><a href="https://github.com/dchud/wikibeat">wikibeat</a>: a musical exploration of how Wikipedia is changing created by <a href="http://twitter.com/dchud">Dan Chudnov</a> and <a href="http://twitter.com/unrulymusic">Chris Burns</a>.
</li></ul>

So wikichanges is there to make it easier to bootstrap applications that want to do things with the Wikipedia update stream. Here is a brief demo of getting wikichanges working on a stock Ubuntu ec2 instance:

<iframe width="420" height="315" src="http://www.youtube.com/embed/XKw7G0y-NJY" frameborder="0" allowfullscreen></iframe>

<h2>What's Next?</h2>

So this was a bit of wild ride, I hope you were able to follow along. I could have spent some time explaining why Node was a good fit for wikistream. Perhaps we can talk about that in the Q/A if there is any time for that. Let's just say I actually reach for Python first when working on a new project, but the particular nature of this application, and tool availability made Node a natural fit. Did we crash it yet?

The combination of the <a href="http://en.wikipedia.org/wiki/Wikipedia:GLAM">GLAM</a> effort with the <a href="http://meta.wikimedia.org/wiki/Wikidata">WikiData</a> are poised to really transform the way cultural heritage organizations contribute to and use Wikipedia. I hope wikistream might help you make the case for Wikipedia in your organization as you make presentations. If you have ideas on how to use the wikistream library to do something with the update stream I would love to hear about them.


<ol>
<li id="cite1"><a href="http://www.w3.org/2001/sw/sweo/public/UseCases/BBC/">Case Study: Use of Semantic Web Technologies on the BBC Web Sites</a> by Yves Raimond, et al.</li>
<li id="cite2"><a href="http://derivadow.com/2009/01/13/the-web-as-a-cms/">The Web as a CMS</a> by Tom Scott.</li>
<li id="cite3"><a href="http://books.google.com/books?id=AVnpwHYP-nIC&lpg=PA3&pg=PA5">Catalog It Once And For All: A History of Cooperative Cataloging in the United States Prior to 1967 (Before MARC)</a> by Barbara Tillett, in Cooperative Cataloging: Past, Present, and Future. Psychology Press, 1993, page 5.</li>
<li id="cite4">After hitting publish on this post I <a href="http://twitter.com/metade/status/223536432684351489">learned</a> that the BBC's bot was written by <a href="http://www.metade.org/">Patrick Sinclair</a></li>

</ol>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4745</wp:post_id>
		<wp:post_date>2012-07-11 13:35:27</wp:post_date>
		<wp:post_date_gmt>2012-07-11 20:35:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wikimania</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Wikimania Revisited</title>
		<link>http://inkdroid.org/2012/07/24/wikimania-revisited/</link>
		<pubDate>Tue, 24 Jul 2012 13:25:32 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4830</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://wikimania2012.wikimedia.org/wiki/File:Wikimania_2012_Group-0001.jpg"><img src="http://inkdroid.org/images/wikimaniacs2012.jpg" /></a> 

I recently attended the <a href="http://wikimania2012.wikimedia.org/wiki/Main_Page">Wikimania</a> conference here in Washington, DC. I really can't express how amazing it was to be a Metro ride away from more than 1,400 people from 87 countries who were <a href="http://en.wikipedia.org/wiki/Wikipedia:Ten_things_you_may_not_know_about_Wikipedia#We.27re_in_it_for_the_long_haul">passionate</a> about creating a world in which every single human being can freely share in the sum of all knowledge. It was my first Wikimania, and I had pretty high expectations, but I was blown away by the level of enthusiasm and creativity of the attendees. Since my employer supported me by allowing me to spend the week there, I thought I would jot down some notes about the things that I took from the conference, from the perspective of someone working in the cultural heritage sector.

<h2>Archivy</h2>

Of course the big news from Wikimania for folks like me who work in libraries and archives was the plenary speech by the Archivist of the United States, <a href="http://blogs.archives.gov/aotus/">David Ferriero</a>. Ferriero did an excellent job of connecting NARA's mission to that of the Wikipedia community. In particular he stressed that NARA cannot solve difficult problems like the preservation of electronic records without the help of open government, transparency and citizen engagement to shape its policies and activities. As a library software developer I'm as interested as the next person in technical innovations in the digital preservation space: be they new repository software, flavors of metadata and digital object packaging, web services and protocols, etc. But over the past few years I've been increasingly convinced that access to the content that is being preserved is an absolutely vital ingredient to its preservation. If open access (as in the case of NARA) isn't possible due to licensing concerns, then it is still essential to let access by <em>some user community</em> drive and ground efforts to collect and preserve digital content. Seeing high level leadership in the cultural heritage space (and from the federal government no less) address this issue was really inspiring. 

<blockquote>
At the Archives our concepts of openness and access are embedded in our mission. The work we do every day is rooted in the belief that citizens have the right to see, examine, and learn from the records that guarantee citizens rights, document government actions, and tell the story of our nation. 

My biggest challenge is visibility: not everyone knows who we are, what we do, or more importantly, the amazing resources we collect and house. The lesson I learned in my time in New York is that it isn't good enough to create great digital collections, and sit back and expect people to find you. You need to be where the people are.
</blockquote>

The astounding thing is that it's not just talk--Ferriero went on to describe several efforts of how the Archives is executing on collaboration with the Wikipedia community, which is also documented at a high level in NARA's <a href="http://www.archives.gov/open/open-plan.html">Open Government Plan</a>. One example that stood out for me was NARA's <a href="http://www.archives.gov/historical-docs/todays-doc">Today's Document</a> website which highlights documents from its collections. On June 1st, 2011 they featured a photograph of <a href="http://www.archives.gov/historical-docs/todays-doc/index.html?dod-date=601">Harry P. Perry</a> who was the first African American to enlist in the US Marine Corps after it was desegregated on June 1st, 1942. NARA's Wikipedian in Residence <a href="http://en.wikipedia.org/wiki/User:Dominic">Dominic McDevitt-Parks'</a> efforts to bring archival content to the attention of Wikipedians resulted in a new article <a href="http://en.wikipedia.org/wiki/Desegregation_in_the_United_States_Marine_Corps">Desegregation in the United States Marine Corps</a> being created that same day...and the photograph on NARA's website was viewed more than <em>4 million times in 8 hours</em>. What proportion of the web traffic was driven by Wikipedia specifically rather than other social networking sites wasn't exactly clear, but the point is that this is what happens when you get your content where the users are. If my blog post is venturing into <a href="http://www.urbandictionary.com/define.php?term=Too+long+didn't+read">tl;dr</a> territory, please be sure to at least watch his speech, it'll just take 20 minutes. 

<iframe width="560" height="315" src="http://www.youtube.com/embed/47pEcmXjt8E" frameborder="0" allowfullscreen></iframe>

<h2>Resident Wikipedians</h2>

In a similar vein <a href="https://twitter.com/sosarasays">Sara Snyder</a> made a strong case for the use of archival materials on Wikipedia in her talk <a href="http://wikimania2012.wikimedia.org/wiki/Submissions/5_Reasons_Why_Archives_are_an_Untapped_Goldmine_for_Wikimedians">5 Reasons Why Archives are an Untapped Goldmine for Wikimedians</a>. She talked about the work that <a href="http://sarahstierch.com/">Sarah Stierch</a> did as the Wikipedia in Residence at the <a href="http://www.aaa.si.edu/">Smithsonian Archives of American Art</a>. The <a href="http://en.wikipedia.org/wiki/Wikipedia:GLAM/AAA">partnership</a> resulted in <a href="http://commons.wikimedia.org/wiki/Category:Images_from_the_Archives_of_American_Art">~300</a> WPA images being uploaded to <a href="http://commons.wikimedia.org">Wikimedia Commons</a>, <a href="http://en.wikipedia.org/wiki/Wikipedia:GLAM/AAA/Outcomes">37</a> new Wikipedia articles, and new connections with a community of <a href="http://en.wikipedia.org/wiki/Wikipedia:GLAM/AAA/Participants#List_of_project_E-Volunteers">volunteers</a> who participated in edit-a-thons to improve Wikipedia and learn more about the AAA collections. She also pointed out that since 2010 Wikipedia has driven more traffic to the Archives of American Art website than all other social media combined.

In the same session Dominic McDevitt-Parks <a href="http://wikimania2012.wikimedia.org/wiki/Submissions/Wikisource_and_the_US_National_Archives:_a_case_study">spoke</a> about his activities as the Wikipedian in Residence at the US National Archives. Dominic focused much of his presentation on NARA's <a href="http://en.wikisource.org/wiki/Wikisource:WikiProject_NARA/To_prepare">digitization work</a>, largely done by volunteers, the use of <a href="http://commons.wikimedia.org/">Wikimedia Commons</a> as a content platform for the images, and ultimately <a href="http://en.wikisource.org">WikiSource</a> as a platform for transcribing the documents. The finished documents are then linked to from NARA's Online Catalog, as in this example: <a href="http://arcweb.archives.gov/arc/action/ExternalIdSearch?id=306647&jScript=true">Appeal for a Sixteenth Amendment</a> from the National Woman Suffrage Association. NARA also prominently links out to the content waiting to be transcribed at WikiSource on its <a href="http://www.archives.gov/citizen-archivist/">Citizens Archivist Dashboard</a>. If you are interested in learning more, Dominic has written a bit about the work with WikiSource on the NARA <a href="http://blogs.archives.gov/TextMessage/2011/07/25/wikimedia-and-the-new-collaborative-digital-archives/">blog</a>. Both Dominic and Sara will be speaking next month at the <a href="http://www2.archivists.org/conference/2012/san-diego">Society of American Archivists Annual Meeting</a> making the case for Wikipedia to the archival community. Their talk is called <a href="http://saa.archivists.org/4DCGI/events/eventdetail.html?Action=Events_Detail&Time=359201045&InvID_W=2353">80,000 Volunteers Can't Be Wrong: The Case for Greater Collaboration with Wikipedia</a>, and I encourage you attend if you will be at SAA.

The arrival of <a href="http://outreach.wikimedia.org/wiki/Wikipedian_in_Residence">Wikipedians in Residence</a> is a welcome seachange in the Wikipedia community, where historically there had been some uncertainty about the best way for cultural heritage organizations to highlight their original content in Wikipedia articles. As Sara pointed out in her talk, it helps both sides (the institutional side, and the Wikipedia side) to have an actual, experienced Wikipedian on site to help the organization understand how they want to engage the community. Having direct contact with archivists, curators and librarians that know their collections backwards and forwards also helps the resident in knowing how to direct their work, and the work of other Wikipedians. The Library of Congress made an announcement at the Wikimania reception that the <a href="http://www.wdl.org">World Digital Library</a> are seeking a Wikipedia in Residence. I don't work directly on the project anymore, but I know people who do, so let me know if you are interested and I can try to connect the dots.

I think in a lot of ways the residency program is an excellent start, but really it's just that--a start. The task at hand of connecting the Wikipedia community and article content with the collections of galleries, libraries, archives and museums is a huge one. One person, especially a temporary volunteer, can only do so much. As you probably know, Wikipedia editors can often be found embedded in cultural heritage organizations. It's one of the reasons why we started having informal Wikipedia lunches at the Library of Congress: to see what can be done at the grass roots level by staff to integrate Wikipedia into our work. When we started to meet I learned about an earlier, 4 year old effort to create a policy that provides guidance to staff about how to interact with the Wikipedia community as editors. Establishing a residency program is an excellent way to signal a change in institutional culture, and to bootstrap and focus the work. But I think the residencies also highlight the need to empower staff throughout the organization to participate as well, so that after the resident leaves the work goes on. In addition to establishing a WDL Wikipedian in Residence I would love to see the Library of Congress put the finishing touches on its Wikipedia policy that would empower staff to use and contribute to Wikipedia as part of their work, without lingering doubt about whether it was correct or not. It would probably be helpful for other organizations to publish theirs as examples for other organizations wanting the same.

<h2>Wikipedia as a Platform</h2>

Getting back to Wikimania, I wanted to highlight a few other GLAM related projects that use Wikipedia as a platform.

<a href="http://dbm.neuro.uni-jena.de/people/alumni/daniel-mietchen/">Daniel Mietchen</a> <a href="http://wikimania2012.wikimedia.org/wiki/Submissions/Open_Access_Media_Importer">spoke</a> about work he was doing around the <a href="">Open Access Media Importer</a> (OAMI). The OAMI is a <a href="https://github.com/erlehmann/open-access-media-importer">tool</a> that harvests media files (images, movies, etc) from open access materials and uploads them to Wikimedia Commons for use in article content. Efforts to date have focused primarily on <a href="http://www.ncbi.nlm.nih.gov/pubmed/">PubMed</a> from the National Institutes of Health. As someone working in the digital preservation field one of the interesting outcomes of the work so far was a table that illustrated the media formats present in PubMed:

<a href="http://wikimania2012.wikimedia.org/wiki/File:MIME_types_of_supplementary_files_in_the_Open_Access_Subset_of_PubMed_Central_as_of_6_July_2012.png"><img src="http://inkdroid.org/images/open-access-media-importer-formats.png"/></a>

Since Daniel and other OAMI collaborators are scientists they have been focused primarily on science related media...so they naturally are interested in working with <a href="http://arxiv.org/">arXiv</a>. arXiv is a heavily trafficked, <a herf="http://arxiv.org/help/support">volunteer supported</a>, pre-print server,  that is normally a poster child for open repositories. But one odd thing about arXiv that Daniel pointed out is that while arXiv collects licensing information from authors as part of deposit, they do not indicate in the user interface which license has been used. This makes it particularly difficult for the OAMI to determine which content can be uploaded to the Wikimedia Commons. I learned from Simeon Warner shortly afterwards that while the licensing information doesn't show up in the UI currently, and isn't present in all the metadata formats that their <a href="http://arxiv.org/help/oa/index">OAI-PMH</a> service provides, it can be found squirreled away in the <a href="http://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:0804.2273&metadataPrefix=arXivRaw">arXivRaw format</a>. So it should be theoretically possible to modify the OAMI to use  arXivRaw. 

Another challenges the OAMI faces is extraction of metadata. For example media files often don't share all the subject keywords that are appropriate for the entire article. So knowing which ones to apply can be difficult. In addition, metadata extraction from Wikimedia Commons was reported to not be optimal, since it involves parsing mediawiki templates, which limits the downstream use of the content added to the Commons. I don't know if the <a href="http://www.plos.org/">Public Library of Science</a> is on the radar for harvesting, but if it isn't it should be. The OAMI work also seems loosely related to the issue of research data storage and citation which seems to be on the <a href="https://www.conftool.net/or2012/sessions.php">front burner</a> for those interested in digital repositories. Jimmy Wales has <a href="http://www.guardian.co.uk/commentisfree/2012/may/01/open-free-access-academic-research">reportedly</a> been advising the UK government on how to making funded research available to the public. I'm not sure if datasets fit the purview of the Wikimedia Commons, but since Excel is #3 in the graph above perhaps it is. It might be interesting to think more about Wikimedia Commons as a platform for publishing (and citing) datasets.

I learned about another interesting use of the Wikimedia Commons from Maarten Dammers and Dan Entous during their <a href="http://wikimania2012.wikimedia.org/wiki/Submissions/The_GLAMwiki_toolset_project_-_work_in_progress">talk</a> about the GLAMwiki Toolset. The project is a partnership between Wikipedia Netherlands and <a href="http://www.europeana.eu/portal/">Europeana</a>. If you aren't already familiar with Europeana it is an EU funded effort to enhance access to European cultural heritage material on the Web. The project is just getting kicked off now, and is aiming to:

<blockquote>
...develop a scalable, maintainable, ease to use system for mass uploading open content from galleries, libraries, archives and museums to Wikimedia Commons and to create GLAM-specific requirements for usage statistics.
</blockquote>

Wikimedia Commons can be difficult to work with in an automated, batch oriented way for a variety of reasons. One that was mentioned above is metadata. The GLAMwiki Toolset will provide some mappings from commonly held metadata formats (starting with Dublin Core) to Commons templates, and will provide a framework for adapting the tool to custom formats. Also there is a perceived need for tools to manage batch imports as well as exports from the Commons. The other big need are usable analytics tools that let you see how content is used and referenced on the Commons once it has been uploaded. Maarten indicated that they are seeking participation in the project from other GLAM organizations. I imagine that there are other organizations that would like to use the Wikimedia Commons as a content platform, to enable collaboration across institutional boundaries. Wikipedia is one of the most popular destinations on the Web, so they have been forced to scale their technical platform to support this demand. Even the largest cultural heritage organizations can often find themselves bound to somewhat archaic legacy systems, that can make it difficult to similarly scale their infrastructure. I think services like Wikimedia Commons and WikiSource have a lot to offer cash strapped organizations that want to do more to provide access to their organizations unique materials on the Web, but are not in a position to make the technical investments to make it happen. I'm hoping that efforts like the GLAMWiki toolset will make this easier to achieve, and is something I personally would like to get involved in.

Incidentally, one of the more interesting technical track talks I attended was a <a href="http://wikimania2012.wikimedia.org/wiki/Submissions/Swift_and_the_Media_Storage_System">talk</a> by Ben Hartshorne from the Wikimedia Foundation Operations Team, about their transition from NFS to <a href="http://www.openstack.org/software/openstack-storage/">Openstack Swift</a> for media storage. I had some detailed notes about this talk, but proceeded to lose them. I seem to remember that in total, the various Wikimedia properties amount to 40T of media storage (images, videos, etc), and they want to be able to grow this to 200T this year. Ben included lots of juicy details about the hardware and deployment of Swift in their infrastructure, so I've got an email out to him to see if he can share his slides <em>(update: he just <a href="http://wikimania2012.wikimedia.org/wiki/File:Swift_Presentation_for_Wikimania.pdf">shared them</a>, thanks Ben!</em>). The placement of various caches (Swift is an HTTP REST API), as well as the hooks into MediaWiki were really interesting to me. The importance of URL addressable object storage for bitstreams in an enterprise that is made up of many different web applications can't be overstated. It was also fun to hear about the impact that digitization projects like Wikipedia Loves Monuments and the NARA work mentioned above, are having on the backend infrastructure. It's great to hear that Wikipedia is planning for growth in the area of media storage, and can scale horizontally to meet it, without paying large sums of money for expensive, proprietary, vendor supplied <a href="http://en.wikipedia.org/wiki/Network-attached_storage">NAS</a> solutions. What wasn't entirely clear from the presentation is whether there is a generic tipping point where investing in staff and infrastructure to support something like Swift becomes more cost-effective than using a storage solution like Amazon S3. Ben did indicate that there use of Swift and the abstractions they built into Mediawiki would allow for using storage APIs like S3.

Before I finish this post, there were a couple other Wikipedia related topics that I didn't happen to see discussed at Wikimania (it's a multi-track event so I may have just missed it). One is the topic of image citation on Wikipedia. <a href="http://www.loc.gov/today/pr/2010/10-243.html">Helena Zinkham</a> (Chief of the Prints and Photographs Division at the Library of Congress) recently floated a project proposal at LC's Wikipedia Lunch to more prominently place the source of an image in Wikipedia articles. For an example of what Helena is talking about take a look at the article for <a href="http://en.wikipedia.org/wiki/Walt_whitman">Walt Whitman</a>: notice how the caption doesn't include information about where the image came from? If you click on the image you get a detail page that does indicate that the photograph is from LC's Prints & Photographs collection, with a link back to the Prints & Photographs Online Catalog. I agree with Helena that more prominent information about the source of photographs and other media in Wikipedia could encourage more participation from the GLAM community. What the best way to proceed with the idea is still in question. I'm new to the way projects get started and <a href="http://en.wikipedia.org/wiki/Wikipedia:Requests_for_comment">RFCs</a> work there. Hopefully we will continue to work on this in the context of the grassroots Wikipedia work at LC. If you are interested please drop me an <a href="mailto:ehs@pobox.com">email</a>

Another Wikipedia project directly related to my $work is the <a href="http://en.wikipedia.org/wiki/Wikipedia:WikiProject_Digital_Preservation">Digital Preservation WikiProject</a> that the <a href="http://www.digitalpreservation.gov/ndsa/">National Digital Stewardship Alliance</a> is trying to kickstart. One of the challenges of digital preservation is the identification of file formats, and their preservation characteristics. English Wikipedia currently has 325 articles about <a href="http://en.wikipedia.org/wiki/Category:Computer_file_formats">Computer File Formats</a>, and one of the goals of the Digital Preservation project is to enhance these with predictable infoboxes that usefully describe the format. External data sources such as <a href="http://www.nationalarchives.gov.uk/PRONOM/Default.aspx">PRONOM</a> and <a href="http://udfr.org/">UDFR</a> also contain information about data formats. It's possible that some of them could be used to improve Wikipedia articles, to more widely disseminate digital preservation information. Also, as Ferriero noted, it's important for cultural heritage organizations to get their information out to where the people are. Jason Scott of <a href="http://www.archiveteam.org/">ArchiveTeam</a> has been talking about a similar <a href="http://www.archiveteam.org/index.php?title=Just_Solve_the_Problem_2012">project</a> to aggregate information about file formats to build better tools for format identification. While I can understand the desire to build a new wiki to support this work, and there are challenges to working with the Wikipedia community, I think <a href="http://en.wikipedia.org/wiki/Linus'_Law">Linus' Law</a> points the way to using Wikipedia.

<h2>Beginning</h2>

So, I could keep going, but in the interests of time (yours and mine) I have to wrap this Wikimania post up (for now). Thanks for reading this far through my library colored glasses. Oddly I didn't even get to mention the most exciting and high profile <a href="http://meta.wikimedia.org/wiki/Wikidata">Wikidata</a> and <a href="http://www.mediawiki.org/wiki/VisualEditor">Visual Editor</a> projects that are under development, and are poised to change what it means to use and contribute to Wikipedia for everyone, not just GLAM organizations. Wikidata is of particular interest to me because if successful it will bring many of the ideas of the Linked Data to solve an eminently practical problem that Wikipedia faces. In some ways the WikiData project is following in the footsteps of the successful dbpedia and Google Freebase projects. But there is a reason why Freebase and Dbpedia have spent time engineering their Wikipedia updates--because it's where the users are creating content. Hopefully I'll be able to attend Wikimania next year to see how they are doing. And I hope that my first Wikimania marks the beginning of a more active engagement in what Wikipedia is doing to transform the Web and the World.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4830</wp:post_id>
		<wp:post_date>2012-07-24 06:25:32</wp:post_date>
		<wp:post_date_gmt>2012-07-24 13:25:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wikimania-revisited</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="images"><![CDATA[images]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="photographs"><![CDATA[photographs]]></category>
		<category domain="post_tag" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="storage"><![CDATA[storage]]></category>
		<category domain="post_tag" nicename="wikidata"><![CDATA[wikidata]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85437";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85434</wp:comment_id>
			<wp:comment_author><![CDATA[Michelle Davison]]></wp:comment_author>
			<wp:comment_author_email>michelledavison@me.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.michelledavison.net</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-24 09:54:42</wp:comment_date>
			<wp:comment_date_gmt>2012-07-24 16:54:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for posting this information. I went to many of the non-GLAM sessions, so it's nice to hear what went on in some of the other tracks.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>454</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1343148882.7887";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:15:"MichelleDavison";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1343149776.1611";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85437</wp:comment_id>
			<wp:comment_author><![CDATA[aubreymcfato.myopenid.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://aubreymcfato.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-25 02:23:42</wp:comment_date>
			<wp:comment_date_gmt>2012-07-25 09:23:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed, thanks for sharing your thougths. It's a pity I missed your talk (I'll check the video when it comes out), and it's a pity we could not get to you for the unconference about Wikisource :-) If you are interested, you can find here some slides (http://commons.wikimedia.org/wiki/Category:Wikisource_presentation_slides), mine are the last ones. Moreover, there is a wikisource-l mailing list in which we would like to continue the work in the next months, and a technically skilled librarian we'd love to have on board :-) 
Cheers, Aubrey]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>455</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1343208222.445";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:25:"aubreymcfato.myopenid.com";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1343210712.9693";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85438</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-25 05:38:42</wp:comment_date>
			<wp:comment_date_gmt>2012-07-25 12:38:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Aubrey - I'm subscribing to the wikisource list now--thanks! Unfortunately circumstances beyond my control prevented me from getting to the unconference at Wikimania.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>85437</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1343219924.0482";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85439</wp:comment_id>
			<wp:comment_author><![CDATA[Breandán]]></wp:comment_author>
			<wp:comment_author_email>b@breandan.org</wp:comment_author_email>
			<wp:comment_author_url>http://breandan.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-25 07:08:24</wp:comment_date>
			<wp:comment_date_gmt>2012-07-25 14:08:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for posting your wikimania experience -- it's so nice to see that these questions of use and accessibility of content are taken so seriously.  We passed around your article here at Europeana, so thanks for the shout-out, as well. We have high hopes for the GLAMwiki toolset -- it's one of a couple of Wikipedia collaborations that we have going on at the moment, along with editathons and some other co-sponsored events.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>456</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1343225305.4638";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:3:"bfk";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1343226009.6051";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85442</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-07-25 20:46:14</wp:comment_date>
			<wp:comment_date_gmt>2012-07-26 03:46:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks Breandán ; now I'm intruiged, what are the other Wikipedia/Europeana collaborations that are going on?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>85439</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1343274374.5984";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85447</wp:comment_id>
			<wp:comment_author><![CDATA[hangingtogether.org &raquo; Blog Archive &raquo; Wikimania 2012: copyright and closing thoughts]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://hangingtogether.org/?p=2047</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-08-14 12:59:52</wp:comment_date>
			<wp:comment_date_gmt>2012-08-14 19:59:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] at some other blog posts summarizing the conference from the LAM perspective, see Ed Summers on Wikimania Revisited and the Biodiversity Heritage Library&#8217;s report, Wikimania 2012 &amp; BHL Related posts:A [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1344974392.6925";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1344975898.4818";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>From Polders to Postmodernism</title>
		<link>http://inkdroid.org/2012/09/01/from-polders-to-postmodernism/</link>
		<pubDate>Sun, 02 Sep 2012 01:53:09 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4898</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.goodreads.com/book/show/6234836-from-polders-to-postmodernism" style="float: left; padding-right: 20px"><img alt="From Polders to Postmodernism: A Concise History of Archival Theory" border="0" src="http://photo.goodreads.com/books/1266587251m/6234836.jpg" /></a><a href="http://www.goodreads.com/book/show/6234836-from-polders-to-postmodernism">From Polders to Postmodernism: A Concise History of Archival Theory</a> by <a href="http://www.goodreads.com/author/show/2837239.John_Ridener">John Ridener</a><br />
My rating: <a href="http://www.goodreads.com/review/show/388346754">3 of 5 stars</a><br /><br />
This was a nice little find for my continuing self-education in archives. As its title suggests, it's a short survey (less than 200 pages), that traces a series of paradigm shifts in archival theory starting in the 19th century Netherlands leading up to the present. Ridener focuses on the approaches to subjectivity and objectivity in archival theory in order to show how the theories have changed and built on each other over the last 200 years. He does a nice job of sketching the context for the theories, the changes in society and technology that drove them, as well as some interesting biographical material about individuals such as Jenkins and Schellenberg. After having just read <a href="http://www.goodreads.com/book/show/11167093-controlling-the-past">Controlling the Past</a> I felt like I had some exposure to contemporary thinking about archives, but was lacking some of the historical background, so this book was very helpful. I think I might have to read Schellenberg's <a href="http://www.goodreads.com/book/show/4996379-modern-archives">Modern Archives</a> now, especially because of the NARA connection. But that might get sidelined to read more of Terry Cook's work on macro-appraisal. My only small complaint is that I noticed quite a few typos in the first half of the book, which got a little distracting at times. 
<br /><br />
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4898</wp:post_id>
		<wp:post_date>2012-09-01 18:53:09</wp:post_date>
		<wp:post_date_gmt>2012-09-02 01:53:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>from-polders-to-postmodernism</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="appraisal"><![CDATA[appraisal]]></category>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[history]]></category>
		<category domain="post_tag" nicename="philosophy"><![CDATA[philosophy]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[no]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>fido test suite</title>
		<link>http://inkdroid.org/2012/09/02/fido-test-suite/</link>
		<pubDate>Mon, 03 Sep 2012 03:29:48 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4904</guid>
		<description></description>
		<content:encoded><![CDATA[I work in a digital preservation group at the Library of Congress where we do a significant amount of work in Python. Lately, I've been spending some time with <a href="https://github.com/openplanets/fido">OpenPlanet's FIDO</a> utility, mainly to see if I could refactor it so that it's a bit easier to use as a Python module, for use in other Python applications. At the moment FIDO is designed to be used from the command line. This work involved more than a little bit of refactoring, and the more I looked at the code, the more it became clear that a test suite would be useful to have as a safety net.

Conveniently, I also happened to have been reading a recent report from the National Library of Australia on <a href="http://www.openplanetsfoundation.org/blogs/2012-08-12-file-characterisation-tools-report-testing-project-conducted-national-library">File Characterization Tools</a>, which in addition to talking about FIDO, pointed me at the <a href="http://digitalcorpora.org/corpora/files">govdocs1</a> dataset. Govdocs1 is a dataset of 1 million files harvested from the .gov domain by the NSF funded <a href="http://digitalcorpora.org">Digital Corpora</a> project. The data was collected to serve as a public domain corpus for forensics tools to use as a test bed. I thought it might be useful to survey the filenames in the dataset, and cherry pick out formats of particular types for use in my FIDO test suite.

So I wrote a little <a href="https://github.com/edsu/fido/blob/master/test-data/download.py">script</a> that crawled all the filenames, and kept track of file extensions used. Here are the results:

<table>
<tr><th>extension</th><th>count</th></tr>
<tr><td>pdf</td><td>232791</td></tr>
<tr><td>html</td><td>191409</td></tr>
<tr><td>jpg</td><td>109281</td></tr>
<tr><td>txt</td><td>84091</td></tr>
<tr><td>doc</td><td>80648</td></tr>
<tr><td>xls</td><td>66599</td></tr>
<tr><td>ppt</td><td>50257</td></tr>
<tr><td>xml</td><td>41994</td></tr>
<tr><td>gif</td><td>36301</td></tr>
<tr><td>ps</td><td>22129</td></tr>
<tr><td>csv</td><td>18396</td></tr>
<tr><td>gz</td><td>13870</td></tr>
<tr><td>log</td><td>10241</td></tr>
<tr><td>eps</td><td>5465</td></tr>
<tr><td>png</td><td>4125</td></tr>
<tr><td>swf</td><td>3691</td></tr>
<tr><td>pps</td><td>1629</td></tr>
<tr><td>kml</td><td>995</td></tr>
<tr><td>kmz</td><td>949</td></tr>
<tr><td>hlp</td><td>660</td></tr>
<tr><td>sql</td><td>632</td></tr>
<tr><td>dwf</td><td>474</td></tr>
<tr><td>java</td><td>323</td></tr>
<tr><td>pptx</td><td>219</td></tr>
<tr><td>tmp</td><td>196</td></tr>
<tr><td>docx</td><td>169</td></tr>
<tr><td>ttf</td><td>104</td></tr>
<tr><td>js</td><td>92</td></tr>
<tr><td>pub</td><td>76</td></tr>
<tr><td>bmp</td><td>75</td></tr>
<tr><td>xbm</td><td>51</td></tr>
<tr><td>xlsx</td><td>46</td></tr>
<tr><td>jar</td><td>34</td></tr>
<tr><td>zip</td><td>27</td></tr>
<tr><td>wp</td><td>17</td></tr>
<tr><td>sys</td><td>8</td></tr>
<tr><td>dll</td><td>7</td></tr>
<tr><td>exported</td><td>5</td></tr>
<tr><td>exe</td><td>5</td></tr>
<tr><td>tif</td><td>3</td></tr>
<tr><td>chp</td><td>2</td></tr>
<tr><td>pst</td><td>1</td></tr>
<tr><td>squeak</td><td>1</td></tr>
<tr><td>data</td><td>1</td></tr>
</table>

With this list in hand, I downloaded an example of each file extension, ran it through the current release of FIDO, and used the output to generate a <a href="https://github.com/edsu/fido/blob/master/test.py">test suite</a> for my new refactored version. Interestingly, <a href="http://travis-ci.org/#!/edsu/fido/builds/2314989">two tests fail</a>:

<pre>
Traceback (most recent call last):
  File "/home/ed/Projects/fido/test.py", line 244, in test_pst
    self.assertEqual(i.puid, "x-fmt/249")
AssertionError: 'x-fmt/248' != 'x-fmt/249'

======================================================================
FAIL: test_pub (test.FidoTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/ed/Projects/fido/test.py", line 260, in test_pub
    self.assertEqual(i.puid, "x-fmt/257")
AssertionError: 'x-fmt/252' != 'x-fmt/257'
</pre>

I'll need to dig in to see what could be different between the two versions that would confuse <a href="http://beta.domd.info/pronom/x-fmt/248">x-fmt/248</a> with <a href="http://beta.domd.info/pronom/x-fmt/249">x-fmt/249</a> and <a href="http://beta.domd.info/pronom/x-fmt/252">x-fmt/252</a> with <a href="http://beta.domd.info/pronom/x-fmt/257">x-fmt/257</a>. Perhaps it is related to Dave Tarrant's <a href="http://www.openplanetsfoundation.org/blogs/2012-08-29-years-registry-why-has-preservation-community-not-solved-problem-well-managed-and">recent post</a> about how FIDO's identification patterns have flip flopped in the past.

You may have noticed that I'm linking the PUIDs to <a href="https://twitter.com/anjacks0n">Andy Jackson</a>'s <a href="http://beta.domd.info/">PRONOM Prototype Registry</a> (<a href="http://www.openplanetsfoundation.org/blogs/2011-01-14-building-collaborative-format-registry-editor">built in 6 days</a> with Drupal) instead of the official PRONOM registry. I did this because a <a href="https://www.google.com/#q=x-fmt/257">Google search</a> for the PRONOM identifier (PUID) pulled up a <a href="http://beta.domd.info/pronom/x-fmt/257">nice detail</a> page for the format in Andy's prototype, and it doesn't seem possible (at least in the 5 minutes I tried) to link directly to a file format record in the official PRONOM registry. I briefly tried the Linked Data prototype, but it proved difficult to search for a given PUID (server errors,  the unforgiving glare of SPARQL query textareas, etc). 

I hope OpenPlanets and/or the National Archives give Andy's Drupal experiment a fair shake. Getting a functional PRONOM registry running in 6 days with an opensource toolkit like Drupal definitely seems more future proof than spending years with a contractor only to get closed source code. The Linked Data prototype looks promising, but as the recent final report on the <a href="http://udfr.org/project/UDFR-final-report.pdf">Unified Digital Format Registry</a> project highlights, choosing to build on a semantic web stack has its risks compared with more mainstream web publishing frameworks or content management systems like Drupal. PRONOM just needs an easy way for digital preservation practitioners to be able to collaboratively update the registry, and for each format to have a unique URL that uses the PUID. My only complaint is that Andy's prototype seemed to advertise RDF/XML in the HTML, but it seemed to return an empty RDF document, for example the HTML at <a href="http://beta.domd.info/pronom/x-fmt/248">http://beta.domd.info/pronom/x-fmt/248</a> has a &lt;link&gt; that points at <a href="http://beta.domd.info/node/1303/rdf">http://beta.domd.info/node/1303/rdf</a>. 

I admit I am a fan of linked data, or being able to get machine readable data back (RDFa, Microdata, JSON, RDF/XML, XML, etc) from <a href="http://www.w3.org/Provider/Style/URI.html">Cool URLs</a>. But using triplestores, and SPARQL don't seem to be terribly important things for PRONOM to have at this point. And if they are there under the covers, there's no need to confront the digital preservation practitioner with them. My guess is that they want to have an application that lets them work with their peers to document file formats, not learn a new query or ontology language. Perhaps Jason Scott's <a href="http://ascii.textfiles.com/archives/3645">Just Solve the Problem</a> effort in October, will be a good kick in the pants to mobilize grassroots community work around digital formats.

Meanwhile, I've finished up the FIDO API changes and the test suite enough to have submitted a <a href="https://github.com/openplanets/fido/pull/31">pull request</a> to OpenPlanets. My fork of the OpenPlanets repository is similarly <a href="https://github.com/edsu/fido">on Github</a>. I'm not really holding my breath waiting for it to be accepted, as it represents a significant change, and they have their own published <a href="http://wiki.opf-labs.org/display/TR/FIDO+roadmap">roadmap</a> of work to do. But I am hopeful that they will recognize the value in having a test suite as a safety net as they change and refactor FIDO going forward. Otherwise I guess it could be the beginnings of a fido2, but I would like to avoid that particular future.

<em>Update: after posting this Ross Spencer tweeted me some instructions for linking to PRONOM

https://twitter.com/beet_keeper/status/242515266146287616

Maybe I missed it, but PRONOM could use a page that describes this.</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4904</wp:post_id>
		<wp:post_date>2012-09-02 20:29:48</wp:post_date>
		<wp:post_date_gmt>2012-09-03 03:29:48</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>fido-test-suite</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="digital-preservation"><![CDATA[digital preservation]]></category>
		<category domain="post_tag" nicename="drupal"><![CDATA[drupal]]></category>
		<category domain="post_tag" nicename="file-formats"><![CDATA[file formats]]></category>
		<category domain="post_tag" nicename="openplanets"><![CDATA[openplanets]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="pronom"><![CDATA[pronom]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_7a7d7d575f3a0a3f247bf226121d6ae0</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p><a href="https://twitter.com/edsu">@edsu</a> Ed, see <a href="http://t.co/BKUvxUxv">http://t.co/BKUvxUxv</a> replace fmt for x-fmt for x-puids, and attach .xml for xml data to link directly to PRONOM data.</p>&mdash; Ross Spencer (@beet_keeper) <a href="https://twitter.com/beet_keeper/statuses/242515266146287616">September 3, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_4847588a0717b2d54f38f36f3ce5b7a0</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>@<a href="https://twitter.com/edsu">edsu</a> Ed, see <a href="http://t.co/BKUvxUxv" title="http://www.nationalarchives.gov.uk/PRONOM/fmt/41">nationalarchives.gov.uk/PRONOM/fmt/41</a> replace fmt for x-fmt for x-puids, and attach .xml for xml data to link directly to PRONOM data.</p>&mdash; Ross Spencer (@beet_keeper) <a href="https://twitter.com/beet_keeper/status/242515266146287616">September 3, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_3a784c379360c896c4317207149ecd62</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>@<a href="https://twitter.com/edsu">edsu</a> Ed, see <a href="http://t.co/BKUvxUxv" title="http://www.nationalarchives.gov.uk/PRONOM/fmt/41">nationalarchives.gov.uk/PRONOM/fmt/41</a> replace fmt for x-fmt for x-puids, and attach .xml for xml data to link directly to PRONOM data.</p>&mdash; Ross Spencer (@beet_keeper) <a href="https://twitter.com/beet_keeper/status/242515266146287616">September 3, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_6bef0f1c27fccc25fea873654cef1663</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="500"><p>@<a href="https://twitter.com/edsu">edsu</a> Ed, see <a href="http://t.co/BKUvxUxv" title="http://www.nationalarchives.gov.uk/PRONOM/fmt/41">nationalarchives.gov.uk/PRONOM/fmt/41</a> replace fmt for x-fmt for x-puids, and attach .xml for xml data to link directly to PRONOM data.</p>&mdash; Ross Spencer (@beet_keeper) <a href="https://twitter.com/beet_keeper/status/242515266146287616">September 3, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85457</wp:comment_id>
			<wp:comment_author><![CDATA[anjackson]]></wp:comment_author>
			<wp:comment_author_email>anj@anjackson.net</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-09-03 01:30:18</wp:comment_date>
			<wp:comment_date_gmt>2012-09-03 08:30:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the feedback. The reason the RDF is missing is simply that you need to choose a set of concept/predicate URIs to map the Drupal content types/fields onto, and I didn't get around to mocking the URIs up or picking them out of the LD PRONOM prototype. I started exploring a Drupal 7 version, which has even better RDF support, but lost the will to carry on as it didn't seem that enough people were interested in using the thing anyway. As a solution, it seems too dependent upon me, as there don't seem to be many Drupal folks in the DigiPres community. Having said that, most of the time was spend on the Python script that syncs with PRONOM, which is perhaps best done in other ways, or avoided altogether. Not sure there's any need to manage the binary signatures etc. outside PRONOM, as that's where TNA are focussing effort.

Stepping back, I fear I fell into the same trap as UDFR - I started building a solution without engaging with the users and letting them lead, but with an 'if I built it they will come' attitude. Who are the users? Who will have time to contribute? Do they really need a pretty UI or is some YAML in a GitHub repo + jekyll scripts to publish it sufficient?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>458</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1346661018.8643";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:9:"anjackson";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1346676534.7591";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85458</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-09-03 06:08:49</wp:comment_date>
			<wp:comment_date_gmt>2012-09-03 13:08:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for responding Andy. I didn't mean to imply that people should use your prototype in place of the official PRONOM registry. I just thought that in the spirit of being a proof of concept it seems to capture what PRONOM itself would do well to emulate: opensource, SEO friendly, easily linkable, editable with authentication, etc. I have a hunch that the <a href="http://ascii.textfiles.com/archives/3645" rel="nofollow">Just Solve the Problem</a> month will show that there are more people than you might think who care about this stuff.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1346677730.0325";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85464</wp:comment_id>
			<wp:comment_author><![CDATA[ross-spencer]]></wp:comment_author>
			<wp:comment_author_email>all.along.the.watchtower2001@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.exponentialdecay.co.uk</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-09-25 10:28:15</wp:comment_date>
			<wp:comment_date_gmt>2012-09-25 17:28:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed, 

This is really good work. I hope to take a better look at FIDO soon. Creating a test suite for DROID/FIDO is something I am quite interested in. I wanted to point you in the direction of a project I have been working on here: 

https://github.com/exponential-decay/skeleton-test-suite-generator

I wonder if some of the suite I output as part of this work is beneficial to you?

There is a testing report here [PDF]: https://github.com/downloads/exponential-decay/skeleton-test-suite-generator/skeleton-test-suite-generator-v0.1-BETA-results.pdf
And the first output of the tool is here [ZIP]: https://github.com/downloads/exponential-decay/skeleton-test-suite-generator/skeleton-suite-v0.1-BETA.zip

This is the first iteration in a longer process, I'm currently gauging its usefulness and we'll see where it can be taken. I've already managed to solve some of the issues in the tool following the BETA results report so now the problems are much more focused around DROID's capabilities, although there are some other parts of the code I will go on to improve.

Ross]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>460</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1348594095.8529";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:12:"ross-spencer";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1348594224.364";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>finding soundcloud users with lastfm</title>
		<link>http://inkdroid.org/2012/09/10/finding-soundcloud-users-with-lastfm/</link>
		<pubDate>Mon, 10 Sep 2012 15:31:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4940</guid>
		<description></description>
		<content:encoded><![CDATA[I stumbled upon the lovely <a href="http://developers.soundcloud.com/docs">Soundcloud API</a> this weekend, and before I knew it I was hacking together something that would use the <a href="http://www.last.fm/api">LastFM API</a> to lookup artists that I listen to, and then look to see if they are on Soundcloud. If you haven't seen it before <a href="http://soundcloud.com">Soundcloud</a> is a social networking site for musicians and audiophiles to share tracks. Sometimes artists will share works in progress, which is really fascinating.

It's kind of amazing what you can accomplish in just HTML and JavaScript these days. It sure makes it easy to deploy, which I did at <a href="http://inkdroid.org/lastcloud/">http://inkdroid.org/lastcloud/</a>. If you want to give it a try enter your LastFM username, or the username of someone you know, like mine: inkdroid. As you can see the hack <em>sorta</em> worked. I say <em>sorta</em> because there seem to be a fair amount of users who are squatting on names of musicians. There also seem to be accounts that are run by fans, pretending to be the artist. Below is a list of seemingly legit Soundcloud accounts I found, and have followed. If you have any ideas for improving the hack, I put the code up on <a href="http://github.com/edsu/lastcloud">GitHub</a>.

<ul style="float: left; display: inline;">
<li style="list-style: none"><a href="http://soundcloud.com/radiohead"><img src="http://userserve-ak.last.fm/serve/126/317976.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/four-tet"><img src="http://userserve-ak.last.fm/serve/126/55697721.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/deathcabforcutie"><img src="http://userserve-ak.last.fm/serve/126/60010487.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/lalipuna"><img src="http://userserve-ak.last.fm/serve/126/52628081.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/frightened-rabbit"><img src="http://userserve-ak.last.fm/serve/126/10397305.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/jamesblackshaw"><img src="http://userserve-ak.last.fm/serve/126/13458341.jpg"/></a></li>
</ul>
<ul style="float: left; display: inline;">
<li style="list-style: none"><a href="http://soundcloud.com/wyeoak"><img src="http://userserve-ak.last.fm/serve/126/52986843.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/the-album-leaf"><img src="http://userserve-ak.last.fm/serve/126/2857325.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/caleparks"><img src="http://userserve-ak.last.fm/serve/126/38401967.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/dirty-projectors"><img src="http://userserve-ak.last.fm/serve/126/79702089.png"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/thurstonmoore"><img src="http://userserve-ak.last.fm/serve/126/4020136.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/johnvanderslice"><img src="http://userserve-ak.last.fm/serve/126/2289629.jpg"/></a></li>
</ul>
<ul style="float: left;">
<li style="list-style: none"><a href="http://soundcloud.com/davidwenngren"><img src="http://userserve-ak.last.fm/serve/126/2247728.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/boombip"><img src="http://userserve-ak.last.fm/serve/126/85278.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/matesofstate"><img src="http://userserve-ak.last.fm/serve/126/13595289.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/dntel"><img src="http://userserve-ak.last.fm/serve/126/4117918.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/gold-panda"><img src="http://userserve-ak.last.fm/serve/126/58226915.jpg"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/grizzlybearband"><img src="http://userserve-ak.last.fm/serve/126/42459789.png"/></a></li>
<li style="list-style: none"><a href="http://soundcloud.com/ramonafalls"><img src="http://userserve-ak.last.fm/serve/126/33597075.png"/></a></li>
</ul>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4940</wp:post_id>
		<wp:post_date>2012-09-10 08:31:52</wp:post_date>
		<wp:post_date_gmt>2012-09-10 15:31:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>finding-soundcloud-users-with-lastfm</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="apis"><![CDATA[apis]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="lastfm"><![CDATA[lastfm]]></category>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<category domain="post_tag" nicename="soundcloud"><![CDATA[soundcloud]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85461";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85461</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8…]]></wp:comment_author>
			<wp:comment_author_email>brian.cassidy@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawlT40Vc7qbygawSgPrTovJFOqeuoz7bH1k</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-09-10 11:21:06</wp:comment_date>
			<wp:comment_date_gmt>2012-09-10 18:21:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I see that the last.fm API returns MBIDs for artists -- you're probably better off checking the Musicbrainz data for soundcloud links.

Here's the MB webservice call for "Jon Fratelli":

http://musicbrainz.org/ws/2/artist/c2190969-53a8-4cab-8762-1abc05cbdb70?inc=url-rels

You can see the soundcloud link there. Obviously this requires the soundcloud metadata to be linked rather than implied, but it's certainly more reliable.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>459</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1347301267.9635";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawlT40Vc7qbygawSgPrTovJ";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1347368914.6999";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>archiving wikitweets</title>
		<link>http://inkdroid.org/2012/09/19/archiving-wikitweets/</link>
		<pubDate>Wed, 19 Sep 2012 20:32:59 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=4970</guid>
		<description></description>
		<content:encoded><![CDATA[Earlier this year I created a little toy webapp called <a href="http://wikitweets.herokuapp.com">wikitweets</a> that uses the <a href="https://dev.twitter.com/docs/api/1.1/post/statuses/filter">Twitter streaming API</a> to identify tweets that reference Wikipedia, which it then displays realtime in your browser. It was basically a fun experiment to kick the tires on NodeJS and <a href="http://socket.io/">SocketIO</a> using a free, single process Heroku instance.

At the time I <a href="http://lists.wikimedia.org/pipermail/wiki-research-l/2012-April/001982.html">announced</a> the app on the wiki-research-l discussion list to see if anyone was interested in it. Out of the responses I received were ones from <a href="http://lists.wikimedia.org/pipermail/wiki-research-l/2012-April/001985.html">Emilio Rodríguez-Posada</a> and <a href="http://lists.wikimedia.org/pipermail/wiki-research-l/2012-April/001989.html">Taha Yasseri</a> where they asked whether the tweets are archived as they stream by. This struck a chord with me, since I'm a software developer working in the field of "digital preservation". You know that feeling when you suddenly see one of your huge gaping blindspots? Yeah.

Anyway, some 6 months or so later I finally got around to adding an <a href="https://github.com/edsu/wikitweets/blob/master/app.js#L131">archive function</a> to wikitweets, and I thought it might be worth writing about very quickly. Wikitweets uses the <a href="http://archive.org/help/abouts3.txt">S3 API</a> at Internet Archive to store every 1000 tweets. So you can visit <a href="http://archive.org/details/wikitweets">this page</a> at Internet Archive and download the tweets. Now I don't know how long Internet Archive is going to be around, but I bet it will be longer than inkdroid.org, so it seemed like a logical (and free) safe harbor for the data. 

In addition to being able to share the files Internet Archive also make a BitTorrent seed available, so the data can easily be distributed around the Internet. For example you could open <a href="http://archive.org/download/wikitweets/wikitweets_archive.torrent">wikitweets_archive.torrent</a> in your BitTorrent client and download a copy of the entire dataset, while providing a redundant copy. I don't really expect this to happen much with the wikitweets collection, but it seems to be a practical offering in the <em>Lots of Copies Keeps Stuff</em> Safe category.

I tried to coerce several of the seemingly excellent s3 libraries for NodeJS to talk to the Internet Archive, but ended up writing my own very small library that works specifically with Internet Archive. <a href="https://github.com/edsu/wikitweets/blob/master/lib/ia.js">ia.js</a> is bundled as part of wikitweets, but I guess I could put it on npm if anyone is really interested. It gets used by wikitweets like this:

<pre lang="javascript">
  var c  = ia.createClient({
    accessKey: config.ia_access_key,
    secretKey: config.ia_secret_key,
    bucket: config.ia_bucket
  });

  c.addObject({name: "20120919030946.json", value: tweets}, function() {
    console.log("archived " + name);
  });
</pre>

The nice thing is that you can use s3 libraries that have support for Internet Archive, like <a href="https://github.com/boto/boto">boto</a> to programatically pull down the data. For example, here is a Python program that goes through each file and prints out the Wikipedia article title that is referenced by the tweet:

<pre lang="python">
  import json
  import boto

  ia = boto.connect_ia()
  bucket = ia.get_bucket("wikitweets")

  for keyfile in bucket:
      content = keyfile.get_contents_as_string()
      for tweet in json.loads(content):
          print tweet['article']['title']
</pre>

The archiving has only been running for last 24 hours or so, so I imagine there will be tweaks that need to be made. I'm considering compression of the tweets as one of them. Also it might be nice to put the files in subdirectories, but it seemed that Internet Archive's API wanted to URL encode object names that have slashes in them. 

If you have any suggestions I'd love to hear them.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4970</wp:post_id>
		<wp:post_date>2012-09-19 13:32:59</wp:post_date>
		<wp:post_date_gmt>2012-09-19 20:32:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>archiving-wikitweets</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="digital-preservation"><![CDATA[digital preservation]]></category>
		<category domain="post_tag" nicename="internet-archive"><![CDATA[internet archive]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="storage"><![CDATA[storage]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>data dumps</title>
		<link>http://inkdroid.org/2012/09/21/data-dumps/</link>
		<pubDate>Fri, 21 Sep 2012 20:22:53 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5005</guid>
		<description></description>
		<content:encoded><![CDATA[<em>As usual, the following comments are the reflections of a software developer working at the Library of Congress and are not an official statement of my employer.</em>.

One of the challenges that we've had at the <a href="http://www.loc.gov/ndnp/">National Digital Newspaper Program'</a>s website <a href="http://chroniclingamerica.loc.gov">Chronicling America</a> has been access to data. At the surface level Chronicling America is a conventional web application that provides access to millions of pages of historic newspapers. Here "access" means a researcher's ability to browse to each newspaper, issue and page, as well as search across the OCR text for each page. 

Digging a bit deeper "access" also means programmatic access via a <a href="http://chroniclingamerica.loc.gov/about/api/">Web API</a>. Chronicling America's API enables custom software to issue queries using the popular <a href="http://opensearch.org">OpenSearch</a> protocol, and it also makes URL addressable data available using  principles of <a href="http://chroniclingamerica.loc.gov/about/api/#linked-data">Linked Data</a>. In addition the website also makes the so called "batch" data that each NDNP awardee sends to the Library of Congress <a href="http://chroniclingamerica.loc.gov/data/batches/">available</a> on the Web. The advantage to making the batch data available is that it allows 3rd parties are then able to build their own custom search indexes on top of the data so their own products and services don't have a runtime dependency on our Web API. Also researchers can choose to index things differently, perform text mining operations, or conduct other experiments. Each batch contains JPEG 2000, PDF, OCR XML and METS XML data for all the newspaper content; and it is in fact the very same data that the Chronicling America web application ingests. The batch data views makes it possible for interested parties to crawl the content using wget or some similar tool that talks HTTP, and fetch a lot of newspaper data.

But partly because of NDNP's participation in the NEH's <a href="http://www.diggingintodata.org/">Digging Into Data</a> program, as well as the interest from other individuals and organizations we've recently started making data dumps of the OCR content available. This same OCR data is available as part of the batch data mentioned above, but the dumps provide two new things:

<ol>
<li>The ability to download a small set of large compressed files with checksums to verify their transfer, as opposed to having to issue HTTP GETs for millions of uncompressed files with no verification.</li>
<li>The ability to easily map each of the OCR files to their corresponding URL on the web. While it is theoretically possible to extract the right bits from the METS XML data in the batch data, the best of expression of how to do this is encapsulated in the Chronicling America ingest code, and is non-trivial.</li>
</ol>

So when you download, decompress and untar one of the files you will end up with a directory structure like this:

<pre>
sn86063381/
|-- 1908
|   |-- 01
|   |   |-- 01
|   |   |   `-- ed-1
|   |   |       |-- seq-1
|   |   |       |   |-- ocr.txt
|   |   |       |   `-- ocr.xml
|   |   |       |-- seq-2
|   |   |       |   |-- ocr.txt
|   |   |       |   `-- ocr.xml
|   |   |       |-- seq-3
|   |   |       |   |-- ocr.txt
|   |   |       |   `-- ocr.xml
|   |   |       `-- seq-4
|   |   |           |-- ocr.txt
|   |   |           `-- ocr.xml
|   |   |-- 02
|   |   |   `-- ed-1
|   |   |       |-- seq-1
|   |   |       |   |-- ocr.txt
|   |   |       |   `-- ocr.xml
|   |   |       |-- seq-2
|   |   |       |   |-- ocr.txt
|   |   |       |   `-- ocr.xml
|   |   |       |-- seq-3
|   |   |       |   |-- ocr.txt
|   |   |       |   `-- ocr.xml
|   |   |       `-- seq-4
|   |   |           |-- ocr.txt
|   |   |           `-- ocr.xml

...
</pre>

The pattern here is:

<pre>
{lccn}/{year}/{month}/{day}/{edition}/{sequence}/
</pre>

If you don't work in a library, an lccn is a Library of Congress Control Number, which is a unique ID for each newspaper title. Each archive file will lay out in a similar way, such that you can process each .tar.bz2 file and will end up with a complete snapshot of the OCR data on your filesystem. The pattern maps pretty easily to URLs of the format:

<pre>
http://chroniclingamerica.loc.gov/lccn/{lccn}/{year}-{month}-{day}/{edition}/{sequence}/
</pre>

This is an obvious use case for a pattern like <a href="https://wiki.ucop.edu/display/Curation/PairTree">PairTree</a>, but there was some perceived elegance to using paths that were a bit more human readable, and easier on the filesystem, which stands a good chance of not being ZFS.

Another side effect of having a discrete set of files to download is that each dump file can be referenced in an <a href="http://chroniclingamerica.loc.gov/ocr/feed">Atom feed</a>, so that you can keep your snapshot up to date with a little bit of automation. Here's a snippet of the feed:

<pre lang="xml">
< ?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title>Chronicling America OCR Data Feed</title>
    <link rel="self" type="application/atom+xml" href="http://chroniclingamerica.loc.gov/ocr/feed/" />
    <id>info:lc/ndnp/ocr</id>
    <author>
        <name>Library of Congress</name>
        <uri>http://loc.gov</uri>
    </author>
    <updated>2012-09-20T10:34:02-04:00</updated>

    <entry>
        <title>part-000292.tar.bz2</title>
        <link rel="enclosure" length="650169965" hash="sha1:bb7fa00e8e07041501a9703bf85afbe5040e3448" type="application/x-bzip2" href="http://chroniclingamerica.loc.gov/data/ocr/part-000292.tar.bz2" />
        <id>info:lc/ndnp/dump/ocr/part-000292.tar.bz2</id>
        <updated>2012-09-20T10:34:02-04:00</updated>
        <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">OCR dump file <a href="http://chroniclingamerica.loc.gov/data/dumps/ocr/part-000292.tar.bz2">part-000292.tar.bz2</a> with size 620.1 MB generated Sept. 20, 2012, 10:34 a.m.</div></summary>
    </entry>

    ...

</feed>
</pre>

As you can see it's a pretty vanilla Atom feed that should play nicely with whatever feed reader or library you are using. You may notice the &lt;link&gt; element has some attributes that you might not be used to seeing. The <code>enclosure</code> and <code>length</code> attributes are directly from <a href="http://tools.ietf.org/html/rfc4287">RFC 4287</a> for giving clients an idea that the referenced resource might be on the large side. The <code>hash</code> attribute is a generally useful attribute from James Snell's <a href="http://tools.ietf.org/html/draft-snell-atompub-link-extensions">Atom Link Extensions</a> IETF draft.

If parsing XML is against your religion, there's also a <a href="http://chroniclingamerica.loc.gov/ocr.json">JSON flavored feed</a> that looks like:

<pre lang="javascript">
{
  ocr: [
    {
      url: "http://chroniclingamerica.loc.gov/data/ocr/part-000337.tar.bz2",
      sha1: "fd73d8e1df33015e06739c897bd9c08a48294f82",
      size: 283454353,
      name: "part-000337.tar.bz2",
      created: "2012-09-21T06:56:35-04:00"
    },
    ...
  ]
}
</pre>

Again, I guess we could've kicked the tires on the emerging <a href="http://www.openarchives.org/rs/0.1/resourcesync">ResourceSync</a> specification to simliar effect. But ResourceSync is definitely still in development, and well, Atom is a pretty nice Internet standard for publishing changes. Syndication technologies like RSS and Atom have <a href="http://dumps.wikimedia.org/enwiki/latest/">already been used</a> by folks like Wikipedia for publishing the availability of data dumps. ResourceSync seems intent <a href="http://www.openarchives.org/rs/0.1/resourcesync#Dump">on using Zip</a> for compressing dump files, and bzip is common enough, and enough better than zip that it's worth diverging. In some ways this blog post has turned into a when-to-eschew-digital-library-standards in favor of more mainstream or straightforward patterns. I didn't actually plan that, but those of you that know me probably are not surprised.

If you plan to use the OCR dumps I, and others on the NDNP team, would love to hear from you. One of the big problems with them so far is that there is no explict statement about how the data is in the public domain, which it is. I'm hopeful this can be rectified soon. If you have feedback on the use of Atom here I would be interested in that too. But the nice thing about using it is really how uncontroversial it is, so I doubt I'll hear much feedback on that front.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5005</wp:post_id>
		<wp:post_date>2012-09-21 13:22:53</wp:post_date>
		<wp:post_date_gmt>2012-09-21 20:22:53</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>data-dumps</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="atom"><![CDATA[atom]]></category>
		<category domain="post_tag" nicename="bzip"><![CDATA[bzip]]></category>
		<category domain="post_tag" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="json"><![CDATA[json]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="ndnp"><![CDATA[ndnp]]></category>
		<category domain="post_tag" nicename="newspapers"><![CDATA[newspapers]]></category>
		<category domain="post_tag" nicename="ocr"><![CDATA[ocr]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_145569c1ad0af3d2e5cd7a241f3bbf5a</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_4a001a267add42a9cc6a159841aee13a</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_02a5e88c1def44fff3ab6962cb5c9c7e</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_4606bbcaf2e5df07f6367eb367024429</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>lcnaf unix hack</title>
		<link>http://inkdroid.org/2012/10/04/lcnaf-unix-hack/</link>
		<pubDate>Fri, 05 Oct 2012 03:02:10 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5046</guid>
		<description></description>
		<content:encoded><![CDATA[I was in a <a href="http://socialarchive.iath.virginia.edu/NAAC_meeting2_agenda.html">meeting</a> today listening to a presentation about the Library of Congress Name Authority File and I got it into my head to see if I could quickly graph record creation by year. Part of this might've been prompted by sitting next to Kevin Ford, who was multi-tasking by what looked like loading some MARC data into id.loc.gov. I imagine this isn't perfect, but I thought it was kind of fun hack that demonstrates what you can get away with on the command line with some open data:

<pre lang="bash">
  curl http://id.loc.gov/static/data/authoritiesnames.nt.skos.gz \
    | zcat - \
    | perl -ne '/terms\/created> "(\d{4})-\d{2}-\d{2}/; print "$1\n" if $1;' \
    | sort \
    | uniq -c \
    | perl -ne 'chomp; @cols = split / +/; print "$cols[2]\t$cols[1]\n";' \
    > lcnaf-years.tsv
</pre>

Which yields a tab delimited file where column 1 is the year and column 2 is the number of records created in that year. The key part is the perl one-liner on line 3 which looks for assertions like this in the ntriples rdf, and pulls out the year:

<pre>
&lt;http://id.loc.gov/authorities/names/n90608287&gt; &lt;http://purl.org/dc/terms/created&gt; "1990-02-05T00:00:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; .
</pre>

The use of <code>sort</code> and <code>uniq -c</code> together is a handy trick my old boss Fred Lindberg taught me, for quickly generating aggregate counts from a stream of values. It works surprisingly well with quite large sets of values, because of all the work that has gone into making <code>sort</code> efficient.

WIth the tsv in hand I trimmed the pre-1980 values, since I think there are lots of records attributed to 1980 since that's when OPAC came online, and I wasn't sure what the dribs and drabs prior to 1980 represented. Then I dropped the data into ye olde chart maker (in this case GoogleDocs) and voilà:

<img src="http://inkdroid.org/images/lcnaf-record-creation.png"/>

It would be more interesting to see the results broken out by contributing NACO institution, but I don't think that data is in the various RDF representations. I don't even know if the records contributed by other NACO institutions are included in the LCNAF. I imagine a similar graph is available somewhere else, but it was neat that the availability of the LCNAF data meant I could get a rough answer to this passing question fairly quickly.

The numbers add up to ~7.8 million which seems within the realm of possibile correctness. But if you notice something profoundly wrong with this display please let me know!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5046</wp:post_id>
		<wp:post_date>2012-10-04 20:02:10</wp:post_date>
		<wp:post_date_gmt>2012-10-05 03:02:10</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>lcnaf-unix-hack</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="statistics"><![CDATA[statistics]]></category>
		<category domain="post_tag" nicename="unix-lcnaf"><![CDATA[unix. lcnaf]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85468</wp:comment_id>
			<wp:comment_author><![CDATA[dr0i]]></wp:comment_author>
			<wp:comment_author_email>christoph@hbz-nrw.de</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-10-10 08:42:15</wp:comment_date>
			<wp:comment_date_gmt>2012-10-10 15:42:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Inspired by your visualisation I did this with our 16 Mio library records: https://wiki1.hbz-nrw.de/display/SEM/2012/10/10/Issues-per-year+diagram]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>462</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1349883735.7081";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:4:"dr0i";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1349886339.2325";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>a look at who makes the LCNAF</title>
		<link>http://inkdroid.org/2012/10/10/a-look-at-who-makes-the-lcnaf/</link>
		<pubDate>Wed, 10 Oct 2012 11:22:37 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5070</guid>
		<description></description>
		<content:encoded><![CDATA[As a follow up to my <a href="http://inkdroid.org/2012/10/04/lcnaf-unix-hack/">last post</a> about visualizing Library of Congress Name Authority File (LCNAF) records created by year, I decided to dig a little bit deeper to see how easy it would be to visualize how participating <a href="http://www.loc.gov/aba/pcc/naco/">Name Authority Cooperative</a> institutions have contributed to the LCNAF over time. This idea was mostly born out of spending the latter part of last week participating in a conversation about the need for a <a href="http://socialarchive.iath.virginia.edu/NAAC_index.html">National Archival Authority Cooperative</a> hosted at NARA. This blog post is one part nerdy technical notes on how I worked with the LCNAF Linked Data, and one part line charts showing who creates and modifies LCNAF records. It might've made more sense to start with the pretty charts, and then show you how I did it...but if the tech details don't interest you can jump to the <a href="#result">second half</a>.

<h2>The Work</h2>

After a very helpful Twitter <a href="https://twitter.com/3windmills/status/254949232052686848">conversation</a> with Kevin Ford I discovered that the Linked Data <a href="http://www.loc.gov/standards/mads/rdf/">MADSRDF</a> representation of the LCNAF includes assertions about the institution responsible for creating or revising the a record. Here's a snippet of Turtle for RDF that describes who created and modified the LCNAF record for <a href="http://id.loc.gov/authorities/names/n97108433">J. K. Rowling</a> (if your eyes glaze over when you see RDF, don't worry keep reading, it's not essential you understand this):

<pre>
@prefix ri: &lt;http://id.loc.gov/ontologies/RecordInfo#&gt; .

&lt;http://id.loc.gov/authorities/names/n97108433&gt;
    madsrdf:adminMetadata [
        ri:recordChangeDate "1997-10-28T00:00:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
        ri:recordContentSource &lt;http://id.loc.gov/vocabulary/organizations/dlc&gt; ;
        ri:recordStatus "new"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
        a ri:RecordInfo
    ],
    [
        ri:recordChangeDate "2011-08-25T06:29:06"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
        ri:recordContentSource &lt;http://id.loc.gov/vocabulary/organizations/dlc&gt; ;
        ri:recordStatus "revised"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; ;
        a ri:RecordInfo
    ] .
</pre> 

So I picked up an <a href="http://aws.amazon.com/ec2/instance-types/">EC2 m1.large</a> spot instance (7.5G of RAM, 2 virtual cores, 850G of storage) for a miserly $0.026/hour, installed 4store (which is a triplestore I'd heard good things about), and loaded the data.

<pre>
% wget http://id.loc.gov/static/data/authoritiesnames.nt.madsrdf.gz
% gunzip authoritiesnames.nt.madsrdf.gz
% sudo apt-get install 4store
% sudo mkdir /mnt/4store
% sudo chown fourstore:fourstore /mnt/4store
% sudo ln -s /mnt/4store /var/lib/4store
% sudo -u fourstore 4s-backend-setup lcnaf --segments 4
% sudo -u fourstore 4s-backend lcnaf
% sudo -u fourstore 4s-import --verbose lcnaf authoritiesnames.nt.madsrdf
</pre>

I used 4 segments as a best guess to match the 4 EC2 compute units available to an m1.large. The only trouble was that after loading 90M of the 226M assertions it began to slow to a crawl as the memory was about used up. 

I thought briefly about upgrading to a larger instance...but it occurred to me that I actually didn't need all the triples. I just need the ones related to the record changes, and the organization that made them. So I filtered out just the assertions I needed. By the way, this is a really nice artifact of the ntriples data format, which is very easy to munge with line oriented Unix utilities and scripting tools:

<pre>
zcat authoritiesnames.nt.madsrdf.gz | egrep '(recordChangeDate)|(recordContentSource)|(recordStatus)'  > updates.nt
</pre>

This left me with 50,313,810 triples which loaded in about 20 minutes! With the database populated I was then able to execute the following query to fetch all the create dates with their institution code using 4s-query:

<pre>
@prefix ri: &lt;http://id.loc.gov/ontologies/RecordInfo#&gt; .

SELECT ?date ?source WHERE { 
  ?s ri:recordChangeDate ?date . 
  ?s ri:recordContentSource ?source . 
  ?s ri:recordStatus "new"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; . 
}
</pre>

This returned a tab delimited file that looked something like:

<pre>
"1991-08-16T00:00:00"^^&gt;http://www.w3.org/2001/XMLSchema#dateTime&gt;      &lt;http://id.loc.gov/vocabulary/organizations/dlc&gt;
"1995-01-07T00:00:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt;      &lt;http://id.loc.gov/vocabulary/organizations/djbf&gt;
"2004-03-04T00:00:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt;      &lt;http://id.loc.gov/vocabulary/organizations/nic&gt;
</pre>


I then wrote a simplistic <a href="https://gist.github.com/3863082">python program</a> to read in the TSV file and output a table of data where each row represented a year and the columns were the institution codes. 

<h2 id="result">The Result</h2>
 
If you'd like to see the table you can check it out as a <a href="https://www.google.com/fusiontables/DataSource?docid=1IcIRXt-H76hrGyGDc7P1ngQKDzgBkpmRXTXE1E8">Google Fusion Table</a>. If you are interested, you should be able to easily pull the data out into your own table, modify it, and visualize it. Google Fusion tables can be really easily rendered in a variety of ways, including a line graph, which I've embedded here, just displaying the top 25 contributors:

<iframe width="600" height="400" scrolling="no" frameborder="no" src="https://www.google.com/fusiontables/embedviz?viz=GVIZ&amp;t=LINE&amp;gco_vAxes=%5B%7B%22title%22%3A%22%22%2C+%22minValue%22%3A0%2C+%22maxValue%22%3Anull%2C+%22useFormatFromData%22%3Atrue%2C+%22viewWindowMode%22%3A%22explicit%22%2C+%22viewWindow%22%3A%7B%22max%22%3Anull%2C+%22min%22%3A0%7D%7D%2C%7B%22useFormatFromData%22%3Atrue%2C+%22viewWindowMode%22%3A%22pretty%22%2C+%22viewWindow%22%3A%7B%22max%22%3Anull%2C+%22min%22%3Anull%7D%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%7D%5D&amp;gco_curveType=function&amp;gco_booleanRole=certainty&amp;gco_lineWidth=2&amp;gco_hAxis=%7B%22useFormatFromData%22%3Atrue%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%2C+%22viewWindow%22%3Anull%2C+%22viewWindowMode%22%3Anull%2C+%22title%22%3A%22Year%22%7D&amp;gco_legend=right&amp;gco_title=LCNAF+Records+Created&amp;gco_legendTextStyle=%7B%22color%22%3A%22%23222%22%2C+%22fontSize%22%3A%2210%22%7D&amp;containerId=gviz_canvas&amp;isXyPlot=true&amp;q=select+col0%2C+col1%2C+col2%2C+col3%2C+col4%2C+col5%2C+col6%2C+col7%2C+col8%2C+col9%2C+col10%2C+col11%2C+col12%2C+col13%2C+col14%2C+col15%2C+col16%2C+col17%2C+col18%2C+col19%2C+col20%2C+col21%2C+col22%2C+col23%2C+col24%2C+col25+from+1IcIRXt-H76hrGyGDc7P1ngQKDzgBkpmRXTXE1E8&amp;qrs=+where+col0+%3E%3D+&amp;qre=+and+col0+%3C%3D+&amp;qe=+order+by+col0+asc+limit+32&amp;width=600&amp;height=400"></iframe>

While I didn't quite expect to see LC tapering off the way it is, I did expect it to dominate the graph. <a href="https://www.google.com/fusiontables/DataSource?docid=1kPtQOFcF6wY9FbbVAHZcCJIZTqgVSa1_X0IfAPs">Removing LC</a> from the mix makes the graph a little bit more interesting. For example you can see the steady climb of the British Library, and the strong role that Princeton University plays:

<iframe width="600" height="400" scrolling="no" frameborder="no" src="https://www.google.com/fusiontables/embedviz?viz=GVIZ&amp;t=LINE&amp;gco_vAxes=%5B%7B%22title%22%3A%22Records%22%2C+%22minValue%22%3A0%2C+%22maxValue%22%3Anull%2C+%22useFormatFromData%22%3Atrue%2C+%22viewWindowMode%22%3A%22explicit%22%2C+%22viewWindow%22%3A%7B%22max%22%3Anull%2C+%22min%22%3A0%7D%7D%2C%7B%22useFormatFromData%22%3Atrue%2C+%22viewWindowMode%22%3A%22pretty%22%2C+%22viewWindow%22%3A%7B%22max%22%3Anull%2C+%22min%22%3Anull%7D%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%7D%5D&amp;gco_curveType=function&amp;gco_booleanRole=certainty&amp;gco_lineWidth=2&amp;gco_hAxis=%7B%22useFormatFromData%22%3Atrue%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%2C+%22viewWindow%22%3Anull%2C+%22viewWindowMode%22%3Anull%2C+%22title%22%3A%22Year%22%7D&amp;gco_legend=right&amp;gco_title=LCNAF+Records+Created+(No+LC)&amp;gco_legendTextStyle=%7B%22color%22%3A%22%23222%22%2C+%22fontSize%22%3A%2210%22%7D&amp;containerId=gviz_canvas&amp;isXyPlot=true&amp;q=select+col0%2C+col2%2C+col3%2C+col5%2C+col6%2C+col7%2C+col8%2C+col9%2C+col10%2C+col12%2C+col13%2C+col14%2C+col15%2C+col16%2C+col17%2C+col18%2C+col19%2C+col20%2C+col22%2C+col23%2C+col24%2C+col25%2C+col26%2C+col27%2C+col28%2C+col29%2C+col30+from+1kPtQOFcF6wY9FbbVAHZcCJIZTqgVSa1_X0IfAPs&amp;qrs=+where+col0+%3E%3D+&amp;qre=+and+col0+%3C%3D+&amp;qe=+order+by+col0+asc+limit+32&amp;width=600&amp;height=400"></iframe>

Out of curiosity I then executed a SPARQL query for record updates (or revisions), repeated the step with stats.py, uploaded to <a href="https://www.google.com/fusiontables/DataSource?docid=1rVZNLnEWzCoSEj9jAs40ZX9WjJ25BnA0GYIuTUo">Google Fusion Tables</a>, and removed LC to better see trends in who is updating records:

<pre>
@prefix ri: &lt;http://id.loc.gov/ontologies/RecordInfo#&gt; .

SELECT ?date ?source WHERE { 
  ?s ri:recordChangeDate ?date . 
  ?s ri:recordContentSource ?source . 
  ?s ri:recordStatus "revised"^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; . 
}
</pre>

<iframe width="600" height="400" scrolling="no" frameborder="no" src="https://www.google.com/fusiontables/embedviz?viz=GVIZ&amp;t=LINE&amp;gco_vAxes=%5B%7B%22title%22%3A%22Records%22%2C+%22minValue%22%3A0%2C+%22maxValue%22%3Anull%2C+%22useFormatFromData%22%3Atrue%2C+%22viewWindowMode%22%3A%22explicit%22%2C+%22viewWindow%22%3A%7B%22max%22%3Anull%2C+%22min%22%3A0%7D%7D%2C%7B%22useFormatFromData%22%3Atrue%2C+%22viewWindowMode%22%3A%22pretty%22%2C+%22viewWindow%22%3A%7B%22max%22%3Anull%2C+%22min%22%3Anull%7D%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%7D%5D&amp;gco_curveType=function&amp;gco_booleanRole=certainty&amp;gco_lineWidth=2&amp;gco_hAxis=%7B%22useFormatFromData%22%3Atrue%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%2C+%22viewWindow%22%3Anull%2C+%22viewWindowMode%22%3Anull%2C+%22title%22%3A%22Year%22%2C+%22titleTextStyle%22%3A%7B%22color%22%3A%22%23222%22%2C+%22fontSize%22%3A%2212%22%2C+%22italic%22%3Atrue%7D%7D&amp;gco_legend=right&amp;gco_title=LCNAF+Records+Revised&amp;gco_legendTextStyle=%7B%22color%22%3A%22%23222%22%2C+%22fontSize%22%3A%2210%22%7D&amp;containerId=gviz_canvas&amp;isXyPlot=true&amp;q=select+col0%2C+col3%2C+col4%2C+col5%2C+col6%2C+col7%2C+col8%2C+col10%2C+col11%2C+col12%2C+col13%2C+col14%2C+col15%2C+col16%2C+col17%2C+col18%2C+col20%2C+col21%2C+col22%2C+col23%2C+col24%2C+col25%2C+col26%2C+col27%2C+col28%2C+col29%2C+col30+from+1rVZNLnEWzCoSEj9jAs40ZX9WjJ25BnA0GYIuTUo&amp;qrs=+where+col0+%3E%3D+&amp;qre=+and+col0+%3C%3D+&amp;qe=+order+by+col0+asc+limit+30&amp;width=600&amp;height=400"></iframe>

I definitely never understood what <a href="http://en.wikipedia.org/wiki/Twin_Peaks">Twin Peaks</a> was about, and I similarly don't really know what the twin peaks in this graph signify (2000 and 2008). I guess these were years where there were a lot of coordinated edits? Perhaps some NACO folks who have been around for a few years may know the answer. You can also see in this graph that Princeton University plays a strong role in updating records as well as creating them.

So I'm not sure I understand the how/when/why of an NAAC any better, but I did learn:

<ul>
<li>EC2 is a big win for quick data munging projects like this. I spent $0.98 with the instance up and running for 3 days.</li>
<li>Filtering ntriples files to what you actually need prior to loading into a triplestore can save time, money.</li>
<li>Working with ntriples is still pretty esoteric, and the options out there for processing a dump of ntriples (or rdf/xml) of LCNAF's size are truly slim. If I'm wrong about this I would like to be corrected.</li>
<li>Google Fusion Tables are a nice way to share data and charts.</li>
<li>It seems like while <a href="http://inkdroid.org/2012/10/04/lcnaf-unix-hack/">more LCNAF records are being created per year</a>, they are being created by a broader base of institutions instead of just LC (who appear to be in decline). I think this is a good sign for NAAC.</li>
<li>Open Data, and Open Data Curators (thanks Kevin) are essential to open, collaborative enterprises.</li>
</ul>

Now I could've made some hideous mistakes here, so in the unlikely event you have the time and inclination I would be interested to hear if you can reproduce these results. If the results confirm or disagree with other views of LCNAF participation I would be interested to see them.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5070</wp:post_id>
		<wp:post_date>2012-10-10 04:22:37</wp:post_date>
		<wp:post_date_gmt>2012-10-10 11:22:37</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-look-at-who-makes-the-lcnaf</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="4store"><![CDATA[4store]]></category>
		<category domain="post_tag" nicename="authority-control"><![CDATA[authority control]]></category>
		<category domain="post_tag" nicename="ec2"><![CDATA[ec2]]></category>
		<category domain="post_tag" nicename="lcnaf"><![CDATA[lcnaf]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="library-of-congress"><![CDATA[library of congress]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[no]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>learning from people that do</title>
		<link>http://inkdroid.org/2012/10/11/learning-from-people-that-do/</link>
		<pubDate>Thu, 11 Oct 2012 17:15:12 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5136</guid>
		<description></description>
		<content:encoded><![CDATA[Anil Dash recently <a href="http://dashes.com/anil/2012/10/the-blue-collar-coder.html">wrote</a> a nice piece about the need for what he calls a Hi-Tech Vo-tech in the technology sector. If you are not familiar with it already, Vo-Tech is shorthand in the US for <a href="http://en.wikipedia.org/wiki/Vocational-technical_school">Vocational-technical school</a>, which provide focused training in specific areas, often on a part time basis. The Vo-Tech experience is markedly different from the typical 4 year university experience, which tends to be focused more on theory than practice.

I <em>totally</em> agree. 

But if you are looking to work as a software developer, and to help build this amazing information space we call the World Wide Web, you don't need to wait for this dream of a better high school curriculum for computer programming, or Hi-Tech Vo-Techs to come to your town. I don't want to minimize the effort involved in finding your way into the workplace...it's hard, especially when there is competition from "qualified" candidates, and the skill sets seem to be constantly shifting. But here are some relatively simple steps you can take to get started.

<h2>Look at Job Ads</h2>

Go to the <a href="http://craigslist.org">CraigsList</a> for your area, look at what jobs are available under the <em>internet engineers</em> and <em>software / qa / dba</em> sections. I suggest Craigslist because of their local flavor, and the low cost to advertise, which typically means the jobs are at smaller companies who are less interested in finding someone with the right college degree, and more interested in finding someone who can get things done. Look for jobs that focus on what you can do rather than schooling. Don't apply for any of the jobs just yet. Note down the tools they want people to know: computer languages, operating systems, web frameworks, etc. Research them on <a href="http://wikipedia.org">Wikipedia</a>. Focus on tools that seem to pop up a lot, are opensource, and can be downloaded and used without cost. You don't need to do anything with them just yet though.

<h2>Go To User Group Meetings</h2>

I say opensource because opensource tools often have open communities around them. You should be able to find user groups in your area where people present on how they use these tools at their place of work. You might have to drive a while, or take a long bus/train ride -- but it's worth it. To find the meetings do some searches by technology and location on <a href="http://meetup.com">Meetup</a>. Alternatively you can Google for whatever the technology is + "user group" + your area (e.g. Philadelphia) and go through a few pages of results. At a user group meeting you will not only learn about the details of the technology, but you will meet actual, real people who are using it. There are often subtle differences in the cultures and communities of practice around software tools. Some user groups will feel more comfortable than others. Pay attention to your gut reactions--they are indicators of how much you would like a job working with the technology, and the people who like it. If you get a bad vibe, don't take it personally, try another meeting. Finding a job is often a matter of who you know, not what you know ... and user groups are a great place to get to know people working in the software development field. There's no online substitute for meeting people in real life.

<h2>Use Social Networks</h2>

At user group meetings you meet people who you can learn from. See if they have a blog, are on Twitter or Facebook. Maybe they use a social bookmarking tool you can follow. Or perhaps there are email discussion lists you can subscribe to. It's not stalking, these people are your mentors, learn from them. Take a dip into sites like <a href="http://news.ycombinator.com/">Hacker News</a> or <a href="http://www.reddit.com/r/programming/">Programming Reddit</a>. Watch the trends, you aren't being a fanboy/girl, you are learning about what people care about in the field. Don't feel bad if it's overwhelming (it's overwhelming to "experts" too), focus on what seems interesting. Also, cultivate your own online identity by posting stuff that you are interested in, or have questions about. Stay positive, and try not to bash things: people (and potential employers) are watching you the same way you are watching them.

<h2>Read</h2>

Sometimes the speakers at User Group meetings will also be authors of books. You will see books reviewed on sites like Hacker News. People you follow may mention the books they read, or have accounts on sites like <a href="http://goodreads.com">GoodReads</a>. See if a library or a bookstore has them, and go skim them. Buy or borrow the ones you like. Take notes about them online, so people can see your interests. Get a Google Reader account and follow blogs related to tools you would like to use. Look for tools that have approachable/readable tutorials. Try out the examples, and get a feel for how well the theory of the tutorial translates into practice. If tools don't install or seem to work the way they are described, don't feel like you did something wrong...move on to tools that work more smoothly, and fit your brain better. The benefit to focusing on opensource projects is that you will find more content about them online. You can can read code. Reading the source code for Ruby or GoLang is definitely not for the faint of heart, though it's nice you can do it. It's more important that you look at code that uses these tools. Go to <a href="http://github.com">GitHub</a> and see what projects there are that use the tool. Browse the source online, or clone the repositories to your workstation. See if you can help out with some low hanging fruit tasks in their issue queue.

<h2>Find a Niche</h2>

You are probably interested in things other than programming. For example I like libraries and archives, and the cultural heritage sector. I've found a virtual community of software developers in this area called <a href="http://code4lib.org">code4lib</a>, which helps me learn more about new projects, tools in the field, and is a way to get to know people. You may be surprised to find a similar community around something you are interested in: be it astrophysics, cartoons, music, maps, real estate, etc. If you don't find one, maybe think about starting one up--you might be surprised by how many people turn up. Sometimes there are collaborative projects that need your help like Wikipedia, <a href="http://www.openstreetmap.org/">Open Street Map</a> where the ability to automate mundane tasks is needed. You might not get paid for this work, but it will broaden your circle of contacts, deepen your technical skills, will build your self confidence, and will be something to put on your resume. The key thing that finding a niche can do is make your job search a bit easier, since technology skills cut across domains. You will also find that your niche has a particular set of tools that it likes to use. These typically aren't hard and fast rules about using X instead of Y, but are norms. Pay attention to them, and learn about things that interest you.

<h2>Be Confident</h2>

I don't mean to imply any of this is easy. It can be extremely difficult to get out of your comfort zone and explore things you don't know. But you will be rewarded for your efforts, by learning from people who actually do things in the world. I've worked with some really excellent software developers that didn't have a compsci degree, and some that I wasn't even sure if they graduated high school. Sometimes I wonder if I even graduated from high school. So be confident in your ability to learn and do this thing we call software development. Show that you are humble about what you don't know, and that you are hungry to learn it. Above all, don't buy into the cult of the "real programmer" ... she doesn't exist. There are just people to learn from, and if you are doing it right, you never stop learning.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5136</wp:post_id>
		<wp:post_date>2012-10-11 10:15:12</wp:post_date>
		<wp:post_date_gmt>2012-10-11 17:15:12</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>learning-from-people-that-do</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[yes]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>aktt_tweeted</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Always Already New</title>
		<link>http://inkdroid.org/2012/10/15/always-already-new/</link>
		<pubDate>Mon, 15 Oct 2012 08:46:28 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5162</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.goodreads.com/book/show/1158322.Always_Already_New" style="float: left; padding-right: 20px"><img alt="Always Already New: Media, History, And The Data Of Culture" border="0" src="http://photo.goodreads.com/books/1347774737m/1158322.jpg" /></a><a href="http://www.goodreads.com/book/show/1158322.Always_Already_New">Always Already New: Media, History, And The Data Of Culture</a> by <a href="http://www.goodreads.com/author/show/398692.Lisa_Gitelman">Lisa Gitelman</a><br />
My rating: <a href="http://www.goodreads.com/review/show/393386341">3 of 5 stars</a><br /><br />
I enjoyed this book, mainly for the author's technique of exploring what media means in our culture by using two examples, separated in time: the phonograph and the Internet. She admits that in some ways this amounts to comparing apples to oranges, and there is definitely a creative tension in the book. Gitelman's emphasis is not that media technologies change society and culture, but that a technology is introduced and is in turn shaped by its particular social and historical context, which then reshapes society and culture.<br /><br /><blockquote>I define media as socially realized structures of communication, where structures include both technological forms and their associated protocols, and where communication is a cultural practice, a ritualized collocation of different people on the same mental map, sharing or engaged with popular ontologies of representation. As such, media are unique and complicated historical subjects.</blockquote><br /><br />It's tempting to talk about media technologies as if their ultimate use is somehow inevitable. For example, Gitelman discusses how the initial commercial placement of the phonograph centered largely around the idea that it would transform dictation and the office. Early demonstrations intended to increase sales of the device focused on recording and playback, rather than simply playback. They didn't initially see the  market for recorded music, which would so transform the device. To some extent we've cynically come to expect this out of marketing and "evangelism" about media technologies all the time. But this mode of thinking is also present in purely technical discussions, which don't account for the placement of the technology in a particular social context. <br /><br />Getting a sense of the social context you are in the middle of, as opposed to one you one you are historically removed from, presents some challenges. I think this difficulty is more evident in the second part of the book which focuses on the Internet and the World Wide Web against a backdrop of libraries and bibliography. Like many others I imagine, my knowledge of JCR Licklider's influence on the development of ARPAnet, and the Internet was largely culled from <a href="http://www.goodreads.com/book/show/281818.Where_Wizards_Stay_Up_Late">Where Wizards Stay Up Late</a>. I had no idea, until reading Always Already New, that Licklider contracted with the Council on Library Resources (now Council on Library and Information Resources) to write a report <em>Libraries of the Future</em> on the topic of how computing would change libraries.<br /><br />I enjoyed the discussion of the role that the Request for Comment (RFC) played on the Internet. How these documents that were initially shared via the post, helped bootstrap the technologies that would create the Internet that allowed them to be shared as electronic documents or text. I didn't know about the <a href="http://www.rfc-editor.org/rfc-online-2008.html" rel="nofollow">RFC-Online</a> project that Jon Postel started right before his death, to recover the earliest RFCs that had been already lost. Gitelman's study of linking, citation and "publishing" on the Web was also really enjoyable, mainly because of her orientation to these topics:<br /><br /><blockquote>I will argue that far from making history impossible, the interpretive space of the World Wide Web can prompt history in exciting new ways.</blockquote><br /><br />All this being said, I finished the book with the sneaking feeling that I needed to reread it. Gitelman's thesis was subtle enough that it was only when I got to the end that I felt like I understood it: the strange loop that thinking and media participate in, and how difficult (and yet fruitful) it is to talk about media and their social context. Maybe this was also partly the effect of reading it on a Kindle :-)<br />
<br /><br />
<a href="http://www.goodreads.com/review/list/5899086-ed-summers">View all my reviews</a>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5162</wp:post_id>
		<wp:post_date>2012-10-15 01:46:28</wp:post_date>
		<wp:post_date_gmt>2012-10-15 08:46:28</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>always-already-new</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="book-review"><![CDATA[book review]]></category>
		<category domain="post_tag" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[history]]></category>
		<category domain="post_tag" nicename="internet"><![CDATA[internet]]></category>
		<category domain="post_tag" nicename="media"><![CDATA[media]]></category>
		<category domain="post_tag" nicename="phonograph"><![CDATA[phonograph]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[no]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>who creates the LCNAF (part 2)</title>
		<link>http://inkdroid.org/2012/10/15/who-creates-the-lcnaf-part-2/</link>
		<pubDate>Mon, 15 Oct 2012 14:07:21 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5170</guid>
		<description></description>
		<content:encoded><![CDATA[I ended my <a href="http://inkdroid.org/2012/10/10/a-look-at-who-makes-the-lcnaf/">A Look at Who Creates the LCNAF</a> post with a hunch that the Library of Congress Name Authority File is increasingly supported by particpants in the Name Authority Cooperative (NACO) rather than by the Library of Congress themself. It didn't occur to me until a few days later that I missed a pretty obvious opportunity to graph the number of records created by LC compared with all the other members of the collective. So, here it is:

<iframe width="500" height="300" scrolling="no" frameborder="no" src="https://www.google.com/fusiontables/embedviz?viz=GVIZ&amp;t=LINE&amp;gco_vAxes=%5B%7B%22title%22%3A%22Records+Created%22%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%2C+%22useFormatFromData%22%3Atrue%2C+%22viewWindowMode%22%3A%22pretty%22%2C+%22viewWindow%22%3A%7B%22max%22%3Anull%2C+%22min%22%3Anull%7D%7D%2C%7B%22useFormatFromData%22%3Atrue%2C+%22viewWindowMode%22%3A%22pretty%22%2C+%22viewWindow%22%3A%7B%22max%22%3Anull%2C+%22min%22%3Anull%7D%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%7D%5D&amp;gco_curveType=function&amp;gco_booleanRole=certainty&amp;gco_lineWidth=2&amp;gco_hAxis=%7B%22useFormatFromData%22%3Atrue%2C+%22minValue%22%3Anull%2C+%22maxValue%22%3Anull%2C+%22viewWindow%22%3Anull%2C+%22viewWindowMode%22%3Anull%2C+%22title%22%3A%22Year%22%7D&amp;gco_legend=right&amp;gco_title=LCNAF+Record+Creation+Overview&amp;containerId=gviz_canvas&amp;isXyPlot=true&amp;q=select+col0%2C+col1%2C+col2+from+1QctNI-hgLhwOO9pE42Ffdw5bQx9i-1iBpI286b4&amp;qrs=+where+col0+%3E%3D+&amp;qre=+and+col0+%3C%3D+&amp;qe=+order+by+col0+asc+limit+32&amp;width=500&amp;height=300"></iframe>

It looks like this has been a trend since about 1996 or so. I think it validates the cooperative aspect of the PCC and NACO. Not that it needs any validating. It's just nice to see libraries and librarians working together to build something. I guess the name <em>Library of Congress</em> Name Authority File is also increasingly ironic...

<em>Update: thanks to Kevin Ford (who emailed me privately) it seems that LC has been quite aware of this trend, and highlighted the event in 1996 when NACO members began contributing more records than LC with a <a href="http://www.loc.gov/today/pr/1996/96-154.html">press release</a>.</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5170</wp:post_id>
		<wp:post_date>2012-10-15 07:07:21</wp:post_date>
		<wp:post_date_gmt>2012-10-15 14:07:21</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>who-creates-the-lcnaf-part-2</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="authority-control"><![CDATA[authority control]]></category>
		<category domain="post_tag" nicename="cataloging"><![CDATA[cataloging]]></category>
		<category domain="post_tag" nicename="lcnaf"><![CDATA[lcnaf]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="loc"><![CDATA[loc]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[no]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85470</wp:comment_id>
			<wp:comment_author><![CDATA[merrilee]]></wp:comment_author>
			<wp:comment_author_email>merrilee@merrilee.org</wp:comment_author_email>
			<wp:comment_author_url>http://www.hangingtogether.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-10-17 10:43:13</wp:comment_date>
			<wp:comment_date_gmt>2012-10-17 17:43:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed, I asked my friend and colleague Karen Smith-Yoshimura about this, and she said that the 2000 bump most likely had to do with the conversion of names of Chinese records from Wade-Jiles to Pinyin romanization. Seems like a likely suspect! More here: http://www.loc.gov/catdir/pinyin/romcover.html]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>464</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1350495793.2658";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:8:"merrilee";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1350500348.3294";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>level 0 linked archival data</title>
		<link>http://inkdroid.org/2012/10/24/level-0-linked-archival-data/</link>
		<pubDate>Wed, 24 Oct 2012 10:29:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5183</guid>
		<description></description>
		<content:encoded><![CDATA[<div style="float: left; font-size: 8pt; text-align: center; margin-right: 10px;"><a href="https://en.wikipedia.org/wiki/File:Fondos_archivo.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fondos_archivo.jpg/220px-Fondos_archivo.jpg"/><br />Depósito del Archivo de la Fundación<br />Sierra-Pambley</a></div>


<strong><em>TLDR; lets see if we can share structured archival data better by adding HTML &lt;link&gt; elements that point at our EAD XML files.</em></strong><br />

A few weeks ago I attended a small meeting of DC museums, archives and libraries that were discussing what Linked Data means for Archives. Hillel Arnold and I took collaborative notes in <a href="http://piratepad.net/IB2zcWvFDz">Pirate Pad</a>. For a good part of the time we went around the room talking about how we describe archival collections with various workflows using Encoded Archival Description (EAD), and how this was mostly working (or not).

Some good work has already been done imagining how Linked Data can transform archival description by the <a href="http://blogs.ukoln.ac.uk/locah/">LOCAH</a> (now <a href="http://archiveshub.ac.uk/linkinglives/">Linking Lives</a>) as well as the <a href="http://socialarchive.iath.virginia.edu/">Social Networks and Archival Context</a> project. I think tools like <a href="http://editorsnotes.org/">Editors' Notes</a>, <a href="http://www.cwrc.ca/projects/infrastructure-projects/technical-projects/cwrc-writer/">CWRC Writer</a>, and Google's <a href="http://googledocs.blogspot.com/2012/05/find-facts-and-do-research-inside.html">Research Pane</a> could provide really useful models for how the work of an archivist could benefit from linking to external resources such as Wikipedia, dbpedia, VIAF, etc. But we really didn't talk about that in too much detail. The focus instead was on various tools people used in their EAD workflows: Archivists' Toolkit, Oxygen, ExistDB, Access databases, etc ... and the hope that <a href="http://www.archivesspace.org/">Archives Space</a> could possibly improve matters. We did touch briefly on what it means to make finding aids available on the Web, but not in a very satisfactory way.

I was really struck by how everyone was using EAD, even if their tools were different. I was also left with the lingering suspicion that not much of this EAD data was linked to from the HTML presentation of the finding aid. After some conversations it was also my understanding that even after 20 years of work on EAD, there is not a listing of websites that make EAD finding aids available. It seems particularly sad that institutions have invested a lot of time and effort in putting EAD into practice, and yet we still aren't really sharing them very well with each other.

So in a bit of a fit of frustration I did some <a href="http://github.com/edsu/ead-finder">hacking</a> to see if I could use <a href="http://google.com?q=ead+filetype:xml">Google</a> and <a href="http://beta.worldcat.org/archivegrid">ArchiveGrid</a> to identify websites that serve up finding aids either as HTML or as EAD XML. I wanted to:

<ol>
<li>Get a list of websites that made HTML and EAD XML finding aids available. We can rely on Google to index the Web, but maybe we could index the archival web a bit better ourselves if we had a better understanding of where the EAD data was available. The idea is that this initial list could be used to bootstrap a list of websites making EAD finding aids available in the Wikipedia entry for <a href="https://en.wikipedia.org/wiki/Encoded_Archival_Description">EAD</a>.</li>
<li>To see which websites have HTML representations that link to an EAD XML representation. The rationale here is to encourage a very simple best practice for linking to structured archival data when it is available. More on that below.</li>
</ol>

I was able to identify 201 hosts that served up finding aids either as HTML or XML. You should be able to see them here in this <a href="https://docs.google.com/spreadsheet/ccc?key=0Ak6uboYXcJbBdEFMODhhN1dSaWlUNTRQX05pcmEtLWc#gid=0">spreadsheet</a>. I also collected URLs for finding aids (both HTML and XML) that I was able to locate, which can be seen in this <a href="https://github.com/edsu/ead-finder/blob/master/dump.json">JSON file</a>. 

With the URLs in hand I wrote a little script to examine which of the 156 hosts serving up HTML representations of finding aids had a link to an XML EAD document. I looked for a very simple kind of link that was <a href="http://www.rssboard.org/rss-autodiscovery">popularized</a> by the RSS and Atom syndication community for autodiscovery of blog feeds. A <code>&lt;link&gt;</code> tag that has a <code>rel</code> attribute of <code>alternate</code> and a <code>type</code> attribute set to <code>application/xml</code>. Out of the 156 websites serving up HTML representations of finding aids I could only find two websites that used this link pattern: Princeton University and Emory University. 

For example if you view the HTML source for the <a href="http://findingaids.princeton.edu/collections/C1022">Einstein Collection</a> finding aid at Princeton you'll see this link:

<pre lang="html">
<link rel="alternate" type="application/xml" href="http://findingaids.princeton.edu/collections/C1022.xml" />
</pre>

Similarly the finding aid for the <a href="http://findingaids.library.emory.edu/documents/rushdie1000/">Salman Rushdie</a> collection at Emory University has this link:

<pre lang="html">
<link rel="alternate" type="application/xml" href="/documents/rushdie1000/EAD/" />
</pre>

As the title of this blog post suggests, I'm calling this pattern <em>level 0 linked data</em>. Linked Data purists would probably say this isn't Linked Data at all since it doesn't involve an RDF serialization. And I guess they would be right. But it does express a graph of HTML and EAD data that is linked, and it serves a real need. If you are interested in Linked Data and archives I encourage you to add these links to your HTML finding aids today. 

So why is are these links important? 

The main reason is they are found in HTML documents, which are the representations that matter most on the Web. HTML documents are read by people. They are hypertext documents that link to and from other places on an archives website and elswewhere on the Web at large. They are well understood technically by the Web development community...if you hire a developer they might have strong feelings about using PHP or Ruby, but they will know HTML backwards and forwards. They are crawled and indexed by search engine bots so that researchers around the world can discover our collections. They are cited in social environments like Twitter, Facebook, blog posts, etc. We have a responsibility to create stable homes (URLs) for our archival descriptions that fit into the Web.

The other reason is these links are important is that they make our investment in EAD visible on the Web for anyone who is looking. Nobody but ArchiveGrid actively crawl EAD XML data. They are the only ones that can find them, because they have been told where they are. If we did a better job of advertising the availability of our EAD documents I think we would see more tools and services around them. ArchiveGrid is a good example of the sort of tool that could be built on top of a web of EAD data. But what about archival collections in your local area? Perhaps it would be useful to have a service that let you look across the archival holdings of institutions in a consortium you belong to. Or perhaps you might want to create an alerting service that lets researchers know what new archival collections are being made available. Or maybe you need to collaborate with archives in a specific domain, and need tools that provide a custom experience for that distributed collection. I imagine there would be lots of ideas for apps if there were just a teensy bit more thought put into how finding aids (both the HTML and the XML) are put on the Web, and how we shared information about their availability.

Going forward I think HTML5 microdata and RDFa present some excellent opportunities for Linked Data representations of finding aids. Especially when you consider some of the vocabulary development <a href="http://purl.org/archival/vocab/arch">being</a> <a href="http://archivi.ibc.regione.emilia-romagna.it/ontology/reference_document/referencedocument.html">done</a> <a href="http://blogs.ukoln.ac.uk/locah/2011/02/16/two-changes-to-the-model-and-some-definitions/">around</a> them; as well as some of the <a href="http://discontents.com.au/tag/linked-data">work</a> being done by Tim Sherratt on using  linked data to create new user experiences around archival data. But if your institution has already invested in creating EAD documents I think trying this link pattern with data you already have could be a good first step towards introducing linked data into your archive. I hope it is a first baby step that archives can take in merging some of the structured data found in the EAD XML document into the HTML they publish about their collections.

I'm planning on getting the list of EAD publishers into the Wikipedia article for EAD, and putting out a call for others to add their website if it is missing. I also think that a simple crawling and aggregation service that use the links in some fashion could also encourage more linking. A lot of this blog post has been mental preparation for my involvement in an IMLS funded project run out of Tufts that will be looking at Linked Archival Metadata, which is about to be kicked off this winter. If you've read this far, and have any thoughts or suggestions about this I'd enjoy hearing them either here, on <a href="http://twitter.com/edsu">Twitter</a> or via <a href="mailto:ehs@pobox.com">email</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5183</wp:post_id>
		<wp:post_date>2012-10-24 03:29:49</wp:post_date>
		<wp:post_date_gmt>2012-10-24 10:29:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>level-0-linked-archival-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archivegrid"><![CDATA[archivegrid]]></category>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="ead"><![CDATA[ead]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="nodejs"><![CDATA[nodejs]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="xml"><![CDATA[xml]]></category>
		<wp:postmeta>
			<wp:meta_key>aktt_notify_twitter</wp:meta_key>
			<wp:meta_value><![CDATA[no]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>linkrot: use your illusion</title>
		<link>http://inkdroid.org/2012/11/01/linkrot-use-your-illusio/</link>
		<pubDate>Thu, 01 Nov 2012 15:01:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5318</guid>
		<description></description>
		<content:encoded><![CDATA[Mike Giarlo <a href="http://www.personal.psu.edu/mjg36/blogs/2012/10/understanding-eg-dois-for-data-sets.html">wrote a bit</a> last week about the issues of citing datasets on the Web with <a href="https://en.wikipedia.org/wiki/Digital_object_identifier">Digital Object Identifiers (DOI)</a>. It's a really nice, concise characterization of why libraries and publishers have promoted and used the DOI, and <em>indirect identifiers</em> more generally. Mike defines indirect identifiers as

<blockquote>
... identifiers that point at and resolve to other identifiers.
</blockquote>

I might be reading between the lines a bit, but I think Mike is specifically talking about any identifier that has some documented or ad-hoc mechanism for turning it into a Web identifier, or <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Locator">URL</a>. A quick look at the Wikipedia <a href="https://en.wikipedia.org/wiki/Category:Identifiers">identifier</a> category yields lots of these, many of which (but not all) can be expressed as a <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URI</a>.

The reason why I liked Mike's post so much is that he was able to neatly summarize the psychology that drives the use of indirect identifier technologies:

<blockquote>
... cultural heritage organizations and publishers have done a pretty poor job of persisting their identifiers so far, partly because they didn’t grok the commitment they were undertaking, or because they weren’t deliberate about crafting sustainable URIs from the outset, or because they selected software with brittle URIs, or because they fell flat on some area of sustainability planning (financial, technical, or otherwise), and so because you can’t trust these organizations or their software with your identifiers, you should use this other infrastructure for minting and managing quote persistent unquote identifiers
</blockquote>

Mike goes on to get to the heart of the problem, which is that indirect identifier technologies don't solve the problem of broken links on the Web, they just push it elsewhere. The real problem of maintaining the indirect identifier when the actual URL changes becomes <em>someone else's problem</em>. Out of sight, out of mind ... except it's not really out of sight right? Unless you don't really care about the content you are putting online. 

We all know that <a href="https://en.wikipedia.org/wiki/Link_rot">linkrot</a> on the Web is <a href="http://arxiv.org/abs/1105.3459">a real thing</a>. I would be putting my head in the sand if I were to say it wasn't. But I would also be putting my head in the sand if I said that things don't <a href="http://www.washingtonpost.com/wp-dyn/content/article/2007/10/23/AR2007102301784.html">go missing</a> from our brick and mortar libraries. But still, we should be able to do better than 1/2 the URLs in <a href="http://arxiv.org">arXiv</a> going dead right? I make a living as a web developer, I'm an occasional advocate for linked data, and I'm a big fan of the <a href="http://www.w3.org/2001/tag/doc/URNsAndRegistries-50">work</a> Henry Thompson and David Orchard did for the W3C analyzing the use of alternate identifier schemes on the Web...so, admittedly, I'm a bit of a zealot when it comes to promoting URLs as identifiers, and taking the Web seriously as an information space.

Mike's post actually kicked off what I thought was a useful Twitter <a href="https://twitter.com/mjgiarlo/status/262373950447837184">conversation</a> (yes they can happen), which left me contemplating the future of libraries and archives on (or in) the Web. Specifically, it got me thinking that perhaps libraries and archives of the not too distant future will be places that take special care in how they put content on the Web, so that it can be accessed over time, just like a traditional physical library or archive. The places where links and the content they reference are less likely to go dead will be the new libraries and archives. These may not be the same institutions we call libraries today. Just like today's libraries, these new libraries may not necessarily be free to access. You may need to be part of some community to access them, or to pay some sort of subscription fee. But some of them, and I hope most, will be public assets.

So how to make this happen? What will it look like? Rather than advocating a particular identifier technology I think these new libraries need to think seriously about providing <a href="https://en.wikipedia.org/wiki/Terms_of_service">Terms of Service</a> documents for their content services. I think these library ToS documents will do a few things.

<ul>
<li>They will require the library to think seriously about the service they are providing. This will involve meetings, more meetings, power lunches, and likely lawyers. The outcome will be an organizational understanding of what the library is putting on the Web, and the commitment they are entering into with their users. It won't simply be a matter of a web development team deciding to put up some new website...or take one down. This will likely be hard, but I think it's getting easier all the time, as the importance of the Web as a publishing platform becomes more and more accepted, even in conservative organizations like libraries and archives.</li>
<li>The ToS will address the institutions commitment for continued access to the content. This will involve a clear understanding of the URL namespaces that the library manages, and a statement about how they will be maintained over time. The Web has built in mechanisms for content moving from place to place (<a href="https://en.wikipedia.org/wiki/HTTP_301">HTTP 301</a>), and for when resources are removed (<a href="https://en.wikipedia.org/wiki/HTTP_401#4xx_Client_Error">HTTP 410</a>), so URLs don't need to be written in stone. But the library needs to commit to how resources will redirect permanently to new locations, and for how long--and how they will be removed.</li>
<li>The ToS will explicitly state the licensing associated with the content, preferably with Creative Commons licenses (hey I'm daydreaming here) so that it can be confidently used.</li>
<li>Libraries and archives will develop a shared palette of ToS documents. Each institution won't have it's own special snowflake ToS that nobody reads. There will be some normative patterns for different types of libraries. They will be shared across consortia, and among peer institutions. Maybe they will be incorporated into, or reflect shared principles found in documents like ALA's <a href="https://en.wikipedia.org/wiki/Library_Bill_of_Rights">Library Bill of Rights</a> or SAA's <a href="http://www2.archivists.org/statements/saa-core-values-statement-and-code-of-ethics">Code of Ethics.</a></li>
</ul>

I guess some of this might be a bit reminiscent of the work that has gone into what makes a <a href="http://www.crl.edu/Archiving%20%2526%20Preservation/Digital%20Archives/Metrics%20for%20Assessing%20and%20Certifying-0">trusted repository</a>. But I think a Terms of Service between a library/archive and its researcher is something a bit different. It's more outward looking, less interested in certification and compliance and more interested in entering into and upholding a contract with the user of a collection.

As I was writing this post, Dan Brickley <a href="https://twitter.com/danbri/status/263834404592427010">tweeted</a> about a <a href="http://ecommons.eu/wp-content/uploads/Tony-Ageh-%E2%80%93%C2%A0The-Economies-of-Sharing.pdf">recent talk</a> Tony Ageh (head of the archive development team at the BBC) gave at the recent <a href="http://ecommons.eu/">Economies of the Commons</a> conference. He spoke about his ideas for a future Digital Public Space, and the role that archives and organizations like the BBC play in helping create it.

<blockquote>
Things no longer ‘need’ to disappear after a certain period of time.  Material that once would have flourished only briefly before languishing under lock and key or even being thrown away — can now be made available forever. And our Licence Fee Payers increasingly expect this to be the way of things. We  will soon need to have a very, *very* good reason for why  anything at all disappears from view or is not permanently accessible in some way or other.

That is why the Digital Public Space has placed the  continuing and permanent availability of all publicly-funded  media, and its associated information, as the default and founding principle.
</blockquote>

I think Tony and Mike are right. Cultural heritage organizations need to think more seriously, and more long term about the content they are putting on the Web. They need to put this thought into clear, and succinct contracts with their users. The organizations that do will be what we call libraries and archives tomorrow. I guess I need to start by getting my own house in order eh?]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5318</wp:post_id>
		<wp:post_date>2012-11-01 08:01:04</wp:post_date>
		<wp:post_date_gmt>2012-11-01 15:01:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>linkrot-use-your-illusio</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linkrot"><![CDATA[linkrot]]></category>
		<category domain="post_tag" nicename="terms-of-service"><![CDATA[terms of service]]></category>
		<category domain="post_tag" nicename="url"><![CDATA[url]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[linkrot-use-your-illusion-2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;s:5:"85480";}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85480</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>markleightonfisher@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawno6PSI9v-DWjDQ6hJORHSiNf9oxVDhrYU</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-11-21 10:26:00</wp:comment_date>
			<wp:comment_date_gmt>2012-11-21 17:26:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[You might want to look at the Purdue University Research Repository, as it provides specific Terms of Service (https://research.hub.purdue.edu/legal) for the research datasets it stores.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>466</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:14:"1353518761.133";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawno6PSI9v-DWjDQ6hJORHS";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1353841798.3325";s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85482</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-11-25 04:22:10</wp:comment_date>
			<wp:comment_date_gmt>2012-11-25 11:22:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Mark, many thanks for the link to the PURR ToS. The details about the personal information that HubZero collects is really important. I guess it's good that it lets people know up front that you can do whatever you want with the account at any time. But I was actually thinking of ToS that help users understand what a "repository" does in the context of Purdue. Can people reliably link to resources housed in the repository? How is this ToS distinguish PURR as a different sort of application that is concerned with the longevity of scholarly publishing on the Web?]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";s:15:"1353842535.5399";s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Inside Out Libraries</title>
		<link>http://inkdroid.org/2012/12/18/inside-out-libraries/</link>
		<pubDate>Wed, 19 Dec 2012 05:30:17 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5330</guid>
		<description></description>
		<content:encoded><![CDATA[Peter Brantley tells a <a href="http://www.publishersweekly.com/pw/by-topic/industry-news/libraries/article/55131-you-have-two-maybe-three-years.html">sad tale</a> about where public library leadership is at, as we plunge headlong into the ebook future, that has been talked about for what seems like forever, and which is now upon us. It's not pretty.

<blockquote>
The general consensus among participants was that public libraries have two, maybe three years to establish their relevance in the digital realm, or risk fading from the central place they have long occupied in the world’s literary culture.
</blockquote>

The fact that a bunch of big-wigs invited by IFLA were seemingly unable to find inspiration and reason to hope that public libraries will continue to exist is not surprising in the least I guess. I'm not sure that libraries were ever the center of the world's literary culture. But for the sake of argument lets assume they were, and that now they're increasingly not. Let us also assume that the economic landscape around ebooks is in incredible turmoil, and that there will continue to be sea changes in technologies, and people's use of them in this area for the foreseeable future.

What can libraries do to stay relevant? I think part of the answer is: stop being libraries...well, sorta.

<h2>The HyperLocal</h2>

<blockquote>
The most serious threat facing libraries does not come from publishers, we argued, but from e-book and digital media retailers like Amazon, Apple, and Google. While some IFLA staff protested that libraries are not in the business of competing with such companies, the library representatives stressed that they are. <strong>If public libraries can’t be better than Google or Amazon at something, then libraries will lose their relevance.</strong>
</blockquote>

In my mind the thing that libraries have to offer, which these big corporations cannot, is authentic, local context for information about a community's past, present and future. But in the past century or so libraries have focused on collecting mass produced objects, and sharing data about said objects. The mission of collecting hyper-local information has typically been a side task, that has fallen to special collections and archives. If I were invited to that IFLA meeting I would've said that libraries need to shift their orientation to caring more about the practices of archives and manuscript collections, by collecting unique, valued, at risk local materials, and adapting collection development and descriptive practices to the realities of more and more of this information being available as data. 

As Mark Matienzo <a href="https://twitter.com/anarchivist/status/281460231769313280">indicated</a> (somewhat indirectly in Twitter) after I published this blog post, a lot of this work involves focusing less on hoarding items like books, and focusing more on the functions, services, and actions that public libraries want to document and engage with in their communities. Traditionally this orientation has been a strength area for archivists in their practice and theory of <a href="https://en.wikipedia.org/wiki/Archival_appraisal">appraisal</a>  where: 

<blockquote cite="https://en.wikipedia.org/wiki/Archival_appraisal">
... considerations ... include how to meet the record-granting body’s organizational needs, how to uphold requirements of organizational accountability (be they legal, institutional, or determined by archival ethics), and how to meet the expectations of the record-using community. <a href="https://en.wikipedia.org/wiki/Archival_appraisal">Wikipedia</a>
</blockquote>

I think this represents a pretty significant cognitive shift for library professionals, and would in fact take some doing. But perhaps that's just because my exposure to archival theory in "library school" was pretty pathetic. Be that as it may here are some practical examples of growth areas for public libraries that I wish came up at the IFLA meeting.

<h2>Web Archiving</h2>

The Internet Archive and national libraries that are part of the International Internet Preservation Consortium don't have the time, resources and often mandate to collect web content that are of interest at the local level. What if the tooling and expertise existed for public libraries to perform some of this work, and to have the results fed into larger aggregations of web archives?

<h2>Municipality Reports and Data</h2>

Increasing amounts of data are being collected as part of the daily working of our local governments. What if your public library had the resources to be a repository for this data? Yeah, I said the R word. But I'm not suggesting that public libraries get the expertise to set up Fedora instances with Hydra heads, or something. I'm thinking about approaches to allowing data to easily flow into an organization, where it is backed up, and made available in a clearinghouse manner similar to <a href="https://public.resource.org/">public.resource.org</a> on the Web, for search engines to pick up. Perhaps even services like <a href="http://jasongriffey.net/librarybox/">LibraryBox</a> offer another lens to look at the opportunities that lie in this area.

<h2>Born Digital Manuscript Collections</h2>

Public libraries should be aggressively collecting the "papers" of local people who have had significant contributions to their communities. Increasingly, these aren't paper at all, but are born digital content. For example: email correspondence, document archives, digital photograph collections. I think that librarians and archivists know, in theory, that this born digital content is out there, but the reality is it's not flowing into the public library/archive. How can we change this? Efforts such as <a href="http://www.personalarchiving.com/">Personal Digital Archiving</a> are important for two reasons: they help set up the right conditions for born digital collections to be donated, and they also make professionals think about how they would like to receive materials so that they are easier to process. Think more things like <a href="http://born-digital-archives.blogspot.com/">AIMS</a>, training and tooling for both professionals and citizens.

<h2>Licensing</h2>

It's not unusual for archives and special collections to have all sorts of donor gift agreements that place restrictions on how their donated materials can be used. To some extent needing to visit the collection, request it, and not being able to leave the room with it, has mitigated some of this special-snowflakism. But when things are online things change a bit. We need to normalize these agreements so that content can flow online, and be used online in clearer ways. What if we got donors to think about Creative Commons licenses when they donated materials? How can we make sure donated material can become a usable part of the Web

<h2>Persistence</h2>

We all know that things come and go on the Web. But it doesn't need to be that way for everything on the Web. Libraries and archives have an opportunity to show how focusing on being a clearninghouse for data assets can allow for things to live persistently on the Web. Thinking about our URLs as identifiers for things we are taking care of is important. Practical strategies for achieving that are possible, and repeatable. What if public libraries were safe harbors for local content on the World Wide Web? This might sound hard to do, but I think it's not as hard as people think.

<h2>Metrics</h2>

As libraries/archives make more local content available publicly on the Web it becomes important to track how this content is accessed and used online. Quick wins like Web analytics tools (Google Analytics) for seeing what is being accessed and from where. Seeing how content is cited in social media applications like Facebook, Twitter, Pinterest and Wikipedia is important for reporting on the value of online collections. But encouraging professionals to use this information to become part of the conversations is equally important. Good metrics are also essential for collection development purposes, seeing what content is of interest, and what is not.

<h2>Inside Out Libraries</h2>

So, no I don't think public libraries need a new open source Overdrive. The ebook market will likely continue to take care of itself. I also am not really convinced we need some overarching organization like the Digital Public Library of America to serve as a single point of failure when the funding runs dry. We need distributed strategies for documenting our local communities, so that this information can take its rightful place on the Web, and be picked up by Google so that people can find it when they are on the other side of the world. Things will definitely keep changing, but I think libraries and archives need to invest in the Web as an enduring delivery platform for information.

I've never been before but I was so excited to read the <a href="http://elag2013.org/program/call-for-papers/">call</a> for the European Library Automation Group (ELAG) this year.

<blockquote>
The theme of this year’s conference is ‘The INSIDE-OUT Library’. This theme was chosen at last year’s conference, because we concluded:

<ul>
	<li>Libraries have been focusing on bringing the world to their users. Now information is globally available.</li>
	<li>Libraries have been producing metadata for the same publications in parallel. Now they are faced with deduplicating redundancy.</li>
	<li>Libraries have been selecting things for their users. Now the users select things themselves.</li>
	<li>Libraries have been supporting users by indexing things locally. Now everything is being indexed in global, shared indexes.</li>
</ul>

Instead of being an OUTSIDE-IN library, libraries should try and stay relevant by shifting their paradigm 180 degrees. Instead of only helping users to find what is available globally, they should also focus on making local collections and production available to the world. Instead of doing the same thing everywhere, libraries should focus on making unique information accessible. Instead of focusing on information trapped in publications, libraries should try and give the world new views on knowledge.
</blockquote>

This blog post is really just a somewhat shabby rephrasing of that call. Maybe IFLA could use some of the folks on the ELAG program commmittee at their next meeting about the future of public libraries? Hopefully 2013 will be a year I can make it to ELAG.

I expect public libraries will continue to exist, but there isn't going to be some magical technical solution to their problems. Their future will be forged by each local relationship they make, which leads to them better documenting their place on the Web. We may not call these places public libraries at first, but that's what they will be.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5330</wp:post_id>
		<wp:post_date>2012-12-18 22:30:17</wp:post_date>
		<wp:post_date_gmt>2012-12-19 05:30:17</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>inside-out-libraries</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="ebooks"><![CDATA[ebooks]]></category>
		<category domain="post_tag" nicename="elag"><![CDATA[elag]]></category>
		<category domain="post_tag" nicename="ifla"><![CDATA[ifla]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{i:0;i:85502;i:1;i:85503;i:2;i:85505;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85502</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-12-19 16:55:13</wp:comment_date>
			<wp:comment_date_gmt>2012-12-19 23:55:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Well said. 

I still think it's important for libraries not to give up the ebook fight... but don't have much optimism on anything but total defeat. 

On "is authentic, local context for information about a community’s past, present and future"

I've been thinking about that for a while. I've been thinking every public library ought to have a program where you can bring in your _personal_ historical recorded memory (snapshots from shoeboxes, old VHS home movies, the invitation from your wedding), and the library will have computers available to scan or otherwise digitize the material, and show you how to do it (you'll do it yourself). THEN the library will offer to host the content publically and 'perpetually' (perhaps in partnership with Internet Archive), and in a UI that lets community members add commetns and tags ("I was at that wedding! That's me third from the right in that photo!")

Wouldn't that be SWEET?  It's history that is being lost, that nobody is competing with the library to preserve, and that actually does match the libraries essential 'spiritual mission'.  The only thing libraries lack is the funding. If I were a library funder or project (DPLA?), I'd be looking to invent programs like this all accross the country.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1355961319.8547160625457763671875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85503</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-12-19 16:57:12</wp:comment_date>
			<wp:comment_date_gmt>2012-12-19 23:57:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[(I also suspect that public libraries had 2, maybe 3 years... about 3 years ago. And academic libraries MAYBE have 2 or 3 years now. And I am despondent about any of them rising to the challenge. But I'd vote for edsu for library czar, you're on the right track completely).]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1355961433.113462924957275390625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85504</wp:comment_id>
			<wp:comment_author><![CDATA[Rebekkah Abraham]]></wp:comment_author>
			<wp:comment_author_email>rebekkah.abraham@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-12-19 17:28:58</wp:comment_date>
			<wp:comment_date_gmt>2012-12-20 00:28:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey bibwild - some libraries are doing what you propose. Check out the program West Hartford Library, CT are running inviting their commmunity to come and have their photos digitised: http://www.westhartfordlibrary.org/research_tools/local_history/historypin]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>468</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1355963339.08961200714111328125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:16:"Rebekkah Abraham";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1355964136.6491219997406005859375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85505</wp:comment_id>
			<wp:comment_author><![CDATA[Peter Murray]]></wp:comment_author>
			<wp:comment_author_email>jester@dltj.org</wp:comment_author_email>
			<wp:comment_author_url>http://dltj.org/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-12-20 19:23:17</wp:comment_date>
			<wp:comment_date_gmt>2012-12-21 02:23:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Right on, Ed!  Libraries used to focus on commercial-physical, then commercial-digital, and now it is time to focus on local-digital.  I called this the <a href="http://dltj.org/article/riding-the-waves/" rel="nofollow">riding the third wave</a>.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>469</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1356056597.344521045684814453125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:4:"dltj";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1356574609.69122791290283203125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85506</wp:comment_id>
			<wp:comment_author><![CDATA[arlenes]]></wp:comment_author>
			<wp:comment_author_email>arlene_schmuland-1@yahoo.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-12-21 15:38:16</wp:comment_date>
			<wp:comment_date_gmt>2012-12-21 22:38:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I think I missed something in your discussion that you may not have meant to leave out: where are the archivists? Other than in books or on Twitter telling you that there's a paradigm shift to be made?  Why not put them in charge--or at least closer to the top--of this shift? There's lots of them out there, clearly nowhere as many as librarians, but these are the people who already have the knowledge you're going to need to convert libraries to archives and localized collections. They know the pitfalls, they know the places where efficiencies can be made. They know the theory, they know the practice. They also know the costs associated with creating archival collections, and what can and can't be done with the existing space because chances are, they've had to convert library space to archives space already. And many of them already speak library-speak because many of them are already situated in libraries or came out of archival-focus programs within library schools and have been translating their work for librarians for a long time now. If archives are the future for libraries, shouldn't you be inviting huge numbers of archivists--directly--to this table? And asking them for their help?

As a long-time archivist, I'd help. It's my future too.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>470</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1356129496.7335479259490966796875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:7:"arlenes";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1356574390.51064395904541015625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85513</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2012-12-26 19:16:20</wp:comment_date>
			<wp:comment_date_gmt>2012-12-27 02:16:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Arlenes ... you make an excellent point. Yes, we need more archivists, and they need to be in charge. I think we also need more archivists that understand "the digital", or issues surrounding digital curation and digital preservation.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>85506</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1356574580.7579309940338134765625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>85525</wp:comment_id>
			<wp:comment_author><![CDATA[(Discover AND deliver) OR else @ CommonPlace.Net]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://commonplace.net/2013/01/discover-and-deliver-or-else/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-01-07 02:54:29</wp:comment_date>
			<wp:comment_date_gmt>2013-01-07 09:54:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] name in common. For a vision on the future of public libraries, see Ed Summer’s excellent post “The inside out library”. As for research and special libraries, some of what I am about to say will apply to these [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1357552470.1732618808746337890625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1357553320.6668798923492431640625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86531</wp:comment_id>
			<wp:comment_author><![CDATA[Libraries and Open Civic Data | @bibliotechy]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://chadbnelson.wordpress.com/2013/08/27/libraries-and-open-civic-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-08-27 18:37:13</wp:comment_date>
			<wp:comment_date_gmt>2013-08-28 01:37:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] year, Ed Summers wrote a post called “Inside Out Library” about how rather than trying to pull in data from all over the world to present to local users, [&#8230;]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1377657191.473040103912353515625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1377653834.0897769927978515625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>archiving tweets</title>
		<link>http://inkdroid.org/2012/12/31/archiving-tweets/</link>
		<pubDate>Mon, 31 Dec 2012 19:42:48 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5364</guid>
		<description></description>
		<content:encoded><![CDATA[If you are an active Twitter user you may have <a href="http://blogs.wsj.com/digits/2012/12/19/how-to-download-your-entire-twitter-history-and-laugh-at-how-bad-it-is/?mod=e2tw">heard</a> that you can now download your complete archive of tweets. The functionality is still being rolled out across the millions of accounts, so don't be surprised if you don't see the function yet in your settings.

The WSJ piece kind of joked about the importance of this move on Twitter's part, which is a bit unfortunate, since it's a pretty important issue. Yes you can use a 3rd party apps for downloading your Twitter data, but it says a lot when a company takes "archiving" seriously enough to offer it as a service to its users.

If you work in the digital preservation space it's kind of fun to take a look at the way that Twitter makes these personal archives available. Luckily (if you don't have the archive download button yet like me) <a href="http://davewiner.com/">Dave Winer</a> has started <a href="http://threads2.scripting.com/2012/december/uploadYourTwitterArchive">collecting</a> some archives, and making them publicly available for browsing and download off of S3. For example we can look at <a href="http:/twitter.com/sarahebourne">Sarah Bourne's</a> (who tipped me off to Dave's work--thanks Sarah!).  After you've downloaded the ZIP file you get a directory that looks like:

<pre>
sarahebourne/
|-- css
|   `-- application.min.css
|-- data
|   |-- csv
|   |   |-- 2008_08.csv
|   |   |-- 2008_09.csv
|   |   |-- 2008_10.csv
|   |   |-- 2008_11.csv
|   |   |-- 2008_12.csv
|   |   |-- 2009_01.csv
|   |   |-- 2009_02.csv
|   |   |-- 2009_03.csv
|   |   |-- 2009_04.csv
|   |   |-- 2009_05.csv
|   |   |-- 2009_06.csv
|   |   |-- 2009_07.csv
|   |   |-- 2009_08.csv
|   |   |-- 2009_09.csv
|   |   |-- 2009_10.csv
|   |   |-- 2009_11.csv
|   |   |-- 2009_12.csv
|   |   |-- 2010_01.csv
|   |   |-- 2010_02.csv
|   |   |-- 2010_03.csv
|   |   |-- 2010_04.csv
|   |   |-- 2010_05.csv
|   |   |-- 2010_06.csv
|   |   |-- 2010_07.csv
|   |   |-- 2010_08.csv
|   |   |-- 2010_09.csv
|   |   |-- 2010_10.csv
|   |   |-- 2010_11.csv
|   |   |-- 2010_12.csv
|   |   |-- 2011_01.csv
|   |   |-- 2011_02.csv
|   |   |-- 2011_03.csv
|   |   |-- 2011_04.csv
|   |   |-- 2011_05.csv
|   |   |-- 2011_06.csv
|   |   |-- 2011_07.csv
|   |   |-- 2011_08.csv
|   |   |-- 2011_09.csv
|   |   |-- 2011_10.csv
|   |   |-- 2011_11.csv
|   |   |-- 2011_12.csv
|   |   |-- 2012_01.csv
|   |   |-- 2012_02.csv
|   |   |-- 2012_03.csv
|   |   |-- 2012_04.csv
|   |   |-- 2012_05.csv
|   |   |-- 2012_06.csv
|   |   |-- 2012_07.csv
|   |   |-- 2012_08.csv
|   |   |-- 2012_09.csv
|   |   |-- 2012_10.csv
|   |   |-- 2012_11.csv
|   |   `-- 2012_12.csv
|   `-- js
|       |-- payload_details.js
|       |-- tweet_index.js
|       |-- tweets
|       |   |-- 2008_08.js
|       |   |-- 2008_09.js
|       |   |-- 2008_10.js
|       |   |-- 2008_11.js
|       |   |-- 2008_12.js
|       |   |-- 2009_01.js
|       |   |-- 2009_02.js
|       |   |-- 2009_03.js
|       |   |-- 2009_04.js
|       |   |-- 2009_05.js
|       |   |-- 2009_06.js
|       |   |-- 2009_07.js
|       |   |-- 2009_08.js
|       |   |-- 2009_09.js
|       |   |-- 2009_10.js
|       |   |-- 2009_11.js
|       |   |-- 2009_12.js
|       |   |-- 2010_01.js
|       |   |-- 2010_02.js
|       |   |-- 2010_03.js
|       |   |-- 2010_04.js
|       |   |-- 2010_05.js
|       |   |-- 2010_06.js
|       |   |-- 2010_07.js
|       |   |-- 2010_08.js
|       |   |-- 2010_09.js
|       |   |-- 2010_10.js
|       |   |-- 2010_11.js
|       |   |-- 2010_12.js
|       |   |-- 2011_01.js
|       |   |-- 2011_02.js
|       |   |-- 2011_03.js
|       |   |-- 2011_04.js
|       |   |-- 2011_05.js
|       |   |-- 2011_06.js
|       |   |-- 2011_07.js
|       |   |-- 2011_08.js
|       |   |-- 2011_09.js
|       |   |-- 2011_10.js
|       |   |-- 2011_11.js
|       |   |-- 2011_12.js
|       |   |-- 2012_01.js
|       |   |-- 2012_02.js
|       |   |-- 2012_03.js
|       |   |-- 2012_04.js
|       |   |-- 2012_05.js
|       |   |-- 2012_06.js
|       |   |-- 2012_07.js
|       |   |-- 2012_08.js
|       |   |-- 2012_09.js
|       |   |-- 2012_10.js
|       |   |-- 2012_11.js
|       |   `-- 2012_12.js
|       `-- user_details.js
|-- img
|   |-- bg.png
|   `-- sprite.png
|-- index.html
|-- js
|   `-- application.min.js
|-- lib
|   |-- bootstrap
|   |   |-- bootstrap-dropdown.js
|   |   |-- bootstrap.min.css
|   |   |-- bootstrap-modal.js
|   |   |-- bootstrap-tooltip.js
|   |   |-- bootstrap-transition.js
|   |   |-- glyphicons-halflings.png
|   |   `-- glyphicons-halflings-white.png
|   |-- hogan
|   |   `-- hogan-2.0.0.min.js
|   |-- jquery
|   |   `-- jquery-1.8.3.min.js
|   |-- twt
|   |   |-- sprite.png
|   |   |-- sprite.rtl.png
|   |   |-- twt.all.min.js
|   |   `-- twt.min.css
|   `-- underscore
|       `-- underscore-min.js
`-- README.txt
</pre>

So why is this interesting?

<h2>The Data</h2>

The archive includes data both as CSV and as JavaScript. The CSV is perfect for throwing into a spreadsheet, and doing stuff with it there. The JavaScript is actually a very light shim over some JSON data that is quite a bit richer than the CSV. The JavaScript shim is needed so that it can be used by the app that comes in the archive (more on that later). For example here's a randomly picked tweet from Sarah:

https://twitter.com/sarahebourne/status/281405942321532929

Here is how the Tweet shows up in the CSV:

<pre lang="csv">

"tweet_id","in_reply_to_status_id","in_reply_to_user_id","retweeted_status_id","retweeted_status_user_id","timestamp","source","text","expanded_urls"
"281405942321532929","281400879465238529","61233","","","2012-12-19 14:29:39 +0000","<a href=""http://janetter.net/"" rel=""nofollow"">Janetter</a>","@monkchips Ouch. Some regrets are harsher than others."

</pre>

And here's the archived JSON for the Tweet:

<pre lang="javascript">
{
  "source" : "<a href=\"http://janetter.net/\" rel=\"nofollow\">Janetter</a>",
  "entities" : {
    "user_mentions" : [ {
      "name" : "James Governor",
      "screen_name" : "monkchips",
      "indices" : [ 0, 10 ],
      "id_str" : "61233",
      "id" : 61233
    } ],
    "media" : [ ],
    "hashtags" : [ ],
    "urls" : [ ]
  },
  "in_reply_to_status_id_str" : "281400879465238529",
  "geo" : {
  },
  "id_str" : "281405942321532929",
  "in_reply_to_user_id" : 61233,
  "text" : "@monkchips Ouch. Some regrets are harsher than others.",
  "id" : 281405942321532929,
  "in_reply_to_status_id" : 281400879465238529,
  "created_at" : "Wed Dec 19 14:29:39 +0000 2012",
  "in_reply_to_screen_name" : "monkchips",
  "in_reply_to_user_id_str" : "61233",
  "user" : {
    "name" : "Sarah Bourne",
    "screen_name" : "sarahebourne",
    "protected" : false,
    "id_str" : "16010789",
    "profile_image_url_https" : "https://si0.twimg.com/profile_images/638441870/Snapshot-of-sb_normal.jpg",
    "id" : 16010789,
    "verified" : false
  }
}
</pre>

So there's quite a bit more structured data in the archived JSON including whether geo coordinates, hash tags, urls mentioned, etc. Also, the avatar images are still referenced out on the Web, where they can change, disappear, etc. It's also interesting to compare the archived JSON against what you get back the from Twitter API for the same Tweet:

<pre lang="javascript">
{
  "user": {
    "follow_request_sent": false, 
    "profile_use_background_image": true, 
    "default_profile_image": false, 
    "id": 16010789, 
    "verified": false, 
    "profile_text_color": "080C0C", 
    "profile_image_url_https": "https://si0.twimg.com/profile_images/638441870/Snapshot-of-sb_normal.jpg", 
    "profile_sidebar_fill_color": "FCFAEF", 
    "entities": {
      "url": {
        "urls": [
          {
            "url": "http://www.linkedin.com/in/sarahbourne", 
            "indices": [
              0, 
              38
            ], 
            "expanded_url": null
          }
        ]
      }, 
      "description": {
        "urls": []
      }
    }, 
    "followers_count": 2367, 
    "profile_sidebar_border_color": "FFFFFF", 
    "id_str": "16010789", 
    "profile_background_color": "DAE0D9", 
    "listed_count": 331, 
    "profile_background_image_url_https": "https://si0.twimg.com/profile_background_images/671143407/8544adf04bc3823d306c7f05efef2351.jpeg", 
    "utc_offset": -18000, 
    "statuses_count": 20090, 
    "description": "Internet technology strategist, Accessibility and assistive technologies. Views expressed/implied are my own. See my Twitter lists for more interests.", 
    "friends_count": 784, 
    "location": "Boston, MA, USA", 
    "profile_link_color": "800326", 
    "profile_image_url": "http://a0.twimg.com/profile_images/638441870/Snapshot-of-sb_normal.jpg", 
    "following": true, 
    "geo_enabled": false, 
    "profile_banner_url": "https://si0.twimg.com/profile_banners/16010789/1348096060", 
    "profile_background_image_url": "http://a0.twimg.com/profile_background_images/671143407/8544adf04bc3823d306c7f05efef2351.jpeg", 
    "screen_name": "sarahebourne", 
    "lang": "en", 
    "profile_background_tile": true, 
    "favourites_count": 3147, 
    "name": "Sarah Bourne", 
    "notifications": null, 
    "url": "http://www.linkedin.com/in/sarahbourne", 
    "created_at": "Wed Aug 27 12:24:25 +0000 2008", 
    "contributors_enabled": false, 
    "time_zone": "Eastern Time (US & Canada)", 
    "protected": false, 
    "default_profile": false, 
    "is_translator": false
  }, 
  "favorited": false, 
  "entities": {
    "user_mentions": [
      {
        "id": 61233, 
        "indices": [
          0, 
          10
        ], 
        "id_str": "61233", 
        "screen_name": "monkchips", 
        "name": "James Governor"
      }
    ], 
    "hashtags": [], 
    "urls": []
  }, 
  "contributors": null, 
  "truncated": false, 
  "text": "@monkchips Ouch. Some regrets are harsher than others.", 
  "created_at": "Wed Dec 19 14:29:39 +0000 2012", 
  "retweeted": false, 
  "in_reply_to_status_id_str": "281400879465238529", 
  "coordinates": null, 
  "in_reply_to_user_id_str": "61233", 
  "source": "<a href=\"http://janetter.net/\" rel=\"nofollow\">Janetter</a>", 
  "in_reply_to_status_id": 281400879465238529, 
  "in_reply_to_screen_name": "monkchips", 
  "id_str": "281405942321532929", 
  "place": null, 
  "retweet_count": 0, 
  "geo": null, 
  "id": 281405942321532929, 
  "in_reply_to_user_id": 61233
}
</pre>

Using <a href="https://npmjs.org/package/json-diff">json-diff</a> it's not too difficult to see what the differences are between the archived version and the API version:

<pre lang="diff">
 {
+  favorited: false
+  contributors: null
+  truncated: false
+  retweeted: false
+  coordinates: null
+  place: null
+  retweet_count: 0
   entities: {
-    media: [
-    ]
   }
-  geo: {
-  }
+  geo: null
   user: {
+    follow_request_sent: false
+    profile_use_background_image: true
+    default_profile_image: false
+    profile_text_color: "080C0C"
+    profile_sidebar_fill_color: "FCFAEF"
+    entities: {
+      url: {
+        urls: [
+          {
+            url: "http://www.linkedin.com/in/sarahbourne"
+            indices: [
+              0
+              38
+            ]
+            expanded_url: null
+          }
+        ]
+      }
+      description: {
+        urls: [
+        ]
+      }
+    }
+    followers_count: 2367
+    profile_sidebar_border_color: "FFFFFF"
+    profile_background_color: "DAE0D9"
+    listed_count: 331
+    profile_background_image_url_https: "https://si0.twimg.com/profile_background_images/671143407/8544adf04bc3823d306c7f05efef2351.jpeg"
+    utc_offset: -18000
+    statuses_count: 20090
+    description: "Internet technology strategist, Accessibility and assistive technologies. Views expressed/implied are my own. See my Twitter lists for more interests."
+    friends_count: 784
+    location: "Boston, MA, USA"
+    profile_link_color: "800326"
+    profile_image_url: "http://a0.twimg.com/profile_images/638441870/Snapshot-of-sb_normal.jpg"
+    following: true
+    geo_enabled: false
+    profile_banner_url: "https://si0.twimg.com/profile_banners/16010789/1348096060"
+    profile_background_image_url: "http://a0.twimg.com/profile_background_images/671143407/8544adf04bc3823d306c7f05efef2351.jpeg"
+    lang: "en"
+    profile_background_tile: true
+    favourites_count: 3147
+    notifications: null
+    url: "http://www.linkedin.com/in/sarahbourne"
+    created_at: "Wed Aug 27 12:24:25 +0000 2008"
+    contributors_enabled: false
+    time_zone: "Eastern Time (US & Canada)"
+    default_profile: false
+    is_translator: false
   }
 }
</pre>

To be fair some of the user profile information has been normalized in the archive (perhaps to save space for the viewing application) out to a user_details.js file, which looks like:

<pre lang="javascript">{
  "screen_name" : "sarahebourne",
  "location" : "Boston, MA, USA",
  "full_name" : "Sarah Bourne",
  "bio" : "Internet technology strategist, Accessibility and assistive technologies. Views expressed/implied are my own. See my Twitter lists for more interests.",
  "id" : "16010789",
  "created_at" : "Wed Aug 27 12:24:25 +0000 2008"
}
</pre>

Notably missing from this is a homepage for the user, their number of favourites, their number of friends, followers, whether geo is enabled, etc.

All these details aside, Twitter deserves a lot of credit for making the data available as CSV for ease of use, and also as JavaScript for programmatic use.

<h2>The Code</h2>

So the really, really neat thing about the archive is that it comes with a pure HTML, CSS and JavaScript application that you can open locally in your browser and view your archive. It looks pretty, for example <a href="http://static.scripting.com/twitterArchives/bourne/index.html">here</a> is Sarah's archive that Dave Winer mounted up on S3. It even has a keyword search across all your tweets, which takes a bit of time (it interactively loads all your tweet JavaScript files mentioned above), but it works. You can zip the data up, give it to someone else, and it all just works. 

The archive uses some third party libraries such as jQuery, Underscore, Twitter Bootstrap and Hogan, which all come minified and bundled statically in the archive. The application itself is called Grailbird and comes minified as well. Grailbird loads the static JavaScript (as needed) and displays it. The only network traffic I saw while it was running was fetching avatar images.

Assuming JavaScript backwards compatibility, and browser support for JavaScript, the Twitter archive's contextual display for the underlying data could last a long, long time. At least that's a possible interpretation based on David Rosenthal's <a href="http://blog.dshr.org/2012/10/formats-through-time.html">hypothesis</a> about the Web's effect on format obsolescence. I think it's safe to say that this app written for the local Web platform is likely last longer than a GUI application written in another language environment. The separation of code and data, and independence from a particular browser implementation are big wins. These are qualities that we all had to fight and work hard for on the Web, and I think it makes sense to re-purpose them here in an archival context.

I doubt anyone from Twitter has read this far, but if someone has, it would be great to see Grailbird show up with the other great stuff you have released to Github. I found myself wanting to quickly search across tweets looking for things, like geo-enabled tweets (to make sure that they are there). I could look at the minified Grailbird source in Chrome using developer tools, but it wasn't good enough for me to figure out how to dynamically load data. I resorted to using NodeJS, and evaling the JavaScript files...and was able to confirm that there is geo data in the archives if you have it enabled. Here's the simplistic script I came up with:

<pre lang="javascript">
var fs = require('fs');

var Grailbird = {data: {}};

// load all the tweet data
eval(fs.readFileSync("data/js/tweet_index.js", "utf8"));
for (var i = 0; i < tweet_index.length; i++) {
  eval(fs.readFileSync(tweet_index[i].file_name, "utf8"));
}

// look at each tweet and print out the date and geolocation if it's there
for (var slice in Grailbird.data) {
  for (var j = 0; j < Grailbird.data[slice].length; j++) {
    var tweet = Grailbird.data[slice][j];
    if (tweet.geo.coordinates) console.log(tweet.created_at, ",", tweet.geo.coordinates.join(","));
  }
}
</pre>

and the output for <a href="https://twitter.com/adactio">Jeremy Keith's</a> archive.

</pre><pre>
% node geo.js
Fri Nov 30 13:08:33 +0000 2012,50.8262027605,-0.138112306595
Sat Nov 17 12:09:18 +0000 2012,54.6000387923,-5.9254288673
Fri Nov 16 22:32:03 +0000 2012,54.5925614526,-5.930852294
Thu Nov 15 13:35:35 +0000 2012,54.595909,-5.922033
Sat Nov 10 12:59:37 +0000 2012,50.825832,-0.142381
Fri Nov 09 13:54:51 +0000 2012,50.8262027605,-0.1381123066
Wed Nov 07 18:07:24 +0000 2012,50.825977,-0.138339
Tue Nov 06 16:58:49 +0000 2012,50.8378257671,-1.1800042739
Tue Oct 30 11:19:53 +0000 2012,50.8262027605,-0.1381123066
Thu Oct 18 17:51:22 +0000 2012,43.0733634985,-89.38608062
Tue Oct 16 17:29:20 +0000 2012,43.0872606735,-89.3659955263
Tue Oct 09 18:11:20 +0000 2012,40.7406891129,-74.0076184273
Sun Oct 07 14:27:50 +0000 2012,50.82906975,-0.126056
Sat Oct 06 16:29:30 +0000 2012,50.825832,-0.142381
Thu Oct 04 16:46:56 +0000 2012,50.8262027605,-0.1381123066
Tue Oct 02 17:46:42 +0000 2012,50.826646,-0.136921
Mon Oct 01 10:46:04 +0000 2012,50.8262027605,-0.1381123066
Mon Oct 01 10:43:46 +0000 2012,50.8262027605,-0.1381123066
Mon Oct 01 09:38:01 +0000 2012,50.8236703111,-0.1387184062
Mon Oct 01 08:53:15 +0000 2012,50.8236703111,-0.1387184062
Thu Sep 27 13:05:16 +0000 2012,59.915652,10.749959
Sun Sep 23 12:54:16 +0000 2012,50.8281663943,-0.128531456
Sat Sep 22 13:44:09 +0000 2012,50.87447886,0.017625
Thu Sep 20 13:16:11 +0000 2012,50.8262027605,-0.1381123066
Thu Sep 20 09:27:55 +0000 2012,50.8262027605,-0.1381123066
Mon Sep 17 07:51:20 +0000 2012,47.9952739036,7.8525775405
Sun Sep 16 09:01:28 +0000 2012,51.1599172667,-0.1787844393
Thu Sep 13 12:40:26 +0000 2012,50.822951,-0.136905
Tue Sep 11 18:41:47 +0000 2012,50.822746,-0.142274
Tue Sep 11 17:19:38 +0000 2012,50.822219,-0.140802
Tue Sep 11 13:05:59 +0000 2012,50.8262027605,-0.1381123066
Tue Sep 11 13:03:35 +0000 2012,50.8262027605,-0.1381123066
Tue Sep 11 12:48:51 +0000 2012,50.8262027605,-0.1381123066
Tue Sep 11 12:06:36 +0000 2012,50.8262027605,-0.1381123066
Tue Sep 11 08:23:00 +0000 2012,50.8262027605,-0.1381123066
Sun Sep 09 19:10:21 +0000 2012,50.826646,-0.136921
Tue Sep 04 17:33:44 +0000 2012,50.826646,-0.136921
Tue Sep 04 12:57:16 +0000 2012,50.822951,-0.136905
Mon Sep 03 16:03:37 +0000 2012,50.8262027605,-0.1381123066
Mon Sep 03 15:26:41 +0000 2012,50.8262027605,-0.1381123066
Sun Sep 02 19:40:38 +0000 2012,50.8229428584,-0.1390289018
Sun Sep 02 19:24:45 +0000 2012,50.8229428584,-0.1390289018
Sun Sep 02 19:08:55 +0000 2012,50.825977,-0.138339
Sun Sep 02 18:25:08 +0000 2012,50.825449,-0.137123
Sun Sep 02 17:04:15 +0000 2012,50.825449,-0.137123
Sun Sep 02 15:34:31 +0000 2012,50.8229428584,-0.1390289018
Fri Aug 31 17:33:20 +0000 2012,50.8291396274,-0.133923449
Fri Aug 31 09:20:04 +0000 2012,50.8311581116,-0.1335176435
Tue Aug 28 20:44:32 +0000 2012,41.8844650304,-87.6257600109
Mon Aug 27 13:57:24 +0000 2012,41.8844650304,-87.6257600109
Sat Aug 25 18:45:51 +0000 2012,41.8851594291,-87.6232355833
Wed Aug 22 12:32:45 +0000 2012,50.824415,-0.134691
Tue Aug 21 11:39:46 +0000 2012,50.8262027605,-0.1381123066
Mon Aug 20 11:01:28 +0000 2012,51.535132,-0.069309
Fri Aug 17 12:03:40 +0000 2012,50.8262027605,-0.1381123066
Sat Aug 11 16:08:13 +0000 2012,50.826646,-0.136921
Fri Aug 10 14:25:15 +0000 2012,50.8262027605,-0.1381123066
Wed Aug 08 11:51:45 +0000 2012,50.8262027605,-0.1381123066
Tue Aug 07 15:45:49 +0000 2012,50.8262027605,-0.1381123066
Fri Aug 03 16:38:55 +0000 2012,50.8262027605,-0.1381123066
Fri Aug 03 14:33:04 +0000 2012,50.8262027605,-0.1381123066
Sat Jul 28 14:57:52 +0000 2012,50.825449,-0.137123
Sat Jul 28 12:09:01 +0000 2012,50.828404,-0.137435
Thu Jul 26 17:17:22 +0000 2012,50.8266230357,-0.1367429505
Tue Jul 24 15:07:39 +0000 2012,50.8262027605,-0.1381123066
Mon Jul 23 12:25:35 +0000 2012,50.823104,-0.139515
Sat Jul 21 12:46:25 +0000 2012,50.827943,-0.136033
Fri Jul 20 13:21:41 +0000 2012,50.8262027605,-0.1381123066
Mon Jul 16 19:28:01 +0000 2012,50.825449,-0.137123
Sun Jul 15 10:48:44 +0000 2012,51.4714930776,-0.4883337021
Sat Jul 14 23:08:27 +0000 2012,41.974037,-87.890239
Tue Jul 10 13:44:08 +0000 2012,30.2655234842,-97.7385378752
Mon Jul 09 19:32:48 +0000 2012,30.2655234842,-97.7385378752
Mon Jul 09 14:40:21 +0000 2012,30.2656095537,-97.7385592461
Sat Jul 07 15:08:12 +0000 2012,51.4726745412,-0.4817537462
Fri Jun 29 10:55:03 +0000 2012,50.8262027605,-0.1381123066
Wed Jun 20 10:23:29 +0000 2012,51.488197,-0.120692
Mon Jun 18 12:12:01 +0000 2012,50.8262027605,-0.1381123066
Mon Jun 18 12:02:43 +0000 2012,50.8262027605,-0.1381123066
Sat Jun 16 15:51:15 +0000 2012,50.8244773427,-0.1387893509
Sat Jun 16 15:10:29 +0000 2012,50.827972412,-0.136271402
Fri Jun 15 22:15:44 +0000 2012,50.947306,0.090209
Fri Jun 15 12:58:27 +0000 2012,50.947306,0.090209
Wed Jun 13 12:12:49 +0000 2012,50.822951,-0.136905
Mon Jun 11 14:05:50 +0000 2012,50.825977,-0.138339
Wed Jun 06 16:31:48 +0000 2012,51.50361668,-0.683839
Wed Jun 06 15:38:45 +0000 2012,51.50361668,-0.683839
Sat Jun 02 15:40:48 +0000 2012,50.825449,-0.137123
Fri Jun 01 13:29:40 +0000 2012,50.8262027605,-0.1381123066
Thu May 31 16:37:18 +0000 2012,50.8262027605,-0.1381123066
Wed May 30 14:58:46 +0000 2012,50.8262027605,-0.1381123066
Wed May 30 12:45:33 +0000 2012,50.8262027605,-0.1381123066
Wed May 30 12:32:27 +0000 2012,50.8262027605,-0.1381123066
Tue May 29 12:12:15 +0000 2012,50.8242644595,-0.1329624653
Tue May 29 08:12:24 +0000 2012,50.8307708894,-0.1330473622
Sun May 27 21:06:57 +0000 2012,47.5608179303,-52.70936785
Mon May 21 19:15:05 +0000 2012,50.824975,3.26387
Mon May 21 13:56:02 +0000 2012,51.0541040608,3.7238935404
Mon May 21 12:19:17 +0000 2012,51.055163,3.720835
Sat May 19 15:52:22 +0000 2012,50.821309,-0.1434404
Sat May 19 14:19:38 +0000 2012,50.822215,-0.154896
Sun May 13 14:08:33 +0000 2012,50.8244462443,-0.139321602
Sun May 13 13:29:30 +0000 2012,50.8192217888,-0.1411056519
Sat May 12 19:32:13 +0000 2012,50.820359,-0.14243
Sat May 12 17:51:57 +0000 2012,50.822623,-0.142676
Fri May 11 09:22:05 +0000 2012,52.366239,4.894655
Tue May 08 12:39:36 +0000 2012,50.8287188784,-0.1423922896
Sun May 06 20:38:27 +0000 2012,50.871762,0.011501
Fri May 04 14:35:37 +0000 2012,50.8262027605,-0.1381123066
Thu May 03 16:03:52 +0000 2012,50.8262027605,-0.1381123066
Thu May 03 12:05:08 +0000 2012,50.8242644595,-0.1329624653
Wed May 02 12:43:38 +0000 2012,50.8262027605,-0.1381123066
Tue May 01 14:50:47 +0000 2012,50.8244094849,-0.1399479955
Tue May 01 13:17:36 +0000 2012,50.8262027605,-0.1381123066
Tue May 01 12:01:59 +0000 2012,50.826779,-0.138462
Tue May 01 11:22:41 +0000 2012,50.8262027605,-0.1381123066
Mon Apr 30 15:58:14 +0000 2012,50.8262027605,-0.1381123066
Fri Apr 27 17:26:19 +0000 2012,50.825449,-0.137123
Thu Apr 26 12:44:54 +0000 2012,50.8262027605,-0.1381123066
Tue Apr 24 11:30:25 +0000 2012,50.8262027605,-0.1381123066
Sat Apr 21 14:37:59 +0000 2012,50.8244773427,-0.1387893509
Wed Apr 18 11:05:28 +0000 2012,51.514461,-0.15415
Tue Apr 17 11:38:39 +0000 2012,50.8262027605,-0.1381123066
Mon Apr 16 17:28:09 +0000 2012,50.825449,-0.137123
Fri Apr 13 17:35:30 +0000 2012,50.825449,-0.137123
Fri Apr 13 11:39:01 +0000 2012,50.8262027605,-0.1381123066
Thu Apr 12 20:59:46 +0000 2012,50.8284865994,-0.1406764984
Thu Apr 12 20:43:24 +0000 2012,50.8284865994,-0.1406764984
Thu Apr 12 12:38:06 +0000 2012,50.8262027605,-0.1381123066
Wed Apr 04 17:35:46 +0000 2012,50.829236,-0.130433
Wed Apr 04 11:20:06 +0000 2012,50.8262027605,-0.1381123066
Wed Mar 28 19:51:57 +0000 2012,50.82533,-0.1371919
Wed Mar 28 17:41:06 +0000 2012,50.8266230357,-0.1367429505
Sat Mar 24 15:24:22 +0000 2012,50.82578,-0.139591
Sat Mar 24 14:42:14 +0000 2012,50.8244773427,-0.1387893509
Thu Mar 22 20:33:36 +0000 2012,50.821049,-0.140416
Thu Mar 15 16:00:20 +0000 2012,32.8975517297,-97.0442533493
Wed Mar 14 15:41:13 +0000 2012,30.265426,-97.740498
Tue Mar 13 19:52:43 +0000 2012,30.2647199679,-97.7443528175
Tue Mar 13 16:29:12 +0000 2012,30.2653850259,-97.7383099888
Mon Mar 12 02:03:53 +0000 2012,30.2669212002,-97.745683415
Sun Mar 11 17:45:31 +0000 2012,30.2626071693,-97.739803791
Sun Mar 11 15:18:53 +0000 2012,30.2647199679,-97.7443528175
Fri Mar 09 15:11:51 +0000 2012,30.2671521557,-97.7396624407
Mon Mar 05 10:56:37 +0000 2012,50.8262027605,-0.1381123066
Thu Mar 01 09:55:16 +0000 2012,50.8304057758,-0.1329698575
Wed Feb 22 23:56:59 +0000 2012,-33.8782765912,151.221249511
Wed Feb 22 02:00:43 +0000 2012,-41.328228677,174.809947014
Thu Feb 16 01:13:27 +0000 2012,-41.2890508786,174.777774995
Wed Feb 15 21:39:06 +0000 2012,-41.2893031956,174.777374268
Wed Feb 15 18:50:42 +0000 2012,-41.2893031956,174.777374268
Wed Feb 15 02:10:18 +0000 2012,-41.29336192,174.776485
Mon Feb 13 04:07:07 +0000 2012,-41.2893031956,174.777374268
Mon Feb 13 03:36:49 +0000 2012,-41.2924914456,174.776140451
Mon Feb 13 03:00:13 +0000 2012,-41.293314,174.776395
Mon Feb 13 02:40:18 +0000 2012,-41.2934345895,174.775958061
Mon Feb 13 01:22:04 +0000 2012,-41.2939726591,174.775840044
Sat Feb 11 23:39:04 +0000 2012,-36.405247,174.65600431
Sat Feb 11 07:32:16 +0000 2012,-36.405247,174.65600431
Sat Feb 11 06:49:42 +0000 2012,-36.405247,174.65600431
Wed Feb 08 23:20:25 +0000 2012,-33.878302,151.221256
Sat Feb 04 11:14:52 +0000 2012,50.828205,-0.1378011703
Thu Feb 02 13:41:42 +0000 2012,50.8262027605,-0.1381123066
Wed Feb 01 16:57:16 +0000 2012,50.8262027605,-0.1381123066
Sat Jan 28 16:57:35 +0000 2012,50.827062,-0.135349
Sat Jan 28 15:55:49 +0000 2012,50.828295,-0.138769
Thu Jan 26 12:42:08 +0000 2012,50.8262027605,-0.1381123066
Mon Jan 23 12:34:45 +0000 2012,50.822219,-0.140802
Sun Jan 22 15:18:32 +0000 2012,50.825832,-0.142381
Sat Jan 21 14:27:51 +0000 2012,50.8213,-0.1409
Fri Jan 20 12:45:34 +0000 2012,51.9479484763,-0.5020558834
Thu Jan 19 20:49:09 +0000 2012,52.9556027724,-1.1504852772
Thu Jan 19 12:38:47 +0000 2012,52.954584773,-1.1563324928
Wed Jan 18 16:42:24 +0000 2012,52.954584773,-1.1563324928
Wed Jan 18 16:39:09 +0000 2012,52.954584773,-1.1563324928
Tue Jan 17 15:00:09 +0000 2012,50.8262027605,-0.1381123066
Mon Jan 16 10:03:12 +0000 2012,50.8303548561,-0.1329055827
Sat Jan 14 16:11:55 +0000 2012,50.824838842,-0.1516896486
Wed Jan 11 21:07:19 +0000 2012,51.522789913,-0.0784921646
Wed Jan 11 19:27:24 +0000 2012,51.5237223711,-0.0770612686
Sat Jan 07 14:49:09 +0000 2012,50.824424,-0.138875
...
Fri Apr 09 01:52:12 +0000 2010,47.4412234282,-122.3010026978
Fri Apr 09 00:00:15 +0000 2010,47.4432422071,-122.3010595342
Thu Apr 08 01:29:11 +0000 2010,47.6873506139,-122.3341637453
Wed Apr 07 00:16:03 +0000 2010,47.6109922102,-122.3480262842
Sun Apr 04 18:47:33 +0000 2010,47.7083958758,-122.3272574643
Sat Apr 03 18:06:54 +0000 2010,47.6687063559,-122.3942997359
Sat Apr 03 18:05:00 +0000 2010,47.6687063559,-122.3942997359
</pre>

I guess it's kind of scary that you can do this, and is perhaps why Twitter doesn't let you export anyone's account, even if it is public. But returning to the issue of Grailbird being on Github, I imagine there would be people that would write code that uses Grailbird as an API to the archive data, to provide extensions that would display a map of where you've been over time for example, or an analysis of your friendship network, or a view on hashtags you've used, events you've been at etc. 

I think from an archival perspective, it would be really useful to be able to receive something like a Tweet archive from a donor, and overlay functionality on top of it. The model of using the Web as a local application platform for this sort of archival content seems like it could be a growth area.





]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5364</wp:post_id>
		<wp:post_date>2012-12-31 12:42:48</wp:post_date>
		<wp:post_date_gmt>2012-12-31 19:42:48</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>archiving-tweets</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="css"><![CDATA[css]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_143fb7bc35e19f5dd68d7a4480196776</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p><a href="https://twitter.com/monkchips">@monkchips</a> Ouch. Some regrets are harsher than others.</p>&mdash; Sarah Bourne (@sarahebourne) <a href="https://twitter.com/sarahebourne/statuses/281405942321532929">December 19, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_0b7eefcf08f4580c32c2a81fa369b7bc</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="500"><p>@<a href="https://twitter.com/monkchips">monkchips</a> Ouch. Some regrets are harsher than others.</p>&mdash; Sarah Bourne (@sarahebourne) <a href="https://twitter.com/sarahebourne/status/281405942321532929">December 19, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_0c15f19d96745320d2683479f69ab25f</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>@<a href="https://twitter.com/monkchips">monkchips</a> Ouch. Some regrets are harsher than others.</p>&mdash; Sarah Bourne (@sarahebourne) <a href="https://twitter.com/sarahebourne/status/281405942321532929">December 19, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_c449002c56c6016313c129cec0ee1165</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>@<a href="https://twitter.com/monkchips">monkchips</a> Ouch. Some regrets are harsher than others.</p>&mdash; Sarah Bourne (@sarahebourne) <a href="https://twitter.com/sarahebourne/status/281405942321532929">December 19, 2012</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85523</wp:comment_id>
			<wp:comment_author><![CDATA[Wat te verwachten van het Twitter archief &laquo; Dee&#039;tjes]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://dymphie.com/2013/01/04/wat-te-verwachten-van-het-twitter-archief/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-01-04 00:05:00</wp:comment_date>
			<wp:comment_date_gmt>2013-01-04 07:05:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] heeft ze ook nog niet, maar vertelt over de structuur (Archiving tweets) : deels CSV, deels JSON, maar die tekst is nogal [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1357283101.080459117889404296875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1357309872.899282932281494140625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Darth Nader</title>
		<link>http://inkdroid.org/2013/01/02/darth-nader/</link>
		<pubDate>Wed, 02 Jan 2013 11:06:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5402</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This may be a bad/shortlived idea, but as part of a New Year's resolution to write more varied material I'm going to try to use my blog (partly) as a dream journal. This will probably drive the few readers I have away, but I'm hoping it might provide some amusement. I barely remember my dreams these days, and would like to remember more of them, so here goes. Feel free to file under TMI.</em>

Walking into a cafe/restaurant in the morning, in what feels like New York, but I'm not sure...it could be any city. It's a cosy, narrow setup, with all the seats taken by people quietly chatting. I manage to get a cup of coffee to go, and stand waiting for a table to open up. I discover a staircase and vaguely remember that there is seating upstairs. I go up the stairs carefully balancing my wide bowl-like cup of coffee.

The upstairs area is quite large and sprawling, dimly lit, with comfortable chairs, wider tables, and in the middle is a life sized sculpture of a woman in motion, looking behind, while walking--who apparently is the owner of the establishment. A hostess shows me to a table nearby, and says she can't remember the name of the server, but that someone would be with me shortly. I sit down with my coffee. 

After just a few minutes I notice that it feels like evening. There are lots of conversations going on nearby, which I'm able to hear fairly easily. One man in his early 30s is standing at his table, and in a kind of spotlight. He is talking quietly, as if on stage, not obviously on a cell phone, about a meeting that he has just had, and how they will need to travel to Austin, Texas to help protect some geographic area. I can't remember the exact details of what he was saying but it is clear he is working for an organization that is trying to save some ecosystem features in Austin. 

There is a bookshelf nearby with a disembodied head on it, which looks like Ralph Nader, and also a bit like Darth Vader when Luke takes his helmet off at the end of Return of the Jedi. The head is animated, and seems to be simulating the other half of the conversation. He is saying that this is important work, and is similar to a recent project in Seattle. The conversation ends, and the man walks out of the coffee shop. 

I notice three other people, with big thick, Ginsbergian beards also leave their tables at the same time, deep in conversation, about something different. There is a counter-culture, occupy-like feeling in the air, of people steadily working to make there corner of the world a better place, it's a good feeling.

<h2>Afterword</h2>

Half awake I found myself thinking about the talking head, and how it reminded me of <a href="http://librarybox.us/">LibraryBox</a>. It was as if the head made it possible to easily tune into public conversations that were going on in the local context of the coffee shop...and it served as an archive or store of these conversations for others to discover later. I don't know if LibraryBox actually lets any of that happen, but it's something I've been meaning to learn more about in the new year.

<em>By the way, dream interpretations as comments are most welcome...</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5402</wp:post_id>
		<wp:post_date>2013-01-02 04:06:41</wp:post_date>
		<wp:post_date_gmt>2013-01-02 11:06:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>darth-nader</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="coffee"><![CDATA[coffee]]></category>
		<category domain="post_tag" nicename="darth-vader"><![CDATA[darth vader]]></category>
		<category domain="category" nicename="dreams"><![CDATA[dreams]]></category>
		<category domain="post_tag" nicename="librarybox"><![CDATA[librarybox]]></category>
		<category domain="post_tag" nicename="occupy"><![CDATA[occupy]]></category>
		<category domain="post_tag" nicename="ralph-nader"><![CDATA[ralph nader]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>spotify vs rdio 2012</title>
		<link>http://inkdroid.org/2013/01/02/spotify-vs-rdio-2012/</link>
		<pubDate>Wed, 02 Jan 2013 15:33:31 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5427</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://inkdroid.org/2011/08/06/spotify-rdio-and-albums-of-the-year/">Back</a> in August of 2011 I wrote a little <a href="https://github.com/edsu/aotycmp/">utility</a> that pulled down Alf Eaton's <a href="http://apps.hubmed.org/aoty/">Album of the Year</a> data. AOTY is nice for two reasons: a) I like Alf's taste in music, so the lists are relevant to me; and b) AOTY is a nice example of layering structured metadata into HTML, for easy processing (aka scraping). With the data in hand it was easy to to check to see if the albums were available on the streaming services <a href="http://www.spotify.com/">Spotify</a> and <a href="http://rdio.com">Rdio</a> using their respective APIs. I was trying to decide which one to use at the time, and wanted to know if there was any significant difference in their catalogs. 

Back then, it looked like 32% of the albums were available on Spotify, and 46% on Rdio. Alf has updated his list for <a href="http://apps.hubmed.org/aoty/2012">2012</a> so I decided to rerun aotycmp, and it appears that coverage of both has improved, with Spotify (41%) closing the gap a bit closer with Rdio (49%) which still has a comfortable lead. If you want the availability data I've updated it on <a href="https://raw.github.com/edsu/aotycmp/master/aoty_cmp.csv">Github</a>.

I've been very happy with Rdio, although pieces like <a href="http://pitchfork.com/features/articles/8993-the-cloud/">Damon Krukowski's</a> (thanks <a href="http://twitter.com/dchud">@dchud</a>) make me wish there was a better way to a) stream music while b) actually putting money in the artists pockets. I'd love to have the ability to pay a little bit more if I knew it was going to the help support the artist in creating more of their art.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5427</wp:post_id>
		<wp:post_date>2013-01-02 08:33:31</wp:post_date>
		<wp:post_date_gmt>2013-01-02 15:33:31</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>spotify-vs-rdio-2012</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Fielding notes</title>
		<link>http://inkdroid.org/2013/01/05/fielding-notes/</link>
		<pubDate>Sun, 06 Jan 2013 04:16:51 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5441</guid>
		<description></description>
		<content:encoded><![CDATA[<div style="float: left; font-size: 8pt; text-align: center; margin-right: 10px;"><a href="http://www.flickr.com/photos/psd/8271699529/"><img src="http://inkdroid.org/images/timbl_roy.jpg"/><br />a tongue-in-cheek change request from @timberners_lee<br />Paul Downey</a></div>

I've been doing a bit of research into the design of the Web for a paper I'm trying to write. In my travels I ran across Jon Udell's <a href="http://jonudell.net/udell/2006-08-25-a-conversation-with-roy-fielding-about-http-rest-webdav-jsr-170-and-waka.html">2006 interview</a> with Roy Fielding. The interview is particularly interesting because of Roy's telling of how (as a graduate student) he found himself working on <a href="http://en.wikipedia.org/wiki/Library_for_WWW_in_Perl">libwww-perl</a> which helped him discover the architecture of the Web that was largely documented by Tim Berners-Lee's <a href="http://en.wikipedia.org/wiki/Libwww">libwww</a> HTTP library for Objective-C. 

For the purposes of note taking, and giving some web spiders some text to index, here are a few moments that stood out:

<blockquote>
<strong>Udell</strong>: A little later on [in Roy's dissertation] you talk about how systems based on what you call control messages are in a very different category from systems where the decisions that get made are being made by human beings, and that that's, in a sense, the ultimate rationale for designing data driven systems that are web-like, because people need to interact with them in lots of ways that you can't declaratively define. 

<strong>Fielding</strong>: Yeah, it's a little bit easier to say that people need to reuse them, in various unanticipated ways. A lot of people think that when they are building an application that they are building something that's going to last forever, and almost always that's false. Usually when they are building an application the only thing that lasts forever is the data, at least if you're lucky. If you're lucky the data retains some semblance of archivability, or reusability over time. 

...

<strong>Udell</strong>: There is a meme out there to the effect that what we now call REST architectural style was in a sense discovered post facto, as opposed to having been anticipated from the beginning. Do you agree with that or not?

<strong>Fielding</strong>: No, it's a little bit of everything, in the sense that there are core principles involved that Berners-Lee was aware of when he was working on it. I first talked to Tim about what I was calling the HTTP Object Model at the time, which is a terrible name for it, but we talked when I was at the W3C in the summer of 95, about the software engineering principles. Being a graduate student of software engineering, that was my focus, and my interest originally. Of course all the stuff I was doing for the Web that was just for fun. At the time that was not considered research. 

<strong>Udell</strong>: But did you at the time think of what you then called the HTTP object model as being in contrast to more API like and procedural approaches?

<strong>Fielding</strong>: Oh definitely. The reason for that was that the first thing I did for the Web was statistical analysis software, which turned out to be very effective at helping people understand the value of communicating over the Web. The second thing was a program called MOMSpider. It was one of the first Web spiders, a mechanism for testing all the links that were on the Web.

<strong>Udell</strong>: And that was when you also worked on libwww-perl?

<strong>Fielding</strong>: Right, and ... at the time it was only the second protocol library available for the Web. It was a combination of pieces from various sources, as well as a lot of my own work, in terms of filling out the details, and providing an overall view of what a Web client should do with an HTTP library. And as a result of that design process I realized some of the things Tim Berners-Lee had designed into the system. And I also found a whole bunch of cases where the design didn't make any sense, or the way it had been particularly implemented over at NCSA, or one of the other clients, or various history of the Web had turned out to be not-fitting with the rest of the design. So that led to a lot of discussions with the other early protocol developers particularly people like Rob McCool, Tony Sanders and Ari Luotonen--people who were building their own systems and understood both what they were doing with the Web, and also what complaints they were getting from their users. And from that I distilled a model of basically what was the core of HTTP. Because if you look back in the 93/94 time frame, the HTTP specification did not look all that similar to what it does now. It had a whole range of methods that were never used, and a lot of talk about various aspects of object orientation which never really applied to HTTP. And all of that came out of Tim's original implementation of libwww, which was an Objective-C implementation that was trying to be as portable as possible. It had a lot of the good principles of interface separation and genericity inside the library, and really the same principles that I ended up using in the Perl library, although they were completely independently developed. It was just one of those things where that kind of interaction has a way of leading to a more extensible design.

<strong>Udell</strong>: So was focusing down on a smaller set of verbs partly driven by the experience of having people starting to use the Web, and starting to experience what URLs could be in a human context as well as in a programmatic context?

<strong>Fielding</strong>: Well, that was really a combination of things. One that's a fairly common paradigm: if you are trying to inter-operate with people you've never met, try to keep it as simple as possible. There's also just inherent in the notion of using URIs to identify everything, which is of course really the basis of what the Web is, provides you with that frame of mind where you have a common resource, and you want to have a common resource interface.
</blockquote>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5441</wp:post_id>
		<wp:post_date>2013-01-05 21:16:51</wp:post_date>
		<wp:post_date_gmt>2013-01-06 04:16:51</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>fielding-notes</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="architecture"><![CDATA[architecture]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="interviews"><![CDATA[interviews]]></category>
		<category domain="post_tag" nicename="jon-udell"><![CDATA[jon udell]]></category>
		<category domain="post_tag" nicename="rest"><![CDATA[rest]]></category>
		<category domain="post_tag" nicename="roy-fielding"><![CDATA[roy fielding]]></category>
		<category domain="post_tag" nicename="uri"><![CDATA[uri]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>aaronsw</title>
		<link>http://inkdroid.org/2013/01/19/aaronsw/</link>
		<pubDate>Sat, 19 Jan 2013 22:07:30 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5460</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Aaron Swartz left us all a week ago. It's strange, I only met Aaron once at the Internet Archive, and had a handful of conversations with him via email/irc ... but not a day has passed since last Saturday that I haven't thought about him, and his principled life.</p>

<p>I've been asked a few times why Aaron has been on my mind so much, and I've struggled to put it into words. Meanwhile, so many thoughtful things have been written about him. The arc of his life, his ideals, and abilities, charisma, and chutzpah, seem larger than life. And yet, he was just a person, a son, a friend, with people who loved him. It's just heartbreaking.</p>

<hr />

<p>I work as a software developer in libraryland, trying to bridge the world of information we've had with the world we are building on the Web. So for me, Aaron was a role model, a teacher whose lessons weren't in textbooks or scholarly journals, but in his blog, in his code, in his talks, in his experiments with real world results. He was only 26 when he died, but he was, and remains, as Tim Berners-Lee paradoxically <a href="http://lists.w3.org/Archives/Public/www-tag/2013Jan/0017.html">called him</a>, a "wise elder".</p>

<p>I wanted to write something here, but more than that I wanted to do something.</p>

<hr />

<p>I noticed that <a href="http://archive.org/">Internet Archive</a> created a <a href="http://archive.org/details/aaronsw">collection</a> devoted to online material related to Aaron, and thought I would try to collect together all the Twitter conversations that mention him. Twitter's search is limited to the last week, so I quickly wrote a <a href="http://github.com/edsu/twarc">command line utility</a> that pages through search results using their <a href="https://dev.twitter.com/docs/api/1.1/get/search/tweets">API</a>, and writes out the complete data as line-oriented JSON. I also pulled in the tweets that mention #pdftribute since they were largely inspired by Aaron's efforts in the open access space. I packaged up the data using <a href="http://en.wikipedia.org/wiki/BagIt">BagIt</a> and <a href="http://archive.org/details/AaronswRelatedTweets">put it up</a> at Internet Archive. Here's the description from the bag-info.txt</p>

<blockquote>
  On January 11, 2013 the Internet activist Aaron Swartz took his own life, and a great deal of grief, anger, and constructive thinking erupted on the Web and in Twitter. In particular the #pdftribute Twitter tag was born, in an attempt to raise awareness about Open Access issues, that Aaron did so much to futher during his life.

This package contains Twitter JSON data for two Twitter search queries that were collected in the week following Aaron's death:

<ul>
<li>"Aaron Swartz" OR aaronsw</li>
<li>#pdftribute</li>
</ul>

aaronsw.json.gz contains 630,397 tweets, for the period starting with 2013-01-11 16:50:22 and ending 2013-01-18 13:50:02.

pdftribute.json.gz contains 42,277 tweets, for the period starting with Jan 13 02:42:26 and ending Jan 17 03:33:46.

In addition the URLs mentioned in the tweets found in aaronsw.tar.gz were extracted, unshortened, and then aggregated to provide a report of what people linked to. These URLs are available in aaronsw-urls.txt.gz.

  It is hoped that this data will help document the Web community's response to Aaron's death, and life.
</blockquote>

<p>Below is a list of the top 50 links shared in tweets about Aaron. There were 36,506 in all.</p>

<p><style>
.aligned-table td + td {text-align: right;}
</style></p>

<table class="aligned-table">
<tr>
<th>Page</th>
<th>Shares</th>
</tr>
<tr><td><a href="http://boingboing.net/2013/01/12/rip-aaron-swartz.html">RIP, Aaron Swartz - Boing Boing</a></td><td>11763</td></tr>
<tr><td><a href="http://unhandled.com/2013/01/12/the-truth-about-aaron-swartzs-crime/">The Truth about Aaron Swartz’s “Crime” « Unhandled Exception</a></td><td>6641</td></tr>
<tr><td><a href="http://tech.mit.edu/V132/N61/swartz.html"> Aaron Swartz commits suicide - The Tech</a></td><td>5539</td></tr>
<tr><td><a href="https://petitions.whitehouse.gov/petition/remove-united-states-district-attorney-carmen-ortiz-office-overreach-case-aaron-swartz/RQNrG1Ck">Remove United States District Attorney Carmen Ortiz from office for overreach in the case of Aaron Swartz.</a></td><td>6478</td></tr>
<tr><td><a href="http://lessig.tumblr.com/post/40347463044/prosecutor-as-bully">Prosecutor as bully - Lessig Blog</a></td><td>3738</td></tr>
<tr><td><a href="http://www.guardian.co.uk/commentisfree/2013/jan/12/aaron-swartz-heroism-suicide1"> The inspiring heroism of Aaron Swartz | Glenn Greenwald | Comment is free | guardian.co.uk </a></td><td>2522</td></tr>
<tr><td><a href="http://thinkprogress.org/justice/2013/01/14/1441211/killers-slavers-and-bank-robbers-all-face-less-severe-prison-terms-than-aaron-swartz-did/?mobile=nc">Aaron Swartz Faced A More Severe Prison Term Than Killers, Slave Dealers And Bank Robbers | ThinkProgress</a></td><td>2367</td></tr>
<tr><td><a href="https://www.eff.org/deeplinks/2013/01/farewell-aaron-swartz">Farewell to Aaron Swartz, an Extraordinary Hacker and Activist - EFF</a></td><td>2042</td></tr>
<tr><td><a href="http://www.nytimes.com/2013/01/13/technology/aaron-swartz-internet-activist-dies-at-26.html">Internet Activist, a Creator of RSS, Is Dead at 26, Apparently a Suicide - New York Times</a></td><td>1927</td></tr>
<tr><td><a href="http://alt1040.com/2013/01/aaron-swartz"> Aaron Swartz muere por suicidio a sus 26 años</a></td><td>1572</td></tr>
<tr><td><a href="http://mashable.com/2013/01/13/aaron-swartz/">Technology's Greatest Minds Say Goodbye to Aaron Swartz</a></td><td>1558</td></tr>
<tr><td><a href="http://alt1040.com/2013/01/aaron-swartz-contribuciones-a-la-red"> Aaron Swartz a través de 5 grandes contribuciones a la red</a></td><td>1495</td></tr>
<tr><td><a href="http://www.washingtonpost.com/blogs/wonkblog/wp/2013/01/12/aaron-swartz-american-hero/">Aaron Swartz, American hero</a></td><td>1397</td></tr>
<tr><td><a href="http://mashable.com/2013/01/12/aaron-swartz-suicide/">Internet Activist Aaron Swartz Commits Suicide</a></td><td>1330</td></tr>
<tr><td><a href="http://news.cnet.com/8301-1023_3-57563752-93/anonymous-hacks-mit-after-aaron-swartzs-suicide/">Anonymous hacks MIT after Aaron Swartz's suicide | Internet & Media - CNET News</a></td><td>1327</td></tr>
<tr><td><a href="http://www.zephoria.org/thoughts/archives/2013/01/13/aaron-swartz.html">danah boyd | apophenia  » processing the loss of Aaron Swartz</a></td><td>1280</td></tr>
<tr><td><a href="http://rememberaaronsw.tumblr.com/post/40372208044/official-statement-from-the-family-and-partner-of-aaron">Official Statement from the family and partner of Aaron Swartz - Remember Aaron Swartz</a></td><td>1199</td></tr>
<tr><td><a href="http://wilwheaton.net/2012/09/depression-lies/">depression lies | WIL WHEATON dot NET: 2.0</a></td><td>1164</td></tr>
<tr><td><a href="http://www.bbc.co.uk/news/world-us-canada-21001452">BBC News - Aaron Swartz, internet freedom activist, dies aged 26</a></td><td>1143</td></tr>
<tr><td><a href="https://www.eff.org/deeplinks/2013/01/aaron-swartz-fix-draconian-computer-crime-law">In the Wake of Aaron Swartz's Death, Let's Fix Draconian Computer Crime Law - EFF</a></td><td>1088</td></tr>
<tr><td><a href="http://www.huffingtonpost.com/2013/01/15/westboro-baptist-church-aaron-swartz-anonymous_n_2479019.html">Westboro Baptist Church Drops Aaron Swartz Funeral Protest After Anonymous Vows Action (VIDEO)</a></td><td>1079</td></tr>
<tr><td><a href="http://soupsoup.tumblr.com/post/40373383323/official-statement-from-the-family-and-partner-of">Soup • Official Statement from the Family and Partner of...</a></td><td>1067</td></tr>
<tr><td><a href="http://rt.com/usa/news/aaron-swartz-funeral-chicago-059/">'Aaron was killed by the government' - Robert Swartz on his son's death  — RT</a></td><td>1066</td></tr>
<tr><td><a href="http://pdftribute.net/">#PDFTribute list of documents</a></td><td>1044</td></tr>
<tr><td><a href="http://www.cnn.com/2013/01/12/us/new-york-reddit-founder-suicide/index.html">Internet prodigy, activist Aaron Swartz commits suicide - CNN.com</a></td><td>1009</td></tr>
<tr><td><a href="http://www.thenation.com/blog/172187/aaron-swartz">Remembering Aaron Swartz | The Nation</a></td><td>1003</td></tr>
<tr><td><a href="http://www.aaronsw.com/2002/continuity">If I get hit by a truck...</a></td><td>991</td></tr>
<tr><td><a href="http://www.lemonde.fr/technologies/article/2013/01/12/suicide-d-aaron-swartz-activiste-a-l-origine-du-format-rss-et-de-reddit_1816246_651865.html">Suicide d'Aaron Swartz, activiste à l'origine du format RSS et de Creative Commons</a></td><td>938</td></tr>
<tr><td><a href="http://www.zdnet.com/hacker-activist-aaron-swartz-commits-suicide-7000009725/">Hacker, Activist Aaron Swartz Commits Suicide | ZDNet</a></td><td>896</td></tr>
<tr><td><a href="http://www.forbiddenknowledgetv.com/videos/activism/how-we-stopped-sopa-by-aaron-swartz1986-2013.html">Activism "How We Stopped SOPA" by Aaron Swartz (1986-2013)</a></td><td>896</td></tr>
<tr><td><a href="http://tecnologia.elpais.com/tecnologia/2013/01/13/actualidad/1358037094_942870.html">Muere a los 26 años el ciberactivista Aaron Swartz | Tecnología | EL PAÍS</a></td><td>887</td></tr>
<tr><td><a href="http://www.alternet.org/10-awful-crimes-get-you-less-prison-time-what-aaron-swartz-faced">10 Awful Crimes That Get You Less Prison Time Than What Aaron Swartz Faced | Alternet</a></td><td>868</td></tr>
<tr><td><a href="http://www.wired.com/threatlevel/2013/01/aaron-swartz/">Aaron Swartz, Coder and Activist, Dead at 26 | Threat Level | Wired.com</a></td><td>856</td></tr>
<tr><td><a href="http://www.newyorker.com/online/blogs/newsdesk/2013/01/everyone-interesting-is-a-felon.html?mbid=social_retweet">How the Legal System Failed Aaron Swartz--and Us : The New Yorker</a></td><td>849</td></tr>
<tr><td><a href="https://aaronsw.jottit.com/howtoget">https://aaronsw.jottit.com/howtoget</a></td><td>811</td></tr>
<tr><td><a href="http://www.theatlanticwire.com/national/2013/01/anonymous-westboro-baptist-church-aaron-swartz-funeral/61036/">How Anonymous Got Westboro to Back Off Aaron Swartz's Funeral - National - The Atlantic Wire</a></td><td>804</td></tr>
<tr><td><a href="http://alt1040.com/2013/01/aaron-swartz-open-data-investigacion"> Muerte de Aaron Swartz: la necesidad del Open Data en el I+D</a></td><td>779</td></tr>
<tr><td><a href="http://rt.com/usa/news/swartz-suicide-court-drops-charges-997/">US court drops charges on Aaron Swartz days after his suicide — RT</a></td><td>772</td></tr>
<tr><td><a href="http://neuroconscience.com/2013/01/13/researchers-begin-posting-article-pdfs-to-twitter-in-pdftribute-to-aaron-swartz/">Researchers begin posting article PDFs to twitter in #pdftribute to Aaron Swartz « Neuroconscience</a></td><td>745</td></tr>
<tr><td><a href="http://www.quinnnorton.com/said/?p=644">  My Aaron Swartz, whom I loved.   | Quinn Said</a></td><td>742</td></tr>
<tr><td><a href="http://www.guardian.co.uk/commentisfree/2013/jan/12/aaron-swartz-heroism-suicide1?CMP=twt_gu"> The inspiring heroism of Aaron Swartz | Glenn Greenwald | Comment is free | guardian.co.uk </a></td><td>713</td></tr>
<tr><td><a href="http://arstechnica.com/tech-policy/2013/01/government-formally-drops-charges-against-aaron-swartz/">Government formally drops charges against Aaron Swartz | Ars Technica</a></td><td>708</td></tr>
<tr><td><a href="http://www.nakedcapitalism.com/2013/01/aaron-swartzs-politics.html">Aaron Swartz’s Politics «  naked capitalism</a></td><td>704</td></tr>
<tr><td><a href="http://www.cnn.com/">CNN.com - Breaking News, U.S., World, Weather, Entertainment & Video News</a></td><td>690</td></tr>
<tr><td><a href="http://mashable.com/2013/01/15/aaron-swartz-tech-world-depression/">After Aaron Swartz: The Tech World Must Talk About Depression</a></td><td>670</td></tr>
<tr><td><a href="http://web.archive.org/web/20130410145744/http://aaronsw.archiveteam.org/">JSTOR liberator</a></td><td>663</td></tr>
<tr><td><a href="http://mashable.com/2013/01/12/aaron-swartz-suicide/">Internet Activist Aaron Swartz Commits Suicide</a></td><td>661</td></tr>
<tr><td><a href="http://alt1040.com/2013/01/anonymous-mit-doj-tributo-aaron-swartz"> Anonymous tumba las webs del MIT y DOJ como tributo a Aaron Swartz</a></td><td>652</td></tr>
<tr><td><a href="http://mashable.com/2013/01/14/anonymous-hacks-mit/">Anonymous Hacks MIT, Leaves Farewell Message for Aaron Swartz</a></td><td>647</td></tr>
</table>

<p>There were 209,839 Twitter users that mentioned Aaron on Twitter in the last week. I was one of them. I wish I could've done more to help.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5460</wp:post_id>
		<wp:post_date>2013-01-19 15:07:30</wp:post_date>
		<wp:post_date_gmt>2013-01-19 22:07:30</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>aaronsw</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="aaronsw"><![CDATA[aaronsw]]></category>
		<category domain="post_tag" nicename="internet-archive"><![CDATA[internet archive]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85671</wp:comment_id>
			<wp:comment_author><![CDATA[Aaron Swartz and Too-Comfortable Research Libraries &#8211; Library Hat]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.bohyunkim.net/blog/archives/2421</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-02-21 16:53:36</wp:comment_date>
			<wp:comment_date_gmt>2013-02-21 23:53:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Summers, Ed. 2013. “Aaronsw | Inkdroid.” Accessed February 21. http://inkdroid.org/2013/01/19/aaronsw/. [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1361522100.312448024749755859375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title> genealogy of a braeburn</title>
		<link>http://inkdroid.org/2013/01/30/genealogy-of-a-braeburn/</link>
		<pubDate>Wed, 30 Jan 2013 15:59:26 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5507</guid>
		<description></description>
		<content:encoded><![CDATA[It has been observed that when systems break down we get to actually see how they operate. I wonder what this breakage below says about the <a href="http://www.guardian.co.uk/technology/2013/jan/19/google-search-knowledge-graph-singhal-interview">use</a> of Freebase and Wikipedia data in Google's <a href="http://www.google.com/insidesearch/features/search/knowledge.html">Knowlege Graph</a>.

<a href="https://www.google.com/#q=braeburn&fp=82bae2c3ce10781c"><img src="http://inkdroid.org/images/braeburn-kg.png"/></a>

Yes, that's an image of <a href="http://mlp.wikia.com/wiki/Braeburn">Braeburn</a> from My Little Pony to the right, and text about the apple to the left. Interestingly it's fine at Wikipedia:

<a href="http://en.wikipedia.org/wiki/Braeburn"><img src="http://inkdroid.org/images/braeburn-wp.png"/></a>

And it's not even there in Freebase (according to a search).

<a href="http://www.freebase.com/search?limit=30&start=0&query=braeburn"><img src="http://inkdroid.org/images/braeburn-fb.png"/></a>

I don't know if this reveals what's going on in the flow of entities between Wikipedia, Freebase and Google. But I thought it was interesting. I wonder where to report such an anomaly. Is there a place?

Thanks to <a href="https://plus.google.com/107581973435023382062/posts">Jeff Godin</a> in <a href="irc:irc.feenode.net/code4lib">#code4lib</a> for noticing the breakage in Knowledge Graph.

See also Hilary Mason's <a href="http://www.hilarymason.com/blog/im-a-dead-celebrity/">post</a> about how her identity got mixed up on Bing. (Thanks <a href="http://improbable.org/chris">Chris</a>).

Update: 2012-02-04

I thought to check a week later, and the The Knowledge Graph results got even funnier, now it's a collage of apples and My Little Pony:

<img src="http://inkdroid.org/images/braeburn-kg2.png"/>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5507</wp:post_id>
		<wp:post_date>2013-01-30 08:59:26</wp:post_date>
		<wp:post_date_gmt>2013-01-30 15:59:26</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>genealogy-of-a-braeburn</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="apples"><![CDATA[apples]]></category>
		<category domain="category" nicename="data"><![CDATA[data]]></category>
		<category domain="post_tag" nicename="freebase"><![CDATA[freebase]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="knowledge-graph"><![CDATA[knowledge graph]]></category>
		<category domain="post_tag" nicename="my-little-pony"><![CDATA[my little pony]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:85886;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[geneaology-of-a-braeburn]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85886</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>tfmorris@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawld-pz_ibfBuIZ2hkuRWFiHcZh9-2OSOCg</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-05-06 12:39:23</wp:comment_date>
			<wp:comment_date_gmt>2013-05-06 19:39:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[It's ancient history now, but the empty Freebase search results most of have been due to an error during the search because Freebase knows about many different Braeburns and has for some time.

As for reporting problems like this, if you click the Feedback/More Info link under the bottom right corner of the card, it'll offer to let you mark various facts (including the image) as being wrong.  You can see it in your screen caps.

@tfmorris]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>353</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1367869168.924704074859619140625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawld-pz_ibfBuIZ2hkuRWFi";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>brief note on Ernst</title>
		<link>http://inkdroid.org/2013/02/01/brief-note-on-ernst/</link>
		<pubDate>Fri, 01 Feb 2013 13:33:38 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5521</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
Although the traditional archive used to be a rather static memory, the notion of the archive in Internet communication tends to move the archive toward an economy of circulation: permanent tranformations and updating. The so-called cyberspace is not primarily about memory as cultural record but rather about a performantive form of memory as communication. Within this economy of permanent recycling of information, there is less need for emphatic but short-term, updatable memory, which comes close to the operative storage management in the von Neumann architecture of computing. <strong>Repositories are no longer final destinations but turn into frequently accessed sites.</strong> Archives become cybernetic systems. The aesthetics of fixed order is being replaced by permanent reconfigurability.

<a href="http://de.wikipedia.org/wiki/Wolfgang_Ernst_(Medienwissenschaftler)">Wolfgang Ernst</a>. "Archives in Transition." <a href="http://www.upress.umn.edu/book-division/books/digital-memory-and-the-archive">Digital Memory and the Archive</a>.  
</blockquote>

I was reading this and remembering Kevin Kelly's idea of <a href="http://www.kk.org/thetechnium/archives/2008/12/movage.php">movage</a>, and  the idea of <a href="http://www.ijdc.net/index.php/ijdc/article/view/102">relay supporting archives</a> from Janée et al. I really like the way Ernst works this idea into the way the Internet works, and the ways that the Web transforms the archival function. I'm only half way through the book, and will likely have more to say when I do, so just taking some notes for myself, carry on...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5521</wp:post_id>
		<wp:post_date>2013-02-01 06:33:38</wp:post_date>
		<wp:post_date_gmt>2013-02-01 13:33:38</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>brief-note-on-ernst</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="internet"><![CDATA[internet]]></category>
		<category domain="post_tag" nicename="storage"><![CDATA[storage]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wolfgang-ernst"><![CDATA[Wolfgang Ernst]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>emoji dick and mo tweets</title>
		<link>http://inkdroid.org/2013/02/25/emoji-dick-and-mo-tweets/</link>
		<pubDate>Mon, 25 Feb 2013 19:39:44 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5536</guid>
		<description></description>
		<content:encoded><![CDATA[The <a href="http://blogs.loc.gov/loc/2013/02/a-whale-of-an-acquisition/">news</a> about <a href="http://lccn.loc.gov/2012454709">Emoji Dick</a> (the version of Moby Dick translated into Emoji) being acquired by the Library of Congress prompted me to capriciously go to Twitter Search to see <a href="https://twitter.com/search?q=emoji%20dick">who was talking about it</a>. As I drilled backwards I was surprised to see the search results went back to Fred Benenson's original Tweet about the project.

https://twitter.com/fredbenenson/status/1195751643

<em>That Tweet is from 4 years ago!</em>

Up until <a href="http://blog.twitter.com/2013/02/now-showing-older-tweets-in-search.html">recently</a> you could only search back a couple of weeks, tops. The only sad thing is that the <a href="https://dev.twitter.com/docs/api/1.1/get/search/tweets">Twitter Search API</a> still seems to have the two week window. I used my little <a href="http://github.com/edsu/twarc">twarc</a> utility to drill back in the search results via the API and the earliest it was able to find for the same query was from 2013-02-18.

Hopefully the search window for the API will be opened up at some point, since it is at least theoretically possible now. If you happen to know any of the details about how the search functionality works I would be most grateful to hear from you.

Oh, and of course, I had to request Emoji Dick from the stacks:

<pre>
PLEASE DO NOT REPLY TO THIS MESSAGE.
 
STATUS: Your request has been received.
REQUEST ID: 243106235
SEND TO: Adams Charge Station (LA 5244) - Staff
REQUEST RECEIVED: Mon Feb 25 12:56:19 EST 2013
TITLE: Emoji Dick ; or The Whale / by Herman Melville ; Edited and Compiled by Fred Benenson ; Translation by Amazon Mechanical Turk. 
AUTHOR: Melville, Herman, 1819-1891. 
CALL#: PS2384 .M6 2012
</pre>

The one-time-cataloger in me thinks that there was a missed opportunity to add a <a href="http://en.wikipedia.org/wiki/Uniform_title">uniform title</a> to the <a href="http://lccn.loc.gov/2012454709">LC catalog record</a>.... But the title statement of responsibility mentioning that it is a translation made by Amazon Turk more than makes up for that!

<em>Thanks <a href="http://twitter.com/lbjay">Jay</a> for letting me know what is going on at my own place of work.</em>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5536</wp:post_id>
		<wp:post_date>2013-02-25 12:39:44</wp:post_date>
		<wp:post_date_gmt>2013-02-25 19:39:44</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>emoji-dick-and-mo-tweets</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="emoji"><![CDATA[emoji]]></category>
		<category domain="post_tag" nicename="herman-melville"><![CDATA[herman melville]]></category>
		<category domain="post_tag" nicename="humor"><![CDATA[humor]]></category>
		<category domain="post_tag" nicename="library-of-congress"><![CDATA[library of congress]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_297b86d927bdd81d7a92b412153d736c</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>I am paying 50 cents a sentence to convert  from Herman Melville&#39;s Moby Dick into Emoji on Amazon&#39;s Mechanical Turk: http://ping.fm/1cVXy</p>&mdash; Fred Benenson (@fredbenenson) <a href="https://twitter.com/fredbenenson/statuses/1195751643">February 10, 2009</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_15849154eb0a1c64c4c90393f0394bd3</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>I am paying 50 cents a sentence to convertfrom Herman Melville's Moby Dick into Emoji on Amazon's Mechanical Turk: <a href="http://ping.fm/1cVXy">http://ping.fm/1cVXy</a></p>&mdash; Fred Benenson (@fredbenenson) <a href="https://twitter.com/fredbenenson/status/1195751643">February 10, 2009</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_f3de5eb81f5549d90455469ef0a55206</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="500"><p>I am paying 50 cents a sentence to convertfrom Herman Melville's Moby Dick into Emoji on Amazon's Mechanical Turk: <a href="http://ping.fm/1cVXy">http://ping.fm/1cVXy</a></p>&mdash; Fred Benenson (@fredbenenson) <a href="https://twitter.com/fredbenenson/status/1195751643">February 10, 2009</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_7cc32668b655f5a60f188f1e3af85554</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>I am paying 50 cents a sentence to convertfrom Herman Melville's Moby Dick into Emoji on Amazon's Mechanical Turk: <a href="http://ping.fm/1cVXy">http://ping.fm/1cVXy</a></p>&mdash; Fred Benenson (@fredbenenson) <a href="https://twitter.com/fredbenenson/status/1195751643">February 10, 2009</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>python heal thyself</title>
		<link>http://inkdroid.org/2013/03/22/python-heal-thyself/</link>
		<pubDate>Fri, 22 Mar 2013 13:26:40 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5552</guid>
		<description></description>
		<content:encoded><![CDATA[https://twitter.com/ginatrapani/status/314552254592069632

After seeing Gina's tweet, I was curious to see if there was any difference by gender in the tweets directed at <a href="http://twitter.com/adriarichards">@adriarichards</a> over the recent <a href="https://news.ycombinator.com/item?id=5391667">controversy</a> at PyCon. I wasn't confident I would find anything. It was more a feeble attempt to try to make Python make sense of something senseless that happened at PyCon; or to paraphrase Physician, heal thyself...for Python to heal itself.

I used <a href="http://github.com/edsu/twarc">twarc</a> to collect 13,472 tweets that mentioned @adriarichards from the search API. I then added a <a href="https://github.com/edsu/twarc/blob/master/utils/gender.py">utility filter</a> that uses <a href="https://github.com/bmuller/genderator">genderator</a> to filter the line oriented JSON based on a guess at the gender (Twitter doesn't track it). genderator identified 2,433 (18%) tweets from women, 5,268 (39%) from men, and 5,771 (42%) that were of unknown gender. I then added another <a href="https://github.com/edsu/twarc/blob/master/utils/wordcloud.py">utility</a> that reads a stream of Tweets and generates a tag cloud as a standalone HTML file using <a href="https://github.com/jasondavies/d3-cloud">d3-cloud</a>.

I put them all together on the command line like this:

<pre>
% twarc.py @adriarichards
% cat @adriarichards-20130321200320.json | utils/gender.py --gender male | utils/wordcloud.py > male.html
% cat @adriarichards-20130321200320.json | utils/gender.py --gender female | utils/wordcloud.py > female.html
</pre>

I realize word clouds <a href="http://www.niemanlab.org/2011/10/word-clouds-considered-harmful/">aren't probably the greatest</a> way to visualize the differences in these messages. If you have better ideas let me know. I made the <a href="http://inkdroid.org/data/adriarichards.json.gz" rel="nofollow">tweet JSON</a> available if you want to try your own visualization.

<a href="http://inkdroid.org/data/adriarichards-male.html"><img src="http://inkdroid.org/images/adriarichards-male.png"/></a>
<a href="http://inkdroid.org/data/adriarichards-female.html"><img src="http://inkdroid.org/images/adriarichards-female.png"/></a>

Looking at these didn't yield much insight. So instead of visualizing all the words that each gender used, I wondered what the clouds would look like if I limited them to words that were uniquely spoken by each gender. In other words, what words did males use in their tweets which were not used by females, and vice-versa. There were 1,333 (11%) uniquely female words, and 4,767 (39%) uniquely male words, with a shared vocabulary of 5,988 (50%) words. 

<a href="http://inkdroid.org/data/adriarichards-male-unique.html"><img src="http://inkdroid.org/images/adriarichards-male-unique.png"/></a>
<a href="http://inkdroid.org/data/adriarichards-female.html"><img src="http://inkdroid.org/images/adriarichards-female-unique.png"/></a>

I'm not sure there is much more insight here either. I guess there is some weak comfort in the knowledge that 1/2 of the words used in these tweets were shared by both sexes.

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5552</wp:post_id>
		<wp:post_date>2013-03-22 06:26:40</wp:post_date>
		<wp:post_date_gmt>2013-03-22 13:26:40</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>python-heal-thyself</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="d3"><![CDATA[d3]]></category>
		<category domain="post_tag" nicename="gender"><![CDATA[gender]]></category>
		<category domain="post_tag" nicename="pycon"><![CDATA[pycon]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="tagcloud"><![CDATA[tagcloud]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wordclouds"><![CDATA[wordclouds]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_0f48e62d6aa9b617c8930c0fafe7cdaa</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>.<a href="https://twitter.com/adriarichards">@adriarichards</a> is currently getting doxed &amp; threatened w/ violence. Search Twitter for her name &amp; report abuse: <a href="http://t.co/GO6Gc1jXoC">http://t.co/GO6Gc1jXoC</a></p>&mdash; Gina Trapani (@ginatrapani) <a href="https://twitter.com/ginatrapani/statuses/314552254592069632">March 21, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_a9c282d3fe82be19f408ac3d3e640ff6</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>.@<a href="https://twitter.com/adriarichards">adriarichards</a> is currently getting doxed &amp; threatened w/ violence. Search Twitter for her name &amp; report abuse: <a href="http://t.co/GO6Gc1jXoC" title="http://bit.ly/Y82Ntx">bit.ly/Y82Ntx</a></p>&mdash; Gina Trapani (@ginatrapani) <a href="https://twitter.com/ginatrapani/status/314552254592069632">March 21, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_fdf516dd1551216fecb3bc368aa2efe5</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>.@<a href="https://twitter.com/adriarichards">adriarichards</a> is currently getting doxed &amp; threatened w/ violence. Search Twitter for her name &amp; report abuse: <a href="http://t.co/GO6Gc1jXoC" title="http://bit.ly/Y82Ntx">bit.ly/Y82Ntx</a></p>&mdash; Gina Trapani (@ginatrapani) <a href="https://twitter.com/ginatrapani/status/314552254592069632">March 21, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_72586834f30c71a6a494a0df6845f883</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="500"><p>.@<a href="https://twitter.com/adriarichards">adriarichards</a> is currently getting doxed &amp; threatened w/ violence. Search Twitter for her name &amp; report abuse: <a href="http://t.co/GO6Gc1jXoC" title="http://bit.ly/Y82Ntx">bit.ly/Y82Ntx</a></p>&mdash; Gina Trapani (@ginatrapani) <a href="https://twitter.com/ginatrapani/status/314552254592069632">March 21, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85770</wp:comment_id>
			<wp:comment_author><![CDATA[pbinkley]]></wp:comment_author>
			<wp:comment_author_email>peter.binkley@ualberta.ca</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-03-22 10:15:14</wp:comment_date>
			<wp:comment_date_gmt>2013-03-22 17:15:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA["so?" (male) vs "OMG!" (female) is eloquent, I think.

A friend who's doing sociological research mentioned LIWC (http://www.liwc.net/), which calculates "the degree any text uses positive or negative emotions, self-references, causal words, and 70 other language dimensions". It might be interesting to see what it can do with a corpus of tweets. But it's $100 for the full version and $30 for the lite version.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>478</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1363972520.562037944793701171875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:8:"pbinkley";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>#75</title>
		<link>http://inkdroid.org/2013/04/18/75/</link>
		<pubDate>Thu, 18 Apr 2013 17:24:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5579</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
When taxes are too high,
people go hungry.
When the government is too intrusive,
people lose their spirit.

Act for the people's benefit.
Trust them; leave them alone.

<cite><a href="http://web.archive.org/web/20130821112136/http://academic.brooklyn.cuny.edu/core9/phalsall/texts/taote-v3.html">Tao Te Ching #75</a>
</cite></blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5579</wp:post_id>
		<wp:post_date>2013-04-18 10:24:19</wp:post_date>
		<wp:post_date_gmt>2013-04-18 17:24:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>75</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:85818;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>85818</wp:comment_id>
			<wp:comment_author><![CDATA[profiles.google.com/11528912267&hellip;]]></wp:comment_author>
			<wp:comment_author_email>ryan.b.shaw@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://profiles.google.com/115289122676740361492</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-04-18 11:50:06</wp:comment_date>
			<wp:comment_date_gmt>2013-04-18 18:50:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<blockquote cite="http://academic.brooklyn.cuny.edu/core9/phalsall/texts/taote-v3.html#53">
When rich speculators prosper
While farmers lose their land;
when government officials spend money
on weapons instead of cures;
when the upper class is extravagant and irresponsible
while the poor have nowhere to turn-
all this is robbery and chaos.
It is not in keeping with the Tao.
</blockquote>

<p><a href="http://web.archive.org/web/20130821112136/http://academic.brooklyn.cuny.edu/core9/phalsall/texts/taote-v3.html" rel="nofollow"><cite>TAO TE CHING</cite> #53</a></p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>481</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1366311007.6656129360198974609375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:41:"profiles.google.com-115289122676740361492";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1366315969.8282001018524169921875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Everything is Data</title>
		<link>http://inkdroid.org/2013/05/02/everything-is-data/</link>
		<pubDate>Thu, 02 May 2013 14:19:18 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5590</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.goodreads.com/book/show/134567.Reassembling_the_Social" style="float: left; padding-right: 20px"><img alt="Reassembling the Social: An Introduction to Actor-Network-Theory" border="0" src="http://d.gr-assets.com/books/1172047445m/134567.jpg" /></a><a href="http://www.goodreads.com/book/show/134567.Reassembling_the_Social">Reassembling the Social: An Introduction to Actor-Network-Theory</a> by <a href="http://www.goodreads.com/author/show/77743.Bruno_Latour">Bruno Latour</a><br />
My rating: <a href="http://www.goodreads.com/review/show/516628434">4 of 5 stars</a><br /><br />
I picked this up because folks over on the <a href="https://groups.google.com/forum/#!searchin/philosophy-in-a-time-of-software/reassembling/philosophy-in-a-time-of-software/T5PrHVZ0upI/tgiEC4dEgLoJ" rel="nofollow">Philosophy in a Time of Software</a> kicked things off by discussing this book by Latour. So, I'm really not terribly knowledgeable about sociology, but I did a fair bit of reading in the social sciences while <strike>getting my library union card</strike> studying library/information science. So I wasn't completely underwater, but I definitely felt like I was swimming in the deep end. I didn't get the connection to computer programming until quite late in the book, but it was definitely a bit of a lightbulb moment when I did. Latour's style (at least that of the unmentioned translator) is refreshingly direct, personal, and unabashedly opinionated. He spends much of the book describing just how complicated social science is, and how far it has gone off the tracks...which is quite entertaining at times.<br /><br />A few things I will take with me from this book and its portrayal of Actor Network Theory:<br /><br />I will never be able to say or write the word "social" without feeling like I'm glossing over a whole lot of stuff, and that this stuff is what I should actually be researching, talking and writing about. Latour stresses that it's important not to dumb things down by appealing to established social forces (class, gender, imperialism, etc) but by tracing the actors, their controversies, and their relations. This work requires discipline because it's tempting to reduce the complexity by using these familiar abstractions instead of expending energy/effort in documenting the scenarios as faithfully as possible. By letting the actors have a voice, and say what they think they are doing, rather than the researcher telling the actor what they are actually doing. I work in libraries/archives, so I particularly liked Latour's insistence on the importance notebooks, writing, and documentation:<br /><br /><blockquote><br />The best way to proceed at this point ... is simply to keep track of all our moves, even those that deal with the very production of the account. This is neither for the sake of epistemic reflexivity nor for some narcissist indulgence into one’s own work, but because from now on <strong>everything is data</strong>: everything from the first telephone call to a prospective interviewee, the first appointment with the advisor, the first corrections made by a client on a grant proposal, the first launching of a search engine, the first list of boxes to tick in a questionnaire. In keeping with the logic of our interest in textual reports and accounting, it might be useful to list the different notebooks one should keep—manual or digital, it no longer matters much. p. 286.<br /></blockquote><br /><br />... and that this is the work of "slowciology"  -- it requires you to slow down, and really describe/dig into things.<br /><br />The other really interesting thing about this book for me was the insistence that social actors do not need to be human. It is fairly typical for social science research to focus on face-to-face interaction between people as the primary focus. Latour doesn't dispute the importance of studying human actors, but emphasizes that it's useful to increase the number of actors under study by studying objects (mediators) as actors. Typically we think of actors as having agency, free will, etc ... but objects are typically complex things, with particular affordances, and extensive relations with other things in the field. You get only a very limited view of what is going on if you don't trace these relations. <br /><br /><blockquote><br />Things, quasi-objects, and attachments are the real center of the social world, not the agent, person, member, or participant—nor is it society or its avatars. (p. 237)<br /></blockquote><br /><br />As a software developer, I really identified with Latour's insistence on the role that objects play in our understanding of activities around us; how this view necessarily complicates things a great deal, and requires us to slow down to really understand/describe what is going on. It is hard work. And it's only when we understand the various actors and their relations, the actual ones, not the abstract ones in the architecture diagram, or in the theory about the software, that we will be in a position to effectively change things or build anew. 
<br /><br />
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5590</wp:post_id>
		<wp:post_date>2013-05-02 07:19:18</wp:post_date>
		<wp:post_date_gmt>2013-05-02 14:19:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>everything-is-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="book-review"><![CDATA[book review]]></category>
		<category domain="post_tag" nicename="latour"><![CDATA[latour]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="science"><![CDATA[science]]></category>
		<category domain="post_tag" nicename="social-science"><![CDATA[social science]]></category>
		<category domain="post_tag" nicename="sociology"><![CDATA[sociology]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>maps on the web with a bit of midlife crisis</title>
		<link>http://inkdroid.org/2013/05/10/maps-on-the-web-with-a-bit-of-midlife-crisis/</link>
		<pubDate>Fri, 10 May 2013 19:08:25 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5679</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>TL;DR -- I created <a href="http://edsu.github.io/wikigeo/">a JavaScript library</a> for getting GeoJSON out of Wikipedia's API in your browser (and Node.js). I also created <a href="http://inkdroid.org/ici/">a little app</a> that uses it to display Wikipedia articles for things near you that need a photograph/image or editorial help.</em></p>

<hr />

<p>I probably don't need to tell you how much the state of mapping on the Web has changed in the past few years. <a href="http://www.youtube.com/watch?v=6xG4oFny2Pk">I was there</a>. I can remember trying to get <a href="http://mapserver.org/">MapServer</a> set up in the late 1990s, with limited success. I was there squinting at how <a href="http://en.wikipedia.org/wiki/Adrian_Holovaty">Adrian Holovaty</a> reverse engineered a mapping API out of Google Maps at <a href="http://web.archive.org/web/20060408105215/http://www.chicagocrime.org/map/">chicagocrime.org</a>. I was there when Google released their official API, which I used some, and then they changed their terms of service. I was there in the late 2000s using OpenLayers and TileCache, which were so much more approachable than MapServer was a decade earlier. I'm most definitely not a mapping expert, or even an amateur--but you can't be a Web developer without occasionally needing to dabble, and pretend you are.</p>

<p>I didn't realize until very recently how easy the cool kids have made it to put maps on the Web. Who knew that in 2013 there would be an open source JavaScript library that lets you add a map to your page in a few lines, and that it's in use by Flickr, FourSquare, CraigsList, Wikimedia, the Wall Street Journal, and others? Even more astounding: who knew there would be an openly licensed source of map tiles and data, that was created collaboratively by a project with over a million registered users, and that it would be good enough to be used by Apple? I certainly didn't even dream about it.</p>

<p>Ok, hold that thought...</p>

<p>So, Wikipedia <a href="https://blog.wikimedia.org/2013/03/28/add-an-image-to-this-article-uploads-now-live-on-mobile-wikipedia/">recently announced</a> that they were making it easy to use your mobile device to add a photograph to a Wikipedia article that lacked an image.</p>

<div style="width: 60%; margin-left: auto; margin-right: auto; margin-top: 10px; margin-bottom: 10px; border: thin solid #eeeeee;">
<a href="http://www.flickr.com/photos/inkdroid/8726826906/"><img src="http://farm8.staticflickr.com/7318/8726826906_2e88f9ab6b_b.jpg" width="400"/></a>
</div>

<p>When I read about this I thought it would be interesting to see what Wikipedia articles there are about my current location, and which lacked images, so I could go and take pictures of them. Before I knew it I had <a href="http://inkdroid.org/ici/">a Web app</a> called ici (French for here) that does just that:</p>

<p><a href="http://inkdroid.org/ici/#lat=38.89591781652618&lon=-77.0342230796814&zoom=16"><img src="http://inkdroid.org/images/ici.png"/></a></p>

<p>Articles that need images are marked with little red cameras. It was pretty easy to add orange markers for Wikipedia articles that had been flagged as needing edits, or citations. Calling it an app is an overstatement: it is just static HTML, JavaScript and CSS that I serve up. HTML's <a href="http://diveintohtml5.info/geolocation.html">geolocation</a> features and <a href="http://en.wikipedia.org/w/api.php">Wikipedia's API</a> (which has <a href="http://www.mediawiki.org/wiki/Extension:GeoData">GeoData</a> enabled) take care of the rest.</p>

<p>After I created the app I got a tweet from a <em>real</em> geo-hacker, Sean Gillies, who asked:</p>

<p>https://twitter.com/sgillies/status/332185543234441216</p>

<p>Sean is right, it would be really useful to have a GeoJSON output from Wikipedia's API. But I was on a little bit of a tear, so rather than figuring out how to get GeoJSON into MediaWiki and deployed to all the Wikipedia servers I wondered if I could extract ici's use of the Wikipedia API into a slightly more generalized JavaScript library, that would make it easy to get GeoJSON out of Wikipedia--at least from JavaScript. That quickly resulted in <a href="http://edsu.github.io/wikigeo/">wikigeo.js</a> which is now getting used in ici. Getting GeoJSON from Wikipedia using wikigeo.js is done in just one line, and then <a href="http://leafletjs.com/examples/geojson.html">adding the GeoJSON to a map in Leaflet</a> can also be done in one line:</p>

<pre lang="javascript">
geojson([-73.94, 40.67], function(data) {
    // add the geojson to a Leaflet map
    L.geoJson(data).addTo(map)
});
</pre>

<p>This call results in callback getting some GeoJSON data that looks something like:</p>

<pre lang="javascript">
{
  "type": "FeatureCollection",
  "features": [
    {
      "id": "http://en.wikipedia.org/wiki/New_York_City",
      "type": "Feature",
      "properties": {
        "name": "New York City"
      },
      "geometry": {
        "type": "Point",
        "coordinates": [
          -73.94,
          40.67
        ]
      }
    },
    {
      "id": "http://en.wikipedia.org/wiki/Kingston_Avenue_(IRT_Eastern_Parkway_Line)",
      "type": "Feature",
      "properties": {
        "name": "Kingston Avenue (IRT Eastern Parkway Line)"
      },
      "geometry": {
        "type": "Point",
        "coordinates": [
          -73.9422,
          40.6694
        ]
      }
    },
    {
      "id": "http://en.wikipedia.org/wiki/Crown_Heights_–_Utica_Avenue_(IRT_Eastern_Parkway_Line)",
      "type": "Feature",
      "properties": {
        "name": "Crown Heights – Utica Avenue (IRT Eastern Parkway Line)"
      },
      "geometry": {
        "type": "Point",
        "coordinates": [
          -73.9312,
          40.6688
        ]
      }
    },
    {
      "id": "http://en.wikipedia.org/wiki/Brooklyn_Children's_Museum",
      "type": "Feature",
      "properties": {
        "name": "Brooklyn Children's Museum"
      },
"geometry": {
        "type": "Point",
        "coordinates": [
          -73.9439,
          40.6745
        ]
      }
    },
    {
      "id": "http://en.wikipedia.org/wiki/770_Eastern_Parkway",
      "type": "Feature",
      "properties": {
        "name": "770 Eastern Parkway"
      },
      "geometry": {
        "type": "Point",
        "coordinates": [
          -73.9429,
          40.669
        ]
      }
    },
    {
      "id": "http://en.wikipedia.org/wiki/Eastern_Parkway_(Brooklyn)",
      "type": "Feature",
      "properties": {
        "name": "Eastern Parkway (Brooklyn)"
      },
      "geometry": {
        "type": "Point",
        "coordinates": [
          -73.9371,
          40.6691
        ]
      }
    },
    {
      "id": "http://en.wikipedia.org/wiki/Paul_Robeson_High_School_for_Business_and_Technology",
      "type": "Feature",
      "properties": {
        "name": "Paul Robeson High School for Business and Technology"
      },
      "geometry": {
        "type": "Point",
        "coordinates": [
          -73.939,
          40.6755
        ]
      }
    },
    {
      "id": "http://en.wikipedia.org/wiki/Pathways_in_Technology_Early_College_High_School",
      "type": "Feature",
      "properties": {
        "name": "Pathways in Technology Early College High School"
      },
      "geometry": {
        "type": "Point",
        "coordinates": [
          -73.939,
          40.6759
        ]
      }
    }
  ]
}
</pre>

<p>There are options for broadening the radius, increasing the number of results, and fetching additional properties of the Wikipedia article such as article summaries, images, categories, templates used. Here's an example using all the knobs:</p>

<pre lang="javascript">
geojson(
  [-73.94, 40.67],
  {
    limit: 5,
    radius: 1000,
    images: true,
    categories: true,
    summaries: true,
    templates: true
  },
  function(data) {
    L.geoJson(data).addTo(map)
  }
);
</pre>

<p>Which results in GeoJSON like this (abbreviated)</p>

<pre lang="javascript">
{
  "type": "FeatureCollection",
  "features": [
    {
      "id": "http://en.wikipedia.org/wiki/Silver_Spring,_Maryland",
      "type": "Feature",
      "properties": {
        "name": "Silver Spring, Maryland",
        "image": "Downtown_silver_spring_wayne.jpg",
        "templates": [
          "-",
          "Abbr",
          "Ambox",
          "Ambox/category",
          "Ambox/small",
          "Basepage subpage",
          "Both",
          "Category handler",
          "Category handler/blacklist",
          "Category handler/numbered"
        ],
        "summary": "Silver Spring is an unincorporated area and census-designated place (CDP) in Montgomery County, Maryland, United States. It had a population of 71,452 at the 2010 census, making it the fourth most populous place in Maryland, after Baltimore, Columbia, and Germantown.\nThe urbanized, oldest, and southernmost part of Silver Spring is a major business hub that lies at the north apex of Washington, D.C. As of 2004, the Central Business District (CBD) held 7,254,729 square feet (673,986 m2) of office space, 5216 dwelling units and 17.6 acres (71,000 m2) of parkland. The population density of this CBD area of Silver Spring was 15,600 per square mile all within 360 acres (1.5 km2) and approximately 2.5 square miles (6 km2) in the CBD/downtown area. The community has recently undergone a significant renaissance, with the addition of major retail, residential, and office developments.\nSilver Spring takes its name from a mica-flecked spring discovered there in 1840 by Francis Preston Blair, who subsequently bought much of the surrounding land. Acorn Park, tucked away in an area of south Silver Spring away from the main downtown area, is believed to be the site of the original spring.\n\n",
        "categories": [
          "All articles to be expanded",
          "All articles with dead external links",
          "All articles with unsourced statements",
          "Articles to be expanded from June 2008",
          "Articles with dead external links from July 2009",
          "Articles with dead external links from October 2010",
          "Articles with dead external links from September 2010",
          "Articles with unsourced statements from February 2007",
          "Articles with unsourced statements from May 2009",
          "Commons category template with no category set"
        ]
      },
      "geometry": {
        "type": "Point",
        "coordinates": [
          -77.019,
          39.0042
        ]
      }
    },
    ...
  ]
}
</pre>

<p>I guess this is a long way of saying, if you want to put Wikipedia articles on a map, or otherwise need GeoJSON for Wikipedia articles for a particular location, take a look at <a href="http://edsu.github.io/wikigeo/">wikigeo.js</a>. If you do, and have ideas for making it better, please let me know. Oh, by the way you can <code>npm install <a href="https://npmjs.org/package/wikigeo">wikigeo</a></code> and use it from <a href="http://nodejs.org">Node.js</a>.</p>

<p>I guess JavaScript, HTML5, NodeJS, CoffeeScript are like my midlife crisis...my red sports car. But maybe being the old guy, and losing my edge isn't really so bad?</p>

<blockquote>
I'm losing my edge
to better-looking people 
with better ideas 
and more talent
and they're actually 
really, really nice.
--- <a href="http://www.youtube.com/watch?v=6xG4oFny2Pk">Jim Murphy</a>
</blockquote>

<p>It definitely helps when the kids coming up from behind have talent and are really, really nice. You know?</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5679</wp:post_id>
		<wp:post_date>2013-05-10 12:08:25</wp:post_date>
		<wp:post_date_gmt>2013-05-10 19:08:25</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>maps-on-the-web-with-a-bit-of-midlife-crisis</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="maps"><![CDATA[maps]]></category>
		<category domain="post_tag" nicename="nodejs"><![CDATA[nodejs]]></category>
		<category domain="post_tag" nicename="openstreetmap"><![CDATA[openstreetmap]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="rest"><![CDATA[rest]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>recent Wikipedia citations as JSON</title>
		<link>http://inkdroid.org/2013/06/07/recent-wikipedia-citations-as-json/</link>
		<pubDate>Fri, 07 Jun 2013 17:49:26 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5735</guid>
		<description></description>
		<content:encoded><![CDATA[Here is a little webcast about some <a href="http://github.com/edsu/wikicites">work in progress</a> to stream recent citations out of Wikipedia. It uses previous work I did on the <a herf="https://npmjs.org/package/wikichanges">wikichanges</a> Node library. Beware, I say "um" and "uh" a lot while showing you my terminal window. This idea could very well be brain damaged since it pings the Wikipedia API for the diff of each change in selected Wikipedias, to see if it contains one or more citations. On the plus side, it emits the citations as JSON, which is suitable for downstream apps of some dimensions, which I haven't thought much about yet. Get in touch if you have some ideas.

https://vimeo.com/67893886
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5735</wp:post_id>
		<wp:post_date>2013-06-07 10:49:26</wp:post_date>
		<wp:post_date_gmt>2013-06-07 17:49:26</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>recent-wikipedia-citations-as-json</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="nodejs"><![CDATA[nodejs]]></category>
		<category domain="post_tag" nicename="streaming"><![CDATA[streaming]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_f8b56a02f2b5e96cbcd0ad7d5c929a72</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe src="http://player.vimeo.com/video/67893886" width="604" height="378" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_ec74a1c76cc2b849abd3d0c472dd542b</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_3d01af8ea153b05b7593abdab2bf4a8d</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe src="http://player.vimeo.com/video/67893886" width="625" height="391" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>thoughts on SHARE</title>
		<link>http://inkdroid.org/2013/06/13/thoughts-on-share/</link>
		<pubDate>Thu, 13 Jun 2013 13:46:35 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5741</guid>
		<description></description>
		<content:encoded><![CDATA[My response to Library Journal's <a href="http://lj.libraryjournal.com/2013/06/oa/arl-launches-library-led-solution-to-federal-open-access-requirements/">ARL Launches Library-Led Solution to Federal Open Access Requirements</a> that I'm posting here as well, because I spent a bit of time on it. Thanks for the heads up Dorothea,

https://twitter.com/LibSkrat/status/345148738488115201

<hr />

In principle I like the approach that SHARE is taking, that of leveraging the existing network of institutional repositories, and the amazingly decentralized thing that is the Internet and the World Wide Web. Simply getting article content out on the Web, where it can be crawled, as Harnad suggests, has bootstrapped incredibly useful services like Google Scholar. Scholar works with the Web we have, not some future Web where we all share metadata perfectly using formats that will be preserved for the ages. They don't use OpenURL, OAI-ORE, SWORD, etc. They do have lots o' crawlers, and some magical PDF parsing code that can locate citations. I would like to see a plan that's a bit scruffier and less neat.

Like Dorothea I have big doubts about building what looks to be a centralized system that will then push out to IRs using SWORD, and support some kind of federated search with OpenURL. Most IRs seem more like research experiments than real applications oriented around access, that could sustain the kind of usage you might see if mainstream media or a MOOC happened to reference their content. Rather than a 4 phase plan, with digital library acronym soup,I'd rather see some very simple things that could be done to make sure that federally funded research *is* deposited in an IR, and it can be traced back to the grant that funded it. Of course, I can' resist to throw out a straw man.

Requiring funding agencies to have a URL for each grant, which can be used in IRs seems like it would be the first logical step. Pinging that URL (kind of like a trackback) when there is a resource (article, dataset, etc) associated with the grant would allow the granting institution to know when something was published that referenced that URL. The granting organization could then look at its grants and see which ones lacked a deposit, and follow up with the grantees. They could also examine pingbacks to see which ones are legit or not. Perhaps further on down the line these resources could be integrated into web archiving efforts, but I digress.

There would probably be a bit of curation of these pingbacks, but nothing a big Federal Agency can't handle right? I think putting data curation first, instead of last, as the icing on the 4 phase cake is important. I don't underestimate the challenge in requiring a URL for every grant, perhaps some agencies already have them. I think this would put the onus on the Federal agencies to make this work, rather than the publishers (who, like or not, have a commercial incentive to not make it too easy to provide open access) and universities (who must have a way of referencing grants if any of their plan is to work). This would be putting Linked Data first, rather than last, as rainbow sprinkles on the cake.

Sorry if this comes off as a bit ranty or incomprehensible. I wish Aaron were here to help guide us... It is truly remarkable that the OSTP memo was issued, and that we have seen responses from the ARL and the AAP. I hope we'll see responses from the federal agencies that the memo was actually directed at.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5741</wp:post_id>
		<wp:post_date>2013-06-13 06:46:35</wp:post_date>
		<wp:post_date_gmt>2013-06-13 13:46:35</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>thoughts-on-share</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="curation"><![CDATA[curation]]></category>
		<category domain="post_tag" nicename="data-curation"><![CDATA[data curation]]></category>
		<category domain="post_tag" nicename="egov"><![CDATA[egov]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_ce0681ecfb7daba00742889e725a6ab7</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>Let the nastygrams begin. <a href="http://t.co/7THbOt8AmZ">http://t.co/7THbOt8AmZ</a></p>&mdash; Ondatra libskoolicus (@LibSkrat) <a href="https://twitter.com/LibSkrat/statuses/345148738488115201">June 13, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_e47c5e5a84b36c4c9962063d0baf7e10</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="500"><p>Let the nastygrams begin. <a href="http://t.co/7THbOt8AmZ">http://t.co/7THbOt8AmZ</a></p>&mdash; Ondatra libskoolicus (@LibSkrat) <a href="https://twitter.com/LibSkrat/statuses/345148738488115201">June 13, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_1a73c87b69564bae4cdb5c0beb17a51b</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p>Let the nastygrams begin. <a href="http://t.co/7THbOt8AmZ">http://t.co/7THbOt8AmZ</a></p>&mdash; Ondatra libskoolicus (@LibSkrat) <a href="https://twitter.com/LibSkrat/statuses/345148738488115201">June 13, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>tiny alien phenomenology review</title>
		<link>http://inkdroid.org/2013/06/20/tiny-alien-phenomenology-review/</link>
		<pubDate>Thu, 20 Jun 2013 12:58:59 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5749</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.goodreads.com/book/show/13222626-alien-phenomenology-or-what-it-s-like-to-be-a-thing" style="float: left; padding-right: 20px"><img alt="Alien Phenomenology, or What It's Like to Be a Thing" border="0" src="http://inkdroid.org/images/bogost.jpg" /></a><a href="http://www.goodreads.com/book/show/13222626-alien-phenomenology-or-what-it-s-like-to-be-a-thing">Alien Phenomenology, or What It's Like to Be a Thing</a> by <a href="http://www.goodreads.com/author/show/38615.Ian_Bogost">Ian Bogost</a><br />
My rating: <a href="http://www.goodreads.com/review/show/605946721">3 of 5 stars</a><br /><br />
I found this book to be quite accessible and totally incomprehensible at the same time. It was kind of a surreal joy to read. I liked how it flipped the artificial intelligence research agenda of getting machines to think (like people), to getting humans to imagine what it was like to be a thing. I also came to appreciate Bogost's variation on Latour's litanies, so called tiny ontology. And I really appreciated his emphasis on making things to guide thinking or philosophical carpentry ... and the importance of cultivating a sense of wonder. His use of real examples and case studies to demonstrate his thinking was also very helpful--and sometimes quite humorous. I'm wandering back to Latour to read <a href="http://www.goodreads.com/book/show/134569.We_Have_Never_Been_Modern">We Have Never Been Modern</a> based on some discussion of it in this book.<br /><br />So, in the spirit of tiny ontology here are some random quotes I highlighted on my Kindle:<br /><br /><blockquote>To be sure, computers often do entail human experience and perception. The human operator views words and images rendered on a display, applies physical forces to a mouse, seats memory chips into motherboard sockets. But not always. Indeed, for the computer to operate at all for us first requires a wealth of interactions to take place for itself. As operators or engineers, we may be able to describe how such objects and assemblages work. But what do they experience? What’s their proper phenomenology? In short, what is it like to be a thing?<br /></blockquote><br /><br /><blockquote><br />Theories of being tend to be grandiose, but they need not be, because being is simple. Simple enough that it could be rendered via screen print on a trucker’s cap. I call it tiny ontology, precisely because it ought not demand a treatise or a tome. I don’t mean that the domain of being is small— quite the opposite, as I’ll soon explain. Rather, the basic ontological apparatus needed to describe existence ought to be as compact and unornamented as possible.<br /></blockquote><br /><br /><blockquote><br />For the ontographer, Aristotle was wrong: nature does not operate in the shortest way possible but in a multitude of locally streamlined yet globally inefficient ways.[ 41] Indeed, an obsession with simple explanations ought to bother the metaphysician. Instead of worshipping simplicity, OOO embraces messiness. We must not confuse the values of the design of objects for human use, such as doors, toasters, and computers, with the nature of the world itself. An ontograph is a crowd, not a cellular automaton that might describe its emergent operation. An ontograph is a landfill, not a Japanese garden. It shows how much rather than how little exists simultaneously, suspended in the dense meanwhile of being.<br /></blockquote><br /><br /><blockquote><br />Yet once we are done nodding earnestly at Whitehead and Latour, what do we do? We return to our libraries and our word processors. We refine our diction and insert more endnotes. We apply “rigor,” the scholarly version of Tinker Bell’s fairy dust, in adequate quantities to stave off interest while cheating death. For too long, being “radical” in philosophy has meant writing and talking incessantly, theorizing ideas so big that they can never be concretized but only marked with threatening definite articles (“ the political,” “the other,” “the neighbor,” “the animal”). For too long, philosophers have spun waste like a goldfish’s sphincter, rather than spinning yarn like a charka. Whether or not the real radical philosophers march or protest or run for office in addition to writing inscrutable tomes— this is a question we can, perhaps, leave aside. Real radicals, we might conclude, make things. Examples aren’t hard to find, and some even come from scholars who might be willing to call themselves philosophers.<br /></blockquote>
<br /><br />
<a href="http://www.goodreads.com/review/list/5899086-ed-summers">View all my reviews</a>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5749</wp:post_id>
		<wp:post_date>2013-06-20 05:58:59</wp:post_date>
		<wp:post_date_gmt>2013-06-20 12:58:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>tiny-alien-phenomenology-review</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="book-review"><![CDATA[book review]]></category>
		<category domain="post_tag" nicename="computers"><![CDATA[computers]]></category>
		<category domain="post_tag" nicename="latour"><![CDATA[latour]]></category>
		<category domain="post_tag" nicename="philosophy"><![CDATA[philosophy]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Open Science Champions of Change</title>
		<link>http://inkdroid.org/2013/06/21/open-science-champions-of-change/</link>
		<pubDate>Fri, 21 Jun 2013 21:09:24 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5752</guid>
		<description></description>
		<content:encoded><![CDATA[I had the opportunity to go the White House yesterday to attend the Open Science <a href="http://www.whitehouse.gov/champions">Champions of Change</a> award ceremony. I'm not sure why I was invited, perhaps because I <a href="https://docs.google.com/document/d/1U5v_gKvLPwUU8xdORKMhq6f6EZAoCU8bq3RwAa47l2c/edit">nominated</a> Aaron Swartz for it, and happen to be local. Unfortunately, Aaron didn't win the award. I guess it would've been sad to award it to him posthumously. But it's a sad story. Whatever the reason, I was sure honored to be there. 

It was just amazing to see some of my heroes like <a href="http://en.wikipedia.org/wiki/Paul_Ginsparg">Paul Ginsparg</a> (arXiv), <a href="http://en.wikipedia.org/wiki/David_J._Lipman">David Lipman</a> (PubMed, Genbank) and <a href="http://en.wikipedia.org/wiki/Jeremiah_P._Ostriker">Jeremiah Ostriker</a> (Sloan Digital Sky Survey) in the same room, and on a panel where they could share ideas about the work they've done--and what remains to be done. The event was live streamed and is now available on the White House Youtube channel. The full list of the other amazing recipients and their bios is available <a href="http://www.whitehouse.gov/sites/default/files/microsites/ostp/openscience_release_6-18-13.pdf">here</a>.

http://www.youtube.com/watch?v=a26cEwbyMGQ

So many things were said over the two hours, it's hard for me to summarize here. But I thought I would jot down the main theme that struck me, absent a lot of the details about the projects that were discussed. Hopefully I can look back later and say, oh wow, I went to that.

<hr />

During his intro, Jerimiah Ostriker talked about how the Sloan Digital Sky Survey was set up from the beginning to require public data sharing on the Internet. He said that it wasn't easy, but that they made it work. David Lipman talked humbly about how PubMed and GenBank make all publicly funded research and data available at an astonishing rate: millions of users, and many terabytes of data a day. There was much discussion about how to incentivize scientists to share their research. Lipman pointed out that while there was a history of sharing pre-prints in the physics community (which helped Ginsparg realize arXiv) the biomedical field lacks this culture to some degree. Ginsparg acknowledged this, while pointing out that compelling, new applications that change what it means to do research can mitigate this to some degree.

I don't remember how it came up, but at one point Ostriker was asked what needed to be done to incentivize more public sharing of research and he responded quickly, simply and with a smile:

<blockquote>
People like to follow rules.
</blockquote>

I think Ostriker was not only referring to the way he helped set up the Sloan Digital Sky Survey, but also to the proposed legislation <a href="http://en.wikipedia.org/wiki/Fair_Access_to_Science_and_Technology_Research_Act">Fair Access to Science and Technology Research Act (FASTR)</a> or Aaron's Other Law, which is still pending, and in need of <a href="https://action.eff.org/o/9042/p/dia/action/public/?action_KEY=9061">support</a>. People kind of laughed a bit when <a href="http://en.wikipedia.org/wiki/Jack_Andraka">Jack Andraka</a> (whose story is  <a href="http://www.youtube.com/watch?feature=player_embedded&v=n9yuAhusVts">freakin' amazing</a>) said he was planning to start a petition to bring down the paywalls in front of publicly funded research. He described how his own research was obstructed by these paywalls. He's wicked smart and just a kid, and has a humorous way to present the issues--so a bit of laughter was ok I guess. But Ostriker who is 76 and Andraka who is 16 were right on key, given where they were sitting:

The rules need to change. It's time...there's still time right?]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5752</wp:post_id>
		<wp:post_date>2013-06-21 14:09:24</wp:post_date>
		<wp:post_date_gmt>2013-06-21 21:09:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>open-science-champions-of-change</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="government"><![CDATA[government]]></category>
		<category domain="post_tag" nicename="politics"><![CDATA[politics]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_00574ae3da1fef71f71decdefbcb1c61</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="604" height="340" src="http://www.youtube.com/embed/a26cEwbyMGQ?feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_d02eabdc20d9e6bd2c1d3b19df1defa7</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="625" height="352" src="http://www.youtube.com/embed/a26cEwbyMGQ?feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_abb0ad44aa4ec242dc584f1a955f77f0</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="500" height="281" src="http://www.youtube.com/embed/a26cEwbyMGQ?feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>It&#039;s your data. It&#039;s your life.</title>
		<link>http://inkdroid.org/2013/06/27/its-your-data-its-your-life/</link>
		<pubDate>Thu, 27 Jun 2013 14:22:28 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5773</guid>
		<description></description>
		<content:encoded><![CDATA[I <a href="http://inkdroid.org/2013/06/21/open-science-champions-of-change/">wrote</a> briefly about the Open Science Champions of Change event last week, but almost a week later the impassioned message that <a href="https://twitter.com/KathyGiusti">Kathy Giusti</a> delivered is still with me. Giusti is the Founder and Chief Executive Officer of the Multiple Myeloma Research Foundation (MMRF), and is herself battling the fatal disease. In her introduction, and later during the panel discussion, she made a strong case for patients to be able to opt-in to open access data sharing. I thought I'd point to these two moments in the 2 hour video stream, and transcribe what she said:

http://www.youtube.com/watch?v=a26cEwbyMGQ#t=1h15m50s

<blockquote>
Patients don't know that data is not shared. They don't know ... If patients knew how long it took to publish, if they knew, it's your tissue, it's your data, it's your life. Believe me, patients would be the first force to start really changing the culture and backing everybody around open access.
</blockquote>

http://www.youtube.com/watch?v=a26cEwbyMGQ#t=1h34m54s

<blockquote>

Q: A lot of people when they hear about the sharing of clinical data talk about concerns of privacy. How do we start to handle those concerns, and how do we actually encourage patients to contribute their data in meaningful ways to research, so that we can actually continue to drive the successes that we are seeing here?

Giusti: When you're a patient, and you're living with a fatal disease, you don't lie awake and wonder what happens with my data. If patients understand the role they can play in personalizing their own risk taking abilities ... We all do this when we work with our banks. There's certain information that we're always giving out when we go online, and there's certain information that we always keep private. And in a future world that's what patients are going to do. So when you start talking with the patients, and you ask them: "Would you be willing to share your information?" It just depends on the patient, and it depends on how much they would be willing to give. For someone like me, I'm an identical twin, the disease of Myeloma skews in my family, my grandfather had it, my identical twin does not, I would be crazy not to be published ... and I've done it, and so has my twin ... biopsies, whatever we need. Put it in the public domain. I know everybody isn't going to be like me, but even if you get us half your information we're making progress, and we can start to match you with the right types of researchers and clinicians that care.
</blockquote>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5773</wp:post_id>
		<wp:post_date>2013-06-27 07:22:28</wp:post_date>
		<wp:post_date_gmt>2013-06-27 14:22:28</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>its-your-data-its-your-life</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="cancer"><![CDATA[cancer]]></category>
		<category domain="category" nicename="data"><![CDATA[data]]></category>
		<category domain="category" nicename="government"><![CDATA[government]]></category>
		<category domain="post_tag" nicename="health"><![CDATA[health]]></category>
		<category domain="post_tag" nicename="open-access"><![CDATA[open access]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_b5ffa7f3fe485864e27381b7ca1c5555</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="604" height="340" src="http://www.youtube.com/embed/a26cEwbyMGQ?start=5694&feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_8b82e489d3d42a4a4ae0231ea7d4f3c4</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="604" height="340" src="http://www.youtube.com/embed/a26cEwbyMGQ?start=4550&feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_f602452656c30842a951e8ea30a5a4c2</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="625" height="352" src="http://www.youtube.com/embed/a26cEwbyMGQ?start=5694&feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_ab43ef24a6f2e222467e50e944aefb0c</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="625" height="352" src="http://www.youtube.com/embed/a26cEwbyMGQ?feature=oembed&start=4550" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_774a161cb5f8ddc18581ec0fda1173ac</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="500" height="281" src="http://www.youtube.com/embed/a26cEwbyMGQ?start=4550&feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_96ff95ca59daf80bf6b894a27c5620c7</wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="500" height="281" src="http://www.youtube.com/embed/a26cEwbyMGQ?start=5694&feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:86191;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86191</wp:comment_id>
			<wp:comment_author><![CDATA[@atomotic]]></wp:comment_author>
			<wp:comment_author_email>raffaele.messuti@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-06-27 23:07:51</wp:comment_date>
			<wp:comment_date_gmt>2013-06-28 06:07:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[http://opensourcecureforcancer.com

Salvatore Iaconesi, an italian artist suffering brain cancer, a year ago published online its clinical data to find a cure for the disease.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>443</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1372399673.4907939434051513671875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawlwAIsdKcHJGYmWov34EWu";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1372406776.6801300048828125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>On Snowden and Archival Ethics</title>
		<link>http://inkdroid.org/2013/07/10/on-snowden-and-archival-ethics/</link>
		<pubDate>Wed, 10 Jul 2013 16:59:27 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5781</guid>
		<description></description>
		<content:encoded><![CDATA[Much like you I've been <a href="http://pinboard.in/u:edsu/t:nsa">watching</a> the evolving NSA Surveillance story following the whistle-blowing by former government contractor <a href="https://en.wikipedia.org/wiki/Edward_Snowden">Edward Snowden</a>. Watching isn't really the right word...I've been <em>glued</em> to it. I don't have a particularly unique opinion or observation to make about the leak, or the ensuing dialogue -- but I suppose calling it "whistle blowing" best summarizes where I stand. I just wanted to share a thought I had on the train to work, after reading Ethan Zuckerman's excellent <a href="http://www.ethanzuckerman.com/blog/2013/07/03/me-and-my-metadata-thoughts-on-online-surveillance/">Me and My Metadata - Thoughts on Online Surveillance</a>. I tried to fit it in into <a href="http://twitter.com/edsu">140 characters</a>, but it didn't quite work.

Zuckerman's post is basically about the value of metadata in research. He opened up his Gmail archive to his students, and they created <a href="https://immersion.media.mit.edu/">Immersion</a>, which lets you visualize his network of correspondence using only email metadata (From, To, Cc and Date). Zuckerman goes on to demonstrate what this visualization says about him. The <a href="http://www.ethanzuckerman.com/blog/2013/07/03/me-and-my-metadata-thoughts-on-online-surveillance/comment-page-1/#comment-2807552">first comment</a> in the post by Jonathan O'Donnell has a nice list of related research on the importance of metadata to discovery. Zuckerman's work immediately reminded me of <a href="http://twitter.com/hangal">Sudheendra Hangal</a>'s work on <a href="http://mobisocial.stanford.edu/muse/">MUSE</a> at Stanford, which he and his team have <a href="http://mobisocial.stanford.edu/muse/muse-papers.html">written</a> about extensively. MUSE is a tool that enables scholarly research using email archives. It was then that I realized why I've been so fascinated with the Snowden/NSA story.

Over the past few years there has been increasing awareness in the archival community about the role of forensics tools in digital preservation, curation and research. <a href="https://twitter.com/mkirschenbaum">Matt Kirschenbaum</a>'s <a href="http://www.amazon.com/Mechanisms-New-Media-Forensic-Imagination/dp/026251740X">Mechanisms</a> had a big role in documenting, and spreading the word about how forensics tools can be (and are) used in the digital humanities. The CLIR report <a href="http://www.clir.org/pubs/abstract/reports/pub149">Digital Forensics and Born-Digital Content in Cultural Heritage Collections</a> (co-authored by Kirschenbaum) brought the topic directly to cultural heritage organizations, as did the <a href="http://www.digitalcurationservices.org/aims/white-paper/">AIMS report</a>. If you're not convinced, a <a href="http://scholar.google.com/scholar?q=digital+curation+forensics">search</a> in Google Scholar shows just how prevalent and timely the topic is. The introduction to the CLIR report has a nice summary of why forensics tools are of interest to archives that are dealing with born digital content:

<blockquote>
The same forensics software that indexes a criminal suspect’s hard drive allows the archivist to prepare a comprehensive manifest of the electronic files a donor has turned over for accession; the same hardware that allows the forensics investigator to create an algorithmically authenticated “image” of a file system allows the archivist to ensure the integrity of digital content once captured from its source media; the same data-recovery procedures that allow the specialist to discover, recover, and present as trial evidence an “erased” file may allow a scholar to reconstruct a lost or inadvertently deleted version of an electronic manuscript—and do so with enough confidence to stake reputation and career. 

Digital forensics therefore offers archivists, as well as an archive’s patrons, new tools, new methodologies, and new capabilities. Yet as even this brief description must suggest, <digital forensics does not affect archivists’ practices solely at the level of procedures and tools. <em>Its methods and outcomes raise important legal, ethical, and hermeneutical questions about the nature of the cultural record, the boundaries between public and private knowledge, and the roles and responsibilities of donor, archivist, and the public in a new technological era.
</digital></blockquote>

When collections are donated to an archive, there is usually a <a href="http://www.archivists.org/publications/deed_of_gift.asp">gift agreement</a> between the donor and the archival organization, which documents how the collection of material can be used. For example, it is fairly common for there to be a period where portions (or all) of the archive are kept dark. Much less often gift agreements can stipulate that the collection must be made open on the Web, and sometimes money can change hands. Born digital content in archives is <a href="https://scholarsphere.psu.edu/files/cn69m429f">new enough</a> that cultural heritage organizations are still grappling with the best way to talk to their donors about donating born digital content. 

There has been a bit of attention to sharing best practices about born digital content between organizations, and rising awareness about the sorts of issues that need to be considered. As a software developer tasked with building applications that can be used across these archival collections, the special-snowflake nature to these gift agreements has been a bit of annoyance. If every collection of born digital content has slightly different stipulations about what, when and how content can be used it makes building access applications difficult. The situation is compounded somewhat because the gift agreements themselves aren't shared publicly (at least at my place of work), so you don't even know what you can and can't do. I've observed that this has a tendency to derail conversations about access to born digital content--and access is an essential ingredient to insuring the long term preservation of digital content. It's not like you can take a digital file and put it on a server and come back in 25 or even 5 years and expect to open it, and use it.

So, what does this have to do with Zuckerman's post, and the intrinsic value of metadata to the NSA? When Zuckerman provided his students with access to his email archive he did it in the context of a particular trust scenario. A gift agreement in an archive serves the same purpose, by documenting a trust scenario between the donor and the institution that is receiving the gift. The NSA allegedly has been collecting information from Verizon, Facebook, Google, et al outside of the trust scenario provided by the <a href="https://en.wikipedia.org/wiki/Fourth_Amendment_to_the_United_States_Constitution">Fourth Amendment to the Constitution</a>. After looking at things this way, the special-snowflakism of gift agreements doesn't seem so annoying any more. It is through these agreements that cultural heritage organizations establish their authenticity and trust. And it is by them that they become a desirable place to deposit born digital content. If they have to be unique per-donor, and this hampers unified access to born digital collections, this seems like a price worth paying. Ideally there would be a standard set of considerations to use when putting the gift agreement together. But if we can't fit everyone into the same framework, maybe that's not such a bad thing.

The other common place thing that strikes me is that the same technology that can be used for good, say digital humanities research, or forensics discovery, can also be used for ill. Having a strong sense of the ethics, as a <a href="http://www2.archivists.org/statements/saa-core-values-statement-and-code-of-ethics">professional</a>, as a citizen, and as a human being is extremely important to establishing the context in which technology is used -- and negotiating between the three can sometimes require finesse, and in the case of Snowden, courage.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5781</wp:post_id>
		<wp:post_date>2013-07-10 09:59:27</wp:post_date>
		<wp:post_date_gmt>2013-07-10 16:59:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-snowden-and-archival-ethics</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="gift-agreements"><![CDATA[gift agreements]]></category>
		<category domain="category" nicename="government"><![CDATA[government]]></category>
		<category domain="post_tag" nicename="nsa"><![CDATA[nsa]]></category>
		<category domain="post_tag" nicename="snowden"><![CDATA[snowden]]></category>
		<category domain="post_tag" nicename="trust"><![CDATA[trust]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86352</wp:comment_id>
			<wp:comment_author><![CDATA[Facebook builds a tool to manage data on both facilities and servers | whatsweb]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://whatsweb.wordpress.com/2013/07/13/facebook-builds-a-tool-to-manage-data-on-both-facilities-and-servers/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-07-13 10:21:56</wp:comment_date>
			<wp:comment_date_gmt>2013-07-13 17:21:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Summers, Ed: On Snowden and Archival Ethics (inkdroid.org) [...]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1373736116.3839099407196044921875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1374195043.8057410717010498046875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Notes on Social Mobilization and the Networked Public Sphere</title>
		<link>http://inkdroid.org/2013/08/07/notes-on-social-mobilization-and-the-networked-public-sphere/</link>
		<pubDate>Wed, 07 Aug 2013 18:27:58 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5854</guid>
		<description></description>
		<content:encoded><![CDATA[If the ecosystem of the Web and graph visualizations are your thing, <a href="https://twitter.com/YochaiBenkler">Yochai Benkler</a> & co's recent <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2295953">Social Mobilization and the Networked Public Sphere: Mapping the SOPA-PIPA Debate</a> has lots of eye candy for you. I was going to include some here to entice you over there, but it doesn't seem to be available under <a href="https://www.ssrn.com/terms_of_use.html">terms</a> that would allow for that (sigh).

You probably remember how Wikipedia went dark to protest the <a href="https://en.wikipedia.org/wiki/Stop_Online_Piracy_Act">Stop Online Piracy Act</a>, and how phone bridges in legislative offices melted down. You may also have watched Aaron Swartz's <a href="http://www.democracynow.org/2013/1/14/freedom_to_connect_aaron_swartz_1986">moving talk</a> about the role that Demand Progress played in raising awareness about the legislation, and mobilizing people to act. If you haven't go watch it now. 

Benkler's article really digs in to take a close look at how the "attention backbone" provided by non-traditional media outlets allowed much smaller players to coordinate and cooperate across the usual political boundaries to disrupt and ultimately defeat some of the most powerful lobby groups and politicians in the United States. I'll risk a quote from the conclusion:

<blockquote>
Perhaps the high engagement of young, net-savvy individuals is only available for the politics of technology; perhaps copyright alone is sufficiently orthogonal to traditional party lines to traverse the left-right divide; perhaps Go Daddy is too easy a target for low-cost boycotts; perhaps all this will be easy to copy in the next cyber-astroturf campaign. Perhaps.

But perhaps SOPA-PIPA follows William Gibson’s “the future is already here—it’s just not very evenly distributed.” Perhaps, just as was the case with free software that preceded widespread adoption of peer production, the geeks are five years ahead of a curve that everyone else will follow. If so, then SOPA-PIPA provides us with a richly detailed window into a more decentral-ized democratic future, where citizens can come together to overcome some of the best-funded, best-connected lobbies in Washington, DC.
</blockquote>

Obviously, I don't know what the future holds. But I hope Benkler's hunch is right, and that we have just started to see how the Web can shift political debate into a much more interactive and productive mode. 

I've just come off of a stint of reading <a href="https://en.wikipedia.org/wiki/Bruno_Latour">Bruno Latour</a>, so the data driven network analysis and identification of human and non-human actors participating in a <em>controversy</em> that needed mapping really struck a chord with me. I felt like I wandered across an extremely practical, relevant and timely example of how Latour's ideas can be put into practice, in a domain I work with every day--the Web.

But I must admit the article was most interesting to me because of some of the technical aspects of how the work was performed. The appendix at the end of the paper provides a very lucid description of how they used a tool called <a href="http://www.mediacloud.org/">MediaCloud</a> to collect, house and analyze the data that formed the foundation of their study. MediaCloud is an open source project devloped at the Berkman Center and is available on <a href="https://github.com/berkmancenter/mediacloud">GitHub</a>. MediaCloud allows researchers to collect material from the Web, and then perform various types of analysis on it. Conceptually it's in a similar space as the (recently IMLS funded) <a href="https://github.com/gwu-libraries/social-feed-manager">Social Feed Manager</a> that <a href="http://twitter.com/dchud">Dan Chudnov</a> is working on at George Washington University, now with help from Yale, the University of North Texas and the Center for Jewish History. The appendix is an excellent picture of the sorts of questions we can expect social science researchers to ask of data collections--thankfully absent the distracting term <em>big data</em>. What's important is the questions they asked, and how they went about answering them -- not whether they were keyword compliant.

A few things about MediaCloud and its use that stood out for me:

<h3>RSS</h3>

Even after all these years, syndication technologies like RSS and Atom continue to be useful. Somewhat paradoxically the death of Google Reader and the increasing awareness of the importance of the <a href="http://indiewebcamp.com/why">indieweb</a> seem to have ushered in a sort of <a href="http://dashes.com/anil/2013/07/the-golden-age-of-rss.html">golden age of RSS</a>. Or at least their is increased awareness of its role. I think it's important for content providers to know that simple technologies like RSS still perform an important function in this age of fit to purpose Web APIs.

<h3>Time</h3>

Time is a very important dimension to doing this sort of research. Benkler's analysis hinges on the view of the graph <em>over time</em> as the controversy of the SOPA debate evolved. It's not just a picture of what the graph of content looks like afterwards. It's only when looking at it over time that the various actors are revealed. Getting the time out of RSS feeds wasn't so bad, but the lather-rinse-repeat nature to finding more stories and media outlets on the web, meant that they had to come up with some heuristics for determining where the content should be situated in time.

<h3>Density</h3>

Similarly not all content and links on pages is relevant for this sort of analysis. Advertisements and boilerplate headers/footers around content can often add unwanted noise to the analysis. The authors drew on some <a href="http://ai-depot.com/articles/the-easy-way-to-extract-useful-text-from-arbitrary-html/">research</a> from 2007 on <em>HTML density</em> to extract just the salient bits of content in the HTML representation. Services like Readability have <a href="http://stackoverflow.com/questions/3652657/what-algorithm-does-readability-use-for-extracting-text-from-urls">algorithms and heuristics</a> for doing similar things, but I hadn't heard it described so succinctly before as it was in Benkler's article. The more dense the HTML tags, the less likely it is that the text is the primary content of the page. Truth be told, I'm writing this post for my future self when I go looking for this technique. I think I found the <a href="https://github.com/berkmancenter/mediacloud/blob/master/lib/MediaWords/Crawler/AnalyzeLines.pm#L38
">MediaCloud function</a> for generating the HTML density. It might be nice to spin this out into a separate reusable piece...

<h3>Perl</h3>

Speaking of code, according to Github, MediaCloud is 85.9% Perl and 10.8% JavaScript, and was coded up mainly by <a href="https://twitter.com/dlarochelle">David Larouchelle</a>, <a href="http://wiki.pypt.lt/">Linas Valiukas</a>, <a href="http://blogs.law.harvard.edu/hroberts/">Hal Roberts</a>. There is a fair bit of text munging going on in MediaCloud, so I shouldn't be surprised to see Perl so heavily used...but I was. I don't reach for Perl much these days, but I imagine there's no shortage of demand in certain circles (nudge, nudge, wink, wink, say no more) for Perl hackers. <a href="https://en.wikipedia.org/wiki/Larry_Wall">Larry Wall</a> was a linguist, and Perl's roots are in this sort of linguistic analysis cooked together with the medium of the Web -- it's nice to see some constants in the software development world as it so often froths over with the latest tech trend or meme.

So, it was a good read. Highly recommended. 5 stars. <a href="https://cyber.law.harvard.edu/wealth_of_networks/Main_Page">Wealth of Networks</a> is now in my queue.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5854</wp:post_id>
		<wp:post_date>2013-08-07 11:27:58</wp:post_date>
		<wp:post_date_gmt>2013-08-07 18:27:58</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>notes-on-social-mobilization-and-the-networked-public-sphere</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="bruno-latour"><![CDATA[bruno latour]]></category>
		<category domain="post_tag" nicename="graphs"><![CDATA[graphs]]></category>
		<category domain="post_tag" nicename="networks"><![CDATA[networks]]></category>
		<category domain="post_tag" nicename="perl"><![CDATA[perl]]></category>
		<category domain="post_tag" nicename="politics"><![CDATA[politics]]></category>
		<category domain="post_tag" nicename="research"><![CDATA[research]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Yu Too?</title>
		<link>http://inkdroid.org/2013/08/07/yu-too/</link>
		<pubDate>Thu, 08 Aug 2013 02:18:24 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5887</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.goodreads.com/book/show/16041826-sorry-please-thank-you" style="float: left; padding-right: 20px"><img alt="Sorry Please Thank You: Stories" border="0" src="http://d202m5krfqbpi5.cloudfront.net/books/1361254827m/16041826.jpg" /></a><a href="http://www.goodreads.com/book/show/16041826-sorry-please-thank-you">Sorry Please Thank You: Stories</a> by <a href="http://www.goodreads.com/author/show/221608.Charles_Yu">Charles Yu</a><br />
My rating: <a href="http://www.goodreads.com/review/show/672342764">5 of 5 stars</a><br /><br />
These short stories were just awesome -- funny, smart, incisive, light, beautifully distracting. I must read more <a href="http://twitter.com/charles_yu">Yu</a> <em>now</em>. There were so many good stories in this collection: all of them hauntingly familiar and strangely different. I couldn't shake a feeling of synesthesia, like I was reading along to my favorite Radiohead songs -- elated, disoriented, integrated, fragmented, jettisoned, ordinary, cute and more than a bit scary. <br /><br />A quote from the story <em>Adult Contemporary</em>

<blockquote>Murray tries to see what Rick is talking about, but all he sees is a kind of factory. A manufacturing process for a way of life. Taking anything, experience, a piece of experimental stuff, a particle of particularity, a sound, a day, a song, a bunch of stuff that happens to people, a thing, that makes you laugh, a visual, a feeling, whatever. A mess. A blob. A chunk. A messy, blobby, chunky glob of stuff. Unformed, raw non-content that gets engineered, honed, and refined until some magical point where it has been processed to sufficient smoothness and can be extruded from the machine: content. A chunk of content, homogenous and perfect for slicing up into Content Units. All of this for the customer-citizens, who demand it, or not even demand it but come to expect it, or not even expect it, as that would require awareness of any alternative to the substitute, an understanding that this was not always so, that, once upon a time, there was the real thing. They don't demand it or expect it. They assume it. The product is not a product, it's built into the very notion of who they are. Content Units everywhere, all of it coming from the same source: jingles, news, ads. Ads, ads, ads. Ads running on every possible screen. Screens at the grocery store, in the coffee line, on the food truck, in your car, on top of taxis, on the sides of buses, in the air, on the street signs, in your office, in the lobby, in the elevator, in your pocket, in your home. Content pipelines productive as ever, churning and chugging, pumping out the content day and night, conceptual smokestacks billowing out content-manufacturing waste product emissions, marginal unit cost of content dropping every day, content just piling up, containers full, warehouses full, cargo ships full, the channels stuffed to bursting with content. So much content that they needed to make new markets just to find a place to put all of it, had to create the Town, and after that, another Town, and beyond that, who knew? What were the limits for American Entertainments Inc., and its managed-narrative experiential lifestyle products? How big could the Content Factory get?</blockquote>

I really just wanted to transcribe that--to type it in, and pretend that I wrote it. If typing is writing I guess I did write it. Yu's voice is infectious, and familiar -- like he's telling you things you already know, but in an interesting way you've never really considered before. I'll give you my copy if you want it. Just send me <a href="mailto:ehs@pobox.com">an email</a> with your mailing address in it. Seriously.

Also,

https://twitter.com/charles_yu/status/365653971139956737]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5887</wp:post_id>
		<wp:post_date>2013-08-07 19:18:24</wp:post_date>
		<wp:post_date_gmt>2013-08-08 02:18:24</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>yu-too</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="book-review"><![CDATA[book review]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_8f056a8088623759d93411d827c5b021</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="500"><p><a href="https://twitter.com/edsu">@edsu</a> see what you just did for us? We are connected now, you and me. That never stops being something worth appreciating. Thank you.</p>&mdash; Charles Yu (@charles_yu) <a href="https://twitter.com/charles_yu/statuses/365653971139956737">August 9, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_b036d25482e3015e523eeff60684bd21</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="474"><p><a href="https://twitter.com/edsu">@edsu</a> see what you just did for us? We are connected now, you and me. That never stops being something worth appreciating. Thank you.</p>&mdash; Charles Yu (@charles_yu) <a href="https://twitter.com/charles_yu/statuses/365653971139956737">August 9, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_7db17abf6742a3ed2d5f4751b649840c</wp:meta_key>
			<wp:meta_value><![CDATA[<blockquote class="twitter-tweet" width="550"><p><a href="https://twitter.com/edsu">@edsu</a> see what you just did for us? We are connected now, you and me. That never stops being something worth appreciating. Thank you.</p>&mdash; Charles Yu (@charles_yu) <a href="https://twitter.com/charles_yu/statuses/365653971139956737">August 9, 2013</a></blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>metadata from Getty&#039;s Open Content Program</title>
		<link>http://inkdroid.org/2013/08/13/metadata-from-gettys-open-content-program/</link>
		<pubDate>Tue, 13 Aug 2013 13:52:56 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5906</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://www.getty.edu/art/gettyguide/artObjectDetails?artobj=335657"><img style="float: left; margin-right: 10px;" width="100" src="http://www.getty.edu/art/collections/images/l/33565701.jpg"></a>
<a href="http://loribyrdphillips.com/">Lori Phillips</a> recently <a href="http://lists.okfn.org/pipermail/open-glam/2013-August/000693.html">mentioned</a> on the open-glam discussion list that Getty are <a href="http://blogs.getty.edu/iris/open-content-an-idea-whose-time-has-come/">starting</a> to make high-resolution images of some of their public domain material available as part of their <a href="http://www.getty.edu/about/opencontent.html">Open Content Program</a>. The announcement mentions that metadata is included in each file, so I thought I'd take a look. It's <a href="https://www.adobe.com/products/xmp/">Adobe Extensible Media Platform (XMP)</a> aka RDF. Here's an example I pulled from <a href="http://www.getty.edu/art/gettyguide/artObjectDetails?artobj=335657">The Portrait of Madame Brunet</a> (I apologize if reading RDF as <a href="http://www.w3.org/TeamSubmission/turtle/">Turtle</a> burns your eyes, but I find it much easier to read than the equivalent RDF/XML).

<pre lang="n3">
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix Iptc4xmpCore: <http://iptc.org/std/Iptc4xmpCore/1.0/xmlns/> .
@prefix Iptc4xmpExt: <http://iptc.org/std/Iptc4xmpExt/2008-02-29/> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .
@prefix photoshop: <http://ns.adobe.com/photoshop/1.0/> .
@prefix xmpRights: <http://ns.adobe.com/xap/1.0/rights/> .

<https://d2hiq5kf5j4p5h.cloudfront.net/33565701.jpg>
    Iptc4xmpCore:CreatorContactInfo [
        Iptc4xmpCore:CiAdrCity "Los Angeles" ;
        Iptc4xmpCore:CiAdrCtry "United States" ;
        Iptc4xmpCore:CiAdrExtadr "1200 Getty Center Drive" ;
        Iptc4xmpCore:CiAdrPcode "90049" ;
        Iptc4xmpCore:CiAdrRegion "California" ;
        Iptc4xmpCore:CiEmailWork "rights@getty.edu" ;
        Iptc4xmpCore:CiUrlWork "www.getty.edu"
    ] ;
    Iptc4xmpExt:ArtworkOrObject [
        a rdf:Bag ;
        rdf:_1 [
            Iptc4xmpExt:AOCreator [
                a rdf:Seq ;
                rdf:_1 "Édouard Manet"
            ] ;
            Iptc4xmpExt:AOSource "The J. Paul Getty Museum, Los Angeles" ;
            Iptc4xmpExt:AOSourceInvNo "2011.53" ;
            Iptc4xmpExt:AOTitle [
                a rdf:Alt ;
                rdf:_1 "Portrait of Madame Brunet"@x-default
            ]
        ] 
    ] ;
    photoshop:Source "The J. Paul Getty Museum, Los Angeles" ;
    xmpRights:UsageTerms [
        a rdf:Alt ;
        rdf:_1 "http://www.getty.edu/legal/image_request/"@x-default
    ] ;
    dc:creator [
        a rdf:Seq ;
        rdf:_1 "The J. Paul Getty Museum" 
    ] ;
    dc:date [
        a rdf:Seq ;
        rdf:_1 "2013-06-30T15:14:52"
    ] ;
    dc:description [
        a rdf:Alt ;
        rdf:_1 "Portrait of Madame Brunet; Édouard Manet, French, 1832 - 1883; France, Europe; about 1860 -1863, reworked by 1867; Oil on canvas; Unframed: 132.4 x 100 cm (52 1/8 x 39 3/8 in.), Framed: 153.7 x 121.9 x 7.6 cm (60 1/2 x 48 x 3 in.); 2011.53"@x-default
    ] ;
    dc:title [
        a rdf:Alt ;
        rdf:_1 "Portrait of Madame Brunet"@x-default
    ] .
</pre>

The description is kind of heavy on information about the Getty, but light on information about the painting. For example it's not clear from the metadata that this is even a painting. You can see from the HTML detail page that there is a fair bit more about it available:

<pre>
Édouard Manet (French, 1832 - 1883)
Portrait of Madame Brunet, about 1860 -1863, reworked by 1867, Oil on canvas
Unframed: 132.4 x 100 cm (52 1/8 x 39 3/8 in.)
Framed: 153.7 x 121.9 x 7.6 cm (60 1/2 x 48 x 3 in.)
The J. Paul Getty Museum, Los Angeles
</pre>

Still, it's an incredible step forward to see these high resolution images being made available on the Web for download.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5906</wp:post_id>
		<wp:post_date>2013-08-13 06:52:56</wp:post_date>
		<wp:post_date_gmt>2013-08-13 13:52:56</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>metadata-from-gettys-open-content-program</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Fresh Data</title>
		<link>http://inkdroid.org/2013/08/23/fresh-data/</link>
		<pubDate>Fri, 23 Aug 2013 13:30:36 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5963</guid>
		<description></description>
		<content:encoded><![CDATA[In his talk <a href="http://minds.wisconsin.edu/handle/1793/44118">Secrecy, Archives and the Public Interest</a> in 1970 <a href="https://en.wikipedia.org/wiki/Howard_Zinn">Howard Zinn</a> famously challenged professional archivists to realize the role of politics in their work. His talk included 7 points of criticism, which are still so relevant today, but the last two really moved me to transcribe and briefly comment on them here:

<blockquote>
6. That the emphasis is on the <strong>past over the present</strong>, on the antiquarian over the contemporary; on the non-controversial over the controversial; the cold over the hot. What about the transcripts of trials? Shouldn't these be made easily available to the public? Not just important trials like the <a href="https://en.wikipedia.org/wiki/Chicago_Seven">Chicago Conspiracy Trial</a> I referred to, but the ordinary trials of ordinary persons, an important part of the record of our society. Even the extraordinary trials of extraordinary persons are not available, but perhaps they do not show our society at its best. The trial of the <a href="https://en.wikipedia.org/wiki/Catonsville_Nine">Catonsville 9</a> would be lost to us if <a href="https://en.wikipedia.org/wiki/Daniel_Berrigan">Father Daniel Berrigan</a> had not gone through the transcript and written a play based on it.

7. That far more resources are devoted to the collection and preservation of what already exists as records, than to recording <strong>fresh data</strong>: I would guess that more energy and money is going for the collection and publication of the Papers of John Adams than for recording the experiences of soldiers on the battlefront in Vietnam. Where are the interviews of <a href="https://en.wikipedia.org/wiki/Seymour_Hersh">Seymour Hersh</a> with those involved in the <a href="https://en.wikipedia.org/wiki/My_Lai_Massacre">My Lai Massacre</a>, or <a href="https://en.wikipedia.org/wiki/Fred_Gardner_(activist)">Fred Gardner</a>'s interviews with those involved in the <a href="https://en.wikipedia.org/wiki/Presidio_mutiny">Presidio Mutiny Trial</a> in California, or <a href="https://en.wikipedia.org/wiki/Wallace_Terry">Wallace Terry</a>'s interviews with black GIs in Vietnam? Where are the recorded experiences of the young Americans in Southeast Asia who quit the <a href="https://en.wikipedia.org/wiki/International_Voluntary_Services">International Volunteer Service</a> in protest of American policy there, or of the Foreign Service officers who have quietly left?
</blockquote>

What if Zinn were to ask archivists today about contemporary events? While the situation is far from perfect, the Web has allowed pheomena like <a href="http://wikipedia.org">Wikipedia</a>, <a href="http://wikileaks.org">Wikileaks</a>, the <a href="https://pressfreedomfoundation.org/bradley-manning-transcripts">Freedom of the Press Foundation</a> and many, many others, to emerge, and substantially level the playing field in ways that we are still grappling with. The Web has widened, deepened and amplified traditional journalism. Indeed, electronic communication media like the Web have copying and distribution cooked into their very essence, and make it almost effortless to share information. <em>Fresh data</em>, as Zinn presciently calls it, is what the Web is about; and the Internet that the Web is built on allows us to largely route around power interests...except, of course, when it <a href="http://www.nytimes.com/2011/10/25/world/europe/blocks-on-wikileaks-donations-may-force-its-end-julian-assange-warns.html?_r=0">doesn't</a>. 

Strangely, I think if Zinn were talking to archivists today he would be asking them to think seriously about where this content will be in 20 years--or maybe even one year. How do we work together as professionals to collect the stuff that needs saving? The <a href="http://archive.org">Internet Archive</a> is awesome...it's simply amazing what such a small group of smart people have been able to do. But this is a heavy weight for them to bear alone, and lots of copies keeps stuff safe right? Where are the copies? Yes there is the <a href="http://netpreserve.org/">IIPC</a>, but can we just assume this job is just being taken care of? What web content is being collected? How do we decide what is collected? How do we share our decisions with others so that interested parties can fill in gaps they are interested in? Maybe I'm just not in the know, but it seems like there's a lot of (potentially fun) work to do.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5963</wp:post_id>
		<wp:post_date>2013-08-23 06:30:36</wp:post_date>
		<wp:post_date_gmt>2013-08-23 13:30:36</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>fresh-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[history]]></category>
		<category domain="post_tag" nicename="howard-zinn"><![CDATA[howard zinn]]></category>
		<category domain="post_tag" nicename="internet-archive"><![CDATA[internet archive]]></category>
		<category domain="post_tag" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86521</wp:comment_id>
			<wp:comment_author><![CDATA[kcoyle]]></wp:comment_author>
			<wp:comment_author_email>kcoyle@kcoyle.net</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-08-25 18:07:08</wp:comment_date>
			<wp:comment_date_gmt>2013-08-26 01:07:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ed, I have often thought about the vast number of photos and videos that have been taken during the recent wars in Iraq and Afghanistan. Nearly every soldier was documenting that war. And documenting it in ways that official sources do not: photos of their living quarters, odd sights by the side of the road, people they met. Some soldiers blogged. Most sent email to friends and family. I hoped that someone (that famed someone which means not me but someone else) would put out a call for submissions to an archive of the personal view of the war. I haven't seen that. Of course, whoever did that would be overwhelmed, but look at how important war time photography and records have been in helping us understand the wars of the past, even when those resources were limited (Mathew Brady in the Civil War). We would all have a better perspective on the contemporary wars if such an archive existed.]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>395</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1377479229.070024013519287109375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:6:"kcoyle";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1377507735.304750919342041015625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Gendered Archivist</title>
		<link>http://inkdroid.org/2013/08/26/gendered-archivist/</link>
		<pubDate>Mon, 26 Aug 2013 09:39:00 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=5996</guid>
		<description></description>
		<content:encoded><![CDATA[Over the past few years I've been trying to deepen my understanding of the literature of and about archives. My own MLIS education was heavy on libraries and light on archives; so I was really quite unaware of how rich the thinking about archives is...and how much more relevant it is for the work of digital preservation. 

After not being a member of any professional organization for over ten years I joined the <a href="http://www.archivists.org/">Society of American Archivists </a>two years ago. I really enjoyed when the SAA's quarterly <a href="http://archivists.metapress.com/home/main.mpx">American Archivist</a> started showing up in my mailbox. Incidentally they have put all their content online for the public, but keep the last 3 years embargoed for SAA members only. 

Since I have so much catching up to do I thought it would be interesting to try to harvest some of the article metadata that could be gleaned from the website, to see if I could get my computer to teach me something about the 76 years of content. If you are interested you can find some code I wrote to do this, and the resulting metadata about the 42,432 articles on <a href="http://github.com/edsu/americanarchivist">Github</a>.

As a quick test I thought it would be interesting to throw the first names of authors through <a href="https://github.com/bmuller/genderator">genderator</a> to see if the gender of authors has changed over time. My first pass just displays the number of authors per year by their gender.

<img src="http://inkdroid.org/images/americanarchivist-chart1.png">

Since the number of authors per article isn't constant, and the number of articles per year is also variable the graph is a bit noisy. But if you calculate the percentage of authors per year that were male, female or unknown you get a much smoother graph.

<img src="http://inkdroid.org/images/americanarchivist-chart2.png">

As you can see genderator isn't perfect: sometimes it can't even guess the author's gender 20% of the time. But even with that noise it's clear to see a gradual increase in the number of women authors, which begins in 1970s and is continuing even to today, where women seem to be represented more than men ... although it's a bit too choppy to tell really.

If you are interested in using this data let me know. I have the publicly available PDF content in an s3 bucket if you have research you'd like to do on it.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5996</wp:post_id>
		<wp:post_date>2013-08-26 02:39:00</wp:post_date>
		<wp:post_date_gmt>2013-08-26 09:39:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>gendered-archivist</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="gender"><![CDATA[gender]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="society-of-american-archivists"><![CDATA[society of american archivists]]></category>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[american-archivist-and-gender]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86681</wp:comment_id>
			<wp:comment_author><![CDATA[My Blog on Professional Blogs | jrynicki]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://jrynicki.wordpress.com/2013/11/23/my-blog-on-professional-blogs/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-11-30 11:43:12</wp:comment_date>
			<wp:comment_date_gmt>2013-11-30 18:43:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Summers, Ed (Aug. 26th 2013). Inkdroid: “Gendered Archivist” [Blog]. Retrieved from http://inkdroid.org/2013/08/26/gendered-archivist/ [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1385836993.037108898162841796875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1385867699.998940944671630859375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Wolfram</title>
		<link>http://inkdroid.org/2013/09/09/wolfram/</link>
		<pubDate>Mon, 09 Sep 2013 15:52:34 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6016</guid>
		<description></description>
		<content:encoded><![CDATA[<div style="margin-right: 20px; width: 200px; float: left; text-align: center; font-size: 10pt; border: thin solid #cccccc;">
  <a href="https://commons.wikimedia.org/wiki/File:The_side_of_the_Pompidou_Center_(8423437535).jpg"><img src="http://inkdroid.org/images/pompidou.jpg" /></a><br /><a href="https://commons.wikimedia.org/wiki/File:The_side_of_the_Pompidou_Center_(8423437535).jpg">The Side of Pompidou Center</a><br />by Frank Kovalchek
</div>

<p>Last week Stephen Wolfram opened the <a href="http://www.loc.gov/today/pr/2013/13-154.html">FEDLINK Innovation Talk</a> series at the Library of Congress. Wolfram is truly an intellectual maverick (check out his <a href="https://en.wikipedia.org/wiki/Stephen_Wolfram">Wikipedia page</a>) and kind of an archetype for what is typically meant by the word genius. I haven't read <a href="http://www.wolframscience.com/">A New Kind of Science</a>, or used <a href="http://wolframalpha.com">Wolfram Alpha</a> or <a href="http://www.wolfram.com/mathematica/">Mathematica</a> very much. Perhaps if I knew more of the details behind his thinking I wouldn't have left the talk feeling a little bit disappointed.</p>

<p>I would have liked to hear him reflect on what motivated him to spend 25 years building a platform to make "knowledge computable". He clearly had a vision of this work, and it would've been fun to hear about it -- and where he sees this type of work in 25 years. Perhaps some discussion of whether there were boundaries to making knowledge computable, and if knowledge itself can be thought of without human intention as part of the equation. It would also have been interesting to hear a few more technical details about the platform itself and how it is orchestrated. Maybe I wasn't listening closely enough, but what we got instead was a lot of pointing, clicking, typing and talking to Wolfram Alpha: showing off how it could answer questions like a good reference librarian--and some quite funny jokes along the way.</p>

<p>But, to be fair, he did mention a few interesting things, especially during the (too brief) question and answer period at the end. Here are the things I took mental note of, that I still remember a week later.</p>

<p>I hadn't heard of the <a href="https://www.wolfram.com/cdf/">Computable Document Format</a> before, but I guess it's been around for a few years. CDF is Wolfram's own custom file format that makes data visualizations interactive.</p>

<blockquote>
  <p>... the CDF standard is a computation-powered knowledge container—as everyday as a document, but as interactive as an app.</p>
</blockquote>

<p>It seems to work as a browser plugin that bridges to Mathematica. One nice side effect of CDF is that it makes the underlying data available. Another side effect is that any CDF document that is composed with FreeCDF is <a href="https://www.wolfram.com/cdf/adopting-cdf/licensing-options.html">automatically licensed</a> CC-BY-SA. You can also pay for EnterpriseCDF, which then provides more licensing options, as well as the ability to add what looks like DRM to CDF documents. The documentation talks about it being a standard, and <a href="https://en.wikipedia.org/wiki/Computable_Document_Format">Wikipedia</a> says it has an assigned MIME type (application/cdf), but I can't seem to find a specification for it, or even a registration of the mime type at the IETF. Considering the level of interactivity that documents have on the Web now, with open tools like D3 that sit on top of features from HTML 5 and JavaScript it's hard to get terribly excited about CDF.</p>

<p>Wolfram also mentioned work on something he called the Wolfram Data Format. I can't seem to find much information about it on the Web. It sounded like something akin to Resource Description Format, for describing entities and their attributes, and relations ... and seemed to primarily be used for getting data into and out of the knowledgebase that Wolfram Alpha sits on top of. During the Q/A session someone asked about Wolfram's views on Linked Data, and he knew enough about it to say that RDF wasn't expressive enough for his needs. He wasn't terribly clear on how it wasn't expressive enough: I remember an example about needing to express the position of Mercury and Venus at various points in a concise way. In my experience I've found that RDF gave me plenty of rope.</p>

<p>There is a Pro version of Wolfram Alpha that lets you do<a href="http://www.wolframalpha.com/pro/"> a variety of things</a> you can't do in the free version. The most interesting one of these is that it lets you upload your own data in <a href="http://www.wolframalpha.com/input/pro/uploadexamples/">a bunch</a> of different formats for analysis by Alpha. Presumably this data could be added to the Wolfram Alpha knowledgebase, and help form what Wolfram called the Wolfram Repository.</p>

<p>The R word is pretty charged at my place of work, and I imagine it might be at yours too. Collectively, many dollars have been spent creating systems, certification guidelines, and research about what the <em>digital repository</em> might be. As with many e-this and e-that words, the word <em>digital</em> doesn't really add so much to the meaning of digital repository as <em>repository</em> does. Wolfram Alpha defines repository as:</p>

<blockquote>
  <p>A facility where things can be deposited for storage and safekeeping</p>
</blockquote>

<p>Historically, repositories of knowledge have been found in the form of libraries, archives and museums that are sometimes part of larger institutions like schools, universities, societies, governments, businesses, or personal collections. So Wolfram wants the research community to use Wolfram Alpha as a repository. The carrot here is that data that is uploaded to the Wolfram Repository will be directly citable in CDF documents. During his response about Linked Data, Wolfram commented on how often URLs break, and how they weren't suitable for linking papers to their underlying data. The solution that he seemed to propose is that data would be citable as long as writers used his document format, editing tools, and repository.</p>

<p>Indeed, when asked about the role of libraries and the library profession Wolfram responded saying that in his view the role of the librarian will be to help educate people who have data, to help make it computable, by massaging it into the correct format. What he didn't say (but I heard) was that the correct format was WDF, and that it would be made computable by pushing it into the Wolfram Alpha data repository.</p>

<p>Don't get me wrong, I think his vision of a future for libraries that help researchers work with data is a compelling one. It's an extension of a trend over the past 10-15 years where libraries have built statistical, textual or geographic data collections, that are made available with educational services around them. Certainly getting data into and out of Wolfram Alpha, and making it citable by CDF documents could be a component of this work.</p>

<p>But what was missing from Wolfram's presentation was a vision for how we build data repositories collaboratively, across cultural, corporate and socio-political borders. There were glimpses of an amazing system that he has built, with algorithms and meta-algorithms for choosing them ... but it wasn't clear how to add your own algorithms, to introspect on the decisions that were being made, and see the sources of data that were used in its computations.</p>

<p>Above all, I didn't hear Wolfram describe how his platform includes the Web as an essential part of its architecture. I know I'm biased towards the Web, but Tim Berners-Lee's enduring insight is that the design of the Web needed (and still needs) to be open. Sometimes open systems can seem ugly (hence the picture of the <a href="https://en.wikipedia.org/wiki/Centre_Georges_Pompidou">Pompidou Center</a> above) since they show you the guts of things. Occasionally things can get nasty when parties have opposing interests. But it's extremely important to try. How do we build a future where libraries, archives and museums collect locally and build repositories of data for systems like Wolfram Alpha, Wikipedia, Google's Knowledge Graph, Facebooks OpenGraph in a sustainable way? Libraries aren't well-equipped to build these types of systems themselves, the state of the art is always changing. But these institutions ought to be in a good position to serve as trusted partners, tied to the interests of particular knowledge communities, that can help make data available to the systems like Wolfram Alpha.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6016</wp:post_id>
		<wp:post_date>2013-09-09 08:52:34</wp:post_date>
		<wp:post_date_gmt>2013-09-09 15:52:34</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>wolfram</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="stephen-wolfram-wolfram-alpha-repositories-data-web"><![CDATA[stephen wolfram; wolfram alpha; repositories; data; web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_post_restored_from</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:20:"restored_revision_id";i:6020;s:16:"restored_by_user";i:2;s:13:"restored_time";i:1378459691;}]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>watermark woodcut indigo octavo</title>
		<link>http://inkdroid.org/2013/09/11/watermark-woodcut-indigo-octavo/</link>
		<pubDate>Wed, 11 Sep 2013 18:43:18 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6066</guid>
		<description></description>
		<content:encoded><![CDATA[<p>You know how sometimes you can get ideas for a subject you are interested in by studying a different but related subject? So, strangely enough, I've found myself reading about paper conservation. Specifically, at the moment, a book called <a href="http://lccn.loc.gov/81186656"><em>Books and Documents: dating, permanence and preservation</em></a> by Julius Grant. It was printed in 1937, so I guess a lot of the material is <em>dated</em> now (haha)...but somehow it's only making it that much more interesting to read.</p>

<p>There are long sections detailing experiments on paper and ink to determine their composition, in order to roughly estimate when a document was likely to have been created. On pages 41-44 he provides a list of supplementary evidence that can be used.</p>

<blockquote>
  <p>There are of course many other minor sources of evidence which may prove helpful in establishing the date of a book or document, but to discuss them all in full detail would bring this volume outside its professed scope. It has, however, been thought desirable to summarize the more important of them in the form of a chronological table, which may be used in conjunction with the information on paper and ink already provided.</p>
</blockquote>

<p>The list was so delightful, and oddly thought provoking, that I took the time to transcribe it below. I randomly linked some of the terms and names to Wikipedia to ensure you get completely lost.</p>

<p><strong>Seventh century.</strong> The first bound books and introduction of <a href="https://en.wikipedia.org/wiki/Quill_pen">quill pens</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/863">863</a>.</strong> The oldest printed book known (printed from blocks by Wang Chieh of Kansau, China).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1020">1020</a>.</strong> Beginning of the gradual transition from carbon to <a href="https://en.wikipedia.org/wiki/Iron_gall_ink">iron-gall</a> writing inks.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1282">1282</a>.</strong> The earliest known <a href="https://en.wikipedia.org/wiki/Watermark">watermark</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1307">1307</a>.</strong> Names of paper-makers first incorporated into watermarks.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1341">1341</a>.</strong> Invention of printing from <a href="https://en.wikipedia.org/wiki/Movable_type">movable type</a> (by <a href="https://en.wikipedia.org/wiki/Bi_Sheng">Pi Sheng</a>) in China.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1400">1400</a> (circ.).</strong> Introduction of alum-tanned white pig-skin bindings.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1440">1440</a> (circ.).</strong> Invention of printing from movable type in Europe (<a href="https://en.wikipedia.org/wiki/Johann_Gutenberg">Johann Gutenberg</a>, Mainz).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1445">1445</a>-<a href="https://en.wikipedia.org/wiki/1500">1500</a>.</strong> Alternate light and dark striations in the look-through of paper due to construction of the mould.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1454">1454</a>.</strong> The first dated publication produced with movable type.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1457">1457</a>.</strong> The first book bearing the name of the printer.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1461">1461</a>.</strong> The first illustrated book (crude <a href="https://en.wikipedia.org/wiki/Woodcuts">woodcuts</a>).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1463">1463</a>.</strong> The first book with a <a href="https://en.wikipedia.org/wiki/Title-page">title-page</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1465">1465</a>.</strong> The earliest <a href="https://en.wikipedia.org/wiki/Blotting_paper">blotting paper</a> (<em>vide infra</em>, 1800) ; this is sometimes found in old books and manuscripts and its presence may help to date them, although of course, the blotting paper may have been inserted subsequently to the date of origin.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1470">1470</a> (circ.).</strong> Great increase in the number of bound books produced, following the advent of printing ; vellum and leather used principally.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1470">1470</a>.</strong> The first book with <a href="https://en.wikipedia.org/wiki/Pagination">pagination</a> and <a href="https://en.wikipedia.org/wiki/Headlines">headlines</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1472">1472</a>.</strong> The first book bearing printed signatures to serve as a guide to the binder.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1474">1474</a>.</strong> The first book published in English (by <a href="https://en.wikipedia.org/wiki/William_Caxton">William Caxton</a>, in Bruges).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1476">1476</a>.</strong> The first work printed in England (by William Caxton).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1483">1483</a>.</strong> The first double watermark.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1500">1500</a>.</strong> Introduction of the small <a href="https://en.wikipedia.org/wiki/Octavo">octavo</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1500">1500</a>.</strong> Introduction of <a href="https://en.wikipedia.org/wiki/Italics">Italics</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1536">1536</a>.</strong> The first book printed in America.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1545">1545</a> (circ.).</strong> Introduction of custom of using italics only for emphasis. Mineral oil and rosin first used in printing inks.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1560">1560</a>.</strong> Introduction of the <a href="https://en.wikipedia.org/wiki/Sextodecimo">sexto decimo</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1570">1570</a>.</strong> Introduction of the I2mo.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1570">1570</a>.</strong> Introduction of thin papers.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1575">1575</a> (circ.).</strong> The first gold-tooling.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1580">1580</a>.</strong> Introduction of the modern forms of "i," "j," "u," and "v."</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1580">1580</a>. (circ.).</strong> The first pasteboards.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1600">1600</a> (circ.).</strong> Copper-plate illustration sufficiently perfected to replace crude woodcuts. Introduction of red morocco bindings.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1650">1650</a>.</strong> Wood covers (covered with silk, plush or tapestry) used for binding.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1670">1670</a>.</strong> Introduction of the <a href="https://en.wikipedia.org/wiki/Hollander_beater">hollander</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1720">1720</a>.</strong> Perfection of the <a href="https://en.wikipedia.org/wiki/Vignette_(graphic_design)">vignette</a> illustration.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1734">1734</a>.</strong> <a href="https://en.wikipedia.org/wiki/Caslon">Caslon</a> type introduced.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1750">1750</a> (circ.).</strong> The first coth-backed paper (used only for maps).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1750">1750</a> (circ.).</strong> Gradual disappearance of vellum for binding and introduction of millboard covered with calf; or half-covered with leather and half with marbled paper, etc. The first wove paper (<a href="https://en.wikipedia.org/wiki/John_Baskerville">Baskerville</a>).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1763">1763</a>.</strong> <a href="https://en.wikipedia.org/wiki/Logwood">Logwood</a> inks probably first introduced.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1770">1770</a>.</strong> <a href="https://en.wikipedia.org/wiki/Indigofera">Indigo</a> first used in inks (Eisler).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1780">1780</a>.</strong> <a href="https://en.wikipedia.org/wiki/Birmingham_pen_trade">Steel pens</a> invented.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1796">1796</a>.</strong> The first <a href="https://en.wikipedia.org/wiki/Lithograph">lithographic</a> machine.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1796">1796</a>.</strong> The first embossed binding.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1800">1800</a>.</strong> Blotting-paper in general use (<em>vide supra</em>, 1465) in England, following an accidental rediscovery at Hagbourne, <a href="https://en.wikipedia.org/wiki/Berkshire">Berkshire</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1803">1803</a> (circ.).</strong> Metal pens first placed on the market.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1816">1816</a> (circ.).</strong> Coloured inks first manufactured in England using pigments.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1820">1820</a> (circ.).</strong> Linen-canvas first used instead of parchment to hold the back of the book into the cover. Introduction of straight-grained red morocco bindings (see 1600).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1820">1820</a>.</strong> The invention of modern type of metal <a href="https://en.wikipedia.org/wiki/Nib_(pen)">nib</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1825">1825</a>.</strong> The first permanent photographic image (<a href="https://en.wikipedia.org/wiki/Nic%C3%A9phore_Ni%C3%A9pce">Niepce</a>).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1830">1830</a> (circ.).</strong> The first <a href="https://en.wikipedia.org/wiki/Linen">linen</a> cover. Beginning of the era of poor leather bindings which have since deteriorated.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1830">1830</a>.</strong> Title printed on paper labels which were stuck on the cloth for the first time.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1835">1835</a> (circ.).</strong> Decoration by machinery introduced.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1836">1836</a>.</strong> Introduction of iron-gall inks containing indigo (Stephens).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1839">1839</a>.</strong> Invention of photography (<a href="https://en.wikipedia.org/wiki/Louis_Daguerre">Daguerre</a>).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1840">1840</a>.</strong> Titles first stamped on cloth.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1845">1845</a>.</strong> Linen board cover in common use. At about this time it became usual to trim the edges of books, and the practice of binding in quarter-leather declined.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1852">1852</a>.</strong> Invention of <a href="https://en.wikipedia.org/wiki/Photogravure">photogravure</a>, leading to the development of lithographic etchings, colour prints, line engravings, etc. (Fox-Talbot).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1855">1855</a>.</strong> <a href="https://en.wikipedia.org/wiki/Cotton">Cotton</a> first used as a cover for binding-boards.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1856">1856</a>.</strong> Discovery of the first coal-tar dyestuff (<a href="https://en.wikipedia.org/wiki/Perkin%27s_mauve">Perkin's mauve</a>), leading to the use of such dyestuffs in coloured inks.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1860">1860</a>.</strong> Beginning of the custom of paring calf binding leathers to the thickness of paper.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1861">1861</a>.</strong> Introduction of synthetic indigo for inks.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1878">1878</a>.</strong> Invention of the <a href="https://en.wikipedia.org/wiki/Fountain_pen">stylographic pen</a>.</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1885">1885</a>.</strong> Invention of the half-tone process (<a href="https://en.wikipedia.org/wiki/Frederic_Eugene_Ives">F. E. Ives</a>).</p>

<p><strong><a href="https://en.wikipedia.org/wiki/1905">1905</a>.</strong> The first <a href="https://en.wikipedia.org/wiki/Offset_printing">offset litho press</a>.</p>

<p>More about the other subject later ...</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6066</wp:post_id>
		<wp:post_date>2013-09-11 11:43:18</wp:post_date>
		<wp:post_date_gmt>2013-09-11 18:43:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>watermark-woodcut-indigo-octavo</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="conservation"><![CDATA[conservation]]></category>
		<category domain="post_tag" nicename="ink"><![CDATA[ink]]></category>
		<category domain="post_tag" nicename="paper"><![CDATA[paper]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>metadata from Getty’s Open Content Program (part 2)</title>
		<link>http://inkdroid.org/2013/09/13/metadata-from-gettys-open-content-program-part-2/</link>
		<pubDate>Fri, 13 Sep 2013 17:16:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6097</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A few weeks ago I wrote <a href="http://inkdroid.org/2013/08/13/metadata-from-gettys-open-content-program/">a brief post</a> about the embedded metadata found in images from the (awesome) <a href="http://www.getty.edu/about/opencontent.html">Getty Open Content Program</a>. This led to a useful exchange with <a href="https://twitter.com/bpod">Brenda Podemski</a> on Twitter, which she gathered together with <a href="http://storify.com/bpod/getty-open-content-and-embedded-metadata">Storify</a>. I promised her I would write another blog post that showed how the metadata could be expressed a little bit better.</p>

<p>It's hard to read RDF as XML and Turtle isn't for everyone, so here's a picture of part of the XMP RDF metadata that is included in the highres download for a <a href="http://search.getty.edu/museum/records/musobject?objectid=70019">photo</a> by <a href="https://en.wikipedia.org/wiki/Eug%C3%A8ne_Atget">Eugène Atget</a> of a sculpture <a href="https://fr.wikipedia.org/wiki/Bosquet_de_l'Arc-de-Triomphe">Bosquet de l'Arc de Triomphe</a> by <a href="https://en.wikipedia.org/wiki/Jean-Baptiste_Tuby">Jean-Baptiste Tuby</a>. I haven't portrayed everything in the file since it would clutter up the point I'm trying to make.</p>

<p><img src="http://inkdroid.org/images/getty1.png" alt="Original Description" /></p>

<p>Depicted here are two resources described in the RDF, the JPEG file itself and what the IPTC vocabulary calls an <a href="http://www.iptc.org/std/photometadata/documentation/GenericGuidelines/index.htm#!Documents/iptcextensionartworkorobjectintheimagesection.htm">Artwork or Object</a>. Now, it is good that the description distinguishes between the file and the photograph. The Dublin Core somewhat famously (in metadata circles) call this the <a href="http://wiki.dublincore.org/index.php/Glossary/One-to-One_Principle">One-To-One Principle</a>. But notice how there is a <em>dc:description</em> attached to the file resource with lots of useful information concatenated together as a string? My question to Brenda was whether that string was actually available as structured data, and could it be expressed differently? Her <a href="https://gist.github.com/bepod/6288070">response</a> seemed to indicate that it was.</p>

<p>My suggestion is to unpack and move that concatenated string to describe the photograph, like so:</p>

<p><img src="http://inkdroid.org/images/getty2.png" alt="Unpacked description" /></p>

<p>Notice how the dimensions, format, type and were broken out into separate assertions about the photograph? I also quickly modified the description to use the Dublin Core vocabulary since it was more familiar to me. I wasn't able to quickly find good properties for height and width, but I imagine they are out there somewhere, and if not there could be.</p>

<p>Of course, one could go further, and say there are really three resources: the file, the photograph, and the sculpture.</p>

<p><img src="http://inkdroid.org/images/getty3.png" alt="Added Sculpture" /></p>

<p>But this could be extra work for the Getty, if they don't have this level of description yet. The half-step of enriching the description by indicating that it is a photograph of particular dimensions in a particular format seems like a useful thing to do for this example though, especially if they have that structured data already. My particular vocabulary choices (dc, foaf, etc) aren't important compared to hanging the descriptions off of the right resources.</p>

<p>But, and this is a <em>doozy</em> of a but, it looks like from other metadata in the RDF that the metadata is being input with Photoshop. So while it is technically possible to embed this metadata in XMP as RDF, it is quite likely that Photoshop doesn't give you the ability to enter it. In fact, it is fairly common for some image processing applications to strip parts or all of the embedded metadata. So to embed these richer descriptions into the files one might need to write a small program to do it.</p>

<p>There is another place where the metadata could be embedded though. What if <a href="http://search.getty.edu/museum/records/musobject?objectid=70019">the webpage</a> for the item had embedded RDFa or Microdata in it that expressed this information? If they could commit to a stable URL for the item it would be a perfect place for both the human and machine readable description. All they would have to do would be to link the XMP metadata to it somehow, and adjust the templates they are using that drive the HTML display.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6097</wp:post_id>
		<wp:post_date>2013-09-13 10:16:07</wp:post_date>
		<wp:post_date_gmt>2013-09-13 17:16:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>metadata-from-gettys-open-content-program-part-2</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="getty"><![CDATA[getty]]></category>
		<category domain="post_tag" nicename="html"><![CDATA[html]]></category>
		<category domain="post_tag" nicename="images"><![CDATA[images]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="photography"><![CDATA[photography]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="sculture"><![CDATA[sculture]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Passport Photos</title>
		<link>http://inkdroid.org/2013/09/13/passport-photos/</link>
		<pubDate>Sat, 14 Sep 2013 02:08:46 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6103</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://inkdroid.org/images/passport-photos.png" alt="Passport Photos" /></p>

<p>He would see faces in movies, on TV, in magazines, and in books. He thought that some of these faces might be right for him. And through the years, by keeping an ideal facial structure fixed in his mind, or somewhere in the back of his mind, that he might, by force of will, cause his face to approach those of his ideal.</p>

<p>The change would be very subtle. It might take ten years or so. Gradually his face would change its shape. A more hooked nose. Wider, thinner lips. Beady eyes. A larger forehead. He imagined that this was an ability he shared with most other people. They had also molded their faces according to some ideal. Maybe they imagined that their new face would better suit their personality. Or maybe they imagined that their personality would be forced to change to fit the new appearance. This is why first impressions are often correct.</p>

<p>Although some people might have made mistakes. They may have arrived at an appearance that bears no relationship to them. They may have picked an ideal appearance based on some childish whim, or momentary impulse. Some may have gotten halfway there, and then changed their minds. He wonders if he too might have made a similar mistake.</p>

<p><a href="https://www.youtube.com/watch?v=Rvjw5xJF8WQ">Seen and Not Seen</a> by Talking Heads</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6103</wp:post_id>
		<wp:post_date>2013-09-13 19:08:46</wp:post_date>
		<wp:post_date_gmt>2013-09-14 02:08:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>passport-photos</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="dreams"><![CDATA[dreams]]></category>
		<category domain="post_tag" nicename="passports"><![CDATA[passports]]></category>
		<category domain="post_tag" nicename="photographs"><![CDATA[photographs]]></category>
		<category domain="post_tag" nicename="talking-heads"><![CDATA[talking heads]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>mentioning webmention</title>
		<link>http://inkdroid.org/2013/09/16/mentioning-webmention/</link>
		<pubDate>Mon, 16 Sep 2013 17:27:27 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6126</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Inspired enough by <a href="https://twitter.com/adactio">Jeremy Keith's</a> <a href="http://adactio.com/journal/6495/">post</a> about <a href="http://indiewebcamp.com/webmention">webmention</a>, and the buzz about the <a href="http://indiewebcamp.com/why">indieweb</a> to give <a href="https://github.com/pfefferle/wordpress-webmention">wordpress-webmention</a> by <a href="https://twitter.com/pfefferle">Matthias Pfefferle</a> a quick try.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6126</wp:post_id>
		<wp:post_date>2013-09-16 10:27:27</wp:post_date>
		<wp:post_date_gmt>2013-09-16 17:27:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>mentioning-webmention</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="blogs"><![CDATA[blogs]]></category>
		<category domain="post_tag" nicename="indieweb"><![CDATA[indieweb]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="webmention"><![CDATA[webmention]]></category>
		<category domain="post_tag" nicename="wordpress"><![CDATA[wordpress]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>where the heart beats</title>
		<link>http://inkdroid.org/2013/09/22/where-the-heart-beats/</link>
		<pubDate>Mon, 23 Sep 2013 03:36:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6132</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.goodreads.com/book/show/13542725-where-the-heart-beats" style="float: left; padding-right: 20px"><img alt="Where the Heart Beats: John Cage, Zen Buddhism, and the Inner Life of Artists" border="0" src="http://d202m5krfqbpi5.cloudfront.net/books/1342020813m/13542725.jpg" /></a><a href="http://www.goodreads.com/book/show/13542725-where-the-heart-beats">Where the Heart Beats: John Cage, Zen Buddhism, and the Inner Life of Artists</a> by <a href="http://www.goodreads.com/author/show/299980.Kay_Larson">Kay Larson</a> My rating: <a href="http://www.goodreads.com/review/show/491141241">4 of 5 stars</a></p>

<p>I'm no expert on <a href="https://en.wikipedia.org/wiki/John_cage">John Cage</a> or <a href="https://en.wikipedia.org/wiki/Zen_Buddhism">Zen Buddhism</a>, so I'm not a good person to speak to the accuracy of the material in this book. But Kay Larson provides a very accessible and inspired look at the life of an artist, who found peace and inspiration in the teachings of <a href="https://en.wikipedia.org/wiki/DT_Suzuki">DT Suzuki</a>, and how he went on to be a formative influence on <a href="https://en.wikipedia.org/wiki/Postmodern_art">postmodern art</a>. The story of Cage's relationship with <a href="https://en.wikipedia.org/wiki/Merce_Cunningham">Merce Cunningham</a> and their inner circle of friends and artists was lovingly told. One of my favorite parts of the book was Larson's discovery of a set of cards that were typed up for each meeting of "The Club", which was a gathering of artists and thinkers in Greenwich Village . She used these postcards to piece together the chronology of Cage's development around the time of his <em>Lecture on Something</em> and <em>Lecture on Nothing</em>. There are so many great Cage quotes scattered throughout the book too. I wish I read the book on my kindle so I could have highlighted more, and included some of them here. I've had a copy of <a href="https://en.wikipedia.org/wiki/Silence:_Lectures_and_Writings">Silence</a> for years, and I think I'm going to reread some of it again, now that I know so much more about the context of John Cage's life. If you've ever spent some time living in New York City, this book is bound to make you miss it just a little bit.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6132</wp:post_id>
		<wp:post_date>2013-09-22 20:36:06</wp:post_date>
		<wp:post_date_gmt>2013-09-23 03:36:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>where-the-heart-beats</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="book-review"><![CDATA[book review]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>on perma.cc</title>
		<link>http://inkdroid.org/2013/09/26/on-perma-cc/</link>
		<pubDate>Thu, 26 Sep 2013 20:23:20 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6145</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In case you missed it, an interesting <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2329161">study</a> by Jonathan Zittrain and Kendra Albert was written up in the New York Times with the provocative title <a href="http://www.nytimes.com/2013/09/24/us/politics/in-supreme-court-opinions-clicks-that-lead-nowhere.html">In Supreme Court Opinions, Web Links to Nowhere</a>. In addition to the article, the study itself is worth reading for its compact review of the study of <a href="https://en.wikipedia.org/wiki/Link_rot">link rot</a> on the Web, and its stunning finding that 49% of the links in US Supreme Court Opinions are broken.</p>

<p>This 49% is in contrast with a similar, recent <a href="http://digitalcommons.law.yale.edu/yjolt/vol15/iss2/2/">study</a> by Raizel Liebler and June Liebert of the same links, which found a much lower rate of 29%. The primary reason for this discrepancy was that Zittrain and Albert looked at <em>reference rot</em> in addition to <em>link rot</em>.</p>

<p>The term <em>reference rot</em> was coined by Rob Sanderson, Mark Phillips and Herbert Van de Sompel in their paper <a href="http://arxiv.org/abs/1105.3459">Analyzing the Persistence of Referenced Web Resources with Memento</a>. The distinction is subtle but important. Link rot typically refers to when a URL returns an HTTP error of some kind that prevents a browser from rendering the referenced content. This error can be the result of the page disappearing, or the webserver being offline. Reference rot refers to when the URL itself seems to work (returning either a 200 OK or redirect of some kind), but the content that comes back is no longer the content that was being referenced.</p>

<p>The New York Times article includes a great example of reference rot. The website <a href="http://ssnat.com/">http://ssnat.com/</a> which was referenced in a Supreme Court Opinion by Justice Alito.</p>

<p><img src="http://inkdroid.org/images/linkrot1.png" alt="LinkRot" /></p>

<p>The DNS registration expired, and was picked up someone who knew its significance and turned it into an opportunity to educate people about links in legal documents. The NY Times article calls this nameless person a "prankster" but it is a wonderful hack</p>

<p>One thing the NY Times article didn't mention is that the website has been captured 140 times by the Internet Archive and the original as referenced by Justice Alito is <a href="http://web.archive.org/web/20110529180722/http://ssnat.com/">available</a> still. It seemed like a missed opportunity to highlight the incredibly important work that Brewster Kahle and his merry band of Web archivists are doing. It would be interesting to see how many of the 555 extracted links are available in the Internet Archive. But I couldn't seem to find the list in or linked to from the article.</p>

<p><img src="http://inkdroid.org/images/linkrot2.png" alt="LinkRot" /></p>

<p>Zittrain and Albert on the other hand do mention the Internet Archive's work in the context of <a href="http://perma.cc">perma.cc</a> which is their proposed solution to the problem of broken links.</p>

<blockquote>
  <p>... the Internet Archive is dedicated to comprehensively archiving web content, and thus only passes through a given corner of the Internet occasionally, meaning there is no guarantee that a given page or set of content would be archived to reflect what an author or editor saw at the moment of citation. Moreover, the IA is only one organization, and there are long-term concerns around placing all of the Internet archiving eggs into one basket. <strong>A system of distributed, redundant storage and ownership might be a better long-term solution.</strong></p>
</blockquote>

<p>This seems like a legitimate concern, that there should be some ability to archive a website at a particular point in time. There are <a href="http://blogs.law.harvard.edu/futureoftheinternet/2013/09/22/perma/">27 founding members</a> of perma.cc. There is a strong legal flavor to some of the participants, but perma.cc doesn't appear to be only for legal authors, the website states:</p>

<blockquote>
  <p>perma.cc helps authors and journals create permanent archived citations in their published work. perma.cc will be free and open to all soon.</p>
</blockquote>

<p>It's good to see Internet Archive as one of the founding members. It remains to be seen what perma.cc's approach to a <em>distributed, redundant storage</em> will be. For the system to actually be distributed there has to be more to it than listing 27 organizations that agree that it's a good idea. It's not like Internet Archive operates on its own, since they work closely with the <a href="http://netpreserve.org">International Internet Preservation Consortium</a> which has <a href="http://netpreserve.org/about-us/members">44 organizational members</a>, many of whom are national libraries. I didn't see the IIPC on the list of founding members for perma.cc.</p>

<p>If perma.cc were to take off I wonder what it would mean for publishers' web analytics. If lots of publishers start putting perma.cc URLs in their publications what would this mean for the publishers of the referenced content, and their web analytics? Would it be possible for publishers to see how often their content is being used on perma.cc, and a rough approximation of who they are, what browsers they are using, etc?</p>

<p>Nit-picking aside, its <em>awesome</em> to see another player in the Web archiving space, especially from people Web-veterans who understand how it works, and its significance for society.</p>

<p><em>Update: Leigh Dodds has an excellent <a href="http://blog.ldodds.com/2013/09/25/its-more-than-the-link/">post</a> about perma.cc's <a href="http://www.perma.cc/static/doc/Perma.cc_TOS.pdf">terms of service</a>.</em></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6145</wp:post_id>
		<wp:post_date>2013-09-26 13:23:20</wp:post_date>
		<wp:post_date_gmt>2013-09-26 20:23:20</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-perma-cc</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="law"><![CDATA[law]]></category>
		<category domain="post_tag" nicename="supreme-court"><![CDATA[supreme court]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web-archiving"><![CDATA[web archiving]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86587</wp:comment_id>
			<wp:comment_author><![CDATA[Neulich im Feedreader&#8230; (Teil III): Permanente Links halten auch einem Mailüfterl stand | Hatori Kibble]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://hatorikibble.wordpress.com/2013/10/03/neulich-im-feedreader-teil-iii-permanente-links-halten-auch-einem-mailufterl-stand/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-03 11:52:04</wp:comment_date>
			<wp:comment_date_gmt>2013-10-03 18:52:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] On perma.cc Auch Links, die in Gerichtsentscheidungen genannt werden, veralten und sind irgendwann einmal nicht mehr zugänglich. [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1380826324.9291169643402099609375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1380839382.5803120136260986328125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>preserving linked data</title>
		<link>http://inkdroid.org/2013/09/30/preserving-linked-data/</link>
		<pubDate>Tue, 01 Oct 2013 03:56:27 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6171</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Earlier this morning <a href="https://twitter.com/geckomarma">Martin Malmsten</a> of the <a href="http://www.kb.se/">National Library of Sweden</a> asked an interesting question on Twitter:</p>

<blockquote class="twitter-tweet">
  <p>
    Do you need help hosting your LOD somewhere else? Could be a valuable excercise in LOD stability <a href="http://t.co/FlOT5KqZXn">http://t.co/FlOT5KqZXn</a> <a href="https://twitter.com/3windmills">@3windmills</a> <a href="https://twitter.com/edsu">@edsu</a>
  </p>— Martin Malmsten (@geckomarma) 
  
  <a href="https://twitter.com/geckomarma/statuses/384655483040448512">September 30, 2013</a>
</blockquote>

<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> Martin was asking about the <a href="http://linkeddata.org/">Linked Open Data</a> that the Library of Congress <a href="http://wiki.dublincore.org/index.php/Vocabulary_Preservation_discussion_paper">publishes</a>, and how the potential shutdown of the US Federal Government could result in this data being unavailable. If you are interested, <a href="https://twitter.com/geckomarma/statuses/384655483040448512">click through to the tweet</a> and take a minute to scan the entire discussion.</p>

<p>Truth be told, I'm sure that many could live without the <a href="http://id.loc.gov/authorities/subjects.html">Library of Congress Subject Headings</a> or <a href="http://id.loc.gov/authorities/names.html">Name Authority File</a> for a day or two...or honestly even a month or two. It's not like this data's currency is essential to the functioning of society, like financial, weather or space data, etc. But Martin's point is that it raises an interesting general question about the longevity of Linked Open Data, and how it could be made more persistent.</p>

<p>In case you are new to it, a key feature of Linked Data is that it uses the URL to allow a <em>distributed</em> database to grow organically on the Web. So, in practice, if you are building a database about books, and you need to describe the novel Moby Dick, your description doesn't need to include everything about Herman Melville. Instead it can assert that the book was authored by an entity identified by the URL</p>

<blockquote>
  <p><a href="http://id.loc.gov/authorities/names/n79006936">http://id.loc.gov/authorities/names/n79006936</a></p>
</blockquote>

<p>When you resolve that URL you can get back data about Herman Melville. For pragmatic reasons you may want to store some of that data locally in your database. But you don't need to store all of it. If you suspect it has been updated, or need to pull down more data you simply fetch the URL again. But what if the website that minted that URL is no longer available? Or what if the website is still available but the original DNS registration expired, and someone is <a href="https://en.wikipedia.org/wiki/Cybersquatting">cybersquatting</a> on it?</p>

<p>Admittedly some work has happened at the Dublin Core Metadata Initiative around the <a href="http://wiki.dublincore.org/index.php/Vocabulary_Preservation_discussion_paper">preservation of Linked Data vocabularies</a>. The DCMI is taking a largely social approach to this problem, where vocabulary owners and memory institutions interact within the context of a particular trust framework centered on DNS. But the preservation of vocabularies (which are also identified with URLs) is really just a subset of the much larger problem of Web preservation more generally. Does web preservation have anything to offer for the preservation of Linked Data?</p>

<p>When reading the conversation Martin started I was reminded of a demo my colleague <a href="http://twitter.com/acdha">Chris Adams</a> gave that showed how the <a href="http://wdl.org">World Digital Library</a> item metadata can be retrieved from the Internet Archive. WDL embed item metadata as <a href="http://www.whatwg.org/specs/web-apps/current-work/multipage/microdata.html">microdata</a> in their HTML, and since the Internet Archive archives that HTML, you can get the metadata back from the Internet Archive.</p>

<p>So take this page from WDL:</p>

<p><a href="http://www.wdl.org/item/2/"><img src="http://inkdroid.org/images/wdl-item.png" alt="Chola Woman" /></a></p>

<p>It turns out this particular page has been archived by the <a href="http://web.archive.org/web/20130605205647/http://www.wdl.org/en/item/2/">Internet Archive</a> 27 times. So with a few lines of Python you can use Internet Archive as a metadata service:</p>

<pre lang="python">import urllib 
import microdata

url = "http://web.archive.org/web/20130605205647/http://www.wdl.org/en/item/2/"
resp = urllib.urlopen(url)
items = microdata.get_items(resp)
print items[0].json() 
</pre>

<p>which yields:</p>

<pre lang="javascript">{
  "name": [
    "Chola Woman, Full-Length Portrait, Standing, Facing Right, La Paz, Bolivia"
  ], 
  "creator": [
    {
      "additionalType": [
        "http://viaf.org/viaf/"
      ], 
      "type": "http://schema.org/Person", 
      "name": [
        "Vargas, Max T., 1874\u20131959"
      ]
    }
  ], 
  "url": [
    "http://hdl.loc.gov/loc.wdl/dlc.2"
  ], 
  "image": [
    "/web/20130605205647im_/http://content.wdl.org/2/thumbnail/308x255.jpg", 
    "/web/20130605205647/http://www.wdl.org/media/2.png"
  ], 
  "dateCreated": [
    "1911"
  ], 
  "provider": [
    {
      "type": "http://schema.org/Organization", 
      "name": [
        "Library of Congress"
      ]
    }
  ], 
  "keywords": [
    "Social sciences", 
    "Customs, etiquette & folklore", 
    "Costume & personal appearance", 
    "Portrait photographs", 
    "Women"
  ], 
  "type": "http://schema.org/ItemPage", 
  "contentLocation": [
    {
      "type": "http://schema.org/Place", 
      "address": [
        {
          "addressCountry": [
            "Bolivia"
          ], 
          "addressLocality": [
            "La Paz"
          ], 
          "type": "http://schema.org/PostalAddress"
        }
      ]
    }
  ], 
  "description": [
    "This photograph of a Bolivian woman is from the Frank and Frances Carpenter Collection at the Library of Congress. Frank G. Carpenter (1855-1924) was an American writer of books on travel and world geography whose works helped to popularize cultural anthropology and geography in the United States in the early years of the 20th century. Consisting of photographs taken and gathered by Carpenter and his daughter Frances (1890-1972) to illustrate his writings, the collection includes an estimated 16,800 photographs and 7,000 glass and film negatives. Max T. Vargas, a noted Peruvian photographer and postcard publisher who worked in La Paz, Bolivia, in the early part of the 20th century, took the photograph."
  ]
}

</pre>

<p>Similarly you can get the <a href="http://id.loc.gov/authorities/names/n79006936">LC Name Authority record for Herman Melville</a> from the Internet Archive using the RDFa that is embedded embedded in the page:</p>

<pre lang="python">import rdflib

g = rdflib.Graph()
url = "http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names/n79006936.html"
g.parse(url, format="rdfa")
print g.serialize(format="turtle")
</pre>

<p>which yields:</p>

<pre lang="turtle" style="height: 500px;">@prefix cs: &lt;http://www.w3.org/2003/06/sw-vocab-status/ns#> .
@prefix dcterms: &lt;http://purl.org/dc/terms/> .
@prefix madsrdf: &lt;http://www.loc.gov/mads/rdf/v1#> .
@prefix owl: &lt;http://www.w3.org/2002/07/owl#> .
@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#> .
@prefix ri: &lt;http://id.loc.gov/ontologies/RecordInfo#> .
@prefix skos: &lt;http://www.w3.org/2004/02/skos/core#> .
@prefix skosxl: &lt;http://www.w3.org/2008/05/skos-xl#> .
@prefix xhv: &lt;http://www.w3.org/1999/xhtml/vocab#> .
@prefix xml: &lt;http://www.w3.org/XML/1998/namespace> .
@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#> .

&lt;http://id.loc.gov/authorities/names/n79006936> dcterms:created "1979-01-31T00:00:00" ;
    dcterms:modified "2012-09-13T07:47:36" ;
    dcterms:source "His Typee ... 1846. ",
        "His Pierre, or, The ambiguities ... c1984: t.p. (Herman Meville)"@en,
        "Jeney, Z. Monody, c1983: t.p. (Hermann Melville)"@en,
        "Mobi Diá¸³, 1951 or 1951: t.p. (Herman Melá¹¿il)"@en,
        "Pai ching, 1990 (1992 printing): t.p. (Ho-erh-man Mai-erh-wei-erh, of U.S.) t.p. verso, etc. (Herman Melville [in rom.]; b. 1819, NY; d. 9-27-1891, NY)"@en,
        "The enchanted isles, 2002: p. vii (first published in 1854 The encantadas, or, Enchanted isles by Salvator R. Tarnmoor)"@en,
        "Theory of the American novel, 1970: p.71 (Hawthorne and his mosses, by a Virginian spending July in Vermont [i.e. Herman Melville])"@en,
        "To Kampanario, c1994: t.p. (Cherman Melvil)"@en,
        "Wikipedia, 1 September 2010 (Herman Melville occupations: novelist, short story writer, teacher, sailor, lecturer, poet, customs inspector)"@en ;
    madsrdf:adminMetadata [ a ri:RecordInfo,
                cs:ChangeSet ;
            ri:recordChangeDate "2012-09-13T07:47:36" ;
            ri:recordStatus "revised" ;
            cs:changeReason "revised" ;
            cs:createdDate "2012-09-13T07:47:36" ],
        [ a ri:RecordInfo,
                cs:ChangeSet ;
            ri:recordChangeDate "1979-01-31T00:00:00" ;
            ri:recordStatus "new" ;
            cs:changeReason "new" ;
            cs:createdDate "1979-01-31T00:00:00" ] ;
    madsrdf:authoritativeLabel "Melville, Herman, 1819-1891"@en ;
    madsrdf:classification "PS2380-PS2388" ;
    madsrdf:editorialNote "[Machine-derived non-Latin script reference project.]",
        "[Non-Latin script references not evaluated.]" ;
    madsrdf:hasSource [ a madsrdf:Source ;
            madsrdf:citation-note "t.p. (Herman Melá¹¿il)"@en ;
            madsrdf:citation-source "Mobi Diá¸³, 1951 or 1951:" ;
            madsrdf:citation-status "found" ],
        [ a madsrdf:Source ;
            madsrdf:citation-note "(Herman Melville occupations: novelist, short story writer, teacher, sailor, lecturer, poet, customs inspector)"@en ;
            madsrdf:citation-source "Wikipedia, 1 September 2010" ;
            madsrdf:citation-status "found" ],
        [ a madsrdf:Source ;
            madsrdf:citation-note "t.p. (Herman Meville)"@en ;
            madsrdf:citation-source "His Pierre, or, The ambiguities ... c1984:" ;
            madsrdf:citation-status "found" ],
        [ a madsrdf:Source ;
            madsrdf:citation-note "p.71 (Hawthorne and his mosses, by a Virginian spending July in Vermont [i.e. Herman Melville])"@en ;
            madsrdf:citation-source "Theory of the American novel, 1970:" ;
            madsrdf:citation-status "found" ],
        [ a madsrdf:Source ;
            madsrdf:citation-note "t.p. (Ho-erh-man Mai-erh-wei-erh, of U.S.) t.p. verso, etc. (Herman Melville [in rom.]; b. 1819, NY; d. 9-27-1891, NY)"@en ;
            madsrdf:citation-source "Pai ching, 1990 (1992 printing):" ;
            madsrdf:citation-status "found" ],
        [ a madsrdf:Source ;
            madsrdf:citation-note "" ;
            madsrdf:citation-source "His Typee ... 1846." ;
            madsrdf:citation-status "found" ],
        [ a madsrdf:Source ;
            madsrdf:citation-note "t.p. (Cherman Melvil)"@en ;
            madsrdf:citation-source "To Kampanario, c1994:" ;
            madsrdf:citation-status "found" ],
        [ a madsrdf:Source ;
            madsrdf:citation-note "t.p. (Hermann Melville)"@en ;
            madsrdf:citation-source "Jeney, Z. Monody, c1983:" ;
            madsrdf:citation-status "found" ],
        [ a madsrdf:Source ;
            madsrdf:citation-note "p. vii (first published in 1854 The encantadas, or, Enchanted isles by Salvator R. Tarnmoor)"@en ;
            madsrdf:citation-source "The enchanted isles, 2002:" ;
            madsrdf:citation-status "found" ] ;
    madsrdf:hasVariant _:N07ab04d5af3544cda62856462e53835c,
        _:N08816667a0ec4188aec135323a811da4,
        _:N1101086203694aeab08270c635e2a511,
        _:N3054c346a2754b47bcb4efe873bae4fd,
        _:N36d03253a6e14b49b6b947744ba2b709,
        _:N5b32566a3cb54cea8a085acec3760c19,
        _:N5da6ce34caea470893af1f8253e5512d,
        _:N633919e4f6cb47ebbc7188ed1975280b,
        _:Nab5721a27261493dadb009ebc00a7237,
        _:Nbc0c93b4224e4baba93062ff34dcb970,
        _:Nd5b72f6d9eec49028fbbe024165951cc,
        _:Ne179c605953e4f97a32512b52f3272dc,
        _:Ne46cdc28f7ac4cfc8eda8d37ac47b130 ;
    madsrdf:isMemberOfMADSCollection &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names/collection_LCNAF>,
        &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names/collection_NamesAuthorizedHeadings> ;
    madsrdf:isMemberOfMADSScheme &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names> ;
    skos:editorial "[Machine-derived non-Latin script reference project.]",
        "[Non-Latin script references not evaluated.]" ;
    skos:inScheme &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names> ;
    skos:prefLabel "Melville, Herman, 1819-1891"@en ;
    skosxl:altLabel _:N07ab04d5af3544cda62856462e53835c,
        _:N08816667a0ec4188aec135323a811da4,
        _:N1101086203694aeab08270c635e2a511,
        _:N3054c346a2754b47bcb4efe873bae4fd,
        _:N36d03253a6e14b49b6b947744ba2b709,
        _:N5b32566a3cb54cea8a085acec3760c19,
        _:N5da6ce34caea470893af1f8253e5512d,
        _:N633919e4f6cb47ebbc7188ed1975280b,
        _:Nab5721a27261493dadb009ebc00a7237,
        _:Nbc0c93b4224e4baba93062ff34dcb970,
        _:Nd5b72f6d9eec49028fbbe024165951cc,
        _:Ne179c605953e4f97a32512b52f3272dc,
        _:Ne46cdc28f7ac4cfc8eda8d37ac47b130 .

&lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names/n79006936.html> madsrdf:isMemberOfMADSScheme &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names> ;
    xhv:alternate &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names/n79006936.json>,
        &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names/n79006936.nt>,
        &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names/n79006936.rdf> ;
    xhv:stylesheet &lt;http://web.archive.org/web/20130614192231cs_/http://id.loc.gov/static/css/2012/loc_print_v2.css>,
        &lt;http://web.archive.org/web/20130614192231cs_/http://id.loc.gov/static/css/2012/styles.css> ;
    skos:inScheme &lt;http://web.archive.org/web/20130614192231/http://id.loc.gov/authorities/names> .

&lt;http://web.archive.org/web/20130614192231/http://viaf.org/viaf/sourceID/LC%7Cn+79006936#skos:Concept> madsrdf:hasExactExternalAuthority "http://viaf.org/viaf/sourceID/LC%7Cn+79006936#skos:Concept" ;
    skos:exactMatch "http://viaf.org/viaf/sourceID/LC%7Cn+79006936#skos:Concept" .

&lt;http://web.archive.org/web/20130614192231/http://www.loc.gov/mads/rdf/v1#Authority> a "MADS/RDF Authority" .

&lt;http://web.archive.org/web/20130614192231/http://www.loc.gov/mads/rdf/v1#PersonalName> a "MADS/RDF PersonalName" .

&lt;http://web.archive.org/web/20130614192231/http://www.w3.org/2004/02/skos/core#Concept> a "SKOS Concept" .

_:N07ab04d5af3544cda62856462e53835c a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "×ž×œ×•×•×™×œ, ×”×¨×ž×Ÿ, 1819Ö¾1891" ;
    skosxl:literalForm "×ž×œ×•×•×™×œ, ×”×¨×ž×Ÿ, 1819Ö¾1891" .

_:N08816667a0ec4188aec135323a811da4 a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "éº¥çˆ¾ç¶­çˆ¾, 1819-1891" ;
    skosxl:literalForm "éº¥çˆ¾ç¶­çˆ¾, 1819-1891" .

_:N1101086203694aeab08270c635e2a511 a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Ù…ÙŠÙ„Ú¤ÙŠÙ„ØŒ Ù‡Ø±Ù…Ù†" ;
    skosxl:literalForm "Ù…ÙŠÙ„Ú¤ÙŠÙ„ØŒ Ù‡Ø±Ù…Ù†" .

_:N3054c346a2754b47bcb4efe873bae4fd a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Tarnmoor, Salvator R., 1819-1891" ;
    skosxl:literalForm "Tarnmoor, Salvator R., 1819-1891" .

_:N36d03253a6e14b49b6b947744ba2b709 a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "×ž×œ×•×•×™×œ, ×”×¨×ž×Ÿ" ;
    skosxl:literalForm "×ž×œ×•×•×™×œ, ×”×¨×ž×Ÿ" .

_:N5b32566a3cb54cea8a085acec3760c19 a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Virginian spending July in Vermont, 1819-1891" ;
    skosxl:literalForm "Virginian spending July in Vermont, 1819-1891" .

_:N5da6ce34caea470893af1f8253e5512d a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Melville, Hermann, 1819-1891" ;
    skosxl:literalForm "Melville, Hermann, 1819-1891" .

_:N633919e4f6cb47ebbc7188ed1975280b a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Melvil, Cherman, 1819-1891" ;
    skosxl:literalForm "Melvil, Cherman, 1819-1891" .

_:Nab5721a27261493dadb009ebc00a7237 a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Melvill, German, 1819-1891" ;
    skosxl:literalForm "Melvill, German, 1819-1891" .

_:Nbc0c93b4224e4baba93062ff34dcb970 a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Mai-erh-wei-erh, Ho-erh-man, 1819-1891" ;
    skosxl:literalForm "Mai-erh-wei-erh, Ho-erh-man, 1819-1891" .

_:Nd5b72f6d9eec49028fbbe024165951cc a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Melá¹¿il, Herman, 1819-1891" ;
    skosxl:literalForm "Melá¹¿il, Herman, 1819-1891" .

_:Ne179c605953e4f97a32512b52f3272dc a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "×ž×œ×•×™×œ, ×”×¨×ž×Ÿ" ;
    skosxl:literalForm "×ž×œ×•×™×œ, ×”×¨×ž×Ÿ" .

_:Ne46cdc28f7ac4cfc8eda8d37ac47b130 a madsrdf:PersonalName,
        madsrdf:Variant,
        skosxl:Label ;
    madsrdf:variantLabel "Meville, Herman, 1819-1891" ;
    skosxl:literalForm "Meville, Herman, 1819-1891" .
</pre>

<p>Since it is linked to directly from the HTML page, Internet Archive have also archived the <a href="https://web.archive.org/web/*/http://id.loc.gov/authorities/names/n79006936.rdf">RDF XML</a> itself, and they actually even have the <a href="https://web.archive.org/web/*/http://id.loc.gov/authorities/names/n79006936.marcxml.xml">MARC XML</a>, if that's the sorta thing you are into.</p>

<p>But, as my <a href="http://inkdroid.org/2013/09/26/on-perma-cc/">previous post about perma.cc</a> touched on, a solution to archiving something as important as the Web can't realistically rely on a single point of failure (the Internet Archive). We can't simply decide not to worry about archiving the Web because Brewster Kahle is taking care of it. We need lots of copies of linked data to keep stuff safe.</p>

<p>Fortunately, web archiving is going on at a variety of institutions. But if you have a URL for a webpage, how do you know what web archives have a copy? Do you have to go and ask each one? How do you even know which ones to ask? How do you ask?</p>

<p>...</p>

<p>The <a href="http://mementoweb.org">Memento</a> project worked on aggregating the holdings of web archives in order to provide a single place to look up a URL for their Firefox plugin. They also ended up proxying some sources like Wikipedia that they couldn't convince to support the Memento protocol. From what little I've heard about this process it was done in an ad-hoc fashion, leaning on personal relationships in the IIPC, and was fairly resource intensive, to the point where it was more efficient to use the <a href="https://en.wikipedia.org/wiki/Sneakernet">sneakernet</a> to get the data. If I'm misremembering that I trust someone will let me know.</p>

<p>Earlier this year, David Rosenthal <a href="http://blog.dshr.org/2013/03/re-thinking-memento-aggregation.html">posted</a> some interesting ideas on how to publish the holdings of web archives so that it is not so expensive to aggregate them. His idea is basically for web archives to publish the hostnames of websites they archive instead of complete lists of URLs and all the times they have been fetched. An aggregator could collect these lists, and then provide <em>hints</em> to clients about what web archive a given URL <em>might</em> be found in. This would push the work of polling archives for a particular URL onto the client, which would receive a hint about what web archives to look in. It would also mean that there would space for more than one aggregator, since it wouldn't be so resource intensive.</p>

<p>I really like Rosenthal's idea, and hope that we will see a simple pattern for publishing the holdings of web archives that doesn't turn running an aggregator service into an expensive problem. At the same time it's important that the solution is simple, and that it's not so complicated it becomes an onerous process that web archives don't end up doing. It would be nice to see the bar lowered so that smaller institutions and even individuals could get in the game, not just national libraries. I also hope we can find a simple place to build a list of where these host inventories live, similar to the Name <a href="http://www.cdlib.org/services/uc3/naan_registry.txt">Assigning Authority Number (NAAN) registry</a> that is used (and mirrored) as part of the ARK identifier scheme.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6171</wp:post_id>
		<wp:post_date>2013-09-30 20:56:27</wp:post_date>
		<wp:post_date_gmt>2013-10-01 03:56:27</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>preserving-linked-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="digital-preservation"><![CDATA[digital preservation]]></category>
		<category domain="post_tag" nicename="dns"><![CDATA[dns]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="post_tag" nicename="memento"><![CDATA[memento]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web-archiving"><![CDATA[web archiving]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{i:0;i:86584;i:1;i:86614;i:2;i:86648;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86648</wp:comment_id>
			<wp:comment_author><![CDATA[Chris Adams]]></wp:comment_author>
			<wp:comment_author_email>chris@improbable.org</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawnjYt4eA4hzgDgYRfMpqMFitgKISVvzTc8</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-29 07:12:24</wp:comment_date>
			<wp:comment_date_gmt>2013-10-29 14:12:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I agree that it would be really interesting to explore client caches but trust is such a problem, particularly given how much more organized and advanced attackers are today.

The easiest solution would be a well-reputed third-parties like Google, the Internet Archive, etc. were willing to publish content hashes from earlier retrievals but that runs into problems if they didn't get a copy before the server goes down. It would be interesting to see whether you could run a service where browsers publish hashes and you try to have some sort of majority consensus but it'd be easy to game without strict oversight – someone with a botnet could cast a LOT of votes for their trojaned version of the page they just DDoS-ed.

What could work quite well would be requiring crypto – if the protocol required a signed content signature and the browser would perform the standard origin checks, the response could be trusted even if it came from an unknown source. The catch would be that this would either require HTTPS or at least one of the HTTP signature proposals using either HTTPS-style x509 or something like a public key published in DNS (preferably with DNSSEC), both of which would require some changes by content providers even if it's arguably work they should be doing anyway.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>86614</wp:comment_parent>
			<wp:comment_user_id>354</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1383055944.650662899017333984375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawnjYt4eA4hzgDgYRfMpqMF";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86617</wp:comment_id>
			<wp:comment_author><![CDATA[preserving linked data | modemlab]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://modemlab.wordpress.com/2013/10/12/preserving-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-15 01:10:26</wp:comment_date>
			<wp:comment_date_gmt>2013-10-15 08:10:26</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] See on inkdroid.org [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1381845122.02709102630615234375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1381824626.2176139354705810546875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86616</wp:comment_id>
			<wp:comment_author><![CDATA[On the use of HTTP URIs and the archiving of Linked Data - Knowledge Representation and Reasoning Group]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://krr.cs.vu.nl/2013/10/on-the-use-of-http-uris-and-the-archiving-of-linked-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-15 01:07:27</wp:comment_date>
			<wp:comment_date_gmt>2013-10-15 08:07:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Summer wrote a blog post [12] recently in which he explains how &#8216;traditional&#8217; Web archiving may be used for LD [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1381845114.0031569004058837890625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86614</wp:comment_id>
			<wp:comment_author><![CDATA[christophe-gueret.myopenid.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://christophe-gueret.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-14 04:55:54</wp:comment_date>
			<wp:comment_date_gmt>2013-10-14 11:55:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Interesting blog post and conversation! Having several copies of the same content would indeed be the most reliable solution, and Bittorrent could be an interesting way to share content and/or indexes.

Staying close to what has been done on the Web so far, could we maybe also pick some ideas from the research done against the so-called "Slashdot effect" ? I remember having read some years ago (&gt; 10) a paper that was about redirecting server queries to client machines that browse the site earlier. This sounded like a workable solution: the more you have people browsing, the more you get content providers (assuming these clients accept to play that role and stay up). There has surely more work that has been done on this topic over the past decade, maybe something worth checking out...
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>1740</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1381845130.1945478916168212890625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1381751754.8373539447784423828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:30:"christophe-gueret.myopenid.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86584</wp:comment_id>
			<wp:comment_author><![CDATA[Chris Adams]]></wp:comment_author>
			<wp:comment_author_email>chris@improbable.org</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawnjYt4eA4hzgDgYRfMpqMFitgKISVvzTc8</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-01 11:16:41</wp:comment_date>
			<wp:comment_date_gmt>2013-10-01 18:16:41</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[There are two ideas which seem worth exploring. One would simply be seeing whether the <a href="http://www.coralcdn.org/overview/" rel="nofollow">CoralCDN</a> crowd would be interested in extending their DHT to include temporal information, which opens this really interesting prospect of making the web archive aspect almost transparent to the public.

The more pragmatic idea, however, which I find increasingly intriguing is whether you could simply use BitTorrent to basically publish the list of domains archived by a particular organization, making it trivial to distribute very large lists and allow new players to volunteer their content without requiring prior gatekeeper approval. That becomes particularly interesting when you consider that non-mainstream groups might start wanting to take control of their collective history – imagine a few decades hence when the people who were involved with Occupy Wall Street want to do the equivalent of <a href="https://twitter.com/mkirschenbaum/status/385064104764788736" rel="nofollow">what the AFL-CIO just did in being able to donate their history</a> to a library so future historians have a more complete copy of some incredibly ephemeral media.

Obviously you'd still need to tackle the question of trustworthiness but something as simple as a ZIP file containing a text file listing domains and a PGP signature would make it easy to allow users to decide who to trust and still make it very straight forward to protect against a single point of failure. It'd also have the nice characteristic of making minimal demands on the implementation stack and bringing a lot of battle-tested network code, with significant precedent for browser integration (e.g. Opera has native BitTorrent support) which would be really nice for closing the final loop by allowing clients to deeply integrate Memento support.

(There's some precedent for deeper integration into a server stack, too: <a href="http://codeascraft.com/2012/01/23/solr-bittorrent-index-replication/" rel="nofollow">Etsy famously uses BitTorrent to distribute Solr indices more efficiently</a>, which would be a really interesting way to bootstrap new TimeGate servers efficiently)
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>354</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1380656350.0041120052337646484375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1380651401.619862079620361328125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawnjYt4eA4hzgDgYRfMpqMF";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86583</wp:comment_id>
			<wp:comment_author><![CDATA[jprante]]></wp:comment_author>
			<wp:comment_author_email>joergprante@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-01 01:24:15</wp:comment_date>
			<wp:comment_date_gmt>2013-10-01 08:24:15</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>While I really love the Internet Archive, this approach won't scale if everybody wants to access http://web.archive.org/... in case of failures or outages.</p>

<p>Some failures may be temporary or non-fatal and some will implement Internet Archive as fallback for every kind of situations while others won't.</p>

<p>As a result, the challenge of coping with failures and outages is just moved over to the Internet Archive team. This will surely work because he team is very clever, but only if the Internet  Archive team is guaranteed to perform well for a long time, as it does today.</p>

<p>Same for Wikipedia.</p>

<p>Here is my idea.</p>

<p>My approach wouldn't depend on other institutions. It uses namespace prefixes and a mechanism of "local resolution" of URIs with lots of caches distributed over the web.</p>

<p>Here is how this works: Every URI should be imported to local systems by transforming them to compact URIs (or CURIEs http://en.wikipedia.org/wiki/CURIE.</p>

<p>My Linked Data Co-Location (LDCL) rules are:</p>

<p>Never import full URIs into your triple stores. Select a preferred namespace prefix. Transform URIs to CURIEs in your triples. Save the map of namespace prefixes you used into your triple store configuration.
When triples are exported in responses, transform the CURIEs back to URIs according to configured namespaces for CURIE to URI resolution.
Copy/mirror the domains you use regularly to a local domain near your triple store and add this cache to your triple store's URI/CURIE resolver. Use this cache first. If you get cache misses, check out other remote sources, and at last, the original source.
Publish your local CURIE cache so others can access it for reading. Provide the cache as timestamped linked open data in bulks. For example, using the BitTorrent protocol or CDN mechanisms might help to move snapshots of caches over the wire.
Maintain priority lists of remote sources, build a trust network of CURIE caches, update them regularly.</p>

<p>I know, my idea means adding a new functionality to existing Linked Data platforms. But I think it's worth it.</p>

<p>Best,</p>

<p>Jörg</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>1699</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1380615855.412417888641357421875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:7:"jprante";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1380635475.5674960613250732421875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Shutdown, Startup</title>
		<link>http://inkdroid.org/2013/10/03/shutdown-startup/</link>
		<pubDate>Thu, 03 Oct 2013 11:24:14 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6235</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.indiegogo.com/projects/freehands-craft-studio--2">TL;DR</a></p>

<p>At the moment, I don't have a job. The government has been shut down, and with it my job at the Library of Congress. I've had the good fortune to be able to pick up some part time work here and there with a few friends, to help make ends meet. I know I shouldn't say it, but it has actually been kind of rejuvenating to scramble and brainstorm outside of the "permanent" job mentality that I supposedly have. It's sounding pretty unlikely that I will be paid when the Federal Government re-opens, and it's not really even clear at this point when it will re-open. Meanwhile there is a mortgage to pay, mouths to feed, and not a whole lot of wiggle room in our budget, or savings to speak of. But we'll scrape by, like everyone else in the same boat.</p>

<p>But this post isn't about the shutdown, and it's not really about me. It's about a startup, and it's about my wife Kesa.</p>

<p><a href="http://www.flickr.com/photos/inkdroid/10063186254/"><img src="http://inkdroid.org/images/kesa-outerbanks.jpg" style="width: 200px; float: left; margin-right: 10px;" /></a></p>

<p>Kesa and I met at a startup in New York City in 2000. It was a magical time. We were helping start a business from the ground up, living in a truly amazing city, in a tiny one room apartment that barely fit us and our bookcase...and we were falling in love. We lived in Brooklyn, but our office was in downtown Manhattan, just off Wall Street, and a few blocks from the World Trade Center.</p>

<p>9/11 was an explosive, searing light that annihilated and destroyed...but somehow it also briefly illuminated delicate, evanescent, and commonplace things, making them easier to see. Most of all, the events of 9/11 made me acutely conscious of how important every day I had with Kesa was. One evening later that year I made <a href="http://en.wikipedia.org/wiki/Wedding_soup">Italian Wedding Soup</a> for dinner, and asked her if she wanted to get married. She said yes. I think she liked the soup too.</p>

<p>Around that same time Kesa also decided to return to teaching. She had applied for a job in the Brooklyn Public School system and heard back the morning of September the 11th. I guess the day crystallized some things for her too. She remembered her experience teaching K-3rd grade kids how to read in New Orleans. She remembered what it felt like to help make the world a better place, one student at a time, instead of working her butt off to make some software better, that would (maybe) give some big corporations a competitive edge over some other big corporations, so they could sell more widgets. She inspired me in a way that I needed to be inspired, as our country slipped into pointless retaliation, and war.</p>

<p>Over the last 13 years, Kesa has largely been doing just that: teaching 5th grade in Brooklyn, Chicago and here in Washington DC. She took some time off to be with our kids when they were born, but she went back each time. Her philosophy as a teacher has always been to understand each student for their uniqueness. Don't get me wrong, she is big into the academics; but at the end of the day, it was about connecting with the kids, and seeing them happy and thriving together. The times I went to her class I got to see the evidence of that first hand.</p>

<p>When Maeve (#3!) was born Kesa decided to give something else a try. She started tutoring kids in the neighborhood to see if she could help make ends meet that way. Somewhere along the way the math and reading transitioned to sewing and other crafts. She had caught the <a href="http://en.wikipedia.org/wiki/Maker_culture">makers bug</a> like millions of other people around the country, who are trying to reconnect our culture. She got talking to people like <a href="http://davidbrunton.com">David</a> and <a href="http://butterpies.com/">Lina Brunton</a> who are trying to bootstrap <a href="http://www.rightfieldfarm.com/">a farm</a> outside Annapolis, MD. The kids she taught got a real kick out of learning to make their own pajamas, bags and pillows...unwittingly they encouraged her to do more, and to think a bit bigger. She felt like she was onto something.</p>

<p><a href="http://www.flickr.com/photos/inkdroid/8967638378/"><img src="http://inkdroid.org/images/kesa.jpg" style="width: 200px; float: right; margin-left: 10px;" /></a></p>

<p>So in May of this year Kesa went to Baltimore to register the business <a href="http://freehandscraftstudio.com">Freehands Craft Studio</a>. She found an inexpensive space to rent on the 2nd floor of a strip mall near our house in Silver Spring. The photo to the right was taken when she was painting the walls in the new space. I like it because it captures how earnest she was (and still is) about getting Freehands off the ground.</p>

<p>I watched as she networked on neighborhood discussion lists, talked to friends, and friends of friends, and somehow pulled together a small group of teachers with specialties in knitting, paper making, quilting, sewing, jewelry making and collage. Freehands had a few exploratory classes over the summer to figure out logistics, and this fall the classes started in full swing. Last weekend they were at the <a href="http://makerfairesilverspring.com/">Silver Spring Mini Maker's Faire</a> where they demonstrated how to quickly make reusable lunch sacks, and answered questions for 5 hours from tons of people who were interested in what Freehands was doing.</p>

<p><a href="http://www.flickr.com/photos/inkdroid/10010944433/"><img src="http://inkdroid.org/images/makers-faire.jpg" style="width: 200px; float: left; margin-right: 10px;" /></a></p>

<p>So Kesa is working at a startup again. But this time it's her startup. As the politicians fight in Congress about how to do their job, it means so much to me to see her building Freehands Craft Studio with her friends. It is a lot of work. I'm having to look after the kids a lot more when she is off teaching a class, or doing outreach of some kind. The startup expenses have set us back a bit more than we expected, and at an awkward time. There's still a lot more to do to get the business rolling, to build momentum, and let folks outside of our little corner of Silver Spring know about it. But I can tell it's what Kesa loves doing, because she is smiling when she's doing it, she gets energy from doing it, the work illuminates her life, and our little family.</p>

<p>So I wrote this post for two people.</p>

<p>It's for you Kesa. To let you know that even when I grumble about having to rush home to look after the kids, and scrape together a meal and clean up our house so that it doesn't look like a tornado hit it-- in my heart of hearts I'm so proud of you. Your Freehands experiment gives me hope and purpose. You make me happy, just like back when I made that Italian Wedding Soup.</p>

<p>And this post is also for you. There's no better time to start things up as when other people are shutting things down right? Take some time to consider or remember what you want to start up. It can just be a side project for now. Who knows what it will grow into?</p>

<p>Oh, and if you want to help Kesa and <a href="http://freehandscraftstudio.com">Freehands Craft Studio</a> please consider donating to their <a href="http://www.indiegogo.com/projects/freehands-craft-studio--2">Indiegogo campaign</a>, or sharing information about it with others using your social-media-platform-of-choice. There's only about a day left, and they could really use your help. You can get a little mug or a reusable lunch sack or handmade card as a thank you ... and you will become part of this little dream too.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6235</wp:post_id>
		<wp:post_date>2013-10-03 04:24:14</wp:post_date>
		<wp:post_date_gmt>2013-10-03 11:24:14</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>shutdown-startup</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="business"><![CDATA[business]]></category>
		<category domain="post_tag" nicename="crafts"><![CDATA[crafts]]></category>
		<category domain="post_tag" nicename="dc"><![CDATA[dc]]></category>
		<category domain="post_tag" nicename="love"><![CDATA[love]]></category>
		<category domain="post_tag" nicename="silver-spring"><![CDATA[silver spring]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86589</wp:comment_id>
			<wp:comment_author><![CDATA[fblanco3]]></wp:comment_author>
			<wp:comment_author_email>fwhite@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-03 17:52:00</wp:comment_date>
			<wp:comment_date_gmt>2013-10-04 00:52:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Beautiful tribute.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>1708</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1380847920.947782039642333984375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:8:"fblanco3";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1381236156.0419158935546875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Suzanne Briet on Ada Lovelace Day</title>
		<link>http://inkdroid.org/2013/10/15/suzanne-briet-on-ada-lovelace-day/</link>
		<pubDate>Tue, 15 Oct 2013 14:47:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6314</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://commons.wikimedia.org/wiki/File:Antelope_1_(PSF).png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Antelope_1_%28PSF%29.png/469px-Antelope_1_%28PSF%29.png" style="float: left; margin-right: 10px; width: 175px;" /></a> Today is <a href="http://findingada.com/">Ada Lovelace Day</a> and I wanted to join <a href="https://twitter.com/LibTechWomen">libtechwomen</a> in celebrating the contribution of <a href="https://en.wikipedia.org/wiki/Suzanne_Briet">Suzanne Briet</a>. Briet's thinking helped found the field of <a href="https://en.wikipedia.org/wiki/Information_science">Information Science</a> or <a href="https://en.wikipedia.org/wiki/Documentation_science">Documentation Science</a> as it was known then. Documentation was a field of study started by <a href="https://en.wikipedia.org/wiki/Paul_Otlet">Paul Otlet</a> and <a href="https://en.wikipedia.org/wiki/Henri_La_Fontaine">Henri La Fontaine</a> which focused on fixed forms of documents (e.g.) books, newspapers, etc. Briet's contribution expanded the purview of the study of documents to include the social context in which documents are situated. Or as Ronald Day <a href="http://www.asis.org/Bulletin/Dec-06/day.html">says</a></p>

<blockquote>
  <p>Briet’s writings stressed the importance of cultural forms and social situations and networks in creating and responding to information needs, rather than seeing information needs as inner psychological events. She challenges our common assumptions about the role and activities of information professionals and about the form and nature of documents. She speaks to our age of digital libraries, with their multi-documentary forms, but she also challenges the very conceptual assumptions about the form and the organization of knowledge in such digital libraries. Readers of What Is Documentation? will find themselves returning to Briet’s book, again and again, coming upon ever new insights into current problems and ever new challenges to still current assumptions about documents and libraries and about the origins, designs and uses of information management and its systems.</p>
</blockquote>

<p>As you may know from previous blog posts here, I'm kind of fascinated with the idea of how the Document is presented in Web Architecture, and how it influences technologies like <a href="https://en.wikipedia.org/wiki/Linked_data">Linked Data</a>. I spent some time trying to organize my thoughts about this intersection of Libraries, Archives, Information and the Web in a paper <a href="http://arxiv.org/abs/1302.4591">Linking Things on the Web: A Pragmatic Examination of Linked Data for Libraries, Archives and Museums</a>. I was lucky to have <a href="http://dsalo.info/">Dorothea Salo</a> read an early draft of the paper. Among her many useful comments was one which encouraged me to be a bit more precise in my attribution of the term document in information science. I wasn't even mentioning Briet's contribution and instead just named Otlet and La Fontaine, with a citation to Michael Buckland. I cited Buckland's <a href="http://people.ischool.berkeley.edu/~buckland/whatdoc.html">What Is A Document</a>, which funnily enough is partly responsible for raising awareness about Briet's contribution. Dorothea rightly encouraged me to dig a bit deeper, and to change this paragraph:</p>

<blockquote>
  <p>The terminology of documents situates Linked Data amidst an even older discourse concerning the nature of documents (Buckland, 1997), or documentation science more generally. <strong>Documentation science is a field a field of inquiry established by Paul Otlet and Henri La Fontaine in the 1930s, which was renamed as information science in the 1960s.</strong></p>
</blockquote>

<p>to this:</p>

<blockquote>
  <p>The terminology of documents situates Linked Data amidst an even older discourse concerning the nature of documents (Buckland, 1997), or documentation science more generally. <strong>Documentation science, or the study of documents is an entire field of study established by Otlet (1934), continued by Briet (1951), and more recently Levy (2001). As the use of computing technology spread in the 1960s documentation science was largely subsumed by the field of information science. In particular, Briet’s contributions expanded the notion of what is commonly understood to be a document, by reorienting the discussion to be in terms of objects that function as organized physical evidence (e.g. an antelope in the zoo, as opposed to an antelope grazing on the African savanna). The evidentiary nature of documents is a theme that is particularly important in archival studies.</strong></p>
</blockquote>

<p>So thanks Dorothea, and thank you Suzanne Briet for grounding what I was finding confounding in Web Architecture. Previously my only exposure to Briet's thinking was revival literature about her, so I decided to take this opportunity to buy a copy of <a href="http://www.amazon.com/What-Documentation-English-Translation-Classic/dp/0810851091">What Is Documentation</a> to have for my bookshelf. It's also available <a href="http://ella.slis.indiana.edu/~roday/what%20is%20documentation.pdf">online</a> on the Web, which seems fitting, right?</p>

<p><a href="http://www.flickr.com/photos/inkdroid/10292093453/"><img src="http://inkdroid.org/images/what-is-documentation.jpg" alt="" /></a></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6314</wp:post_id>
		<wp:post_date>2013-10-15 07:47:16</wp:post_date>
		<wp:post_date_gmt>2013-10-15 14:47:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>suzanne-briet-on-ada-lovelace-day</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ada-lovelace"><![CDATA[ada lovelace]]></category>
		<category domain="post_tag" nicename="document"><![CDATA[document]]></category>
		<category domain="post_tag" nicename="information-science"><![CDATA[information science]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="semweb"><![CDATA[semweb]]></category>
		<category domain="post_tag" nicename="suzanne-briet"><![CDATA[suzanne briet]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>archival sliver</title>
		<link>http://inkdroid.org/2013/10/16/archival-sliver/</link>
		<pubDate>Wed, 16 Oct 2013 17:36:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6331</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Just a quick note for my future self, that Verne Harris' notion of the "archival sliver" seems like a great sanity inducing antidote to the notion of <a href="http://www2.archivists.org/glossary/terms/t/total-archives">total archives</a>.</p>

<blockquote>
  <p>The archival record is best understood as a sliver of a sliver of a sliver of a window into process. It is a fragile thing, an enchanted thing, defined not by its connection to "reality", but by its open-ended layerings of construction and reconstruction.</p>
</blockquote>

<p><a href="http://www.nyu.edu/pages/classes/bkg/methods/harris.pdf">The Archival Sliver: Power, Memory and Archives in South Africa</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6331</wp:post_id>
		<wp:post_date>2013-10-16 10:36:57</wp:post_date>
		<wp:post_date_gmt>2013-10-16 17:36:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>archival-sliver</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:0:{}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87332</wp:comment_id>
			<wp:comment_author><![CDATA[April discussion: First half of Delete | Reading Archivists]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://readingarchivists.wordpress.com/2015/04/29/april-discussion-first-half-of-delete/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-04-29 07:22:47</wp:comment_date>
			<wp:comment_date_gmt>2015-04-29 14:22:47</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] day that what we choose to preserve is only a very small &#8220;sliver of a sliver&#8221; as stated so eloquently by South African archivist Verne [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1430317367.7236459255218505859375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1430318851.2605979442596435546875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>NYPL&#039;s Building Inspector</title>
		<link>http://inkdroid.org/2013/10/22/nypls-building-inspector/</link>
		<pubDate>Tue, 22 Oct 2013 15:25:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6333</guid>
		<description></description>
		<content:encoded><![CDATA[<p>You probably already saw the <a href="http://www.wired.com/wiredscience/2013/10/phone-map-game-new-york-city/">news</a> about NYPL's <a href="http://buildinginspector.nypl.org">Building Inspector</a> that was released yesterday. If you haven't, definitely check it out...it's a beautiful app. I hope Building Inspector represents the shape of things to come for engagement with the Web by cultural heritage institutions.</p>

<p>I think you'll find that it is strangely addictive. This is partly because you get to zoom in on random closeups of historic NYC maps: which is like candy if you are a map junkie, or have spent any time in the city. More importantly you get the feeling that you are helping NYPL build and enrich a dataset for further use. I guess you could say it's <a href="https://en.wikipedia.org/wiki/Gamification">gamification</a>, but it feels more substantial than that.</p>

<p>Building Inspector hits a sweet spot for a few reasons:</p>

<ol>
<li>It has a great name. Building Inspector describes the work you will be doing, and contextualizes the activity with a profession you may already be familiar with.</li>
<li>It opens with some playful yet thoughtfully composed instructions that describe how to do the inspection. The instructions aren't optional, but can easily be dismissed. They are fun while still communicating essential facts about what you are going to be doing.</li>
<li>There is an easy way to review the work you've done so far by clicking on the <em>View Progress</em> link. You use your Twitter, Facebook or Google account to login. It would be cool to be able to see the progress view from a global view: everyone's edits, in realtime perhaps. </li>
<li>The app is <em>very</em> responsive, displaying new parts of the map with sub-second response times.</li>
<li>The webapp <a href="http://www.flickr.com/photos/inkdroid/10423047076/">looks</a> and works great as a mobile app. I'd love to hear more about how they did this, since they don't appear to be using anything like Twitter Bootstrap to help. The mobile experience might be improved a little bit if you could zoom and pan with touch gestures. </li>
<li>It uses <a href="http://leafletjs.com/">LeafletJS</a>. I've done some very <a href="http://inkdroid.org/ici/">simplistic</a> work with Leaflet in the past, so it is good to see that it can be customized this much.</li>
<li>NYPL is embracing the cloud. Building Inspector is deployed on <a href="http://heroku.com">Heroku</a>, with map tiles on <a href="http://aws.amazon.com/cloudfront/">Amazon's CloudFront</a>. This isn't a big deal for lots of .com properties, but for libraries (even big research libraries like NYPL) I reckon it is a bigger deal than you might suspect.</li>
<li>The truly hard part of recognizing the outlines of buildings with OpenCV and other tools has been made available by NYPL <a href="https://github.com/NYPL/map-vectorizer">on Github</a> for other people to play around with.</li>
</ol>

<p>Another really fun thing about the way this app was put together was its release, with the (apparent) coordination with an <a href="http://www.wired.com/wiredscience/2013/10/phone-map-game-new-york-city/">article</a> at Wired, and subsequent follow up on the <a href="https://twitter.com/nypl_labs">nypl_labs</a> Twitter account.</p>

<h2>6:35 AM</h2>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet" data-conversation="none">
  <p>
    Our latest project, Building Inspector, is now live! <a href="https://twitter.com/wiredmaps">@wiredmaps</a> has the full story <a href="http://t.co/32M51bjbkS">http://t.co/32M51bjbkS</a> <a href="http://t.co/gNbK8WZkaj">http://t.co/gNbK8WZkaj</a>
  </p>— NYPL Labs (@nypl_labs) 
  
  <a href="https://twitter.com/nypl_labs/statuses/392237770472955905">October 21, 2013</a>
</blockquote>

<h2>7:22 AM</h2>

<blockquote class="twitter-tweet">
  <p>
    Building Inspector has been live for just under an hour and you’ve already inspected more than 800 buildings! <a href="http://t.co/gNbK8WZkaj">http://t.co/gNbK8WZkaj</a>
  </p>— NYPL Labs (@nypl_labs) 
  
  <a href="https://twitter.com/nypl_labs/statuses/392249595180634112">October 21, 2013</a>
</blockquote>

<h2>10:12 AM</h2>

<blockquote class="twitter-tweet">
  <p>
    You’re incredible, Historical Building Inspectors! You’ve already inspected 18,658 buildings this morning! <a href="http://t.co/gNbK8WZkaj">http://t.co/gNbK8WZkaj</a>
  </p>— NYPL Labs (@nypl_labs) 
  
  <a href="https://twitter.com/nypl_labs/statuses/392292206029324288">October 21, 2013</a>
</blockquote>

<h2>6:43 PM</h2>

<blockquote class="twitter-tweet">
  <p>
    Nice work, inspectors. You’ve made 77,447 inspections with the Building Inspector today! Not bad for your first day. <a href="http://t.co/gNbK8WZkaj">http://t.co/gNbK8WZkaj</a>
  </p>— NYPL Labs (@nypl_labs) 
  
  <a href="https://twitter.com/nypl_labs/statuses/392420805403242497">October 21, 2013</a>
</blockquote>

<p>Or in other words:</p>

<p><img src="http://inkdroid.org/images/buildinginspector-chart-1.png" alt="BuildingInspector" /></p>

<p>Quite a first day! It would be interesting to know what portion of the work this represents. Also, I'd be curious to see if NYPL is able to sustain this level of engagement to get the work done.</p>

<h2>Day 2 Update</h2>

<h2>2:22 PM</h2>

<blockquote class="twitter-tweet" data-conversation="none">
  <p>
    <a href="https://twitter.com/nypl_labs">@nypl_labs</a> here we are, midday, day 2... 125,068 buildings checked! Nearing double check on each of the 66k!
  </p>— mattknutzen (@mattknutzen) 
  
  <a href="https://twitter.com/mattknutzen/statuses/392717633822277632">October 22, 2013</a>
</blockquote>

<h2>4:07 PM</h2>

<blockquote class="twitter-tweet">
  <p>
    Building Inspectors, you’re amazing; 131,567 inspections! <a href="http://t.co/gNbK8WZkaj">http://t.co/gNbK8WZkaj</a> Show those space nerds at <a href="https://twitter.com/the_zooniverse">@the_zooniverse</a> who’s boss!
  </p>— NYPL Labs (@nypl_labs) 
  
  <a href="https://twitter.com/nypl_labs/statuses/392744102028386304">October 22, 2013</a>
</blockquote>

<p>If I'm doing the math right (double check me if you really care), between those two data points there were 6,499 inspections over 63,000 seconds -- so an average of 1.03 inspections/second. Whereas between points 3 and 4 of yesterday it looks like they had an average of 1.91 inspections/second.</p>

<p><img src="http://inkdroid.org/images/buildinginspector-chart-2.png" alt="Days 1-2" /></p>

<h2>Day 3 Update</h2>

<blockquote class="twitter-tweet">
  <p>
    We’re up to 163,035 Building inspections. And Building Inspector’s now available on the Firefox Marketplace! <a href="https://t.co/TPadi2I7yI">https://t.co/TPadi2I7yI</a>
  </p>— NYPL Labs (@nypl_labs) 
  
  <a href="https://twitter.com/nypl_labs/statuses/393003845401731072">October 23, 2013</a>
</blockquote>

<p><img src="http://inkdroid.org/images/buildinginspector-chart-3.png" alt="Days 1-3" /></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6333</wp:post_id>
		<wp:post_date>2013-10-22 08:25:57</wp:post_date>
		<wp:post_date_gmt>2013-10-22 15:25:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>nypls-building-inspector</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="amazon"><![CDATA[amazon]]></category>
		<category domain="post_tag" nicename="crowdsourcing"><![CDATA[crowdsourcing]]></category>
		<category domain="post_tag" nicename="heroku"><![CDATA[heroku]]></category>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="leafletjs"><![CDATA[leafletjs]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="maps"><![CDATA[maps]]></category>
		<category domain="post_tag" nicename="nypl"><![CDATA[nypl]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_post_restored_from</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:20:"restored_revision_id";i:6378;s:16:"restored_by_user";i:2;s:13:"restored_time";i:1382455692;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>broken wordpress links</title>
		<link>http://inkdroid.org/2013/10/28/broken-wordpress-links/</link>
		<pubDate>Mon, 28 Oct 2013 16:05:34 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6409</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Internet Archive <a href="http://blog.archive.org/2013/10/25/fixing-broken-links/">recently announced</a> their new <a href="http://archive.org/help/wayback_api.php">Availability API</a> for checking if a representation for a given URL is in their archive with a simple HTTP call. In addition to the API they highlighted a few IA related projects, including a Wordpress plugin called <a href="http://wordpress.org/plugins/broken-link-checker/">Broken Link Checker</a> which will check the links in your Wordpress site, and offer to fix any broken ones using an Internet Archive URL, if it is available based on a call to the Availability API.</p>

<p>I installed the plugin here and let it run for a bit. It detected 3898 unique URLs in 4910 links of which 482 are broken. This amounts to 12% link rot ... but there were also 1038 redirects that resulted in a 200 OK ; so there may be a fair bit of reference rot lurking there. The plugin itself doesn't provide a summary of HTTP status codes for the "broken URLS" but they are listed one by one in the broken link report. Since I could see the HTTP status codes in the table, I figured out you can easily log into your Wordpress database and run a query like this to get a summary:</p>

<pre style="font-family: monospace;">mysql> select http_code, count(*) from wp_blc_links where broken is true group by http_code;
+-----------+----------+
| http_code | count(*) |
+-----------+----------+
|         0 |      113 |
|       400 |        9 |
|       403 |       15 |
|       404 |      333 |
|       410 |        5 |
|       416 |        1 |
|       500 |        3 |
|       502 |        1 |
|       503 |        2 |
+-----------+----------+
</pre>

<p>I'm assuming the 113 (23% of the broken links) are DNS lookup failure, or connection failures. Once the broken links are identified, you have to manually curate each link to decide whether you want to link out to the Internet Archive based on whether it's possible, and whether the most recent link is appropriate or not. This can take some time, but it is useful given I uncovered a number of fat-fingered URLs that looked like they never worked, which I was able to fix. Of the legitimately broken URLs, 136 URLs (~28%) were available at the Internet Archive. Once you've decided to use an IA URL the plugin can quickly rewrite the original content without requiring you to do in and tweak the content yourself.</p>

<p>One thing that would be nice would be for the API to be queried for a representation of the resource based on when the post was authored. For example my Snakes and Rubies <a href="http://inkdroid.org/2005/12/04/snakes-and-rubies/">post</a> had a broken link to http://snakesandrubies.com and the plugin correctly found that it was available at the Internet Archive with an API query like:</p>

<pre>% curl --silent 'http://archive.org/wayback/available?url=snakesandrubies.com' | python -mjson.tool
{
    "archived_snapshots": {
        "closest": {
            "available": true,
            "status": "200",
            "timestamp": "20130131030609",
            "url": "http://web.archive.org/web/20130131030609/http://snakesandrubies.com/"
        }
    }
}
</pre>

<p>When requesting that URL you get this hit in the archive: <a href="http://web.archive.org/web/20130131030609/http://snakesandrubies.com/">http://web.archive.org/web/20130131030609/http://snakesandrubies.com/</a> but that's an archive of a cyerbsquatted version of the page:</p>

<p><a href="http://web.archive.org/web/20130131030609/http://snakesandrubies.com"><img src="http://inkdroid.org/images/snakes-and-rubies-archive-2.png" alt="Snakes, Rubies and Cybersquatter" /></a></p>

<p>If the timestamp of the blogpost were used perhaps a better representation like this could've been found automatically, or at least it could have been offered first?</p>

<p><a href="https://web.archive.org/web/20051208055453/http://snakesandrubies.com/event"><img src="http://inkdroid.org/images/snakes-and-rubies-archive-1.png" alt="Snakes and Rubies" /></a></p>

<p>Based on the svn log for the plugin it appears to have been 2007-10-08 and has been downloaded 2,099,072 times since then. When people gripe about the Web being broken by design I think it's good to remember that tools like this exist to help make it better, one website and link at a time.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6409</wp:post_id>
		<wp:post_date>2013-10-28 09:05:34</wp:post_date>
		<wp:post_date_gmt>2013-10-28 16:05:34</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>broken-wordpress-links</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:86647;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86647</wp:comment_id>
			<wp:comment_author><![CDATA[Chris Adams]]></wp:comment_author>
			<wp:comment_author_email>chris@improbable.org</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawnjYt4eA4hzgDgYRfMpqMFitgKISVvzTc8</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-10-28 14:54:09</wp:comment_date>
			<wp:comment_date_gmt>2013-10-28 21:54:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Using the post date would be a great way to start windowing that search for then-current page but I think it would run into issues getting metadata – i.e. you're using WordPress and it appears that  it doesn't set the Last-Modified header, which has been standard since the mid-90s:

http://redbot.org/?uri=http%3A%2F%2Finkdroid.org%2Fjournal%2F2005%2F12%2F04%2Fsnakes-and-rubies%2F

Maybe this is a good time to see if the IA folks are interested in adding support for the OpenGraph article:published_time or dcterms.created meta values which are present in the template? The Schema.org BlogPost date created / published would also be great candidates for inclusion.

https://archive.org/web/wb404.js doesn't appear to do any parsing; it's not clear whether the code for the available.php endpoint is open-source or not.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>354</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1382997250.0679728984832763671875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawnjYt4eA4hzgDgYRfMpqMF";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>future archives</title>
		<link>http://inkdroid.org/2013/10/31/future-archives/</link>
		<pubDate>Thu, 31 Oct 2013 15:18:10 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6695</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://en.wikipedia.org/wiki/File:Magtape1.jpg"><img style="width: 200px; margin-right: 10px; float: left;" src="http://inkdroid.org/images/magtape.jpg" /></a> It's hard to read <a href="https://twitter.com/moustaki">Yves Raimond</a> and <a href="https://twitter.com/tristanf">Tristan Ferne</a>'s paper <a href="http://challenge.semanticweb.org/2013/submissions/swc2013_submission_5.pdf">The BBC World Service Archive Prototype </a> and not imagine a possible future for radio archives, archives on the Web, and archival description in general.</p>

<p>Actually, it's not just the future, it's also the present, as embodied in the <a href="http://worldservice.prototyping.bbc.co.uk/">BBC World Service Archive</a> prototype itself, where you can search and listen to 45 years of radio, and pitch in by helping describe it if you want.</p>

<p>As their paper describes, Raimond and Ferne came up with some automated techniques to connect up text about the programs (derived directly from the audio, or indirectly through supplied metadata) to Wikipedia and DBPedia. This resulted in some 20 million RDF assertions, that form the database that the (very polished) web application sits on top of. Registered users can then help augment and correct these assertions. I can only hope that some of these users are actually BBC archivists, who can also help monitor and tune the descriptions provided from the general public.</p>

<p>Their story is full of win, so it's understandable why the paper won the <a href="http://challenge.semanticweb.org/2013/winners.html">2013 Semantic Web Challenge</a>:</p>

<ul>
<li>They used <a href="http://wikipedia-miner.cms.waikato.ac.nz/">WikipedidMiner</a> to take a first pass at entity extraction of the text they were able to collect for each program. The <a href="http://maphub.github.io/">MapHub</a> project uses WikipediaMiner for the same purpose of adding structure to otherwise unstructured text.</li>
<li>They used Amazon Web Services (aka the cloud) to do what would have taken them 4 years in the space of 2 weeks, for a fixed, one time cost. </li>
<li>They use ElasticSearch for search, instead of trying to squeeze that functionality and scalability out of a triple store. </li>
<li>They wanted to encourage curation of the content, so they put an emphasis on usability and design that is often absent from Linked Data prototypes. </li>
<li>They have written in <a href="http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-11.pdf">more detail</a> about the algorithms that they used to connect up their text to Wikipedia/DBpedia.</li>
<li>Their <a href="http://github.com/bbcrd">github</a> account reflects the nuts and bolts of how they did this work. Specifically their <a href="https://github.com/bbcrd/rdfsim">rdfsim</a> Python project that vectorizes a SKOS hierarchy, for determining the distance between concepts, seems like a really useful approach to disambiguating terms in text.</li>
</ul>

<p>But it is the (implied) role of the archivist, as the professional responsible for working with developers to tune these algorithms, evaluating/gauging user contributions, and helping describe the content themselves that excites me the most about this work. It's also the future role of the archive that is at stake too. In <a href="http://downloads.bbc.co.uk/rd/pubs/whp/whp-pdf-files/WHP260.pdf">another paper</a> Raimond, Smethurst, McParland and Lowiswhich describe how having this archival data allows them to augment live BBC News subtitles with links to the audio archive, where people can follow their nose (or ears in this case) to explore the context around news stories.</p>

<p>The fact that it's RDF and Linked Data isn't terribly important in all this. But the importance of using world curated, openly licensed entities derived from Wikipedia cannot be understated. It's the conceptual glue that allows connections to be made. As Wikidata grows in importance at Wikipedia it will be interesting to see if it supplants the role that DBpedia has been playing to date.</p>

<p>And of course, it's exciting because it's not just anyone doing this, it's the BBC.</p>

<p>My only nit is that it would be nice to see some of the structured data they've collected expressed more in their HTML. For example they have minted a URI for <a href="http://worldservice.prototyping.bbc.co.uk/tags/Brian_Eno">Brian Eno</a> which lists radio programs that are related to him. Why not display his bio, and perhaps a picture? Why not put links to other radio programs for people he is associated with him, like <a href="http://worldservice.prototyping.bbc.co.uk/tags/David_Byrne">David Byrne</a> or <a href="http://worldservice.prototyping.bbc.co.uk/tags/David_Bowie">David Bowie</a>, etc. Why not express some of this semantic metadata in microdata or RDFa in the page, to enable search engine optimization and reuse?</p>

<p>Luckily, it sounds like they have invested in the platform and data they would need to add these sorts of features.</p>

<p>PS. Apologies to the <a href="http://www.youtube.com/watch?v=eAbkh4TMRqg">Mighty Boosh</a> for the title of this post. "The future's dead ... Everyone's looking back, not forwards."</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6695</wp:post_id>
		<wp:post_date>2013-10-31 08:18:10</wp:post_date>
		<wp:post_date_gmt>2013-10-31 15:18:10</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>future-archives</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="audio"><![CDATA[audio]]></category>
		<category domain="post_tag" nicename="bbc"><![CDATA[bbc]]></category>
		<category domain="post_tag" nicename="dbpedia"><![CDATA[dbpedia]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="post_tag" nicename="skos"><![CDATA[skos]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86655</wp:comment_id>
			<wp:comment_author><![CDATA[Digital BBC World Service radio archives | all things cataloged]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://allthingscataloged.wordpress.com/2013/11/01/digital-bbc-world-service-radio-archives/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-11-01 06:26:20</wp:comment_date>
			<wp:comment_date_gmt>2013-11-01 13:26:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] are used to annotate, correct and add metadata for search and navigation. Ed Summers has a blog post about this project, making a comment I wholeheartedly agree with: &#8221;… [I]t is the (implied) [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1383312380.60064792633056640625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1383313330.4729099273681640625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Gaiman</title>
		<link>http://inkdroid.org/2013/11/08/gaiman/</link>
		<pubDate>Fri, 08 Nov 2013 20:50:39 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6727</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Neil Gaiman <a href="http://www.theguardian.com/books/2013/oct/15/neil-gaiman-future-libraries-reading-daydreaming">wrote recently</a> in the Guardian about reading, writing and libraries. I suspect you've seen it already, but it sat in my to-read pile (on my Kindle) till now. I think I'm going to read it to my kids tonight.</p>

<p>One of the nice things about reading on my Kindle is I'm much more likely to highlight and annotate. Really, I would've highlighted the whole thing if I could have. But anyway, here is what I highlighted:</p>

<h3>The Web</h3>

<blockquote>
  <p>There were noises made briefly, a few years ago, about the idea that we were living in a post-literate world, in which the ability to make sense out of written words was somehow redundant, but those days are gone: words are more important than they ever were: we navigate the world with words, and as the world slips onto the web, we need to follow, to communicate and to comprehend what we are reading. People who cannot understand each other cannot exchange ideas, cannot communicate, and translation programs only go so far.</p>
</blockquote>

<h3>The Book</h3>

<blockquote>
  <p>I do not believe that all books will or should migrate onto screens: as Douglas Adams once pointed out to me, more than 20 years before the Kindle turned up, a physical book is like a shark. Sharks are old: there were sharks in the ocean before the dinosaurs. And the reason there are still sharks around is that sharks are better at being sharks than anything else is. Physical books are tough, hard to destroy, bath-resistant, solar-operated, feel good in your hand: they are good at being books, and there will always be a place for them. They belong in libraries, just as libraries have already become places you can go to get access to ebooks, and audiobooks and DVDs and web content.</p>
</blockquote>

<h3>The (Public) Library</h3>

<blockquote>
  <p>A library is a place that is a repository of information and gives every citizen equal access to it. That includes health information. And mental health information. It's a community space. It's a place of safety, a haven from the world. It's a place with librarians in it. What the libraries of the future will be like is something we should be imagining now ... If you do not value libraries then you do not value information or culture or wisdom. You are silencing the voices of the past and you are damaging the future.</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6727</wp:post_id>
		<wp:post_date>2013-11-08 13:50:39</wp:post_date>
		<wp:post_date_gmt>2013-11-08 20:50:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>gaiman</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Web as a Preservation Medium</title>
		<link>http://inkdroid.org/2013/11/26/the-web-as-a-preservation-medium/</link>
		<pubDate>Wed, 27 Nov 2013 00:59:46 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6738</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>This is the text of a talk I gave at the (wonderful) <a href="http://www.ndf.org.nz/">National Digital Forum</a> in Wellington, New Zealand on November 27th, 2013. You can also find my slides <a href="http://edsu.github.io/webpresmed/">here</a>, and the video <a href="https://www.youtube.com/watch?v=HpJgX8a9d3I">here</a>. If you do happen to watch the video, you'll probably notice I spent more time thinking about the text than I did practicing my talk.</em></p>

<hr />

<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/ragesoss/3835494997/"><img width="250" src="http://inkdroid.org/images/webpresmed/aaron.jpg" /></a>
</div>

<p>Hi there. Thanks for inviting me to NDF 2013, it is a real treat and honor to be here. I'd like to dedicate this talk to <a href="https://en.wikipedia.org/wiki/Aaron_Swartz">Aaron Swartz</a>. Aaron cared deeply about the Web. In a heartbreaking way I think he may have cared more than he was able to. I'm not going to talk much about Aaron specifically, but his work and spirit underly pretty much everything I'm going to talk about today. If there is one message that I would like you to get from my talk today it's that we need to work together as professionals to care for the Web in the same way Aaron cared for it.</p>

<p>Next year it will be 25 years since Tim Berners-Lee wrote his <a href="http://www.w3.org/History/1989/proposal.html">proposal</a> to build the World Wide Web. I've spent almost half of my life working with the technology of the Web. The Web has been good to me. I imagine it has been good to you as well. I highly doubt I would be standing here talking to you today if it wasn't for the Web. Perhaps the National Digital Forum would not exist, if it was not for the Web. Sometimes I wonder if we need the Web to continue to survive as a species. It's certainly hard for my kids to imagine a world without the Web. In a way it's even hard for me to remember it. This is the way of media, slipping into the very fabric of experience. Today I'd like to talk to you about what it means to think about the Web as a <em>preservation medium</em>.</p>

<div style="float:right; margin-left: 10px;">
  <a href="https://en.wikipedia.org/wiki/File:Marshall_McLuhan_holding_a_mirror.jpg"> <img width="250" src="http://inkdroid.org/images/webpresmed/mcluhan.jpg" /> </a>
</div>

<p>Medium and preservation are some pretty fuzzy, heavy words, and I'm not going to try to pin them down too much. We know from Marshall McLuhan that the medium is the message. I like this definition because it disorients more than it defines. McLuhan reminds us of how we are shaped by our media, just as we shape new forms of media. In her book <a href="http://www.amazon.com/Always-Already-New-History-Culture/dp/0262572478">Always Already New</a>, <a href="http://steinhardt.nyu.edu/faculty_bios/view/Lisa_Gitelman">Lisa Gitelman</a> offers up a definition of media that gives us a bit more to chew on:</p>

<blockquote>
  <p>I define media as socially realized structures of communication, where structures include both technological forms and their associated protocols, and where communication is a cultural practice, a ritualized collocation of different people on the same mental map, sharing or engaged with popular ontologies of representation.</p>
</blockquote>

<p>I like Gitelman's definition because it emphasizes how important the social dimension is to our understanding of media. The affordances of media, how media are used by people to do things, and how media does things to us, are just as important as the technical qualities of media. In the spirit of Latour she casts media as a fully fledged actor, not as some innocent bystander or tool to be used by the real and only actors, namely people.</p>

<p>When Matthew Oliver wrote to invite me to speak here he said that in recent years NDF had focused on the museum, and that there was some revival of interest in libraries. The spread of the Web has unified the cultural heritage sector, showing how much libraries, archives and museums have in common, despite their use of subtly different words to describe what they do. I think preservation is a similar unifying concept. We all share an interest in keeping the stuff (paintings, sculptures, books, manuscripts, etc) around for another day, so that someone will be able to see it, use it, cite it, re-interpret it.</p>

<p>Unlike the traditional media we care for, the Web confounds us all equally. We've traditionally thought of preservation and access as different activities, that often were at odds with each other. <a href="https://twitter.com/mkirschenbaum">Matthew Kirschenbaum</a> dispels this notion:</p>

<blockquote>
  <p>... the preservation of digital objects is logically inseparable from the act of their creation -- the lag between creation and preservation collapses completely, since a digital object may only ever be said to be preserved if it is accessible, and each individual access creates the object anew. <a href="http://www.digitalhumanities.org/dhq/vol/7/1/000151/000151.html">The .txtual Condition</a></p>
</blockquote>

<p>Or, as my colleague David Brunton has said, in a McLuhan-esque way:</p>

<blockquote>
  <p>Digital preservation is access...in the future.</p>
</blockquote>

<p>The underlying implication here is that if you are not providing meaningful access in the present to digital content, then you are not preserving it.</p>

<p>In light of these loose definitions I'm going to spend the rest of the time exploring what the Web means as a preservation medium by telling some stories. I'm hoping that they will help illuminate what preservation means in the context of the Web. By the end I hope to convince you of two things: the Web needs us to care for it, and more importantly, we need the Web to do our jobs effectively. For those of you who don't need convincing about either of these points, I hope to give you a slightly different lens for looking at preservation and the Web. It's a hopeful and humanistic lens, that is informed by thinking about the Web as an archive. But more on that later.</p>

<h2>Everything is Broken</h2>

<p>Even the casual user of the Web has run across the problem of the 404 Not Found. In a recent survey of Web citations found in Thompson Reuter's Web of Science, <a href="http://www.biomedcentral.com/1471-2105/14/S14/S5">Hennessey and Ge</a> found that only 69% of the URLs were still available, and the median lifetime for a URL was 9.3 years. The Internet Archive had archived 62% of these URLs. In a similar study of URLs found in recent papers in the popular arXiv pre-print repository <a href="http://arxiv.org/abs/1105.3459">Sanderson, Phillips and Van de Sompel</a> found that of the 144,087 unique URLs referenced in papers, only 70% were still available and of these, 45% were not archived in the Internet Archive, Web Citation, the Library of Congress or the UK National Archive.</p>

<p><a href="http://arxiv.org/abs/1105.3459"><img style="border: thin solid #eeeeee;" src="http://inkdroid.org/images/ndf-01.png" /></a></p>

<p>Bear in mind, this isn't the World Wild Web of dot com bubbles, failed business plans, and pivots we're talking about. These URLs were found in a small pocket of the Web for academic research, a body of literature that is built on a foundation of citation, and written by practitioners whose very livelihood is dependent on how they are cited by others.</p>

<p>A few months ago the 404 made mainstream news in the US when Adam Liptak's story <a href="http://www.nytimes.com/2013/09/24/us/politics/in-supreme-court-opinions-clicks-that-lead-nowhere.html">In Supreme Court Opinions, Web Links to Nowhere</a> broke in the New York Times. Liptak's story spotlighted a recent <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2329161">study</a> by Zittrain and Albert which found that 50% of links in United States Supreme Court opinions were broken. As its name suggests, the Supreme Court is the highest federal court in the United States...it is the final interpreter of our Constitution. These opinions in turn document the decisions of the Supreme Court, and have increasingly referenced content on the Web for context, which becomes important later for interpretation. 50% of the URLs found in the opinions suffered from what the authors call <em>reference rot</em>. Reference rot includes situations of link rot (404 Not Found and other HTTP level errors), but it also includes when the URL appears to technically work, but the content that was cited is no longer available. The point was dramatically and humorously illustrated by the New York Times since someone had bought <a href="http://ssnat.com/">one of the lapsed domains</a> and put up a message for Justice Alito:</p>

<p><a href="http://ssnat.com"><img style="border: thin solid #eeeeee; padding-top: 40px; padding-bottom: 40px;" src="http://inkdroid.org/images/ndf-02.png" /></a></p>

<p>Zittrain and Albert propose a new web archiving project called <a href="http://perma.cc">perma.cc</a>, which relies on libraries to select web pages and websites that need to be archived. As proposed perma.cc would be similar in principle to <a href="http://webcitation.org">WebCite</a>, which is built around submission of URLs by scholars. But WebCite's future is uncertain due to a fund drive to raise money to support its operation. perma.cc also has the potential to offer a governance structure similar to how cultural heritage organizations support the Internet Archive in their crawls of the Web.</p>

<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/joi/3869613988/"> <img width="200" src="http://inkdroid.org/images/webpresmed/brewster.jpg" /> </a>
</div>

<p><a href="http://archive.org">Internet Archive</a> was started by Brewster Kahle in 1996. It now contains 366 billion web pages or captures (not unique URLs). In 2008 Google Engineers reported that their index contained 1 trillion unique URLs. That's 5 years ago now. If we assume it hasn't grown since then, and overlook the fact that there are often multiple captures of a given URL over time, Internet Archive contains about 37% of the Web. This is overly generous since the Web has almost certainly grown in the past 5 years, and we're comparing apples and oranges, web captures to unique URLs.</p>

<p>Of course, it's not really fair (or prudent) to put the weight of preserving the Web on one institution. So thankfully, the Internet Archive isn't alone. The <a href="http://netpreserve.org">International Internet Preservation Consortium</a> is a member organization made up of national libraries, universities, and other organizations that do Web archiving. The National Library of New Zealand is a member, and has its own <a href="http://natlib.govt.nz/collections/a-z/new-zealand-web-archive">Web archive</a>. According to the <a href="https://en.wikipedia.org/wiki/List_of_Web_archiving_initiatives#Archived_data">list of Web archiving initiatives</a> Wikipedia article the archive is comprised of 346 million URLs. Perhaps someone in the audience has a rough idea of how big this is relative to the size of the Kiwi Web. It's a bit of a technical problem even to identify national boundaries on the Web. Since the <a href="http://www.legislation.govt.nz/act/public/2003/0019/latest/DLM191962.html">National Library of New Zealand Act of 2003</a>, the National Library has been authorized to crawl the New Zealand portion of the Web. In this regard, New Zealand is light years ahead of the United States, which still is required by law to ask for permission to collect selected, non-governmental websites.</p>

<p>Protocols and tools for sharing the size and makeup of these IIPC collections are still lacking, but the <a href="http://www.mementoweb.org/">Memento project</a> spurred on some <a href="http://arxiv.org/abs/1309.4008">possible</a> approaches out of necessity. For the Memento prototype to work they needed to collect the URL/timestamp combinations for all archived webpages. This turned out to be difficult both for the archive to share, and to aggregate in one place efficiently--and the moment it was done it was already out of date. David Rosenthal has some <a href="http://blog.dshr.org/2013/03/re-thinking-memento-aggregation.html">interesting ideas</a> for aggregators to collect summary data from web archives, which is used to instead provide <em>hints</em> about where a given URL may be archived. Hopefully we'll see some development in this area, as it's increasingly important that Web archives do collection development more closely, to encourage diversity of approaches, and ensure that one isn't a single point of failure.</p>

<p>Even when you consider the work of the International Internet Preservation Consortium, which adds roughly 75 billion URLs (also not unique) we still are only seeing 44% of the Web being archived. And of course this is a very generous guesstimate, since the 366 billion Internet Archive captures are not unique URLs: e.g. a given URL like the <a href="http://www.bbc.co.uk">BBC homepage</a> has been fetched 13,863 times between December 21, 1996 and November 14, 2013. And there is almost certainly overlap between the various IIPC web archives and the Internet Archive.</p>

<h2>The Archival Sliver</h2>

<p>I am citing these statistics not to say the job of archiving the Web is impossible, or a waste of resources. Much to the contrary. I raise it here to introduce one of the archival lenses I want to encourage you to look at Web preservation through: Verne Harris' notion of the <a href="http://www.nyu.edu/pages/classes/bkg/methods/harris.pdf">archival sliver</a>. Harris is a South African archivist, writer and director of the Archive at the Nelson Mandela Centre of Memory. He participated in the transformation of South Africa’s apartheid public records system, and got to see up close how the contents of archives are shaped by the power structures in which they are embedded. Harris' ideas have a distinctly post-modern flavor, and contrast with positivist theories of the archive that assert that the archive's goal is to reflect reality.</p>

<blockquote>
  <p>Even if archivists in a particular country were to preserve every record generated throughout the land, they would still have only a sliver of a window into that country’s experience. But of course in practice, this record universum is substantially reduced through deliberate and inadvertent destruction by records creators and managers, leaving a sliver of a sliver from which archivists select what they will preserve. And they do not preserve much.</p>
</blockquote>

<p>I like Harris' notion of the archival sliver, because he doesn't see it as a cause for despair, but rather as a reason to celebrate the role that this archival sliver has in the process of social memory, and the archivist who tends to it.</p>

<blockquote>
  <p>The archival record ... is best understood as a sliver of a sliver of a sliver of a window into process. It is a fragile thing, an enchanted thing, defined not by its connections to “reality,” but by its open-ended layerings of construction and reconstruction. Far from constituting the solid structure around which imagination can play, it is itself the stuff of imagination.</p>
</blockquote>

<h2>The First URL</h2>

<p>So instead of considering the preservation of billions of URLs, lets change tack a bit and take a look at the preservation of one, namely the first URL.</p>

<blockquote>
  <p>http://info.cern.ch/hypertext/WWW/TheProject.html</p>
</blockquote>

<p>On April 30th, 1993 CERN made (in hindsight) the momentous decision to freely-release the Web technology software that Tim Berners-Lee, Ari Luotonen and Henrik Nielsen created for making the first website. But 20 years later, that website was no longer available. To celebrate the 20th anniversary of the software release Dan Noyes from CERN led a project to bring the original website back online, at the same address using a completely different software stack. The original content was collected from a variety of places: some from the W3C, some from a 1999 backup of Tim Berners-Lee's NeXT. While the content is how it looked then, the resurrected website isn't running the original Web server software, it's running a modern version of Apache.</p>

<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/adactio/9818910816/"> <img width="200" src="http://inkdroid.org/images/webpresmed/lmb.jpg" /> </a>
</div>

<p>CERN also hosted a group of <a href="http://first-website.web.cern.ch/blog/line-mode-browser-dev-days-participants-announced">11 volunteer developers</a> to spend 2 days coding at CERN (expenses paid) to recreate the experience of using the line mode browser (LMB). The LMB allowed users with an Internet connection to use the Web without having to install any software: they could simply telnet to info.cern.ch and start browsing the emerging Web using their terminal. These developers created a NodeJS JavaScript <a href="http://line-mode.cern.ch/www/hypertext/WWW/TheProject.html">application</a> that simulates the experience of using the early Web. You can even use it to navigate to other pages, for example the current <a href="http://line-mode.cern.ch/www/proxy?url=http://www.w3.org">World Wide Web Consortium page</a>.</p>

<p>In a lot of ways I think this work illustrates James Governor's <a href="http://redmonk.com/jgovernor/2007/04/05/why-applications-are-like-fish-and-data-is-like-wine/">adage</a>:</p>

<blockquote>
  <p>Applications are like fish, data is like wine. Only one improves with age.</p>
</blockquote>

<p>As any old school LISP programmer will tell you, sometimes <a href="https://en.wikipedia.org/wiki/Homoiconicity">code is data and data is code</a>. But it is remarkable that this 20 year old HTML still renders just fine in a modern Web browser. This is no accident, but is the result of thoughtful, just-in-time <a href="http://www.ics.uci.edu/~fielding/pubs/dissertation/net_app_arch.htm">design</a> that encouraged the evolvability, extensibility and customizability of the Web. I think we as a community still have lots to learn from the Web's example, and lots more to import into our practices. More about HTML in a bit.</p>

<h2>Permalinks</h2>

<div style="float: right; margin-left: 10px;">
  <a href="http://www.onfocus.com/about"> <img width="200" src="http://inkdroid.org/images/webpresmed/bausch.jpg" /> </a>
</div>

<p>Now obviously this sort of attention can't be paid to all broken URLs on the Web. But it seems like an interesting example of how an archival sliver of the Web was cared for, respected and valued. Despite popular opinion, the care for URLs is not something foreign to the Web. For example lets take a look at the idea of the <a href="https://en.wikipedia.org/wiki/Permalink">permalink</a> that was popularized by the blogging community. As you know, a blog is typically a stream of content. In 2000 <a href="https://twitter.com/pbausch">Paul Bausch</a> at Blogger <a href="http://www.sixapart.com/blog/2003/09/interview_with.html">came up with</a> a way to assign URLs to individual posts in the stream. This practice is so ubiquitous now it's difficult to see what an innovation it was at the time. As its name implies, the idea of the permalink is that it is stable over time, so that the content can be persistently referenced. Apart from longevity, permalinks have beneficial SEO characteristics: the more that people link to the page over time, the higher its <a href="https://en.wikipedia.org/wiki/PageRank">PageRank</a>, and the more people who will find it in search results.</p>

<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/jimgris/65769319/"> <img width="200" src="http://inkdroid.org/images/webpresmed/timbl.jpg" /> </a>
</div>

<p>A couple years before the blogging community started talking about permalinks, Tim Berners-Lee wrote a short W3C design note entitled <a href="http://www.w3.org/Provider/Style/URI.html">Cool URIs Don't Change</a>. In it he provides some (often humorously snarky) advice for people to think about their URLs, and namespaces with an eye to their future. One of Berners-Lee's great insights was to allow any HTML document to link to any other HTML document, without permission. This decision allowed the Web to grow in a decentralized fashion. It also means that links can break when pages drift apart, and move to new locations, or disappear. But just because a link <em>can</em> break doesn't mean that it <em>must</em> break.</p>

<div style="float: right; margin-left: 10px;">
  <a href="http://www.flickr.com/photos/anjeve/72048767/"> <img width="200" src="http://inkdroid.org/images/webpresmed/lod.jpg" /> </a>
</div>

<p>The idea of Cool URIs saw new life in 2006 when Leo Sauerman and Richard Cyganiak began work on <a href="http://www.w3.org/TR/cooluris/">Cool URIs for the Semantic Web</a>, which became a seminal document for the Linked Data movement. Their key insight was that identity (URLs) matters on the Web, especially when you are trying to create a distributed database like the Semantic Web.</p>

<p>Call them permalinks or Cool URIs, the idea is the same. Well managed websites will be rewarded by more links to their content, improved SEO, and ultimately more users. But most of all they will be rewarded by a better understanding of what they are putting on the Web. Organizations, particularly cultural heritage organization should take note -- especially their "architects". Libraries, archives and museums need to become regions of stability on the Web, where URLs don't capriciously fail because some exhibit is over, or some content management system is swapped out for another. This doesn't mean content can't change, move or even be deleted. It just means we need to know when we are doing it, and say where something has <a href="http://httpstatusdogs.com/301-moved-permanently">moved</a>, or say that what was once there is now <a href="http://httpstatusdogs.com/410-gone">gone</a>. If we can't do it, the websites that do will become the new libraries and archives of the Web.</p>

<h2>Community</h2>

<div style="float: left; margin-right: 10px;">
  <a href="http://www.flickr.com/photos/laughingsquid/3227372389/"> <img width="200" src="http://inkdroid.org/images/webpresmed/textfiles.jpg" /> </a>
</div>

<p>Clearly there is a space between large scale projects to archive the entire Web, and efforts to curate a particular website. Consider the work of <a href="http://www.archiveteam.org/">ArchiveTeam</a>, a volunteer organization formed in 2009 that keeps an eye on when websites are in danger of, actually are, closing their doors and shutting down. Using their <a href="http://www.archiveteam.org/index.php?title=Main_Page">wiki</a>, IRC chatrooms, and software tools they have built up a community of practice around archiving <a href="http://www.archiveteam.org/index.php?title=Category:Rescued_Sites">websites</a>, which have included some 60 sites, such as Geocities and Friendster. They maintain a page called the <a href="http://www.archiveteam.org/index.php?title=Deathwatch">Death Watch</a> where they list sites that are dying (pining for the fjords), or in danger of dying (pre-emptive alarm bells). These activist archivists run something called the <a href="http://www.archiveteam.org/index.php?title=Warrior">Warrior</a> which is a virtual appliance you can install on a workstation, which gets instructions from the <a href="http://tracker.archiveteam.org/">Archive Team tracker</a> about which URLs to download, and coordinates the collection. The tracker then collects statistics, that allow participants to see how much they have contributed relative to others. The collected data is then packed up as WARC files and delivered to the Internet Archive where it is reviewed by an adminstrator, and added to their Web collection.</p>

<p>ArchiveTeam is a labor of love for its creator <a href="http://twitter.com/textfiles">Jason Scott Sadofsky</a> (aka Jason Scott) who is himself an accomplished documenter of computing history, with films such as <a href="http://www.bbsdocumentary.com/">BBS: The Documentary</a> (early bulletin board systems), <a href="http://www.getlamp.com/">Get Lamp</a> (interactive fiction) and <a href="http://ascii.textfiles.com/archives/3984">DEFCON: The Documentary</a>. Apart from mobilizing action, his regular talks have raised awareness about the impermanence on the Web, and have connected with other like minded Web archivists in a way that traditional digital preservation projects have struggled to. I suspect that this self-described "collective of rogue archivists, programmers, writers and loudmouths dedicated to saving our digital heritage" is the shape of things to come for the profession. ArchiveTeam are not the only activists archiving parts of the Web, lets take a look at a few more examples.</p>

<p>Earlier this year Google <a href="http://googlereader.blogspot.com/2013/03/powering-down-google-reader.html">announced</a> that they were pulling Google Reader offline. This caused much grief and anger to be vented from the blogging community...but it spurred one person into action. <a href="http://blog.persistent.info/">Mihai Parparita</a> was an engineer who helped create Google Reader at Google, but he no longer worked there, and wanted to do something to help people retain both their data and the experience of Google Reader. Because he felt that the snapshots of data provided weren't complete, he quickly put together a project <a href="http://readerisdead.com/">ReaderIsDead</a>, which is also <a href="https://github.com/mihaip/readerisdead">available on GitHub</a>. ReaderIsDead is actually a collection of different tools: one for pulling down your personal Reader data from Google while the Google Reader servers were still alive, and a simple web application called ZombieReader that serves that data up, for when the Google Reader servers actually went dead. Mihai put his knowledge of how the Google Reader talked to backend Web service APIs to build ZombieReader.</p>

<iframe width="420" height="315" src="//www.youtube.com/embed/Xjbso_9-yGg" frameborder="0" allowfullscreen></iframe>

<p>Ordinarily fat client interfaces like Google Reader pose problems for traditional Web archiving tools like Internet Archive's Heretrix. Fat Web clients are applications that largely run in your browser, using JavaScript. These applications typically talk back to Web service APIs to fetch more data (often JSON) based on interaction in the browser. Web archiving crawlers don't typically execute JavaScript that is crawled from the Web, and have a hard (if not impossible) time simulating user behavior, which then triggers the calls back to the Web service. And of course the Web Service is what goes dead as well, so even if the Web archive has a snapshot of the requested data, the JavaScript would need to be changed to fetch it. This means the Web archive is left with a largely useless shell.</p>

<p>But in the case of Zombie Reader, the fat client provided a data abstraction that proved to be an excellent way to preserve both the personal data and the user experience of using Google Reader. Mihai was able to use the same HTML, CSS and JavaScript from GoogleReader, but instead of the application talking back to Google's API he had it talk back to a local Web Service that sat on top of the archived data. Individual users could continue to use their personal archives. ZombieReader became a read-only snapshot of what they were reading on the Web, and their interactions with it. Their sliver of a sliver of a sliver.</p>

<h2 id="impermanence">Impermanence</h2>

<p>Of course .com failures aren't the only reason why content disappears from the Web. People intentionally remove content from the Web all the time for a variety of reasons. Let's consider the strange, yet fascinating cases of Mark Pilgrim and Jonathan Gillette. Both were highly prolific software developers, bloggers, authors and well known spokespeople for open source and the Web commons.</p>

<div style="float: left; margin-right: 10px;">
  <a href="https://en.wikipedia.org/wiki/File:Mark_Pilgrim.jpg"> <img width="200" src="http://inkdroid.org/images/webpresmed/pilgrim.jpg" /> </a>
</div>

<p>Among other things, Mark Pilgrim was very active in the area of feed syndication technology (RSS, Atom). He wrote the feed validator and Universal Feed Parser that makes working with syndicated much easier. He also pushed the boundaries of technical writing by writing <a href="http://diveintohtml5.info/">Dive Into HTML 5</a> and <a href="http://www.diveinto.org/python3/">Dive Into Python 3</a> which were published traditionally as books, but also made available on the Web with a CC-BY license. Pilgrim also worked at Google, where he helped promote and evolve the Web with his involvement in the Web Hypertext Application Technology Working Group <a href="http://www.whatwg.org/">WHATWG</a>.</p>

<div style="float: right; margin-left: 10px;">
  <a href="http://www.flickr.com/photos/pragdave/173650575/"> <img width="200" src="http://inkdroid.org/images/webpresmed/why.jpg" /> </a>
</div>

<p>Jonathan Gillette, also known as Why the Lucky Stiff or _why, was a prolific writer, cartoonist, artist, and computer programmer who helped popularize the Ruby programming language. His online book <a href="http://mislav.uniqpath.com/poignant-guide/">Why's (Poignant) Guide to Ruby</a> introduced people of all ages to the practice of programming with wit and humor that will literally make you laugh out loud as you learn. His projects such like <a href="http://tryruby.org/levels/1/challenges/0">Try Ruby</a> and <a href="http://hackety.com/">Hackety Hack!</a> lowered the barriers to getting a working software development environment set up by moving it to the Web. He also wrote a great deal of software such as hpricot for parsing HTML, and the minimalist Web framework <a href="http://camping.io/">camping</a>.</p>

<p>Apart from all these similarities Mark Pilgrim and Jonathan Gillette share something else in common: on October 4, 2011 and August 19, 2009 respectively they both decided to completely delete their online presence from the Web. They committed <em>info-suicide</em>. Their online books, blogs, social media accounts, and github projects were simply removed. No explanations were made, they just blinked out of existence. They are still alive here in the physical world, but they aren't participating online as they were previously...or at least not using the same personas. I like to think Pilgrim and _why were doing performance art to illustrate the fundamental nature of the Web, its nowness, its fragility, it's impermanence. As Dan Connolly said once:</p>

<blockquote>
  <p>The point of the Web arch[itecture] is that it builds the illusion of a shared information space.</p>
</blockquote>

<p>If someone decides to turn off a server or delete a website it's gone for the entire world, the illusion dissolves. Maybe it lives on buried in a Web archive, but it's previous life out on the Web is over. Or is it?</p>

<p>It's interesting to see what happened after the info-suicides. Why's (Poignant) Guide to Ruby was <a href="http://mislav.uniqpath.com/poignant-guide/">rescued</a> by <a href="http://mislav.uniqpath.com/">Mislav Marohnic</a> a software developer living in Croatia. He was able to piece the book back together based on content in the Internet Archive, and put it back online at a new URL, as if nothing had happened. In addition he has continued to curate it: updating code samples to work with the latest version of Ruby, enabling syntax highlighting, converting it to use Markdown, and more.</p>

<p>Similarly Mark Pilgrim's Dive Into HTML 5 and Dive Into Python 3 were assembled from copies and re-deployed to the Web. Prior to his departure Pilgrim used <a href="http://github.com">Github</a> to manage the content for his books. Github is a distributed revision control system, where everyone working with the code has a full copy of it local on their machine. So rather than needing to get content out of the Internet Archive, developers created the <a href="http://github.com/diveintomark">diveintomark</a> organization account on Github, and pushed their clones of the original repositories there.</p>

<p>Much of Why and Pilgrim's code was also developed on GitHub. So even though the master was deleted, many people had clones, and were able to work together to establish a new master. Philip Cromer created the <a href="https://github.com/whymirror/">whymirror</a> on Github, which collected _why's code. Jeremy Ruten created <a href="http://viewsourcecode.org/why/">_why's Estate</a> which is a hypertext archive collects pointers to the various software archives, and writings that have been preserved in Internet Archive and elsewhere.</p>

<p>So, it turns out that the supposedly brittle medium of the Web, where a link can easily break, and a whole website can be capriciously turned off, is a bit more persistent than we think. These events remind me of Matthew Kirschenbaum's book <a href="http://www.amazon.com/Mechanisms-New-Media-Forensic-Imagination/dp/026251740X">Mechanisms</a> which deconstructs notions of electronic media being fleeting or impermanent to show how electronic media (especially that which is stored on hard drives) is actually quite resilient and resistent to change. Mechanisms contains a fascinating study of how <a href="https://en.wikipedia.org/wiki/William_Gibson">William Gibson</a>'s poem <a href="https://en.wikipedia.org/wiki/Agrippa_(a_book_of_the_dead)">Agrippa</a> (which was engineered to encrypt itself and become unreadable after a single reading) saw new life on the Internet, as it was copied around on FTP, USENET, email listservs, and ultimately the Web:</p>

<blockquote>
  <p>Agrippa owes its transmission and continuing availability to a complex network of individuals, communities, ideologies, markets, technologies, and motives ... from its example we can see that the preservation of digital media has a profound social dimension that is at least as important as purely technical considerations. <a href="http://agrippa.english.ucsb.edu/kirschenbaum-matthew-g-hacking-agrippa-the-source-of-the-online-text">Hacking Agrippa</a></p>
</blockquote>

<h2>Small Data</h2>

<p>In the forensic spirit of Mechanisms, let's take a closer look at Web technology, specifically HTML. Remember the first URL and how CERN was able to revive it? When you think about it, it's kind of amazing that you can still look at that HTML in your modern browser, right? Do you think you could view your 20 year old word processing documents today? Jeff Rothenberg cynically <a href="http://www.clir.org/pubs/archives/ensuring.pdf">observed</a></p>

<blockquote>
  <p>digital information lasts forever—or five years, whichever comes first</p>
</blockquote>

<p>Maybe if we focus on the archival sliver instead of the impossibility of <em>everything</em> we're not doing so bad.</p>

<p>As we saw in the cases of Pilgirm and _why the Internet Archive and other web archiving projects play an important role in snapshotting Web pages. But we are also starting to see social media companies are building tools that allow their users to easily extract or "archive" their content. These tools are using HTML in an interesting new way that is worth taking a closer look at.</p>

<p>How many Facebook users are there here? How many of you have requested your archive? If you navigate to the right place in your settings you can "Download a copy of your Facebook data." When you click on the button you set in motion a process that gathers together your profile, contact information, wall, photos, synced photos, videos, friends, messages, pokes, events, settings, security and (ahem) ads. This takes Facebook a bit of time, it took a day the last time I tried it, and you get an email when it's finished which contains a link to download a zip file. The zip file contains HTML, JPEG, MP4 files which you can open in your browser. You don't need to be connected to the Internet, everything is available locally.</p>

<p>Similarly Twitter allow you to request an archive periodically, which triggers an email when it is ready for you to pick up. Much like the Facebook archive it is delivered as a zip file, which contains an easily browsable HTML package. The Twitter archive is actually more like a dynamic application, since it includes a JavaScript application called Grailbird. Grailbird lets you search your tweets, and examine tweets from different time periods. Just like the Facebook archive everything Grailbird needs is available locally, and the application will work when you are disconnected from the Internet. Although user's avatar thumbnail images are still loaded directly from the Web. But all your tweet data is available as JavaScript and CSV. The application depends on some popular JavaScript libraries like jQuery and Underscore, but those also are bundled right with the archive. It would be nice to see Twitter <a href="https://dev.twitter.com/discussions/14148">release Grailbird as a project on Github</a> as many of their other software projects are. Thinking of Grailbird as a visualization framework for tweets would allow interested parties to add new visualizations (e.g. tweets on a map, network graphs, etc). You could also imagine tools for reaching out into the network of an individual's tweets to fetch tweets that they were replying to, and persisting them back locally to the package.</p>

<p>Some of you may remember that the <a href="http://www.dataliberation.org/">Data Liberation Front</a> (led by <a href="https://twitter.com/therealfitz">Brian Fitzpatrick</a> at Google) and the eventual product offering <a href="https://en.wikipedia.org/wiki/Google_Takeout">Google Takeout</a> were early innovators in this area. Google Takeout allows you to download data from 14 of their products as a zip file. The service isn't without <a href="http://www.pcworld.com/article/255920/liberating_your_data_from_google_and_what_that_really_means.html">criticism</a>, because it doesn't include things like your Gmail archive or your search history. The contents of the archive are also somewhat more difficult to work with, compared to the Facebook and Twitter equivalents. For example, each Google+ update is represented as a single HTML file, and there isn't a notion of a minimal, static application that you can use to browse them. The HTML also references content out on the Web, and isn't as self-contained as Twitter and Facebook's archive. But having snapshots of your Youtube videos, and contents of your Google Drive is extremely handy. As Brad Fitzpatrick <a href="http://queue.acm.org/detail.cfm?id=1868432">wrote</a> in 2010, Google Takeout is kind of a remarkable achievement, or realization for a big publicly traded behometh to make:</p>

<blockquote>
  <p>Locking your users in, of course, has the advantage of making it harder for them to leave you for a competitor. Likewise, if your competitors lock their users in, it is harder for those users to move to your product. Nonetheless, it is far preferable to spend your engineering effort on innovation than it is to build bigger walls and stronger doors that prevent users from leaving. Making it easier for users to experiment today greatly increases their trust in you, and they are more likely to return to your product line tomorrow.</p>
</blockquote>

<p>I mention Facebook, Twitter and Google here because I think these archiving services are important for memory institutions like museums, libraries and archives. They allow individuals to download their data from the huge corpus that is available--a sliver of a sliver of a sliver. When a writer or politician donates their papers, what if we accessioned their Facebook or Twitter archive? <a href="https://en.wikipedia.org/wiki/Dave_Winer">Dave Winer</a> for example has started <a href="http://threads2.scripting.com/2012/december/uploadYourTwitterArchive">collecting</a> Twitter archives that have been donated to him, that meet a certain criteria, and making them public. If we have decided to add someone's papers to a collection, why not acquire their social media archives and store them along with their other born digital and traditional content? Yes, Twitter (as a whole) is being archived by the Library of Congress, as so called <em>big data</em>. But why don't we consider these personal archives as small data, where context and <a href="http://www2.archivists.org/glossary/terms/o/original-order">original order</a> are preserved with other relevant material in a highly usable way? As <a href="https://twitter.com/rufuspollock">Rufus Pollock</a> of the <a href="http://okfn.org/">Open Knowledge Foundation</a> <a href="http://blog.okfn.org/2013/04/22/forget-big-data-small-data-is-the-real-revolution/">said</a></p>

<blockquote>
  <p>This next decade belongs to distributed models not centralized ones, to collaboration not control, and to small data not big data.</p>
</blockquote>

<div style="float: left; margin-right: 10px;">
  <a href="http://thenounproject.com/noun/box/#icon-No17863"> <img src="http://inkdroid.org/images/webpresmed/package.png" /> </a>
</div>

<p>The other interesting thing about these services is their use of HTML as a packaging format. My coworker <a href="http://twitter.com/acdha">Chris Adams</a> once remarked that the one format he expects to be able to read in 100 years is HTML. Of course we can't predict the future. But I suspect he may be right. We need best practices, or really just patterns for creating HTML packages of archival content. We need to make sure our work sits on top of common tools for the Web. We need to support the Web browser, particularly open source ones. We need to track and participate in Web standardization efforts such as the W3C and the WHATWG. We must keep the usability of the archive in mind: is it easy to open up in your browser and wander around in the archive as with the Twitter and Facebook examples? And most importantly, as Johan van der Knijff of the National Library of the Netherlands discusses in his study of <a href="http://www.openplanetsfoundation.org/blogs/2013-05-23-epub-archival-preservation-update">EPUB</a>, it is important that all resources are local to the package. Loading images, JavaScript, etc from a remote location makes the archive vulnerable, since it becomes dependent on some part of the Web staying alive. Perhaps we also need tools like <a href="http://www.archiveready.com/">ArchiveReady</a> for inspecting local HTML packages (in addition to websites) and reporting on their archivability?</p>

<h2>Conclusion</h2>

<p>So how to wrap up this strange, fragmented, incomplete tour through Web preservation? I feel like I should say something profound, but I was hoping these stories of the Web would do that for me. I can only say for myself that I want to give back to the Web the way it has given to me. With 25 years behind us the Web needs us more than ever to help care for the archival slivers it contains. I think libraries, museums and archives that realize that they are custodians of the Web, and align their mission with the grain of the Web, will be the ones that survive, and prosper. Brian Fitzpatrick, Jason Scott, Brewster Kahle, Mislav Marohnic, Philip Cromer, Jeremy Ruten and Aaron Swartz demonstrated their willingness to work with the Web as a medium in need of preservation, as well as a medium for <em>doing</em> the preservation. We need more of them. We need to provide spaces for them to do their work. They are the new faces of our profession.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6738</wp:post_id>
		<wp:post_date>2013-11-26 17:59:46</wp:post_date>
		<wp:post_date_gmt>2013-11-27 00:59:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-web-as-a-preservation-medium</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archiveteam"><![CDATA[archiveteam]]></category>
		<category domain="post_tag" nicename="facebook"><![CDATA[facebook]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="internetarchive"><![CDATA[internetarchive]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="web-archiving"><![CDATA[web archiving]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:86676;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86984</wp:comment_id>
			<wp:comment_author><![CDATA[Thoughts on Archival Practice | 0000006 | enjangada]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://enjangada.wordpress.com/2014/07/08/thoughts-on-archival-practice-0000006/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-08 05:06:23</wp:comment_date>
			<wp:comment_date_gmt>2014-07-08 12:06:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] to their institutional web presence. My colleague, Ed Summers at the Library of Congress, recently spoke to the National Digital Forum in Wellington, New Zealand and stressed that “if you are not [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1404821183.5060389041900634765625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1404901689.302361011505126953125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86683</wp:comment_id>
			<wp:comment_author><![CDATA[Editor&#8217;s Choice: The Web as a Preservation Medium | Digital Humanities Now]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://digitalhumanitiesnow.org/2013/12/editors-choice-the-web-as-a-preservation-medium/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-12-03 10:01:13</wp:comment_date>
			<wp:comment_date_gmt>2013-12-03 17:01:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] By Ed Summers | December 3, 2013   Next year it will be 25 years since Tim Berners-Lee wrote his proposal to build the World Wide Web. I’ve spent almost half of my life working with the technology of the Web. The Web has been good to me. I imagine it has been good to you as well. I highly doubt I would be standing here talking to you today if it wasn’t for the Web. Perhaps the National Digital Forum would not exist, if it was not for the Web. Sometimes I wonder if we need the Web to continue to survive as a species. It’s certainly hard for my kids to imagine a world without the Web. In a way it’s even hard for me to remember it. This is the way of media, slipping into the very fabric of experience. Today I’d like to talk to you about what it means to think about the Web as a preservation medium.  Read Full Post Here. [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386090554.5500628948211669921875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386090074.0427761077880859375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86685</wp:comment_id>
			<wp:comment_author><![CDATA[Happenings in the Web Archiving World | The Signal: Digital Preservation]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blogs.loc.gov/digitalpreservation/2013/12/happenings-in-the-web-archiving-world/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-12-04 11:11:45</wp:comment_date>
			<wp:comment_date_gmt>2013-12-04 18:11:45</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Library of Congress gave the keynote address at the National Digital Forum in New Zealand titled The Web as  Preservation Medium. Ed is a software developer and offers a great perspective into some technical aspects of [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386180705.77564907073974609375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386183620.3079349994659423828125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86687</wp:comment_id>
			<wp:comment_author><![CDATA[6 Digital Historiography and Strategy Grad Seminars I&#8217;d Love to Teach | Trevor Owens]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.trevorowens.org/2013/12/6-digital-historiography-and-strategy-grad-seminars-id-love-to-teach/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-12-06 10:20:30</wp:comment_date>
			<wp:comment_date_gmt>2013-12-06 17:20:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Digital Strategy for Cultural Heritage Organizations:  Digital is increasingly becoming a key part of nearly every function of cultural heritage organizations (Libraries, Archives, Museums etc.). We are increasingly acquiring, preserving and exhibiting born-digital and digitized materials, using social media for outreach and public relations, supporting researchers and fielding reference questions through digital channels, and supporting all of that work with a substantive IT infrastructure.  Looking across each of these areas, this course would focus on exploring ideas for how organizations should be structured, about the role of software development should play, embedding &#8220;digital into the design, decision making, strategy and all the operations&#8221; of cultural heritage orgs and the role that the web should play as a platform and organizing principle for orgs. [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386351186.98832798004150390625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386350430.95691394805908203125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86689</wp:comment_id>
			<wp:comment_author><![CDATA[Archiviare il web | Bicycle Mind]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://bicyclemind.it/2013/12/08/archiviare-il-web/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-12-08 08:17:51</wp:comment_date>
			<wp:comment_date_gmt>2013-12-08 15:17:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Archiviare il web [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1386599319.035931110382080078125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86981</wp:comment_id>
			<wp:comment_author><![CDATA[Thoughts on Archival Practice | 0000005 | enjangada]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://enjangada.wordpress.com/2014/06/18/thoughts-on-archival-practice-0000005/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-06-18 16:03:06</wp:comment_date>
			<wp:comment_date_gmt>2014-06-18 23:03:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Read Ed Summers’ talk here (Web as a Preservation Medium): http://inkdroid.org/2013/11/26/the-web-as-a-preservation-medium/. [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1403861069.8975570201873779296875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1403132586.5262119770050048828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86749</wp:comment_id>
			<wp:comment_author><![CDATA[Weekly web archiving roundup: January 8, 2014 | Web Archiving Roundtable]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://webarchivingrt.wordpress.com/2014/01/08/weekly-web-archiving-roundup-january-8-2014/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-01-08 14:10:00</wp:comment_date>
			<wp:comment_date_gmt>2014-01-08 21:10:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Discussion about issues surrounding web-based content: http://inkdroid.org/2013/11/26/the-web-as-a-preservation-medium/ [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1389215400.178318023681640625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1389232508.341617107391357421875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86676</wp:comment_id>
			<wp:comment_author><![CDATA[google.com/accounts/o8&hellip;]]></wp:comment_author>
			<wp:comment_author_email>azaroth42@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>https://www.google.com/accounts/o8/id?id=AItOawmHT2lGj3VrhMg1rx3kD7yImR_BtvfcZc0</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-11-27 14:28:08</wp:comment_date>
			<wp:comment_date_gmt>2013-11-27 21:28:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[A little more information about the Memento work (<a href="http://www.mementoweb.org/" rel="nofollow">http://www.mementoweb.org/</a> as Ed didn't link to it) ...


The issue referred to is a classic distributed search problem to which there are many possible solutions, from parallel searching to centralized federation and all of the hybrids in between.  There's also very strong caching possibilities, as it's very rare that archived copies go away once published.
The prototype referred to was intended exclusively for members of the IIPC.  The live Memento Aggregators use various techniques, but they do <i>not</i> use the data provided by IIPC partner institutions.   Mostly these are aggressive caching and pre-fetching based on common requests, and quickest-response on-demand with the other archives being filled in and cached for future requests.


Hope that clarifies a bit,

-- Rob Sanderson
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>1914</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1385587688.686172008514404296875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:60:"www.google.com-accounts-o8-id-id-AItOawmHT2lGj3VrhMg1rx3kD7y";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1385593526.2172870635986328125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86930</wp:comment_id>
			<wp:comment_author><![CDATA[Archiving the Internet | z657 Digital Humanities Spring 2014]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://biblicon.org/14z657/?p=619</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-13 17:28:00</wp:comment_date>
			<wp:comment_date_gmt>2014-04-14 00:28:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] important part of the web that most people might not be aware of. Ed Summers&#8217;s article &#8220;The Web as a Preservation Medium&#8221; provides a good introduction to the many forms of Internet archiving going on today, and I [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397435280.4023640155792236328125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397738351.71364307403564453125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86698</wp:comment_id>
			<wp:comment_author><![CDATA[The Dramatic Creation Story of HTML5 | MagiclogixBlog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.magiclogixstaging.com/tooblerbeta/blog/?p=4442</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-12-16 02:54:27</wp:comment_date>
			<wp:comment_date_gmt>2013-12-16 09:54:27</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] The Web as a Preservation Medium [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1387188036.7968900203704833984375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1387187668.07297992706298828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86677</wp:comment_id>
			<wp:comment_author><![CDATA[The Dramatic Creation Story of HTML5]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.magiclogix.com/blog/the-dramatic-creation-story-of-html5/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-11-27 15:56:11</wp:comment_date>
			<wp:comment_date_gmt>2013-11-27 22:56:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] The Web as a Preservation Medium [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1385593547.1011369228363037109375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86679</wp:comment_id>
			<wp:comment_author><![CDATA[Your item is my story; my story is your item &#8211; talkingtothecan]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://talkingtothecan.com/your-item-my-story/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2013-11-28 13:42:50</wp:comment_date>
			<wp:comment_date_gmt>2013-11-28 20:42:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] small data and collections needing to be &#8216;regions of stability&#8217; in his keynote talk, The web as a preservation medium. And more on that from Michael Lascarides in his talk and this pertinent [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1385671370.3451960086822509765625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1385684602.87094211578369140625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87207</wp:comment_id>
			<wp:comment_author><![CDATA[What is still on the web after 10 years of archiving? par Andy Jackson (Web Archiving Technical Lead, The British Library) | Web90 &#8211; Patrimoine, Mémoires et Histoire du Web dans les années 1990]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://web90.hypotheses.org/557</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-11-16 13:07:51</wp:comment_date>
			<wp:comment_date_gmt>2014-11-16 20:07:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] long since gone? If those URLs are still working, is the content the same as it was? How has our archival sliver of the web [&#8230;]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1416496931.0702788829803466796875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1416168471.7233469486236572265625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>public email archives</title>
		<link>http://inkdroid.org/2014/01/14/public-email-archives/</link>
		<pubDate>Tue, 14 Jan 2014 18:22:03 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=6992</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.flickr.com/photos/epublicist/3509141851/"><img src="http://inkdroid.org/images/email.jpg" style="float: left; margin-right: 10px; width: 175px;" /></a></p>

<p>I noticed on the <a href="http://www.archivists.org/listservs/arch_listserv_terms.asp">Archives and Archivists</a> discussion list today that the Library of Virginia <a href="http://www.virginiamemory.com/blogs/out_of_the_box/2014/01/13/adam-raised-a-kaine-archiving-the-records-of-a-governor/">has made</a> 66,422 of the approximately 1.3 million emails (167 GB) of <a href="https://en.wikipedia.org/wiki/Tim_Kaine">Governor Tim Kaine</a>'s 2006-2010 administration available on the Web. Even though this is only 5% of the entire collection, it still represents a significant step forwards for open access to government information.</p>

<p>Thankfully, the Library of Virginia took the extra step of <a href="http://www.exlibrisgroup.com/category/DigiToolOverview">describing</a> how they went about processing the email collection. Along the way they tried a series of proprietary tools, some of which crashed regularly to convert pst to pdf and csv, which they then ingested into their digital asset management system, <a href="http://www.exlibrisgroup.com/category/DigiToolOverview">DigiTool</a> from ExLibris (which apparently couldn't load more than 3,000 emails without keeling over).</p>

<p>What's simply astounding is that archivists looked at every email to determine whether it contained <em>restricted material</em>. The result of this sifting was that only 5% of the emails were made available.</p>

<p>I was drawn to the announcement initially because I wondered if they would simply make mailbox data available on the Web, similar to the <a href="https://www.cs.cmu.edu/~enron/">Enron email dataset</a>. But quickly I noticed that while metadata about the email was readable, I wasn't able to read the contents of the message--the so called email body. Instead I was presented with a PDF icon, which had a lock over it. At first I suspected the content was only available at the Library of Virginia, but then after some more reading elsewhere I discovered that I needed to login to see the PDFs.</p>

<p>I was surprised to find that the username and password were simply listed on the <a href="http://digitool1.lva.lib.va.us:8881/pds?func=load-login&amp;calling_system=digitool&amp;url=http://digitool1.lva.lib.va.us:8881/R/67C839Y1UT16P2FTIQ6R4H9KPV7AGMA4NDLXLC3Y1L3CRPDQ1Y-03822?func=search-simple">login page</a> -- you don't get your own login, everyone uses the same one. This login form is accompanied by the following text:</p>

<pre><code>While great care has been taken during the processing of this collection to locate, identify, and restrict access to privacy protected information within this collection, some relevant materials may have been missed.

By logging in and accessing this collection, the user agrees:

* That if privacy-protected information is discovered during use of this material to make no notes or other recordation of the confidential information.
* Not to publish, publicize, or disclose any confidential material to any other party for any purpose. 
* That no direct or indirect contact will be made with the individuals to whom the confidential information relates.
* To contact the Library of Virginia at archdesk@lva.virginia.gov to report any confidential information found. 

Improper disclosure of privacy-protected information is a breach of confidentiality that could result in the loss of access to the archival collections housed and maintained by Library of Virginia, and could result in legal penalties (Code of Virginia, §18.2-186.3).

Name: LVA Password: LVA

Login is required only once during each session.
</code></pre>

<p>In all honesty I'm surprised that this collection has been made available at all, given how recent the material is (relative to how long email has been around), and the rush to make it available in time for Kaine's run for the US Senate. It's hard not to imagine politics going on there behind the scenes.</p>

<p>But the thing that gave me pause in this agreement is the researcher needs to know what <em>privacy protected</em> or <em>confidential</em> information is. I wonder if many archivists are clear on what these words mean in this context? Without a clear understanding of what these terms mean this language seems to prevent researchers from actually using any of the material that they discover.</p>

<p>Deep breath.</p>

<p>Something that might not be apparent to folks outside the archives world is that this is still a huge step forwards for the Library of Virginia to start making collections like this available on the Web. If it were me, I would probably have started out with a less politicized collection, and used opensource Web tools to convert the content, and make it available. ElasticSearch or Solr over static HTML documents comes to mind. Making the emails indexable by Google, and not hiding them behind a public username and password that the bots can't figure out, would be a priority for me as well. So people who are searching for the content can find it. This would take the burden off of Library of Virginia's search tool as well. Terms of service that make sense to researchers, and don't scare them out of doing their work seems pretty important too.</p>

<p>Still, it looks like progress. Keep on keeping on Library of Virginia. 5% is still a <a href="http://inkdroid.org/2013/10/16/archival-sliver/">sliver of a sliver of a sliver</a> that the Web didn't have before.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6992</wp:post_id>
		<wp:post_date>2014-01-14 11:22:03</wp:post_date>
		<wp:post_date_gmt>2014-01-14 18:22:03</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>public-email-archives</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="email"><![CDATA[email]]></category>
		<category domain="post_tag" nicename="library-of-virginia"><![CDATA[library of virginia]]></category>
		<category domain="post_tag" nicename="pdf"><![CDATA[pdf]]></category>
		<category domain="post_tag" nicename="politics"><![CDATA[politics]]></category>
		<category domain="post_tag" nicename="virginia"><![CDATA[virginia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86777</wp:comment_id>
			<wp:comment_author><![CDATA[Roger Christman]]></wp:comment_author>
			<wp:comment_author_email>roger.christman@lva.virginia.gov</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-01-28 13:17:36</wp:comment_date>
			<wp:comment_date_gmt>2014-01-28 20:17:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed,

Thanks for the feedback.  We are adding this paragraph to the Login page:

The Virginia Freedom of Information Act (Code of Virginia, §2.2-3705.5) and the Virginia Public Records Act (Code of Virginia, §42.1- 78) establish guidelines for restricting access to privacy-protected information. Privacy-protected records include, but are not limited to, certain educational, medical, financial, criminal, attorney-client, and personnel records. Privacy-protected information is considered confidential and is restricted from public access for 75 years following the date of record creation. In accordance with this legislation, the Library of Virginia can and will restrict, in whole or in part, access to records containing privacy-protected information.

This mirrors the language on the archives research room agreement that in-person users sign.

As to the numbers, we could have done a better job explaining them.  The 1.3 million is the approximate total number of emails transferred to the Library - however not all of them are permanent archival records.  For this initial release, I reviewed 138,532 Executive Office emails.  Here is the breakdown:  66,422 open records, 13,594 restricted records, 10,808 Virginia Tech records (subject to further review) and 47,708 non-records/non-permanent records.  This constitutes approximately 10% of the collection.  We are going to add this to information to the "Under the Hood" section of the web site ("Kaine Email Project @ LVA - By the Numbers") and update it with each release.   My colleague Ben Bromley and I have reviewed an additional 348,225 emails from four offices.  We hope to release these records soon.

Best,

Roger Christman
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2027</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1390940256.485167026519775390625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:15:"Roger Christman";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1391014124.391725063323974609375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>natus digital</title>
		<link>http://inkdroid.org/2014/01/16/natus-digital/</link>
		<pubDate>Thu, 16 Jan 2014 18:41:53 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7024</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A few days ago I <a href="https://twitter.com/edsu/status/423294386223722496">asked</a> folks on Twitter if they had any <a href="https://en.wikipedia.org/wiki/Born-digital">born-digital</a> archival collections that were Internet accessible (web, ftp, etc). I'm testing a little prototype application called <a href="http://github.com/edsu/fondz">fondz</a> (which I will hopefully write more about later if it proves to be headed in a useful direction) and wanted a collection I could actually talk about in a blog post (as opposed to the content I'm testing with and cannot). I specifically wanted born-digital archival content with well defined <a href="http://www2.archivists.org/glossary/terms/p/provenance">provenance</a>, because fondz assumes the content isn't just a <a href="http://digitalcorpora.org/corpora/files">random assortment of things</a>, but forms a thematic unit of some kind: e.g. content that is donated to an archive as part of a personal or organizational collection of some kind.</p>

<p>Call me crazy, but if you squint right, the Web looks like its full of born-digital archives--they are called websites! But fondz is oriented around files that have been collected offline: collections of word processing or image files, that may have accumulated on your hard drive and perhaps gotten backed up to a disk of some kind, and then ultimately gifted to an archive. So, like a lot of archival content, they also have access rights associated with them.</p>

<p>I received several helpful responses, and thought I would jot them down here in case you go looking for born-digital collections too. If you have one you would like me to add just tweet them at <a href="http://twitter.com/edsu">me</a> and I'll add them.</p>

<ul>
<li><a href="https://twitter.com/aaccomazzi">Alberto Accomazzi</a> suggested <a href="http://arxiv.org">arXiv</a> which contains lots of scientific material in pdf and <a href="https://en.wikipedia.org/wiki/LaTeX">LaTeX</a>. Depending on the scope you could get content from a particular author, organization or discipline. If you squint right I guess any digital repository that has a strong sense of an author identity and/or the subject of the content could work as a source of born-digital archival content. There are lots of so called "institutional repositories" on the Web. But pre-print repositories are particularly interesting because they often represent work in progress, not the finished, polished thing that people often think of as "published". Pre-prints are more like the documents you have lying around on your computer, that you happen to have pushed out to let people know what you are working on, and to share research that is still underway.</li>
<li><a href="http://twitter.com/anarchivist">Mark Matienzo</a> pointed me at the <a href="http://ucispace.lib.uci.edu/handle/10575/7">Richard Rorty born digital files</a> at the University of California at Irvine. In order to download the files you need to <a href="http://special.lib.uci.edu/using/virtual-reading-room-application-form.html">apply</a> for an account in their <a href="http://ucispace.lib.uci.edu/">UCISpace</a> application. I filled out the form, and was pleasantly surprised when I received an email the next day granting me access--way to go UCI!. Are there many other examples of this sort of Web enabled interaction with researchers? Or have I been asleep for a while and this is the new normal for archives on the Web? I particularly enjoyed Mark's suggestion because I'm a big fan of Rorty's work, so it will be fun to look at the content. Subsequently Matthew <a href="https://twitter.com/mmmatthew/status/424067098134855680">followed up</a> on Twitter to let me know that UCISpace has other born digital collections available. <a href="http://twitter.com/abrennr">Aaron Brenner</a> also <a href="https://twitter.com/abrennr/status/424218212230656001">pointed</a> me at some <a href="http://www.slideshare.net/librarchivist/in-the-virtual-reading-room-providing-access-to-the-born-digital">slides</a> about UCI's virtual reading room.</li>
<li><a href="https://twitter.com/diplomaticaerin">Erin O'meara</a> <a href="https://twitter.com/diplomaticaerin/status/423310563897122816">pointed</a> me to <a href="https://twitter.com/jillsexton">Jill Sexton</a> and <a href="https://twitter.com/megtuo">Meg Tuomala</a> at the University of North Carolina at Chapel Hill who have some born-digital collections. Erin then pointed me to the <a href="https://cdr.lib.unc.edu/">Carolina Digital Repository</a> which seems to have a fair bit of born digital material: for example this <a href="https://cdr.lib.unc.edu/record/uuid:fc3f5c38-a6b1-4a81-922b-50da82e906fd">Word document</a> from a folder named <em>Dad's laptop</em> in the John Chapman collection. This content is available without having to login, which is nice. I haven't poked around to see how much more is available yet.</li>
<li><a href="https://twitter.com/jordanheit">Mark Jordan</a> <a href="https://twitter.com/jordanheit/status/423351828860788736">referred</a> me to <a href="https://twitter.com/ruebot">Nick Ruest</a> of York University. Nick has been kind enough to bag up some content from the <a href="http://digital.library.yorku.ca/yul-f0529/allan-robb-fleming">Allan Fleming</a> collection and put it on the Web for me to download. I noticed that the system at York provided a way to login, so maybe someday they could offer a similar service to UC Irvine.</li>
<li><a href="https://twitter.com/tjowens">Trevor Owens</a> suggested I get in touch with the <a href="http://mith.umd.edu/">Maryland Institute for Technology in the Humanities</a> to see if they might have some of their born digital content online. MITH is local for me, so I could conceivably head over their with a thumb drive.</li>
</ul>

<p>If you have other ideas I'd still be interested to hear about them, and will add them here if you comment here, or <a href="http://twitter.com/edsu">tweet</a> them at me. I'm especially interested in collections that fit the UC Irvine model of making born digital collections available on the Web via a researcher request step, or where they are simply publicly available. It's great to see traditional archives moving onto the Web this way to make born digital collections available.</p>

<p>As an aside it's interesting to me how the category of content we call <em>born-digital</em> is beginning to be coterminous with <em>Web content</em> at a particular point in time -- especially as more and more of our born digital content lives on the Web in cloud services of some ilk.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7024</wp:post_id>
		<wp:post_date>2014-01-16 11:41:53</wp:post_date>
		<wp:post_date_gmt>2014-01-16 18:41:53</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>natus-digital</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Ignoring Our Visual Heritage</title>
		<link>http://inkdroid.org/2014/01/20/ignoring-our-visual-heritage/</link>
		<pubDate>Mon, 20 Jan 2014 17:55:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7047</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I recently ran across <a href="http://www.nst.com.my/opinion/letters-to-the-editor/digital-storage-losing-our-visual-heritage-1.465485">Digital Storage: Losing Our Virtual Heritage</a> over on the (surprisingly interesting) <a href="http://www2.archivists.org/listservs/archives">SAA Archives &amp; Archivists</a> discussion list. Strangely, the editorial struck me as both emblematic of a problem in the archival community, and a guidepost for how archives need to move forward.</p>

<p>The key point Bromberg makes is that archives will no longer be able to function if our collections (specifically photographic collections) become digital:</p>

<blockquote>
  <p>Once I do basic work to care for my collections, I can put them on the shelf and pretty much not have to put any more money into their care. You cannot keep a digital file, however, without continually having to put money into it for the constant <strong>migration to new forms</strong>.</p>
  
  <p>You have to buy new software and equipment, and pay for the labour to migrate them to be able to continue to get access to your images. Right now, I can just walk to the shelf and open a box to get access to my photographs.</p>
  
  <p>This high cost of caring for digital files means that archives and museums which hold much of the world's recorded history will most likely not be able to afford to care for them. We already have small budgets to care for our materials and that is unlikely to change.</p>
</blockquote>

<p>Fear of format obsolescence is real and justified. But as David Rosenthal has been <a href="http://blog.dshr.org/2007/04/format-obsolescence-scenarios.html">pointing out</a> <a href="http://blog.dshr.org/2007/05/format-obsolescence-prostate-cancer-of.html">for a while</a> the shared information space of the Web, and its open-source viewers (browsers), have mitigated some of these concerns. We have yet to see evidence that prospective format migration actually helps preserve content. But our continued obsession with format migration, and describing resources so they can be migrated is making the task of archiving digital content (like photographs) cost prohibitive, especially for smaller archives. Do we really think that billions of JPEGs are going to become unreadable overnight?</p>

<p>Bromberg's piece contains a useful example:</p>

<blockquote>
  <p>I can get the Smith family photographs that Grandmother Smith put into a shoebox 50 years ago and forgot about it until her family cleaned out the house. I have packed up photo collections from families, businesses and organisations that contained images well over 100 years old that are perfectly fine. But if Grandmother Smith sticks some photo disks in her shoebox, by the time an archive gets them, they will be long gone.</p>
</blockquote>

<p>Is it really useful for us to put our collective head in the sand and say that digital photography is going away? Or would we be better off <a href="http://www.digitalpreservation.gov/personalarchiving/photos.html">helping</a> photographers take care of their digital collections, so that when it comes time to donate them, they have a digital equivalent of a box of photographs to hand over? I'm reminded of the rich literature about the <a href="http://www.paradigm.ac.uk/workbook/collection-development/post-custodial.html">post-custodial archive</a> where there is an emphasis on helping content owners manage their content, which in turn makes it easier to eventually transfer to an archive if desired. <a href="http://www.digitalpreservation.gov/personalarchiving/padKit/">Personal Digital Archiving Day</a> and <a href="http://dp.la/info/get-involved/reps/">DPLA's Community</a> program are good examples of this sort of effort.</p>

<p>I'm not suggesting that there isn't work to do on this front. Bromberg is right. In our quest for the holy grail of digital preservation our content management systems have raised the bar way too high, for everyday people, and small libraries and archives to continue to do for digital content what they have for physical content, like photographs. To succeed I think archives and libraries need simple solutions that let them easily collect digital content, manage it, and let it feed into larger collections like <a href="http://dp.la">DPLA</a>.</p>

<p>By simple solutions I mean mostly a process that content owners and archivists can keep in their heads, that involves very little software, and mostly represents an investment in digital storage and backup systems in the same way that they have invested in physical space, containers, etc. I suspect many individuals and small archives already have storage solutions in operation for their <em>business data</em>, so this won't be as big a leap as they imagine. But as long as we keep promulgating things like Fedora, DSpace etc as pre-requisites for doing <em>real</em> digital preservation Bromberg will be right.</p>

<p>To put it another way, we need a digital equivalent to the <a href="http://archivists.metapress.com/content/c741823776k65863/fulltext.pdf">More Product, Less Process</a> manifesto. This is the spirit that <a href="http://en.wikipedia.org/wiki/BagIt">BagIt</a> was created in at the Library of Congress. We needed to start processing an influx of digital content from NDIIPP partners, and we didn't have the time, resources, or collective will to describe everything with METS, PREMIS, MODS and put it into Fedora or iRods, or whatever.</p>

<p>You can think of a Bag as a digital analog for a physical container. It's just a directory with files in it, that includes a manifest, and some (optional) high level, human readable metadata. Certainly PREMIS, METS, etc can be layered on top of this, and we've done just that at LC with <a href="http://www.dlib.org/dlib/january09/littman/01littman.html">some</a> of our some of our internal systems ... but BagIt helps with the absolute basics of bundling up data so that it can be moved through space and time.</p>

<p>So I'm not suggesting that people should start using BagIt to manage their digital photographs. I actually think BagIt could be simplified even more (look for more on that later). My point is rather that we need super simple solutions, like BagIt, that involve very little working software, and that everyday people and archives can use. We need to educate, and move forward. A new army of archivist computer scientists isn't going to solve this problem. In a lot of ways, the computer savvy (but not Luddite) archivists we have are perfect for the job of educating artists, business people, and Grandma Smith since the solutions we give them need to fit inside their heads too. We are doing a disservice to them if we just say we can't handle the new medium, and pine for the good old days. Or as my colleague <a href="https://twitter.com/blefurgy/">Bill Lefurgy</a> <a href="https://twitter.com/blefurgy/status/425334701495418880">said</a> in response to this post:</p>

<blockquote>
  <p>... archivists at smaller institutions also need to push beyond digital fear and build capacity, even if slowly</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7047</wp:post_id>
		<wp:post_date>2014-01-20 10:55:49</wp:post_date>
		<wp:post_date_gmt>2014-01-20 17:55:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>ignoring-our-visual-heritage</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="bagit"><![CDATA[bagit]]></category>
		<category domain="post_tag" nicename="digital-preservation"><![CDATA[digital preservation]]></category>
		<category domain="post_tag" nicename="format-migration"><![CDATA[format migration]]></category>
		<category domain="post_tag" nicename="photographs"><![CDATA[photographs]]></category>
		<category domain="post_tag" nicename="software"><![CDATA[software]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86848</wp:comment_id>
			<wp:comment_author><![CDATA[archives and change | metahater]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://metahata.wordpress.com/2014/02/27/archives-and-change/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-02-26 18:29:39</wp:comment_date>
			<wp:comment_date_gmt>2014-02-27 01:29:39</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Summers makes a solid case for agile digital preservation in Ignoring Our Visual Heritage. The agile response to change is adaptation. Summers [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393494274.7087249755859375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393464580.0861759185791015625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87264</wp:comment_id>
			<wp:comment_author><![CDATA[Accepting change | Adam&#039;s LS 566 Metadata Course Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://adamls566metadatacourseblog.wordpress.com/2015/03/04/accepting-change/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-03-04 09:34:37</wp:comment_date>
			<wp:comment_date_gmt>2015-03-04 16:34:37</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] blog post in Inkdroid entitled &#8220;Ignoring Our Visual Heritage&#8221; really struck a chord with me. As anxious as I may feel at times about learning new technologies [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1425487667.3473250865936279296875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1425486877.2653141021728515625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87262</wp:comment_id>
			<wp:comment_author><![CDATA[Simple, simple, simple | MetaWhat! Data!]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://metawhatdata.wordpress.com/2015/02/27/simple-simple-simple/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-03-01 18:47:58</wp:comment_date>
			<wp:comment_date_gmt>2015-03-02 01:47:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] http://inkdroid.org/2014/01/20/ignoring-our-visual-heritage/ [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1425487689.772357940673828125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1425260878.7296879291534423828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87261</wp:comment_id>
			<wp:comment_author><![CDATA[Ignoring Our Visual Heritage | Metadata Blog LS566]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://beccabillings3.wordpress.com/2015/02/28/ignoring-our-visual-heritage/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-02-28 09:58:50</wp:comment_date>
			<wp:comment_date_gmt>2015-02-28 16:58:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] reading this blog post (linked here) I find myself a little worried about photographs and digital photography. Inkdroid (or whoever the [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1425142730.381866931915283203125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1425487695.0869181156158447265625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87258</wp:comment_id>
			<wp:comment_author><![CDATA[How much will we preserve? | The Story Doesn&#039;t Have to End at &quot;The End&quot;]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://tonyaolson.wordpress.com/2015/02/23/how-much-will-we-preserve/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-02-23 12:24:19</wp:comment_date>
			<wp:comment_date_gmt>2015-02-23 19:24:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] week I read a blog post entitled Ignoring Our Visual Heritage, which discussed the challenges of digital preservation, especially in regards to small libraries [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1424719460.085956096649169921875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1425307143.5938589572906494140625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Paris Review Interviews and Wikipedia</title>
		<link>http://inkdroid.org/2014/02/02/paris-review-interviews-and-wikipedia/</link>
		<pubDate>Mon, 03 Feb 2014 03:29:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7085</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://parisreview.org"><img src="http://inkdroid.org/images/parisreview-35.jpg" style="margin-right: 20px; float: left;" /></a> I was recently reading an amusing piece by <a href="https://twitter.com/David_Dobbs">David Dobbs</a> about <a href="http://daviddobbs.net/smoothpebbles/william-faulkner-is-one-tough-interview/">William Faulkner being a tough interview</a>. Dobbs has been working through the <a href="http://www.theparisreview.org/interviews">Paris Review archive of interviews</a> which are available on the Web. The list of authors is really astonishing, and the interviews are great examples of longform writing on the Web.</p>

<p>The 1965 interview with <a href="http://www.theparisreview.org/interviews/4424/the-art-of-fiction-no-36-william-s-burroughs">William S. Burroughs</a> really blew me away. So much so that I got to wondering how many Wikipedia articles reference these interviews.</p>

<p>A few years ago, I experimented with a site called <a href="http://linkypedia.info">Linkypedia</a> for visualizing how a particular website is referenced on Wikipedia. It's actually pretty easy to write a script to see what Wikipedia articles point at a Website, and I've done it enough times that it was convenient to wrap it up in a little <a href="https://github.com/edsu/wplinks">Python module</a>.</p>

<pre lang="python">from wplinks import extlinks 

for src, target in extlinks('http://www.theparisreview.org/interviews'):
    print wikipedia_url, website_url

</pre>

<p>But I wanted to get a picture not only of what Wikipedia articles pointed at the Paris Review, but also Paris Review interviews which were not referenced in Wikipedia. So I wrote a little <a href="https://github.com/edsu/parisreview/blob/master/crawl.py">crawler</a> that collected all the Paris Review interviews, and then figured out which ones were pointed at by English Wikipedia.</p>

<p>This was also an excuse to learn about <a href="http://json-ld.org">JSON-LD</a>, which became a <a href="http://www.w3.org/TR/json-ld/">W3C Recommendation</a> a few weeks ago. I wanted to use JSON-LD to serialize the results of my crawling as an RDF graph so I could visualize the connections between authors, their interviews, and each other (via influence links that can be found on <a href="http://dbpedia.org">dbpedia</a>) using <a href="https://github.com/mbostock/d3/wiki/Force-Layout">D3's Force Layout</a>. Here's a little portion of the larger graph, which you can find by clicking on it.</p>

<p><a href="http://edsu.github.io/parisreview"><img style="border: thin solid gray;" src="http://inkdroid.org/images/parisreview.png" /></a></p>

<p>As you can see it's a bit of a hairball. If you want to have a go at visualizing the data the JSON-LD can be found <a href="https://github.com/edsu/parisreview/blob/master/js/parisreview.json">here</a>. The blue nodes are Wikipedia articles, the white and red nodes are Paris Review interviews. The red ones are interviews that are not yet linked to from Wikipedia. 322 of the 362 interviews are already linked to Wikipedia. Here is the list of 40 that still need to be linked, in the unlikely event that you are a Wikipedian looking for something to do:</p>

<ul>
<li><a href="http://www.theparisreview.org/interviews/1105/the-art-of-fiction-no-153-ismail-kadare">Ismail Kadaré</a></li>
<li><a href="http://www.theparisreview.org/interviews/3195/the-art-of-fiction-no-68-carlos-fuentes">Carlos Fuentes</a></li>
<li><a href="http://www.theparisreview.org/interviews/4779/the-art-of-fiction-no-22-james-jones">James Jones</a></li>
<li><a href="http://www.theparisreview.org/interviews/5197/the-art-of-fiction-no-2-francois-mauriac">François Mauriac</a></li>
<li><a href="http://www.theparisreview.org/interviews/2631/the-art-of-fiction-no-99-peter-taylor">Peter Taylor</a></li>
<li><strike><a href="http://www.theparisreview.org/interviews/2819/the-art-of-fiction-no-91-alain-robbe-grillet">Alain Robbe-Grillet</a></strike></li>
<li><strike><a href="http://www.theparisreview.org/interviews/1432/the-art-of-screenwriting-no-1-billy-wilder">Billy Wilder</a></strike></li>
<li><strike><a href="http://www.theparisreview.org/interviews/1551/the-art-of-humor-no-2-garrison-keillor">Garrison Keillor</a></strike></li>
<li><a href="http://www.theparisreview.org/interviews/3741/the-art-of-poetry-no-20-james-dickey">James Dickey</a></li>
<li><a href="http://www.theparisreview.org/interviews/4502/the-art-of-fiction-no-33-louis-ferdinand-celine">Louis-Ferdinand Céline</a></li>
<li><a href="http://www.theparisreview.org/interviews/1963/an-interview-sybille-bedford">Sybille Bedford</a></li>
<li><a href="http://www.theparisreview.org/interviews/5093/the-art-of-fiction-no-6-alberto-moravia">Alberto Moravia</a></li>
<li><a href="http://www.theparisreview.org/interviews/1218/the-art-of-poetry-no-76-robert-pinsky">Robert Pinsky</a></li>
<li><a href="http://www.theparisreview.org/interviews/2442/the-art-of-fiction-no-108-william-trevor">William Trevor</a></li>
<li><a href="http://www.theparisreview.org/interviews/5863/the-art-of-fiction-no-198-marilynne-robinson">Marilynne Robinson</a></li>
<li><a href="http://www.theparisreview.org/interviews/1395/the-art-of-fiction-no-146-william-f-buckley-jr">William F. Buckley Jr.</a></li>
<li><a href="http://www.theparisreview.org/interviews/3773/the-art-of-fiction-no-60-p-g-wodehouse">P. G. Wodehouse</a></li>
<li><a href="http://www.theparisreview.org/interviews/4463/the-art-of-theater-no-1-lillian-hellman">Lillian Hellman</a></li>
<li><a href="http://www.theparisreview.org/interviews/4537/the-art-of-fiction-no-30-evelyn-waugh">Evelyn Waugh</a></li>
<li><a href="http://www.theparisreview.org/interviews/4134/the-art-of-poetry-no-12-charles-olson">Charles Olson</a></li>
<li><strike><a href="http://www.theparisreview.org/interviews/5601/the-art-of-nonfiction-no-1-joan-didion">Joan Didion</a></strike></li>
<li><a href="http://www.theparisreview.org/interviews/5991/the-art-of-fiction-no-202-ha-jin">Ha Jin</a></li>
<li><a href="http://www.theparisreview.org/interviews/3228/the-art-of-fiction-no-66-donald-barthelme">Donald Barthelme</a></li>
<li><a href="http://www.theparisreview.org/interviews/2576/the-art-of-fiction-no-100-hortense-calisher">Hortense Calisher</a></li>
<li><a href="http://www.theparisreview.org/interviews/4052/the-art-of-poetry-no-16-john-berryman">John Berryman</a></li>
<li><strike><a href="http://www.theparisreview.org/interviews/1550/the-art-of-humor-no-1-woody-allen">Woody Allen</a></strike></li>
<li><a href="http://www.theparisreview.org/interviews/4283/the-art-of-poetry-no-9-conrad-aiken">Conrad Aiken</a></li>
<li><strike><a href="http://www.theparisreview.org/interviews/3605/the-art-of-fiction-no-64-kurt-vonnegut">Kurt Vonnegut</a></strike></li>
<li><a href="http://www.theparisreview.org/interviews/4242/the-art-of-fiction-no-42-isaac-bashevis-singer">Isaac Bashevis Singer</a></li>
<li><a href="http://www.theparisreview.org/interviews/2/the-art-of-fiction-no-182-haruki-murakami">Haruki Murakami</a></li>
<li><a href="http://www.theparisreview.org/interviews/5003/the-art-of-fiction-no-10-james-thurber">James Thurber</a></li>
<li><strike><a href="http://www.theparisreview.org/interviews/2467/the-art-of-theater-no-7-tom-stoppard">Tom Stoppard</a></strike></li>
<li><a href="http://www.theparisreview.org/interviews/4350/the-art-of-theater-no-4-edward-albee">Edward Albee</a></li>
<li><strike><a href="http://www.theparisreview.org/interviews/3209/the-art-of-theater-no-5-tennessee-williams">Tennessee Williams</a></strike></li>
<li><a href="http://www.theparisreview.org/interviews/4503/the-art-of-fiction-no-32-norman-mailer">Norman Mailer</a></li>
<li><strike><a href="http://www.theparisreview.org/interviews/3439/the-art-of-fiction-no-71-joan-didion">Joan Didion</a></strike></li>
<li><a href="http://www.theparisreview.org/interviews/5740/the-art-of-fiction-no-192-jorge-semprun">Jorge Semprún</a></li>
<li><a href="http://www.theparisreview.org/interviews/1396/the-art-of-fiction-no-145-camilo-jose-cela">Camilo José Cela</a></li>
<li><strike><a href="http://www.theparisreview.org/interviews/1250/the-art-of-fiction-no-149-john-le-carre">John le Carré</a></strike></li>
<li><strike><a href="http://www.theparisreview.org/interviews/3441/the-art-of-fiction-no-72-joyce-carol-oates">Joyce Carol Oates</a></strike></li>
</ul>

<p>I ran into my friend <a href="http://twitter.com/dchud">Dan</a> over coffee who sketched out a better way to visualize the relationships between the writers, the interviews and the time periods. Might be a good excuse to get a bit more familiar with D3 ...</p>

<p><a href="http://www.flickr.com/photos/inkdroid/12256021415/"><img src="http://inkdroid.org/images/parisreview-sketch.jpg" alt="@dchud sketch" /></a></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7085</wp:post_id>
		<wp:post_date>2014-02-02 20:29:06</wp:post_date>
		<wp:post_date_gmt>2014-02-03 03:29:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>paris-review-interviews-and-wikipedia</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="post_tag" nicename="literature"><![CDATA[literature]]></category>
		<category domain="post_tag" nicename="paris-review"><![CDATA[paris review]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86852</wp:comment_id>
			<wp:comment_author><![CDATA[February Rundown | Library Manifesto]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://librarymanifesto.com/2014/03/february-rundown/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-02-28 22:20:53</wp:comment_date>
			<wp:comment_date_gmt>2014-03-01 05:20:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] are using data analysis to show interesting relationships. Ed Summer&#8217;s data visualization shows how many times each Paris Review interview appears on [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393670112.975493907928466796875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Design for the Rubble</title>
		<link>http://inkdroid.org/2014/02/18/design-for-the-rubble/</link>
		<pubDate>Tue, 18 Feb 2014 16:23:21 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7135</guid>
		<description></description>
		<content:encoded><![CDATA[<p>If you are interested in issues of preservation and technology you might get a kick out of <a href="http://longnow.org/seminars/02014/jan/21/long-now-now/">this chat</a> between inventor <a href="https://en.wikipedia.org/wiki/Danny_Hillis">Danny Hillis</a> and musician/artist <a href="https://en.wikipedia.org/wiki/Brian_Eno">Brian Eno</a>. It's the latest seminar in long-term thinking from the <a href="http://longnow.org/seminars/02014/jan/21/long-now-now/">Long Now Foundation</a>. The backdrop to the conversation is their work over the last 10 years designing the <a href="http://longnow.org/clock/">10,000 year clock</a>. It's a subtle, and (appropriately) long talk, that was a real pleasure to listen to over the course of three evening dog walks.</p>

<p>A theme they explored was how difficult (and rewarding) it is to think over really long time frames. The further they tried to look into the future, the more they found themselves looking into the past, and getting in touch with the present. They considered the therapeutic value of considering our small parts to play in the sweep of history, the need to surrender to it...to embrace complexity, and work with the muddle of everything, rather than trying to bend it to our will, and our plans.</p>

<p>Plus there are lots of humorous moments along the way about Microsoft Excel's Y10K bug, white on white paintings, the singularity that already happened, and more.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7135</wp:post_id>
		<wp:post_date>2014-02-18 09:23:21</wp:post_date>
		<wp:post_date_gmt>2014-02-18 16:23:21</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>design-for-the-rubble</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="brian-eno"><![CDATA[brian eno]]></category>
		<category domain="post_tag" nicename="danny-hillis"><![CDATA[danny hillis]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="time"><![CDATA[time]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Summoner</title>
		<link>http://inkdroid.org/2014/02/19/summoner/</link>
		<pubDate>Wed, 19 Feb 2014 10:14:28 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7150</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I've recently been working with the Serials Solutions <a href="http://api.summon.serialssolutions.com/">Summon API</a> for some side work at <a href="http://library.gwu.edu">George Washington University Libraries</a>. Since the work is largely in Python I created a very simple reusable module called <a href="http://github.com/edsu/summoner">summoner</a> for talking to the API. It largely just does the authentication bits, and gets out of the way...but it may be of interest to you if you use Python and work at a library that subscribes to Summon.</p>

<p>Anyway, I found myself wanting to get a picture of the metadata fields that are used in Summon responses, so I wrote a <a href="https://github.com/edsu/summoner/blob/master/examples/fields.py">little script</a> that does an open ended search and then walks through lots of results tallying up the fields used by <em>content type</em>. You can find the results for a scan of 50,000 records below.</p>

<p>As you can see the majority (78%) of the results were newspaper and journal articles. There was a fair bit of diversity in the fields returned for these. It was kind of interesting that 83% of the journal articles had ISSNs (I was expecting it to be higher). 50% had EISSNs so perhaps the 17% that lacked ISSNs had an EISSN instead. Only 41% of journals had subject terms. The sampling of the other content areas was probably too low to make any guesses.</p>

<p>The big caveat here is that these numbers reflect GWU's particular holdings and subscription options. Still, if you use the Summon API you might find them interesting; and if you are really curious you can run <a href="https://github.com/edsu/summoner/blob/master/examples/fields.py">the script</a> for your own institution.</p>

<h3>Newspaper Articles (54.0%)</h3>

<table>
  <tr>
    <th>
      Field Name
    </th>
    
    <th>
      Coverage
    </th>
  </tr>
  
  <tr>
    <td>
      PublicationYear
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      openUrl
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DatabaseTitle
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Title
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Score
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationTitle
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      link
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDecade
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Publisher_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LinkModel
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DBID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ISSN
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      MergedId
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      isFullTextHit
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PQPubID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      StartPage
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Genre
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Publisher
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SourceType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsScholarly
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Language
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ExternalDocumentID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      URI
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationCentury
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsPeerReviewed
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      inHoldings
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ContentType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SSID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_l
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_m
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_s
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      hasFullText
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      EISSN
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PQID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Snippet
    </td>
    
    <td>
      88.9%
    </td>
  </tr>
  
  <tr>
    <td>
      DatabaseTitleList
    </td>
    
    <td>
      88.9%
    </td>
  </tr>
  
  <tr>
    <td>
      AbstractList
    </td>
    
    <td>
      88.9%
    </td>
  </tr>
  
  <tr>
    <td>
      Abstract
    </td>
    
    <td>
      88.9%
    </td>
  </tr>
  
  <tr>
    <td>
      SubjectTerms
    </td>
    
    <td>
      74.1%
    </td>
  </tr>
  
  <tr>
    <td>
      GeographicLocations
    </td>
    
    <td>
      14.8%
    </td>
  </tr>
  
  <tr>
    <td>
      GeographicLocations_xml
    </td>
    
    <td>
      14.8%
    </td>
  </tr>
</table>

<h3>Journal Articles (24.0%)</h3>

<table>
  <tr>
    <th>
      Field Name
    </th>
    
    <th>
      Coverage
    </th>
  </tr>
  
  <tr>
    <td>
      PublicationYear
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      openUrl
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Score
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationTitle
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      link
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDecade
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      isFullTextHit
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LinkModel
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Title
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      MergedId
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ExternalDocumentID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsScholarly
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Language
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      hasFullText
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsPeerReviewed
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      inHoldings
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ContentType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SourceType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationCentury
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author
    </td>
    
    <td>
      91.7%
    </td>
  </tr>
  
  <tr>
    <td>
      Author_xml
    </td>
    
    <td>
      91.7%
    </td>
  </tr>
  
  <tr>
    <td>
      StartPage
    </td>
    
    <td>
      91.7%
    </td>
  </tr>
  
  <tr>
    <td>
      ISSN
    </td>
    
    <td>
      83.3%
    </td>
  </tr>
  
  <tr>
    <td>
      Issue
    </td>
    
    <td>
      83.3%
    </td>
  </tr>
  
  <tr>
    <td>
      Volume
    </td>
    
    <td>
      83.3%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_l
    </td>
    
    <td>
      83.3%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_m
    </td>
    
    <td>
      83.3%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_s
    </td>
    
    <td>
      83.3%
    </td>
  </tr>
  
  <tr>
    <td>
      Publisher_xml
    </td>
    
    <td>
      75.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Publisher
    </td>
    
    <td>
      75.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SSID
    </td>
    
    <td>
      75.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DatabaseTitle
    </td>
    
    <td>
      58.3%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright
    </td>
    
    <td>
      58.3%
    </td>
  </tr>
  
  <tr>
    <td>
      DBID
    </td>
    
    <td>
      58.3%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright_xml
    </td>
    
    <td>
      58.3%
    </td>
  </tr>
  
  <tr>
    <td>
      URI
    </td>
    
    <td>
      50.0%
    </td>
  </tr>
  
  <tr>
    <td>
      EISSN
    </td>
    
    <td>
      50.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Snippet
    </td>
    
    <td>
      41.7%
    </td>
  </tr>
  
  <tr>
    <td>
      Genre
    </td>
    
    <td>
      41.7%
    </td>
  </tr>
  
  <tr>
    <td>
      SubjectTerms
    </td>
    
    <td>
      41.7%
    </td>
  </tr>
  
  <tr>
    <td>
      PQID
    </td>
    
    <td>
      41.7%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace_xml
    </td>
    
    <td>
      33.3%
    </td>
  </tr>
  
  <tr>
    <td>
      Database_xml
    </td>
    
    <td>
      33.3%
    </td>
  </tr>
  
  <tr>
    <td>
      PQPubID
    </td>
    
    <td>
      33.3%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace
    </td>
    
    <td>
      33.3%
    </td>
  </tr>
  
  <tr>
    <td>
      EndPage
    </td>
    
    <td>
      33.3%
    </td>
  </tr>
  
  <tr>
    <td>
      Discipline
    </td>
    
    <td>
      25.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PageCount
    </td>
    
    <td>
      25.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Notes
    </td>
    
    <td>
      25.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DatabaseTitleList
    </td>
    
    <td>
      25.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Audience
    </td>
    
    <td>
      25.0%
    </td>
  </tr>
  
  <tr>
    <td>
      AbstractList
    </td>
    
    <td>
      25.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Abstract
    </td>
    
    <td>
      25.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DOI
    </td>
    
    <td>
      16.7%
    </td>
  </tr>
  
  <tr>
    <td>
      RelatedPersons
    </td>
    
    <td>
      16.7%
    </td>
  </tr>
  
  <tr>
    <td>
      RelatedPersons_xml
    </td>
    
    <td>
      16.7%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationSeriesTitle
    </td>
    
    <td>
      16.7%
    </td>
  </tr>
  
  <tr>
    <td>
      ClassificationCodes
    </td>
    
    <td>
      16.7%
    </td>
  </tr>
  
  <tr>
    <td>
      CODEN
    </td>
    
    <td>
      8.3%
    </td>
  </tr>
  
  <tr>
    <td>
      GeographicLocations
    </td>
    
    <td>
      8.3%
    </td>
  </tr>
  
  <tr>
    <td>
      DLL_JC
    </td>
    
    <td>
      8.3%
    </td>
  </tr>
  
  <tr>
    <td>
      GeographicLocations_xml
    </td>
    
    <td>
      8.3%
    </td>
  </tr>
  
  <tr>
    <td>
      PCID
    </td>
    
    <td>
      8.3%
    </td>
  </tr>
</table>

<h3>Reference (10.0%)</h3>

<table>
  <tr>
    <th>
      Field Name
    </th>
    
    <th>
      Coverage
    </th>
  </tr>
  
  <tr>
    <td>
      PublicationYear
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      openUrl
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Title
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Source
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Score
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationTitle
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      link
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ISBN
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDecade
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Publisher_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsPeerReviewed
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LinkModel
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      MergedId
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      isFullTextHit
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ExternalDocumentID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Publisher
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsScholarly
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Language
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationCentury
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SourceType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      inHoldings
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ContentType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SSID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_l
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_m
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_s
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      hasFullText
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright
    </td>
    
    <td>
      80.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Snippet
    </td>
    
    <td>
      80.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DBID
    </td>
    
    <td>
      80.0%
    </td>
  </tr>
  
  <tr>
    <td>
      URI
    </td>
    
    <td>
      80.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright_xml
    </td>
    
    <td>
      80.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Discipline
    </td>
    
    <td>
      60.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SubjectTerms
    </td>
    
    <td>
      40.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace_xml
    </td>
    
    <td>
      20.0%
    </td>
  </tr>
  
  <tr>
    <td>
      EISBN
    </td>
    
    <td>
      20.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author_xml
    </td>
    
    <td>
      20.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Edition
    </td>
    
    <td>
      20.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author
    </td>
    
    <td>
      20.0%
    </td>
  </tr>
  
  <tr>
    <td>
      StartPage
    </td>
    
    <td>
      20.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace
    </td>
    
    <td>
      20.0%
    </td>
  </tr>
</table>

<h3>Patents (6.0%)</h3>

<table>
  <tr>
    <th>
      Field Name
    </th>
    
    <th>
      Coverage
    </th>
  </tr>
  
  <tr>
    <td>
      Discipline
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationYear
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      openUrl
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Snippet
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Score
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      link
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDecade
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsPeerReviewed
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LinkModel
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DBID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Title
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      MergedId
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      isFullTextHit
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ExternalDocumentID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsScholarly
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Language
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Notes
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      URI
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DatabaseTitleList
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationCentury
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      AbstractList
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      inHoldings
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ContentType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SourceType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Abstract
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      hasFullText
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
</table>

<h3>Books (2.0%)</h3>

<table>
  <tr>
    <th>
      Field Name
    </th>
    
    <th>
      Coverage
    </th>
  </tr>
  
  <tr>
    <td>
      PublicationYear
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      openUrl
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Source
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Score
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      link
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LCCallNum
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDecade
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsPeerReviewed
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LinkModel
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LCCN
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DBID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Title
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      MergedId
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      isFullTextHit
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ExternalDocumentID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsScholarly
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Language
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      URI
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationCentury
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      OCLC
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      FullText_t_NoSnippeting
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      inHoldings
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ContentType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SourceType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      hasFullText
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
</table>

<h3>Web Resources (2.0%)</h3>

<table>
  <tr>
    <th>
      Field Name
    </th>
    
    <th>
      Coverage
    </th>
  </tr>
  
  <tr>
    <td>
      PublicationYear
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      openUrl
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Score
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      link
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDecade
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Author_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsPeerReviewed
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LinkModel
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DBID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Title
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      MergedId
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      isFullTextHit
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SubjectTerms
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ExternalDocumentID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsScholarly
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Language
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Notes
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      URI
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      hasFullText
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      inHoldings
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ContentType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SourceType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationCentury
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
</table>

<h3>Book Chapters (2.0%)</h3>

<table>
  <tr>
    <th>
      Field Name
    </th>
    
    <th>
      Coverage
    </th>
  </tr>
  
  <tr>
    <td>
      Discipline
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationYear
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DOI
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      openUrl
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Title
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      EISBN
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Snippet
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Score
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationTitle
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      link
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ISBN
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDecade
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Publisher_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsPeerReviewed
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      LinkModel
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      MergedId
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      isFullTextHit
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      StartPage
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationPlace
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationDate_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SubjectTerms
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Publisher
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SourceType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      IsScholarly
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Language
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ExternalDocumentID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      DatabaseTitleList
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      PublicationCentury
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      AbstractList
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      SSID
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      inHoldings
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      ContentType
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      EndPage
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Abstract
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_l
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_m
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      thumbnail_s
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      hasFullText
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
  
  <tr>
    <td>
      Copyright_xml
    </td>
    
    <td>
      100.0%
    </td>
  </tr>
</table>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7150</wp:post_id>
		<wp:post_date>2014-02-19 03:14:28</wp:post_date>
		<wp:post_date_gmt>2014-02-19 10:14:28</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>summoner</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Presidential Papers</title>
		<link>http://inkdroid.org/2014/02/19/presidential-papers/</link>
		<pubDate>Wed, 19 Feb 2014 16:05:19 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7177</guid>
		<description></description>
		<content:encoded><![CDATA[<p>There's an interesting <a href="http://www.nytimes.com/2014/02/19/opinion/obama-and-his-library-go-small.html">piece</a> in the New York Times this morning: about the future of Obama's Presidential library</p>

<blockquote>
  <p>There are reports that Mr. Obama used to be skeptical of having a library at all; a bold move would be to revert to tradition and deposit his papers at the Library of Congress. (The National Archives and Records Administration manages the 13 presidential libraries, which nowadays are built and maintained with private funds. The Herbert C. Hoover Library opened in 1962.) I knew that the <a href="http://memory.loc.gov/ammem/presprvw/23pres.html">Presidential Papers</a> collection at the Library of Congress, but never knew that LC was the place Presidents would deposit their papers before FDR built the first presidential library.</p>
</blockquote>

<p>I don't know if this list is completely accurate, but I diffed the list of pre-FDR presidents with the list of Presidential Papers and came up with this short list of presidents who didn't choose to deposit their papers with the Library of Congress:</p>

<ul>
<li>John Adams</li>
<li>John Quincy Adams</li>
<li>Millard Fillmore</li>
<li>James Buchanan</li>
<li>Rutherford B Hayes</li>
<li>Warren Harding</li>
</ul>

<p>At least <a href="http://www.bloomberg.com/news/2014-02-17/obama-s-presidential-library-belongs-in-the-cloud.html?alcmpid=view">one person</a> thinks Obama's library belongs in the cloud. I'm not 100% sure what they mean by the cloud, but embracing the distributed nature of archival collections, and using the Web to achieve that sounds like a good idea.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7177</wp:post_id>
		<wp:post_date>2014-02-19 09:05:19</wp:post_date>
		<wp:post_date_gmt>2014-02-19 16:05:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>presidential-papers</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>OCLC Works</title>
		<link>http://inkdroid.org/2014/02/26/oclc-works/</link>
		<pubDate>Wed, 26 Feb 2014 17:44:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7183</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The news about OCLC's Linked Data service <a href="http://dataliberate.com/2014/02/oclc-preview-194-million-open-bibliographic-work-descriptions/">circulated</a> <a href="http://hangingtogether.org/?p=3614">widely</a> on Twitter yesterday. I've never been a big OCLC cheerleader, but the news really hit home for me. I've been writing in my rambling way about <a href="http://inkdroid.org/tag/linked-data/">Linked Data</a> here for about <a href="http://inkdroid.org/journal/2008/01/04/following-your-nose-to-the-web-of-data/">6 years</a>. Of course there are many others who've been at it much longer than I have ... and in a way I think librarians and archivists feel a kinship with the effort because it is cooked into the DNA of how we think about the Web as an information space.</p>

<h2>Like Button</h2>

<p>This new OCLC service struck me as an excellent development for the library Web community for a few reasons, that I thought I would quickly jot down:</p>

<ul>
<li><strong>it's evolutionary</strong>: OCLC didn't let the perfect be the enemy of the good. It's great to hear links to VIAF, FAST, LCSH, etc are planned. But you have to start somewhere, and there is already significant value in expressing the FRBR workset data they have as Linked Data on the Web for others to use. Also, the domain <code>experiment.worldcat.org</code> clearly reflects this is an experiment...but they didn't let anxiety about changing URLs prevent them from publishing what they can now. The future is longer than the past.</li>
<li><strong>it's snappy</strong>: I don't know if they've written about the technical architecture they are using, but the views are quite responsive. Of course I have no idea what kind of load it is under, but so far so good. <em>Update: Ron Buckley of OCLC <a href="https://twitter.com/buckley_ron/status/438819400313868288">let me know</a> the service is built on top of a shared <a href="https://twitter.com/buckley_ron/status/438819400313868288">Apache HBase</a> Hadoop cluster.</em></li>
<li><strong>schema.org</strong>: OCLC has the brains and the market position to create their own vocabulary for bibliographic data. But they worked hard at engaging openly with the <a href="http://www.w3.org/community/schemabibex/">Web community</a> to help clarify and adapt the Schema.org vocabulary so that it can be used by our community. There is lots of thrashing going on in this space at the moment, and OCLC is being a great model in trying to work with the Web we have, and iterating to make it better, instead of trying to take a quantum leap forward.</li>
<li><strong>json-ld</strong>: JSON-LD has been cooking for a while, but it's a brand new W3C standard for representing RDF as idiomatic JSON. RDF has been somewhat plagued in the past by esoteric and/or hard to understand representations. JSON-LD really seems to have <a href="http://manu.sporny.org/2014/json-ld-origins-2/">hit</a> a sweet-spot between the expressivity of RDF and the usability of the Web. It's refreshing to see OCLC kicking JSON-LD's tires.</li>
</ul>

<h2>Rubber Meet Road</h2>

<p>So how do you discover these Work URIs? Richard's <a href="http://dataliberate.com/2014/02/oclc-preview-194-million-open-bibliographic-work-descriptions/">post</a> led me to believe I could get them directly from the xID service using an ISBN. But I found it to be a two step process: first get any OCLC Number associated with an ISBN from xID, and then use the OCLC Number to get the Work Identifier from the xID service:</p>

<p>So for example, to discover the Work URI for Tim Berners-Lee's <a href="http://www.w3.org/People/Berners-Lee/Weaving/">Weaving the Web</a> you first look up the ISBN:</p>

<p><code>http://xisbn.worldcat.org/webservices/xid/isbn/0062515861?method=getMetadata&amp;format=json&amp;fl=*</code></p>

<p>which should yield:</p>

<pre language="json">{
    "list": [
        {
            "author": "Tim Berners-Lee with Mark Fischetti.",
            "city": "San Francisco",
            "ed": "1st ed.",
            "form": [
                "AA",
                "BA"
            ],
            "isbn": [
                "0062515861"
            ],
            "lang": "eng",
            "lccn": [
                "99027665",
                "00039593"
            ],
            "oclcnum": [
                "300691968",
                "318261941",
                "410824754",
                "41238513",
                "470718156",
                "558595430",
                "628749869",
                "768228949",
                "807901805",
                "43903751",
                "699807622"
            ],
            "publisher": "HarperSanFrancisco.",
            "title": "Weaving the Web : the original design and ultimate destiny of the World Wide Web by its inventor",
            "url": [
                "http://www.worldcat.org/oclc/300691968?referer=xid"
            ],
            "year": "1999"
        }
    ],
    "stat": "ok"
}
</pre>

<p>Then pick one of the OCLC Numbers (oclcnum) at random and use it to do an xID call:</p>

<p><code>http://xisbn.worldcat.org/webservices/xid/oclcnum/300691968?method=getMetadata&amp;format=json&amp;fl=*</code></p>

<p>Which should return:</p>

<pre language="json">{
    "list": [
        {
            "isbn": [
                "9780062515865",
                "9780062515872"
            ],
            "lccn": [
                "99027665"
            ],
            "oclcnum": [
                "300691968"
            ],
            "owi": [
                "owi27331745"
            ]
        }
    ],
    "stat": "ok"
}
</pre>

<p>You can then dig out the Work Identifier (owi), trim off the owi prefix, and put it on the end of a URL like:</p>

<p><code>http://experiment.worldcat.org/entity/work/data/27331745</code></p>

<p>or, if you want the JSON-LD without doing content negotiation:</p>

<p><code>http://experiment.worldcat.org/entity/work/data/27331745.jsonld</code></p>

<p>This returns a chunk of JSON data that I won't reproduce here, but do <a href="http://experiment.worldcat.org/entity/work/data/27331745.jsonld">check it out</a>.</p>

<p><em>Update: After hitting publish on this blog post I've corresponded a bit with Stephan Schindehette at OCLC and <a href="http://twitter.com/invisiblecomma">Alf Eaton</a> about some inconsistencies in my blog post (which I've fixed), and uncertainty about what the xID API should be returning. Hopefully xID can be updated to return the OCLC Work Identifier when you lookup by ISBN. I'll update this blog post if I am notified of a change.</em></p>

<h2>Peanut Gallery</h2>

<p>One bit of advice that I was given by <a href="https://github.com/dlongley">Dave Longley</a> on the #json-ld IRC channel, which I will pass along to OCLC, is that it might be better to use CURIE-less properties, e.g. <code>name</code> instead of <code>schema:name</code>, to make it easier to use (and read) the JSON from JavaScript. To do this you would need a more expressive <code>@context</code> but I think it might make sense to <a href="http://www.w3.org/TR/json-ld/#the-context">reference an external context document</a> and cut down on the size of the JSON-LD document even more.</p>

<p>It's wonderful to see that the data is being licensed ODC-BY, but maybe assertions to that effect should be there in the data as well? I think schema.org have steered clear of licensing properties, but cc:license seems like a reasonable property to use, assuming it's used with the right subject URI.</p>

<p>And one last tiny suggestion I have is that it would be nice to see the service mainstreamed into <a href="http://www.oclc.org/data/data-sets-services.en.html">other</a> <a href="http://oclc.org/research/activities/linkeddata.html">parts</a> of OCLC's website. But I understand all too well the divides between R&amp;D and production ... and how challenging it can be to integrate them sometimes, even in the simplest of ways.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7183</wp:post_id>
		<wp:post_date>2014-02-26 10:44:23</wp:post_date>
		<wp:post_date_gmt>2014-02-26 17:44:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oclc-works</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="isbn"><![CDATA[isbn]]></category>
		<category domain="post_tag" nicename="json-ld"><![CDATA[json-ld]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="linked-data"><![CDATA[linked data]]></category>
		<category domain="post_tag" nicename="oclc"><![CDATA[oclc]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_post_restored_from</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:20:"restored_revision_id";i:7230;s:16:"restored_by_user";i:2;s:13:"restored_time";i:1393674306;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86846</wp:comment_id>
			<wp:comment_author><![CDATA[Can you please remove &#8216;meaningful punctuation&#8217; from field contents, librarians? |Bende]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://ben.companjen.name/2014/02/can-you-please-remove-meaningful-punctuation-from-field-contents-librarians/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-02-26 16:02:13</wp:comment_date>
			<wp:comment_date_gmt>2014-02-26 23:02:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] structure and semantics can be improved, as Richard Wallis of OCLC says in a blog post. The example that Ed took in his blog post, &#8220;Weaving the Web&#8221; by Tim Berners-Lee, demonstrates my issue (which is not touched upon [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393494286.3435161113739013671875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393455733.7828509807586669921875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86844</wp:comment_id>
			<wp:comment_author><![CDATA[sschindehette]]></wp:comment_author>
			<wp:comment_author_email>schindes@oclc.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-02-26 13:26:16</wp:comment_date>
			<wp:comment_date_gmt>2014-02-26 20:26:16</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank you for the suggestions.  We are already working on some of the ideas and we will add the others to our list.

Just a note on retrieving Work URIs - Your screen shot of xISBN results lists the OWI so you could have taken the value from the first step.

The good thing about your examples is that they show you can come at it from a number of angles.  Knowing any one of the xID identifiers (ISBN, OCN or LCCN) will allow you to get the Work identifier.  Having the Work URI released in the WorldCat linked data will make this step even easier.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2139</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393494292.576983928680419921875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393446376.667480945587158203125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:13:"sschindehette";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86849</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-02-27 02:55:50</wp:comment_date>
			<wp:comment_date_gmt>2014-02-27 09:55:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Thanks for noticing that -- I'm kind of embarrassed I didn't! That simplifies things significantly, so I've adjusted the blog post appropriately.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>86844</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393494950.2320709228515625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86853</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-03-01 04:57:54</wp:comment_date>
			<wp:comment_date_gmt>2014-03-01 11:57:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>I've flipped the blog post back to the original content (2 xID lookups) but with the correct JSON response this time. Hopefully xID can be updated to return the OCLC Work Identifier consistently. Thanks very much Stephan Schindehette and Alf Eaton for noticing the inconsistencies.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>86844</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393675074.520845890045166015625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86847</wp:comment_id>
			<wp:comment_author><![CDATA[194 Million Linked Open Data Bibliographic Work Descriptions Released by OCLC - Semanticweb.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://semanticweb.com/194-million-linked-open-data-bibliographic-work-descriptions-released-oclc_b41921</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-02-26 16:31:53</wp:comment_date>
			<wp:comment_date_gmt>2014-02-26 23:31:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] now, it&#8217;s great to see this start, and as Ed Summers says in this post, &#8220;OCLC didn’t let the perfect be the enemy of the good.&#8221; [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393494281.6455290317535400390625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1393457514.3198759555816650390625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Dissecting GettyImage Embeds</title>
		<link>http://inkdroid.org/2014/03/10/dissecting-gettyimage-embeds/</link>
		<pubDate>Mon, 10 Mar 2014 09:23:04 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7250</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Yes, GettyImages have decided to <a href="http://en.blog.wordpress.com/2014/03/06/embed-support-getty-images/">encourage</a> people to embed their images. Despite <a href="http://www.poststat.us/getty-images-now-free-embed/">opinions to the contrary</a> I think this is A Good Thing. So what happens when you embed a Getty image into your HTML? To get something like this in your page:</p>

<iframe src="//embed.gettyimages.com/embed/81901686?et=4td6Xm2f0k6pMgQVX7pNFA&sig=fhRom4eoepnZbyWjZ0_2N3SdVG1dxQTC2GUAK4XrPjg=" width="462" height="440" frameborder="0" scrolling="no"></iframe>

<p>you need to include a little snippet of HTML in your pages:</p>

<pre>&lt;iframe src="//embed.gettyimages.com/embed/81901686?et=4td6Xm2f0k6pMgQVX7pNFA&sig=fhRom4eoepnZbyWjZ0_2N3SdVG1dxQTC2GUAK4XrPjg=" width="462" height="440" frameborder="0" scrolling="no"&gt;&lt;/iframe&gt;
</pre>

<p>which in turn embeds this HTML into your page:</p>

<pre>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;base target="_parent" /&gt;
    &lt;title&gt;20 - 30 year old female worker pulls box off of warehouse shelf [Getty Images]&lt;/title&gt;
    &lt;meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /&gt;
    &lt;!--[if lt IE 10]&gt;
    &lt;script src="//html5shiv.googlecode.com/svn/trunk/html5.js"&gt;&lt;/script&gt;
    &lt;![endif]--&gt;
  &lt;/head&gt;
  &lt;body&gt;
  &lt;link rel="stylesheet" type="text/css" href="//embed.gettyimages.com/css/style.css" /&gt;
&lt;section id="embed-body" data-asset-id="81901686" data-collection-id="41"&gt;
  &lt;a href="http://gty.im/81901686" target="_blank"&gt;&lt;img src="http://d2v0gs5b86mjil.cloudfront.net/xc/81901686.jpg?v=1&c=IWSAsset&k=2&d=F5B5107058D53DF50D8BA2399504758256BF753C679B89B417A38C0E9F1FBB9F&Expires=1394499600&Key-Pair-Id=APKAJZZHJ4LGWQENK3OQ&Signature=UC1YXxhGwSAY0BduwMZqnFQ7fcAQTdCksDvYu4WVmNWlTou7NktH7rZ8uk7BLbupJ4sp0ijiDaA93Yi2XijnC-TtcUO1Kylcew4nZpM~Al9jD0OSfx5yNe7jcIalweGpLGOdMLTXn0wRs6XfEh3~1fc~csMrAesHJkUayhBqNxo6Xja-35XQLx98d5fg6UXazOsCRT-UzebWA4dFURz~BSxXgq0RtU~LhKVKRZvkUTvl2RrsqBcN4bW3i~dbNMwHKn~7s9dMy5CxH-7k4ELyJaBClWEO2Jgr5WV9cXy~WGBQnNd-5Lb7CMcZclzn88-LbmDnFcO~BVLgtSU5x-KTpw__" /&gt;&lt;/a&gt;
  &lt;footer&gt;
    &lt;ul class="meta"&gt;
      &lt;li class="gi-logo icon icon-logo"&gt;&lt;/li&gt;
      &lt;li&gt;Bob O&#39;Connor / Stone&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul class="reblog"&gt;
      &lt;li&gt;
        &lt;a href="//twitter.com/share" title="Share on Twitter" class="twitter-share-button" data-lang="en" data-count="none" data-url="http://gty.im/81901686"&gt;&lt;/a&gt;        
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a class="icon-tumblr" target="_self" title="Share on Tumblr" href="//www.tumblr.com/share/video?embed=%3Ciframe%20src%3D%22%2f%2fembed.gettyimages.com%2fembed%2f81901686%3fet%3d4td6Xm2f0k6pMgQVX7pNFA%26sig%3dfhRom4eoepnZbyWjZ0_2N3SdVG1dxQTC2GUAK4XrPjg%3d%22%20width%3D%22462%22%20height%3D%22440%22%20frameborder%3D%220%22%20%3E%3C%2Fiframe%3E"&gt;&lt;/a&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a href="javascript:void(0);" title="Re-embed this image"&gt;&lt;i class="icon-code"&gt;&lt;/i&gt;&lt;/a&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/footer&gt;
&lt;/section&gt;
  &lt;aside class='modal embed-modal' style='display: none;'&gt;
  &lt;div class='contents'&gt;
    &lt;a class="icon modal-close icon-close" href="#close" title="Close"&gt;&lt;/a&gt; 
    &lt;span id="re-embed-body"&gt;
      &lt;h3&gt;Embed this image&lt;/h3&gt;
      &lt;p&gt;Copy this code to your website or blog. &lt;a href="http://www.gettyimages.com/helpcenter" target="_blank" id="learn-more"&gt;Learn more&lt;/a&gt;&lt;/p&gt;
      &lt;p class="commercial-use"&gt;
        Note: Embedded images may not be used for commercial purposes.&lt;/p&gt;        
      &lt;p id="embed-link"&gt;
        &lt;textarea&gt;&lt;/textarea&gt;&lt;/p&gt;
      &lt;p class="terms"&gt;
        By embedding this image, you agree to Getty Images
        &lt;a href="http://www.gettyimages.com/corporate/terms.aspx" target="_blank"&gt;terms of use&lt;/a&gt;.&lt;/p&gt;
    &lt;/span&gt;
  &lt;/div&gt;
&lt;/aside&gt;
&lt;script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt;
&lt;script type="text/javascript" src="/script/embed.js"&gt;&lt;/script&gt;
    &lt;script src="//platform.tumblr.com/v1/share.js"&gt;&lt;/script&gt;
    &lt;script src="//platform.twitter.com/widgets.js"&gt;&lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;</pre>

<p>You can see <a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html">Amazon's CloudFront</a> is being used as a CDN for the images, and that Getty are using CloudFront's <a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls-overview.html">Signed URLs</a> to expire the images...it looks like after 24 hours? This isn't a problem because Getty are serving the page up, but anyone that's tried to snag the image URL for reuse (Google Images?) will end up getting a 400 error.</p>

<p>I thought it was interesting that the embedded iframe gives you not only the image, author and collection, but also links to re-share the image on Twitter and Tumblr. I guess this is Viral Marketing 101, but it's smart I think, since it encourages reuse, and the recycling of content on the Web. Conspicuously absent from the reshare buttons is Facebook -- maybe there's a story there? Also, as we'll see in a second, the description of the image is missing from the embedded view:</p>

<blockquote>
  <p>20 - 30 year old female worker pulls box off of warehouse shelf</p>
</blockquote>

<p>Of course the other big thing the iframe does is gives Getty an idea of where their content is being used. Anyone who uses this one line embed iframe will trigger an HTTP request to a embed.gettyimages.com URL (hosted on Amazon EC2 incidentally). These requests, and their referral information can be stashed away and analyzed, so that Getty can get a picture of who is using their content, and how. Embedded images and the Twitter and Tumblr reshares are automatically linked to Getty's specific short URLs, such as:</p>

<blockquote>
  <p>http://gty.im/81901686</p>
</blockquote>

<p>The number used in the short URL is also used in the expanded URL:</p>

<blockquote>
  <p>http://www.gettyimages.com/detail/photo/year-old-female-worker-pulls-box-off-of-high-res-stock-photography/81901686</p>
</blockquote>

<p>But the title text is just there for SEO, it can be changed to anything:</p>

<blockquote>
  <p>http://www.gettyimages.com/detail/photo/wikileaks-storage-annex/81901686</p>
</blockquote>

<p>Ordinarily I'd be down on the use of a short URL, but in this case it's role is more of a permalink. Of course these short URLs have the same problem as Handles and PURLs in that people won't ordinarily bookmark them. But, <a href="http://www.youtube.com/watch?v=xZbKHDPPrrc&amp;feature=kp">Que Sera Sera</a>. As the Verge <a href="http://www.theverge.com/2014/3/5/5475202/getty-images-made-its-pictures-free-to-use">pointed out</a> these embedded iframes could end up depriving Web content of lead images, if the GettyImages decides to pull the plug on the embeds and they suddenly 404. But their credibility would suffer quite a bit by a decision like that. I think it's important that they are encouraging the Web to rely on these URLs, and that they are putting their reputation on the line.</p>

<p>Of course lots of inbound links to those pages should do wonders for their <a href="http://en.wikipedia.org/wiki/PageRank">PageRank</a>. Plus, following that link allows you to purchase the image, explore other images by the photographer, related images in the GettyImages collection, as well as see some additional metadata about the photo: item number, rights, license type, original file dimensions, size, dots-per-inch. Some of this metadata is even expressed using RDFa (Facebook's OpenGraph metadata) ... which makes the lack of a Facebook share button even more interesting. In addition there is also some minimal use of schema.org HTML microdata for the search engine's to nibble on. If you are curious, Google's Structured Data Testing Tool provides <a href="http://www.google.com/webmasters/tools/richsnippets?q=http%3A%2F%2Fwww.gettyimages.com%2Fdetail%2Fphoto%2Fyear-old-female-worker-pulls-box-off-of-high-res-stock-photography%2F81901686%3Fsuri%3D1">a view</a> on this metadata.</p>

<p>It seems like there's an opportunity to express more information in RDFa or microdata, specifically the details about the original, as well as licensing/rights metadata. Oddly the RDFa doesn't even mark up the author of the image, I suppose because <a href="https://developers.facebook.com/docs/opengraph/howtos/maximizing-distribution-media-content">Facebook's OpenGraph</a> doesn't give a way of expressing it. They could start by marking up the author of the image, but what if Getty established photographer pages, so instead of Bob O'Connor linking to:</p>

<blockquote>
  <p>http://www.gettyimages.com/search/2/image?artist=Bob+O%27Connor&amp;family=Creative</p>
</blockquote>

<p>What if it linked to a vanity URL like:</p>

<blockquote>
  <p>http://www.gettyimages.com/people/bob-oconnor</p>
</blockquote>

<p>This would be a perfect place to share links to author's other social media accounts, a bio, their photographer friends, etc. I'm thinking of the sort of work that National Geographic are doing with their <a href="http://yourshot.nationalgeographic.com/">YourShot</a> application, for example <a href="http://yourshot.nationalgeographic.com/profile/147353/">this Profile page</a> for Bahareh Mohamadian.</p>

<p>The licensing restrictions and iframes around these images would have ordinarily turned me off. But given Getty's market position in this space it's completely understandle, and seems like a useful compromise for now. These landing pages are a perfect place to make more structured metadata available that could be used by integrating applications. Getty should invest in this real estate, not only for the Web, but also for data resuse across their enterprise. The landing pages are an example of just how influential Facebook and Google have been in promoting the use of metadata on the Web. Without them, I think it is safe to assume we wouldn't have seen any structured metadata on these pages at all.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7250</wp:post_id>
		<wp:post_date>2014-03-10 02:23:04</wp:post_date>
		<wp:post_date_gmt>2014-03-10 09:23:04</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>dissecting-gettyimage-embeds</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[dissecting-gettyimages-embeds]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[embedden]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86879</wp:comment_id>
			<wp:comment_author><![CDATA[Resource: Getty Images Available for Embedding | Digital Humanities Now]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://digitalhumanitiesnow.org/2014/03/resource-getty-images-available-for-embedding/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-03-11 11:31:26</wp:comment_date>
			<wp:comment_date_gmt>2014-03-11 18:31:26</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Editors&#8217; Note: You also may be interested in Ed Summer&#8217;s overview of what happens when you embed a Getty Image. [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394592755.429007053375244140625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1394562686.7031300067901611328125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Incompleteness</title>
		<link>http://inkdroid.org/2014/03/18/incompleteness/</link>
		<pubDate>Tue, 18 Mar 2014 14:26:37 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7308</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>Zen spirit has come to mean not only peace and understanding, but devotion to art and to work, the rich unfoldment of contentment, opening the door to insight, the expression of innate beauty, the intangible charm of incompleteness. Zen carries many meanings, none of them entirely definable. If they are defined, they are not Zen.</p>
  
  <p><a href="http://en.wikipedia.org/wiki/Zen_Flesh,_Zen_Bones">Zen Flesh, Zen Bones</a>, p. 18</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7308</wp:post_id>
		<wp:post_date>2014-03-18 07:26:37</wp:post_date>
		<wp:post_date_gmt>2014-03-18 14:26:37</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>incompleteness</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="happiness"><![CDATA[happiness]]></category>
		<category domain="post_tag" nicename="nyogen-senzaki"><![CDATA[nyogen senzaki]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<category domain="post_tag" nicename="zen"><![CDATA[zen]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>To Mr. Hazard</title>
		<link>http://inkdroid.org/2014/03/19/to-mr-hazard/</link>
		<pubDate>Wed, 19 Mar 2014 15:10:38 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7313</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://www.lockss.org/about/">Lots of Copies Keeps Stuff Safe, 1791-Style</a>.</p>

<blockquote>
  <p>Time and accident are committing daily havoc on the originals deposited in our public offices. The late war has done the work of centuries in this business. The last cannot be recovered, but let us save what remains; not by vaults and locks, which fence them from the public eye and use in consigning them to the waste of time, but by such a multiplication of copies as shall place them beyond the reach of accident.</p>
  
  <p>-- Thomas Jefferson</p>
</blockquote>

<p><a href="http://books.google.com/books?id=JWIFAAAAQAAJ&amp;lpg=PA211&amp;ots=rY_Nvt_YXr&amp;pg=PA211#v=onepage&amp;q&amp;f=false">GoogleBooks</a></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7313</wp:post_id>
		<wp:post_date>2014-03-19 08:10:38</wp:post_date>
		<wp:post_date_gmt>2014-03-19 15:10:38</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>to-mr-hazard</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="lockss"><![CDATA[lockss]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="repositories"><![CDATA[repositories]]></category>
		<category domain="post_tag" nicename="thomas-jefferson"><![CDATA[Thomas Jefferson]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>dump truck</title>
		<link>http://inkdroid.org/2014/03/25/dump-truck/</link>
		<pubDate>Tue, 25 Mar 2014 11:34:39 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7328</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I had a strange dream last night.</p>

<p>I was working on a consulting project with an archivist friend, and a group of others I didn't know as well. I knew that the work was politically sensitive, and that it was important for some reason that escapes me now.</p>

<p>Before the work could start I was required to sign a document. I trusted my friend implicitly, and didn't really read it closely. Afterwards my friend let me know that I had signed a document stating that said I had committed suicide, and that I no longer legally existed. Apparently, this would provide a framework for the work to happen more easily.</p>

<p>I remember being concerned about my family. I walked outside to smoke a cigarette (something I don't do anymore). A few of the other people joined me, and we got in a dump truck.</p>

<hr>

<p>I don't know why, but I woke up from the dream feeling strangely relaxed. I looked up a few things in our our <a href="http://www.amazon.com/Dreamers-Dictionary-Stearn-Robinson/dp/0446342963">cheesy dreamer's dictionary</a>.</p>

<blockquote>
  <p><strong>Archives.</strong> Anything to do with archives in a dream is a forerunner of unexpected legal entanglements.</p>
  
  <p><strong>Document.</strong> Business or legal documents in a dream are usually a warning against speculations, unless the documents were in an indigenous place such as a notary or lawyer's office, in which case they portend a coming increase, possibly for inheritance.</p>
  
  <p><strong>Suicide.</strong> This dream is a signal that you need a change of scene or more mental relaxation. Try sharing your troubles with a trusted friend or adviser, but in any event stop brooding.</p>
  
  <p><strong>Cigar (or Cigarette)</strong> Whether you were offering them, smoking yourself, or observing someone else with them, this form of tobacco in a dream is a lucky omen pertaining to prosperity.</p>
</blockquote>

<p>Unfortunately, there wasn't an entry for dump truck.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7328</wp:post_id>
		<wp:post_date>2014-03-25 04:34:39</wp:post_date>
		<wp:post_date_gmt>2014-03-25 11:34:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>dump-truck</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="dreams"><![CDATA[dreams]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:86907;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86908</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-03-27 06:36:09</wp:comment_date>
			<wp:comment_date_gmt>2014-03-27 13:36:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>In the cab, on the passenger side :-)</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1395927369.104651927947998046875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86907</wp:comment_id>
			<wp:comment_author><![CDATA[bibwild.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email>rochkind@jhu.edu</wp:comment_author_email>
			<wp:comment_author_url>http://bibwild.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-03-27 05:24:38</wp:comment_date>
			<wp:comment_date_gmt>2014-03-27 12:24:38</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Were you in the cab or the bed?
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>152</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1395923078.92467498779296875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:21:"bibwild.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Glass Houses</title>
		<link>http://inkdroid.org/2014/04/07/glass-houses/</link>
		<pubDate>Mon, 07 Apr 2014 16:29:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7348</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://inkdroid.org/images/flickr-brooklyn.png" alt="" />You may have noticed Brooklyn Museum's recent <a href="http://www.brooklynmuseum.org/community/blogosphere/2014/04/04/social-change/">announcement</a> that they have pulled out of <a href="https://secure.flickr.com/commons">Flickr Commons</a>. Apparently they've seen a "steady decline in engagement level" on Flickr, and decided to remove their content from that platform, so they can focus on their own website as well as Wikimedia Commons.</p>

<p>Brooklyn Museum <a href="http://www.brooklynmuseum.org/community/blogosphere/2010/04/12/cross-posting-the-collection-to-wikimedia-commons-and-the-internet-archive/">announced</a> three years ago that they would be cross-posting their content to Internet Archive and Wikimedia Commons. Perhaps I'm not seeing their current bot, but they <a href="https://commons.wikimedia.org/wiki/Brooklyn_Museum">appear</a> to have two, neither of which have done an upload since March of 2011, based on their <a href="https://commons.wikimedia.org/wiki/Special:Contributions/BrooklynMuseum">user</a> <a href="https://commons.wikimedia.org/wiki/Special:Contributions/BrooklynMuseumBot">activity</a>. It's kind of ironic that content like <a href="https://commons.wikimedia.org/wiki/File:Paris_Exposition_Champ_de_Mars_and_Eiffel_Tower,_Paris,_France,_1900_n1.jpg">this</a> was uploaded to Wikimedia Commons by <a href="https://commons.wikimedia.org/wiki/User:Flickr_upload_bot">Flickr Uploader Bot</a> and not by one of their own bots.</p>

<p>The announcement stirred up a fair bit of <a href="https://twitter.com/benfinoradin/status/452422242467864577">discussion</a> about how an institution devoted to the preservation and curation of cultural heritage material could delete all the curation that has happened at Flickr. The theory being that all the comments, tagging and annotation that has happened on Flickr has not been migrated to Wikimedia Commons. I'm not even sure if there's a place where this structured data could live at Wikimedia Commons. Perhaps some sort of template could be created, or it could live in <a href="http://wikidata.org">Wikidata</a>?</p>

<p>Fortunately, <a href="https://twitter.com/thisisaaronland">Aaron Straup-Cope</a> has a backup copy of Flickr Commons metadata, which includes <a href="https://github.com/straup/flickr-commons-metadata/tree/master/data/83979593@N00">a snapshot</a> of the Brooklyn Museum's content. He's been harvesting this metadata out of concern for Flickr's future, but surprise, surprise -- it was an organization devoted to preservation of cultural heritage material that removed it. It would be interesting to see how many comments there were. I'm currently unpacking a tarball of Aaron's metadata on an ec2 instance just to see if it's easy to summarize.</p>

<p>But:</p>

<div style="text-align: center; margin: 15px;">
  <a title="People who live in glass houses shouldn't throw stones." href="https://en.wikipedia.org/wiki/File:Billy_Joel_-_Glass_Houses.jpg"><img src="https://upload.wikimedia.org/wikipedia/en/8/81/Billy_Joel_-_Glass_Houses.jpg" /></a>
</div>

<p>I'm pretty sure I'm living in one of those.</p>

<p>I agree with Ben:</p>

<blockquote class="twitter-tweet" lang="en">
  <p>
    <a href="https://twitter.com/edsu">@edsu</a> <a href="https://twitter.com/textfiles">@textfiles</a> <a href="https://twitter.com/dantobias">@dantobias</a> <a href="https://twitter.com/waxpancake">@waxpancake</a> <a href="https://twitter.com/brooklynmuseum">@brooklynmuseum</a> Yep. Unfortunately this is a blind spot even for orgs doing things relatively well
  </p>— Ben Fino-Radin (@benfinoradin) 
  
  <a href="https://twitter.com/benfinoradin/statuses/453181565976473600">April 7, 2014</a>
</blockquote>

<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> It would help if we had a bit more method to the madness of our own Web presence. Too often the Web is treated as a marketing platform instead of our culture's predominant content delivery mechanism. Brooklyn Museum deserves a lot of credit for talking about this issue openly. Most organizations just sweep it under the carpet and hope nobody notices.</p>

<p>What do you think? Is it acceptable that Brooklyn Museum discarded the user contributions that happened on Flickr, and that all the people who happened to be pointing at said content from elsewhere now have broken links? Could Brooklyn Museum instead decided to leave the content there, with a banner of some kind indicating that it is no longer actively maintained? Don't lots of copies keep stuff safe?</p>

<p>Or perhaps having too many copies detracts from the perceived value of the currently endorsed places of finding the content? Curators have too many places to look, which aren't synchronized, which add confusion and duplication. Maybe it's better to have one place where people can focus their attention?</p>

<p>Perhaps these two positions aren't at odds, and what's actually at issue is a framework for thinking about how to migrate Web content between platforms. And different expectations about content that is self hosted, and content that is hosted elsewhere?</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7348</wp:post_id>
		<wp:post_date>2014-04-07 09:29:16</wp:post_date>
		<wp:post_date_gmt>2014-04-07 16:29:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>glass-houses</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="brooklyn-museum"><![CDATA[Brooklyn Museum]]></category>
		<category domain="post_tag" nicename="flickr"><![CDATA[flickr]]></category>
		<category domain="post_tag" nicename="internet-archive"><![CDATA[internet archive]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86925</wp:comment_id>
			<wp:comment_author><![CDATA[Where Brooklyn At? | inkdroid]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2014/04/08/where-brooklyn-at/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-08 12:41:24</wp:comment_date>
			<wp:comment_date_gmt>2014-04-08 19:41:24</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This Article was mentioned on <a href="http://inkdroid.org/2014/04/08/where-brooklyn-at/" rel="nofollow">inkdroid.org</a>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>webmention</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1396986084.4016749858856201171875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1396986195.3056170940399169921875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86924</wp:comment_id>
			<wp:comment_author><![CDATA[bernsteins]]></wp:comment_author>
			<wp:comment_author_email>shelley.bernstein@brooklynmuseum.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-07 12:24:29</wp:comment_date>
			<wp:comment_date_gmt>2014-04-07 19:24:29</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Ed,

Thanks for your thoughts here and allowing comments on your blog, where I can respond.  I've also been following the discussion on twitter @shell7.

So, a few things to clarify.

The bots you are referencing from three years ago did upload the bulk of our collection objects to Wikimedia Commons and the Internet Archive, but there's an important distinction in the content.  The Flickr Commons images were part of our archives not images of accessioned objects in our the collection so they were not part of the bot migration three years ago.  That's one reason why you are seeing a discrepancy.

Why are you not seeing more uploads via the bot?  A couple of issues there - first Wikimedia really dislikes bots.  We coded the bot, but in the end we had to constantly watch over the upload process and it was never fully automated to the degree where we felt we could "set it and forget it" and just allow it to run in the background.  (By comparison, the transfer to IA was much easier.)  In the end, we did one full dump of the appropriately licensed collection objects and then stopped using the bot.

What's our Wiki upload strategy now?  Ever since the bot debacle, we have worked on a series of projects to contribute content to Wikipedia and Wikimedia, but more in a manner which works better for the wiki community.  Assets are more carefully looked at, uploaded to wikimedia, and then seeded into articles.  It's a process which we do by hand and with a lot of thinking behind it.  We've been lucky to have funding to do so and <a href="http://www.brooklynmuseum.org/community/blogosphere/tag/kress/" rel="nofollow">these posts</a> by our former Kress fellow might be of interest.

In terms of this migration (Flickr Commons to Wikimedia), we had a number of volunteers each take a set of the Flickr images, migrate them to wikicommons and now we are in the process of seeding them into appropriate articles.

However, just moving assets to Wikimedia Commons is not everything.  As stated, what happens to the community driven content?  <a href="http://www.brooklynmuseum.org/community/posse/profiles/FlickrCommons" rel="nofollow">Tags</a> were being fairly consistently brought over into our collection online, so those contributions have been retained.  Throughout the years, our archivist has also corrected official records through community input and that remains vitally important and has made being a part of The Commons a worthwhile endeavor from the start.

That said, did we get every single thing migrated, archived, updated, etc?   No, for sure that's not the case.  But, we felt like we did a lot of due diligence and much of the really valuable information has been migrated even if some of it is only internal at this point.

I hope that answers some of the questions and I'm happy to field more.

Shelley
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2199</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1396899145.8895380496978759765625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1396898671.8912200927734375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:10:"bernsteins";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Where Brooklyn At?</title>
		<link>http://inkdroid.org/2014/04/08/where-brooklyn-at/</link>
		<pubDate>Tue, 08 Apr 2014 19:41:20 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7378</guid>
		<description></description>
		<content:encoded><![CDATA[<p>As a follow up to my <a href="http://inkdroid.org/2014/04/07/glass-houses/">last post</a> I added a <a href="https://github.com/edsu/py-flarchive/blob/master/flarchive/activity.py">script</a> to my fork of Aaron's <a href="https://github.com/straup/py-flarchive">py-flarchive</a> that will load up a <a href="http://redis.io/">Redis</a> instance with comments, notes, tags and sets for Flickr images that were uploaded by Brooklyn Museum. The script assumes you've got a snapshot of the archived metadata, which I <a href="https://github.com/straup/flickr-commons-metadata/releases">downloaded</a> as a tarball. It took several hours to unpack the tarball on a medium ec2 instance; so if you want to play around and just want the redis database <a href="mailto:ehs@pobox.com">let me know</a> and I'll get it to you.</p>

<p>Once I loaded up Redis I was able to generate some high level stats:</p>

<ul>
<li>images: 5,697</li>
<li>authors: 4,617</li>
<li>tags: 6,132</li>
<li>machine tags: 933</li>
<li>comments: 7,353</li>
<li>notes: 963</li>
<li>sets: 141</li>
</ul>

<p>Given how many images there were there it represents an astonishing number of authors: unique people who added tags, comments or notes. If you are curious I generated a list of the tags and saved them as a <a href="https://docs.google.com/spreadsheets/d/1_YAfB2PdRZ-PLtEkPaQzlgbmqaxEQLTGlNKBLhTWEEk/edit?usp=sharing">Google Doc</a>. The machine tags were particularly interesting to me. The majority (849) of them look like Brooklyn Museum IDs of some kind, for example:</p>

<blockquote>
  <p>bm:unique=S10_08_Thebes/9928</p>
</blockquote>

<p>But there were also 51 geotags, and what looks like 23 links to items in <a href="http://pleiades.stoa.org/home">Pleiades</a>, for example:</p>

<blockquote>
  <p>tag:pleiades:depicts=721417202</p>
</blockquote>

<p>If I had to guess I'd say this particular machine tag indicated that the Brooklyn Museum image depicted <a href="http://pleiades.stoa.org/places/721417202">Abu Simbel</a>. Now there weren't tons of these machine tags but it's important to remember that other people use Flickr as a scratch space for annotating images this way.</p>

<p>If you aren't familiar with them, Flickr notes are annotations of an image, where the user has attached a textual note to a region in the image. Just eyeballing the list, it appears that there is quite a bit of diversity in them, ranging from the whimsical:</p>

<ul>
<li>cool! they look soo surreal</li>
<li>teehee somebody wrote some graffiti in greek</li>
<li>Lol are these painted?</li>
<li>Steaks are ready!</li>
</ul>

<p>to the seemingly useful:</p>

<ul>
<li>Hunter's Island</li>
<li>Ramesses III Temple</li>
<li>Lapland Village</li>
<li>Lake Michigan</li>
<li>Montuemhat Crypt</li>
<li>Napoleon's troops are often accused of destroying the nose, but they are not the culprits. The nose was already gone during the 18th century.</li>
</ul>

<p>Similarly the general comments run the gamut from:</p>

<ul>
<li>very nostalgic...</li>
<li>always wanted to visit Egypt</li>
</ul>

<p>to:</p>

<ul>
<li>Just a few points. This is not 'East Jordan' it is in the Hauran region of southern Syria. Second it is not Qarawat (I guess you meant Qanawat) but Suweida. Third there is no mention that the house is enveloped by the colonnade of a Roman peripteral temple.</li>
<li>The fire that destroyed the buildings was almost certainly arson. it occurred at the height of the Pullman strike and at the time, rightly or wrongly, the strikers were blamed.</li>
<li>You can see in the background, the TROCADERO with two towers .. This "medieval city" was built on the right bank where are now buildings in modern art style erected for the exposition of 1937.</li>
</ul>

<p>Brooklyn Museum pulled over 48 tags from Flickr before they deleted the account. That's just 0.7% of the tags that were there. None of the comments or notes were moved over.</p>

<p>In the data that Aaron archived there was one indicator of user engagement: the datetime included with comments. Combined with the upload time for the images it was possible to create a <a href="https://docs.google.com/spreadsheets/d/1oONIt77ASpVHmeL1zwbJytCdn5cGLYWR1AFs46KrHCE/edit?usp=sharing">spreadsheet</a> that correlates the number of comments with the number of uploads per month:</p>

<p><a href="https://docs.google.com/spreadsheets/d/1oONIt77ASpVHmeL1zwbJytCdn5cGLYWR1AFs46KrHCE/edit?usp=sharing"><img src="http://inkdroid.org/images/flickr-brooklyn-graph.png" alt="Brooklyn Museum Flickr Activity" /></a></p>

<p>I'm guessing the drop off in December of 2013 is due to that being the last time Aaron archived Brooklyn Museum's metadata. You can see that there was a decline in user engagement: the peak in late 2008 / early 2009 was never matched again. I was half expecting to see that user engagement fell off when Brooklyn Museum's interest in the platform (uploads) fell off. But you can see that they continued to push content to Flickr, without seeing much of a reward, at least in the shape of comments. It's impossible now to tell if tagging, notes or sets trended differently.</p>

<p>Since Flickr includes the number of times each image was viewed it's possible to look at all the images and see how many times images were viewed, the answer?</p>

<blockquote>
  <p>9,193,331</p>
</blockquote>

<p>Not a bad run for 5,697 images. I don't know if Brooklyn Museum downloaded their metadata prior to removing their account. But luckily Aaron did.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7378</wp:post_id>
		<wp:post_date>2014-04-08 12:41:20</wp:post_date>
		<wp:post_date_gmt>2014-04-08 19:41:20</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>where-brooklyn-at</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="brooklyn-museum"><![CDATA[Brooklyn Museum]]></category>
		<category domain="post_tag" nicename="flickr"><![CDATA[flickr]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="redis"><![CDATA[redis]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86927</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-09 07:42:08</wp:comment_date>
			<wp:comment_date_gmt>2014-04-09 14:42:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Re: the 48 tags. My only insight into what tags were pulled over was the link you included in your <a href="http://inkdroid.org/2014/04/07/glass-houses/comment-page-1/#comment-86924" rel="nofollow">last reply</a>. That page appears to only have 49 tags on it? I'm glad to hear you managed to get a snapshot of the comments, notes and tags, even if it can only be kept internal for now. I must admit, the more I look at the data, the worse it makes me feel that it was removed. But it is what it is. I can well understand how it could simplify things to focus more on your local web presence.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>86926</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397054528.69013500213623046875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86928</wp:comment_id>
			<wp:comment_author><![CDATA[bernsteins]]></wp:comment_author>
			<wp:comment_author_email>shelley.bernstein@brooklynmuseum.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-09 08:09:53</wp:comment_date>
			<wp:comment_date_gmt>2014-04-09 15:09:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Right, that page only displays the recent and does not show the aggregated total, unfortunately, so it was an easy misread.  I'm happy to clarify things in advance of publication, always (for what it's worth in the future - you can always email me if you want to).

It is what it is.  We pulled the plug because our focus changed.  The metrics helped us make that decision, but the goals are what is most important.

Generally, however, we think this may be a much bigger issue down the line as platforms continue to change.  We think it's better to talk about this now and set examples (for bringing data back, correcting records, etc) than when the sun sets :/
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>86927</wp:comment_parent>
			<wp:comment_user_id>2199</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397056193.7069289684295654296875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:10:"bernsteins";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86926</wp:comment_id>
			<wp:comment_author><![CDATA[bernsteins]]></wp:comment_author>
			<wp:comment_author_email>shelley.bernstein@brooklynmuseum.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-09 06:36:02</wp:comment_date>
			<wp:comment_date_gmt>2014-04-09 13:36:02</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for this...

Keep in mind, we had a "blended" account and were the only Commons member to have that.  So, uploads included museum goings on (artist load-ins) as well as Commons material.  If you are pulling stats, make sure you are pulling only Commons material so you can do an apples to apples comparison - there's a flag for Commons in the metadata.

Tags...

48 tags - that's not correct.  I think you were just looking at the posse home page for that Flickr user, which only displays the latest tags.  All tags were pulled over via a nightly running script.  Interestingly, however, our script pulled 15,098 tags over the years.  I'm not sure where the discrepancy lies in the numbers you have vs. our own.  We did have trouble with the script every so often and could have dupes, but 48 is not right :)

The machine tags "bm=" helped us match up tags to objects in the scripting, so we could pull things back over.  My understanding, though, is that all of the commons images had those, so 849 does not seem right.  The geotags were coming in b/c we had placed suggestify links in the descriptions of every image asking users to geotag...you can see few did -- it was difficult to get participation at this level.

Comments were moved over when they corrected records and became part of our internal records.  Ditto for notes, which we only took screenshots of b/c the note without the image doesn't tell the whole story.  Unfortunately, we can't surface that material in our own collection online, so this info will remain internal (at least for now).

Flickr peaked in 2008, for sure, but that's because that is when we joined.  We were the third institution to sign up and there was a lot of press at that time.  I wouldn't necessarily say that's a good basis for a peak, but it has been on steady decline since then.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2199</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397050562.660149097442626953125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:10:"bernsteins";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86929</wp:comment_id>
			<wp:comment_author><![CDATA[Flickr Commons LAMs | inkdroid]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2014/04/11/flickr-commons-lams/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-11 13:58:42</wp:comment_date>
			<wp:comment_date_gmt>2014-04-11 20:58:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This Article was mentioned on <a href="http://inkdroid.org/2014/04/11/flickr-commons-lams/" rel="nofollow">inkdroid.org</a>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>webmention</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397249922.1893870830535888671875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397250271.442222118377685546875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Flickr Commons LAMs</title>
		<link>http://inkdroid.org/2014/04/11/flickr-commons-lams/</link>
		<pubDate>Fri, 11 Apr 2014 20:58:39 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7401</guid>
		<description></description>
		<content:encoded><![CDATA[<p>After the <a href="http://inkdroid.org/2014/04/08/where-brooklyn-at/">last post</a> <a href="http://twitter.com/sebchan">Seb</a> <a href="https://twitter.com/sebchan/status/454588043778097154">got me</a> wondering if there were any differences between libraries, archives and museums when looking at upload and comment activity in Flickr Commons in <a href="http://twitter.com/thisisaaronland">Aaron's</a> <a href="https://github.com/straup/flickr-commons-metadata/">snapshot</a> of the Flickr Commons metadata.</p>

<p>First I had to get a list of Flickr Commons organizations and classify them as either a library, museum or archive. It wasn't always easy to pick, but you can see the result <a href="https://github.com/edsu/py-flarchive/blob/master/flarchive/orgs.json">here</a>. I lumped galleries in with museums. I also lumped historical societies in with archives. Then I wrote a <a href="https://github.com/edsu/py-flarchive/blob/master/flarchive/engagement.py">script</a> that walked around in the Redis database I already had from <a href="https://github.com/edsu/py-flarchive/blob/master/flarchive/activity.py">loading</a> Aaron's data.</p>

<p>In doing this I noticed there were some Flickr Commons organizations that were missing from Aaron's snapshot:</p>

<ul>
<li>Tasmanian Archive and Heritage Office Commons</li>
<li>The Royal Library, Denmark</li>
<li>The Finnish Museum of Photography</li>
<li>Musée McCord Museum</li>
</ul>

<p><em>Update: Aaron quickly <a href="https://github.com/straup/flickr-commons-metadata/issues/16">fixed</a> this.</em></p>

<p>I didn't do any research to see if these organizations had significant activity. Also, since there were close to a million files, I didn't load the British Library activity yet. If there's interest in adding them into the mix I'll splurge for the larger ec2 instance.</p>

<p>Anyhow, below are the results. You can find the spreadsheet for these graphs up in <a href="https://docs.google.com/spreadsheets/d/16ykEftF5BZwUfHug_uKmnjht2cd0WV8fUA4YYm6axrE/edit?usp=sharing">Google Docs</a></p>

<iframe height=371 width=600 src="//docs.google.com/spreadsheets/d/16ykEftF5BZwUfHug_uKmnjht2cd0WV8fUA4YYm6axrE/gviz/chartiframe?oid=474842735" seamless frameborder=0 scrolling=no></iframe>

<iframe height=371 width=600 src="//docs.google.com/spreadsheets/d/16ykEftF5BZwUfHug_uKmnjht2cd0WV8fUA4YYm6axrE/gviz/chartiframe?oid=823979235" seamless frameborder=0 scrolling=no></iframe>

<p>This was all done rather quickly, so if you notice anything odd or that looks amiss please let me know. Initially it seemed a bit strange to me that libraries, archives and museums trended so similarly in each graph, even if the volume was different.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7401</wp:post_id>
		<wp:post_date>2014-04-11 13:58:39</wp:post_date>
		<wp:post_date_gmt>2014-04-11 20:58:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>flickr-commons-lams</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="flickr"><![CDATA[flickr]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:86931;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>86931</wp:comment_id>
			<wp:comment_author><![CDATA[Trevor Owens]]></wp:comment_author>
			<wp:comment_author_email>trevor.johnowens@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://tjowens.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-14 06:39:42</wp:comment_date>
			<wp:comment_date_gmt>2014-04-14 13:39:42</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for sharing this Ed! The graphs you shared got me thinking. Since the museums have so much more content, it makes a lot of sense that they are bringing in more comments. I'd be curious to know what the ratio of comments per upload looks like if that were plotted?

I'd also be curious to see these made relative to the number of libraries, archives and museums participating. That is, in the Brooklyn Museum analysis you did we saw commenting falling off over time. Here, we see a bit of that, but it mostly looks like commenting started a bit higher and then stabilized around 2010. With that said, I imagine there has been steady growth of the total number of organizations participating, which would potentially suggest that while the total number is staying relatively consistent each org is seeing a steady decline in the number of comments their photos are getting.

Altogether, I think this is an amazingly rich data set for exploring a lot of ideas and notions of participation and engagement with collections. I hope we see some more folks dive into this data as I think further exploration like this could be invaluable.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2211</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397486407.8178598880767822265625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397482783.1592910289764404296875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:20:"tjowens.myopenid.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86933</wp:comment_id>
			<wp:comment_author><![CDATA[jonvoss]]></wp:comment_author>
			<wp:comment_author_email>jon.voss@wearewhatwedo.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-16 22:59:17</wp:comment_date>
			<wp:comment_date_gmt>2014-04-17 05:59:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[uh, sorry, answering my own question from your data:
archives as of 12/13: 69,274
libraries: 74,337
museums: 188,804
Total: 332,415

Then there's the 1mil from BL on top of that, right?
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>86932</wp:comment_parent>
			<wp:comment_user_id>2215</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397737922.1409609317779541015625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397714357.2251040935516357421875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:7:"jonvoss";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86935</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-17 05:42:35</wp:comment_date>
			<wp:comment_date_gmt>2014-04-17 12:42:35</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Trevor, yes I thought of graphing the ratio of comments/uploads. I'll see if I can put something together.</p>

<p>Jon, yeah the BL data is missing from these stats, which kind of a glaring omission. If getting more complete and timely stats is of interest let <a href="http://twitter.com/edsu" rel="nofollow">let me know</a>.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>86933</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397738555.6335361003875732421875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86932</wp:comment_id>
			<wp:comment_author><![CDATA[jonvoss]]></wp:comment_author>
			<wp:comment_author_email>jon.voss@wearewhatwedo.org</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-04-16 22:54:26</wp:comment_date>
			<wp:comment_date_gmt>2014-04-17 05:54:26</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Rockin Ed. do you have any other basic stats from this, like how many images are there in Flickr Commons?  I was just trying to quantify the groundswell of institutions putting stuff out there for reuse and there doesn't seem to be much out there for stats on Flickr Commons.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2215</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397737932.9839580059051513671875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1397714067.20145893096923828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:7:"jonvoss";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>The Archive as Data Platform</title>
		<link>http://inkdroid.org/2014/04/25/the-archive-as-data-platform/</link>
		<pubDate>Fri, 25 Apr 2014 20:40:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7428</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Yesterday Wikileaks announced the availability of a new collection, the <a href="https://wikileaks.org/WikiLeaks-releases-the-Carter.html">Carter Cables</a>, which are a new addition to the <a href="https://search.wikileaks.org/plusd/">Public Library of US Diplomacy (PlusD)</a>. One thing in particular in the announcement caught my attention:</p>

<blockquote>
  <p>The Carter Cables were obtained by WikiLeaks through the process described <a href="https://wikileaks.org/plusd/pressrelease">here</a> after formal declassification by the US National Archives and Records Administration earlier this year.</p>
</blockquote>

<p>If you follow the link you can see that this content was obtained in a similar manner as the Kissinger Files, that were released just over a year ago. Perhaps this has already been noted, but I didn't notice before that the Kissinger Files (the largest Wikileaks release to date) were not leaked to Wikileaks, but were legitimately obtained directly from NARA's website:</p>

<blockquote>
  <p>Most of the records were reviewed by the United States Department of State's systematic 25-year declassification process. At review, the records were assessed and either declassified or kept classified with some or all of the metadata records declassified. Both sets of records were then subject to an additional review by the National Archives and Records Administration (NARA). Once believed to be releasable, they were placed as individual PDFs at the National Archives as part of their Central Foreign Policy Files collection.</p>
</blockquote>

<p>The Central Foreign Policy Files are a <a href="http://www2.archivists.org/glossary/terms/s/series">series</a> from the General Records of the Department of State <a href="http://www2.archivists.org/glossary/terms/r/record-group">record group</a>. Anyone with a web browser can view these documents on NARA's <a href="http://aad.archives.gov/aad/series-description.jsp?s=4073">Access to Archival Databases</a> website. If you try to access them you'll notice that the series is broken up into 15 separate <a href="http://www2.archivists.org/glossary/terms/f/file">files</a>. Each file is a set of documents that can be searched individually. There's no way to browse the contents of a file, series or the entire group: you must do a search and click through each of the results (more on this in a moment).</p>

<blockquote>
  <p>The form in which these documents were held at NARA was as 1.7 million individual PDFs. To prepare these documents for integration into the PlusD collection, WikiLeaks obtained and reverse-engineered all 1.7 million PDFs and performed a detailed analysis of individual fields, developed sophisticated technical systems to deal with the complex and voluminous data and corrected a great many errors introduced by NARA, the State Department or its diplomats, for example harmonizing the many different ways in which departments, capitals and people's names were spelt.</p>
</blockquote>

<p>It would be super to hear more details about their process for doing this work. I think archives could potentially learn a lot about how to enhance their own workflows for doing this kind of work at scale.</p>

<p>And yet I think there is another lesson here in this story. It's actually important to look at this PlusD work as a success story for NARA...and one that can potentially be improved upon. I mentioned above that it doesn't appear to be possible to browse a list of documents and that you must do a search. If you do a search and click on one of the documents you'll notice you get a URL like this:</p>

<p><a href="http://aad.archives.gov/aad/createpdf?rid=99311&amp;dt=2472&amp;dl=1345">http://aad.archives.gov/aad/createpdf?<strong>rid=99311</strong>&amp;dt=2472&amp;dl=1345 </a></p>

<p>And if you browse to another you'll see something like:</p>

<p><a href="http://aad.archives.gov/aad/createpdf?rid=841&amp;dt=2472&amp;dl=1345">http://aad.archives.gov/aad/createpdf?<strong>rid=841</strong>&amp;dt=2472&amp;dl=1345</a></p>

<p>Do you see the pattern? Yup, the <em>rid</em> appears to be a record number, and it's an integer that you can simply start at 1 and keep going until you've got to the last one for that file, in this case 155278.</p>

<p>It turns out the other <code>dt</code> and <code>dl</code> parameters change for each file, but they are easily determined by looking at the <a href="http://aad.archives.gov/aad/series-description.jsp?s=4073">overview page</a> for the series. Here they are if you are curious:</p>

<ul>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2472&amp;dl=1345</li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2473&amp;dl=1348 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2474&amp;dl=1345 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2475&amp;dl=1348 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2492&amp;dl=1346 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2493&amp;dl=1347 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2476&amp;dl=1345 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2477&amp;dl=1348 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2494&amp;dl=1346 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2495&amp;dl=1347 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2082&amp;dl=1345 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2083&amp;dl=1348 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2084&amp;dl=1346 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2085&amp;dl=1347 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2532&amp;dl=1629 </li>
<li>http://aad.archives.gov/aad/createpdf?rid={{n}}&amp;dt=2533&amp;dl=1630</li>
</ul>

<p>Of course it would be trivial to write a harvesting script to pull down the ~380 gigabytes of PDFs by creating a loop with a counter and using one of the many many HTTP libraries. Maybe even with a bit of sleeping in between requests to be nice to the NARA website. I suspect that this how Wikileaks were able to obtain the documents.</p>

<p>But, in an ideal world, this sort of URL inspection shouldn't be necessary right? Also, perhaps it could be done in such a way that the burden of distributing the data doesn't fall on NARA alone? It feels like a bit of an accident that it's possible to download the data in bulk from NARA's website this way. But it's an accident that's good for access.</p>

<p>What if instead of trying to build the ultimate user experience for archival content, archives focused first and foremost on providing simple access to the underlying data first. I'm thinking of the sort of work Carl Malamud has been doing for years at <a href="https://public.resource.org/">public.resource.org</a>. With a solid data foundation like that, and simple mechanisms for monitoring the archive for new accessions it would then be possible to layer other applications on top within the enterprise and (hopefully) at places external to the archive, that provide views into the holdings.</p>

<p>I imagine this might sound like ceding the responsibility of the archive to some. It may also sound a bit dangerous to those that are concerned about connecting up public data that is currently unconnected. I'm certainly not suggesting that user experience and privacy aren't important. But I think Cassie is right:</p>

<blockquote class="twitter-tweet" lang="en">
  <p>
    <a href="https://twitter.com/edsu">@edsu</a> Yes it is; I think more participation from independent actors in archival access is a great thing we need more of
  </p>— Cassie Findlay (@CassPF) 
  
  <a href="https://twitter.com/CassPF/statuses/459503237909843968">April 25, 2014</a>
</blockquote>

<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> I imagine there are some that feel that associating this idea of the archive as data platform with the Wikileaks project might be counterproductive to an otherwise good idea. I certainly paused before hitting publish on this blog post, given the continued sensitivity around the issue of Wikileaks. But as other archivists have noted there is a <a href="http://www.emergingissues.ala.org/wikileaks/">great</a> <a href="http://www.tandfonline.com/doi/pdf/10.1080/01576895.2013.779926">deal</a> to be <a href="http://www.cjh.org/pages.php?pid=45&amp;evID=1791">learned</a> from the phenomenon that is Wikileaks. Open and respectful conversations about what is happening is important, right?</p>

<p>Most of all I think it's important that we don't look at this bulk access and distant reading of archival material as a threat to the archive. Researchers should feel that downloading data from the archive is a legitimate activity. Where possible they should be given easy and efficient ways to do it. Archives need environments like <a href="https://www.opengov.nsw.gov.au/api">OpenGov NSW</a> (<a href="https://twitter.com/CassPF/status/459868498018045953">thanks Cassie</a>) and the Government Printing Office's <a href="http://www.gpo.gov/fdsys/bulkdata/">Bulk Data</a> website (see this <a href="http://www.archives.gov/press/press-releases/2011/nr11-165.html">press release</a> about the <a href="http://www.federalregister.gov">Federal Register</a>) where this activity can take place, and where a dialogue can happen around it.</p>

<p><em>Update: May 8, 2014</em></p>

<p><a href="https://twitter.com/carwinb">Alexa O'Brien</a>'s <a href="https://www.youtube.com/watch?v=TgAdXibXgMk#t=2456">interview</a> on May 6th with <a href="https://en.wikipedia.org/wiki/Sarah_Harrison_(journalist)">Sarah Harrison</a> of Wikileaks at <a href="http://re-publica.de/en/republica-2014">re:publica14</a> touched on lots of issues related to Wikileaks the archive. In particular the discussion of redaction, accessibility and Wikileaks role in publishing declassified information for others (including journalists) was quite relevant the topic of this blog post.</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/TgAdXibXgMk" frameborder="0" allowfullscreen></iframe>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7428</wp:post_id>
		<wp:post_date>2014-04-25 13:40:06</wp:post_date>
		<wp:post_date_gmt>2014-04-25 20:40:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-archive-as-data-platform</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="wikileaks"><![CDATA[wikileaks]]></category>
		<wp:postmeta>
			<wp:meta_key>_post_restored_from</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:20:"restored_revision_id";i:7464;s:16:"restored_by_user";i:2;s:13:"restored_time";i:1398458947;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>linking spoken quotes of quotes</title>
		<link>http://inkdroid.org/2014/05/02/linking-spoken-quotes-of-quotes/</link>
		<pubDate>Fri, 02 May 2014 09:36:53 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7481</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>An ancient <a href="https://en.wikipedia.org/wiki/Gautama_Buddha">buddha</a> said, "If you do not wish to incur the cause for Unceasing <a href="https://en.wikipedia.org/wiki/Hell">Hell</a>, do not slander the true <a href="https://en.wikipedia.org/wiki/Dharmacakra">dharma wheel</a> of the <a href="https://en.wikipedia.org/wiki/Tathagata">Tathagata</a>. You should <a href="https://en.wikipedia.org/wiki/Carve">carve</a> these <a href="https://en.wikipedia.org/wiki/Word">words</a> on your <a href="https://en.wikipedia.org/wiki/Skin">skin</a>, flesh, <a href="https://en.wikipedia.org/wiki/Bones">bones</a> and marrow; on your <a href="https://en.wikipedia.org/wiki/Body_(metaphysics)">body</a>, <a href="https://en.wikipedia.org/wiki/Mind">mind</a> and <a href="https://en.wikipedia.org/wiki/Ecology">environment</a>; on <a href="https://en.wikipedia.org/wiki/%C5%9A%C5%ABnyat%C4%81">emptiness</a> and on form. They are already carved on <a href="https://en.wikipedia.org/wiki/Tree">trees</a> and <a href="https://en.wikipedia.org/wiki/Rocks">rocks</a>, on <a href="https://en.wikipedia.org/wiki/Field_(agriculture)">fields</a> and <a href="https://en.wikipedia.org/wiki/Villages">villages</a>."</p>
</blockquote>

<p>From <a href="https://en.wikipedia.org/wiki/Gary_Snyder">Gary Snyder's</a> reading of <a href="http://www.amazon.com/The-Teachings-Zen-Master-Dogen/dp/1597771570">The Teachings of Zen Master Dogen</a> (about 1:26:00 in).</p>

<p>His delivery is just a delight to listen to. The puzzling strangeness of the text are made whole in the precision, earthiness and humor of his words.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7481</wp:post_id>
		<wp:post_date>2014-05-02 02:36:53</wp:post_date>
		<wp:post_date_gmt>2014-05-02 09:36:53</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>linking-spoken-quotes-of-quotes</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Broken World</title>
		<link>http://inkdroid.org/2014/05/20/broken-world/</link>
		<pubDate>Tue, 20 May 2014 21:01:59 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7506</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://inkdroid.org/images/wikipedia.png" style="float: left; margin-right: 10px;" />You know that tingly feeling you get when you read something, look at a picture, or hear a song that subtly and effortlessly changes the way you think?</p>

<p>I don't know about you, but for me thoughts, ideas and emotions can often feel like puzzles that stubbornly demand a solution, until something or someone helps make the problem evaporate or dissolve. Suddenly I can zoom in, out or around the problem, and it is utterly transformed. As that philosophical trickster Ludwig Wittgenstein <a href="http://www.plzi.com/tekstia/others/tract_4.htm">wrote</a>:</p>

<blockquote>
  <p>It is not surprising that the deepest problems are in fact not problems at all.</p>
</blockquote>

<p>A few months ago, a tweet from <a href="http://mkirschenbaum.wordpress.com/">Matt Kirschenbaum</a> had this effect on me.</p>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet" lang="en">
  <p>
    Steven Jackson's "Rethinking Repair," an "exercise in broken world thinking," is one of the most remarkable things I've read in some time.
  </p>— Matthew Kirschenbaum (@mkirschenbaum) 
  
  <a href="https://twitter.com/mkirschenbaum/statuses/445254736070311937">March 16, 2014</a>
</blockquote>

<p>It wasn't the tweet itself, so much as what the tweet led to: <a href="http://sjackson.infosci.cornell.edu/">Steven Jackson</a>'s <a href="http://sjackson.infosci.cornell.edu/RethinkingRepairPROOFS(reduced)Aug2013.pdf">Rethinking Repair</a>, which recently appeared in the heady sounding <a href="https://mitpress.mit.edu/books/media-technologies">Media Technologies Essays on Communication, Materiality, and Society</a>.</p>

<p><a href="https://secure.flickr.com/photos/inkdroid/14251969913/"><img src="http://inkdroid.org/images/repair.jpg" style="float: left; margin-right: 10px;" width="200" /></a></p>

<p>I've since read the paper three or four times, taken notes, underlined stuff to follow up on, etc. I've been meaning to write about it here, but I couldn't start...I suppose for reasons that are (or will become) self evident. The paper is like a <a href="https://en.wikipedia.org/wiki/Rhizome_(philosophy)">rhizome</a> that brings together many strands of thought and practice that are directly relevant to my personal and professional life.</p>

<p>I've spent the last decade or so working as a software developer in the field of digital preservation and archives. On good days this seems like a surprisingly difficult thing, and on the bad days it seems more like an oxymoron...or an elaborate joke.</p>

<p>At home I've spent the last year helping my wife <a href="http://inkdroid.org/2013/10/03/shutdown-startup/">start a business</a> to change our culture <a href="http://freehandscraftstudio.com">one set of hands at a time</a>, while trying to raise children in a society terminally obsessed with waste, violence and greed...and a city addicted to power, or at least the illusion of power.</p>

<p>In short, how do you live and move forward amidst such profound brokenness? Today <a href="https://twitter.com/quinnnorton">Quinn Norton</a>'s impassioned <a href="https://medium.com/message/81e5f33a24e1">Everything is Broken</a> reminded me of Jackson's <em>broken world thinking</em>, and what a useful hack (literally) it is...especially if you are working with information technology today.</p>

<p>Writing software, especially for the Web, is still fun, even after almost 20 years. It keeps changing, spreading into unexpected places, and the tools just keep evolving, getting better, and more varied. But this same dizzying rate of change and immediacy poses some real problems if you are concerned about stuff staying around so people can look at it tomorrow.</p>

<p>When I was invited to the <a href="http://www.ndf.org.nz/">National Digital Forum</a> I secretly fretted for months, trying to figure out if I had anything of substance to say to that unique blend of folks interested in the cross section of the Web and the cultural heritage. The thing I eventually landed on was taking a look at the <a href="http://inkdroid.org/2013/11/26/the-web-as-a-preservation-medium/">Web as a preservation medium</a>, or rather the Web as a process, which has a preservation component to it. In the wrapup I learned that the topic of "web preservation" had already been covered a few years earlier, so there wasn't much new there ... but there was some evidence that the talk connected with a few folks.</p>

<p>If I could do it all again I would totally (as Aaron <a href="https://twitter.com/search?q=from:thisisaaronland%20i%20would%20totally">would say</a>) look at the Web and preservation through Jackson's prism of broken world thinking.</p>

<p>The bit where I talked about how Mark Pilgrim and Why's online presence was brought back from virtual suicide using Git repositories, the Internet Archive and a lot of TLC was totally broken world thinking. Verne Harris' notion that the archive is always just a sliver of a sliver of a sliver of a window into process, and that as such it is extremely, extremely valuable is broken world thinking. Or the evolution of permalinks, cool URIs in the face of swathes of linkrot is at its heart broken world thinking.</p>

<p>The key idea in Jackson's article (for me) is that there are very good reasons to remain hopeful and constructive while at the same time being very conscious of the problems we find ourself in today. The ethics of care that he outlines, with roots in the feminist theory, is a deeply transformative idea. I've got lots of lines of future reading to follow, in particular in the area of sustainability studies, which seems very relevant to the work of digital preservation.</p>

<p>But most of all Jackson's insight that innovation doesn't happen in lightbulb moments (the mythology of the Silicon Valley origin story) or the latest tech trend, but in the recognition of brokenness, and the willingness to work together with others to repair and fix it. He positions repair as an ongoing process that fuels innovation:</p>

<blockquote>
  <p>... broken world thinking asserts that breakdown, dissolution, and change, rather than innovation, development, or design as conventionally practiced and thought about are the key themes and problems facing new media.</p>
</blockquote>

<p>I should probably stop there. I know I will return to this topic again, because I feel like a lot of my previous writing here has centered on the importance of repair, without me knowing it. I just wanted to stop for a moment, and give a shout out to some thinking that I'm suspecting will guide me for the next twenty years.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7506</wp:post_id>
		<wp:post_date>2014-05-20 14:01:59</wp:post_date>
		<wp:post_date_gmt>2014-05-20 21:01:59</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>broken-world</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="life"><![CDATA[life]]></category>
		<category domain="category" nicename="preservation"><![CDATA[preservation]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>RealAudio, AAC and Archivy</title>
		<link>http://inkdroid.org/2014/05/23/realaudio-aac-and-archivy/</link>
		<pubDate>Fri, 23 May 2014 10:25:52 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7568</guid>
		<description></description>
		<content:encoded><![CDATA[<p>A few months ago I happened to read a <a href="http://pitchfork.com/features/paper-trail/9370-records-ruin-the-landscape/">Pitchfork interview</a> with <a href="https://en.wikipedia.org/wiki/David_Grubbs">David Grubbs</a> about his book <a href="http://www.dukeupress.edu/Records-Ruin-the-Landscape/">Records Ruin the Landscape</a>. In the interview Grubbs mentioned how his book was influenced by a 2004 <a href="https://en.wikipedia.org/wiki/Kenneth_Goldsmith">Kenny Goldsmith</a> <a href="https://wfmu.org/playlists/shows/10521">interview</a> with <a href="https://en.wikipedia.org/wiki/Henry_Flynt">Henry Flynt</a>...and Pitchfork usefully linked to the <a href="https://wfmu.org/playlists/shows/10521">interview</a> in the <a href="https://wfmu.org/playlists">WFMU archive</a>.</p>

<p>You know, books linking to interviews linking to interviews linking to archives, the wondrous beauty and utility of hypertext.</p>

<p>I started listening to the interview on my Mac with Chrome and the latest RealAudio plugin but after a few minutes it went into a feedback loop of some kind, and became full of echoes and loops, and was completely unlistenable. This is WFMU so I thought maybe this was part of the show, but it went on for a while, which seemed a little bit odd. I tried reloading thinking it might be some artifact of the stream, but the exact thing happened again. I noticed a prominent <a href="https://wfmu.org/audiostream.shtml">Get Help</a> link right next to the link for listening to the content. I clicked on it and filled out a brief form, not really expecting to hear back.</p>

<p><a href="https://wfmu.org/playlists/shows/10521"><img src="http://inkdroid.org/images/wfmu-archive.png" alt="" /></a></p>

<p>As you can see the WFMU archive view for the interview is sparse but eminently useful.</p>

<p>Unexpectedly, just a few hours later I received an email from <a href="https://www.facebook.com/jbmjbm">Jeff Moore</a> who wrote that playback of Real Audio had been reported to be a problem before on some items in the archive, and that they were in the process of migrating them to <a href="https://en.wikipedia.org/wiki/Advanced_Audio_Coding">AAC</a>. My report had pushed this particular episode up in the queue, and I could now reload the page and listen to an AAC stream via their Flash player. I guess now that it's AAC there is probably something that could be done with the <a href="https://developer.mozilla.org/en-US/docs/HTML/Supported_media_formats">audio HTML element</a> to avoid the Flash bit. But now I could listen to the interview (which, incidentally, is awesome) so I was happy.</p>

<p>I asked Jeff about how they were converting the RealAudio, because we have a fair bit of RealAudio laying around at my place of work. He wrote back with some useful notes that I thought I would publish on the Web for others googling for how to do it at this particular point in time. I'd be curious to know if you regard RealAudio as a preservation risk, and good example of a format we ought to be migrating. The playback options seem quite limited, and precarious, but perhaps that's just my own limited experience.</p>

<p>The whole interaction with WFMU, from discovery, to access, to preservation, to <a href="http://www.ftrain.com/wwic.html">interaction</a> seemed like such a perfect illustration of what the Web can do for archives, and vice-versa.</p>

<h2>Jeff's Notes</h2>

<p><em>The text below is from Jeff's email to me. Jeff, if you are reading this and don't really want me quoting you this way, just <a href="mailto:ehs@pobox.com">let me know</a>.</em></p>

<p>I'm still fine-tuning the process, which is why the whole bulk transcode isn't done yet. I'm trying to find the sweet spot where I use enough space / bandwidth for the resulting files so that I don't hear any obvious degradation from the (actually pretty terrible-sounding) Real files, but don't just burn extra resources with nothing gained.</p>

<p>Our Real files are mostly mono sampled at 22.04khz, using a codec current decoders often identify as "Cook".</p>

<p>I've found that ffmpeg does a good job of extracting a WAV file from the Real originals - oh, and since there are two warring projects which each provide a program called ffmpeg, I mean this one:</p>

<p>http://ffmpeg.org/</p>

<p>We've been doing our AAC encoding with the Linux version of the Nero AAC Encoder released a few years ago:</p>

<p>http://www.nero.com/enu/company/about-nero/nero-aac-codec.php</p>

<p>...although I'm still investigating alternatives.</p>

<p>One interesting thing I've encountered is that a straight AAC re-encoding from the Real file (mono, 22.05k) plays fine as a file on disk, but hasn't played correctly for me (in the same VLC version) when streamed from Amazon S3. If I convert the mono archive to stereo and AAC-encode that with the Nero encoder, it's been streaming fine.</p>

<p>Oh, and if you want to transfer tags from the old Real files to any new files, and your transcoding pipeline doesn't automatically copy tags, note that ffprobe (also from the ffmpeg package) can extract tags from Real files, which you can then stuff back in (with neroAacTag or the tagger of your choice).</p>

<h2>Afterword</h2>

<p>Here is Googlebot coming to get the content a few minutes after I published this post.</p>

<pre>54.241.82.166 - - [23/May/2014:10:36:22 +0000] "GET http://inkdroid.org/2014/05/23/realaudio-aac-and-archivy/ HTTP/1.1" 200 20752 "-" "Googlebot/2.1 (+http://www.google.com/bot.html)"
</pre>

<p>So someone searching for how to convert RealAudio to AAC might stumble across it. This decentralized Web thing is kinda neat. We need to <a href="http://indiewebcamp.com/">take care of it</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7568</wp:post_id>
		<wp:post_date>2014-05-23 03:25:52</wp:post_date>
		<wp:post_date_gmt>2014-05-23 10:25:52</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>realaudio-aac-and-archivy</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="format-migration"><![CDATA[format migration]]></category>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<category domain="post_tag" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="wfmu"><![CDATA[wfmu]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>One Big Archive</title>
		<link>http://inkdroid.org/2014/08/14/one-big-archive/</link>
		<pubDate>Thu, 14 Aug 2014 17:05:14 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7608</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Several months ago <a href="http://hillelarnold.com/">Hillel Arnold</a> asked me to participate in a panel at the Society of American Archivists, <a href="http://archives2014.sched.org/event/57ce8b8ded5c6f74d721b5902f259d14#.U-ShqoBdXfs">A Trickle Becomes a Flood: Agency, Ethics and Information</a>. The description is probably worth repeating here:</p>

<blockquote>
  <p>From the Internet activism of Aaron Swartz to Wikileaks’ release of confidential U.S. diplomatic cables, numerous events in recent years have challenged the scope and implications of privacy, confidentiality, and access for archives and archivists. With them comes the opportunity, and perhaps the imperative, to interrogate the role of power, ethics, and regulation in information systems. How are we to engage with these questions as archivists and citizens, and what are their implications for user access?</p>
</blockquote>

<p>It is broad and important topic. I had 20 minutes to speak. Here's what I said.</p>

<p><em>Thanks to <a href="http://twitter.com/dchud">Dan Chudnov</a> for his <a href="http://onebiglibrary.net">One Big Library</a> idea, which I've adapted/appropriated.</em></p>

<hr />

<p>Thanks for inviting me to participate in this panel today. I've been a Society of American Archivists member for just two years, and this is my first SAA. So go easy on me, I'm a bit of a newb. I went to "Library School" almost 20 years ago, and was either blind to it or just totally missed out on the rich literature of the archive. As I began to work in the area of digital preservation 8 years ago several friends and mentors encouraged me to explore how thinking about the archive can inform digital repository systems. So here I am today. It's nice to be here with you.</p>

<p>One thing from the panel description I'd like to focus on is:</p>

<blockquote>
  <p>the opportunity, perhaps the imperative, to interrogate the role of power, ethics, and regulation in information systems</p>
</blockquote>

<p>I'd like to examine a specific, recent <a href="https://en.wikipedia.org/wiki/Mapping_controversies">controversy</a>, in order to explore how the dynamics of power and ethics impact the archive. I'm going to stretch our notion of what the archive is, perhaps to its breaking point, but I promise to bring it back to its normal shape at the end. I hope to highlight just how important archival thinking and community are to the landscape of the Web.</p>

<h2>Ethics</h2>

<p>Perhaps I've just been perseverating about what to talk about today, but it seems like the news has been full of stories about the role of ethics of information systems lately. One that is still fresh in most people's mind is the recent debate about Facebook's <a href="http://www.pnas.org/content/111/24/8788.full">emotional contagion study</a>, where users news feeds were directly manipulated to test theories about how emotions are transferred in social media.</p>

<p>As <a href="https://twitter.com/tcarmody">Tim Carmody</a> <a href="http://kottke.org/14/08/the-problem-with-okcupid-is-the-problem-with-the-social-web">pointed out</a>, this is significant not only for the individuals that had their news feeds tampered with, but also for the individuals who posted content to Facebook, only to have it manipulated. He asks:</p>

<blockquote>
  <p>What if you were one of the people whose posts were filtered because your keywords were too happy, too angry, or too sad?</p>
</blockquote>

<p>However, much of the debate centered on the Terms of Service that Facebook users agree to when they use that service. Did the ToS allow Facebook and Cornell to conduct this experiment?</p>

<p>At the moment there appears to be language in the Facebook Terms of Service that allows user contributed data to be used for research purposes. But that language <a href="http://www.washingtonpost.com/news/the-intersect/wp/2014/07/01/9-answers-about-facebooks-creepy-emotional-manipulation-experiment/">was not present</a> in the 2012 version of the ToS, when the experiment was conducted. But just because the fine print of a ToS document allows for something doesn't necessarily mean it is ethical. Are Terms of Service documents the right way to be talking about the ethical concerns and decisions of an organization? Aren't they just an artifact of the ethical decisions and conversations that have already happened?</p>

<p>Also, it <a href="http://www.washingtonpost.com/news/morning-mix/wp/2014/07/01/facebooks-emotional-manipulation-study-was-even-worse-than-you-thought/">appears</a> that the experiment itself was conducted before Cornell University’s Institutional Review Board had approved the study. Are IRB's functioning the way we want?</p>

<h2>Power</h2>

<p>The Facebook controversy got an added dimension when <a href="http://www.theguardian.com/technology/2014/jul/04/facebook-denies-emotion-contagion-study-government-military-ties">dots were connected</a> between one of the Cornell researchers involved in the study and the Department of Defense funded <a href="http://minerva.dtic.mil/">Minerva Initiative</a>, which,</p>

<blockquote>
  <p>aims to determine "the critical mass (tipping point)" of social contagions by studying their "digital traces" in the cases of "the 2011 Egyptian revolution, the 2011 Russian Duma elections, the 2012 Nigerian fuel subsidy crisis and the 2013 Gazi park protests in Turkey.”</p>
</blockquote>

<p>I'm currently reading <a href="https://twitter.com/BettyMedsger">Betty Medsger</a>'s excellent <a href="https://www.goodreads.com/book/show/17262123-the-burglary">book</a> about how a group of ordinary peace activists in 1971 came to steal files from a small FBI office in Media, Pennsylvania that provided evidence for the FBI's Counter Intelligence Program (<a href="https://en.wikipedia.org/wiki/COINTELPRO">COINTELPRO</a>). COINTELPRO was an illegal program for "surveying, infiltrating, discrediting, and disrupting domestic political organizations". Medsger was the Washington Post journalist who received the documents. It's hard not to see the parallels to today where prominent Muslim-American academics and politicians <a href="https://firstlook.org/theintercept/article/2014/07/09/under-surveillance/">are having their email monitored</a> by the FBI and the NSA. Where the NSA are <a href="http://www.theguardian.com/world/2013/jun/06/nsa-phone-records-verizon-court-order">collecting</a> millions of phone records from Verizon everyday. And where the NSA's <a href="http://www.theguardian.com/world/2013/jul/31/nsa-top-secret-program-online-data">XKeyScore</a> allows analysts to search through the emails, online chats and the browsing histories of millions of people.</p>

<p>Both Facebook and Cornell University later <a href="http://www.theguardian.com/technology/2014/jul/04/facebook-denies-emotion-contagion-study-government-military-ties">denied</a> that this particular study was funded through the Defense Department's Minerva Project, but did not deny that Professor Hancock had previously received funding from it. I don't think you need to be an information scientist (although many of you are) to see how one person's study of social contagions might inform another study of social contagion, <em>by that very same person</em>. But information scientists have bills to pay just like anyone else. I imagine if many of us traced our sources of income back sufficiently far enough we would find the Defense Department as a source. Still, the degrees of separation matter, don't they?</p>

<h2>Big Data</h2>

<p>In her excellent post <a href="https://medium.com/message/what-does-the-facebook-experiment-teach-us-c858c08e287f">What does the Facebook Experiment Teach Us</a>, social media researcher Danah Boyd explores how the debate about Facebook's research practices surfaces a general unease about the so called era of big data:</p>

<blockquote>
  <p>The more I read people’s reactions to this study, the more I’ve started to think the outrage has nothing to do with the study at all. There is a growing amount of negative sentiment towards Facebook and other companies that collect and use data about people. In short, there’s anger at the practice of big data. This paper provided ammunition for people’s anger because it’s so hard to talk about harm in the abstract.</p>
</blockquote>

<p>Certainly part of this anxiety is also the result of what we have learned our own government is doing in willing (and unwilling) collaboration with these companies, based on documents leaked by whistleblower Edward Snowden, and the subsequent journalism from the Guardian and the Washington Post that won them the Pulitzer Prize for Public Service. But as <a href="https://twitter.com/baconmeteor">Maciej Ceglowski</a> argues in his (awesome) <a href="http://idlewords.com/bt14.htm">The Internet With a Human Face</a>.</p>

<blockquote>
  <p>You could argue (and I do) that this data is actually safer in government hands. In the US, at least, there's no restriction on what companies can do with our private information, while there are stringent limits (in theory) on our government. And the NSA's servers are certainly less likely to get hacked by some kid in Minsk.</p>
  
  <p>But I understand that the government has guns and police and the power to put us in jail. It is legitimately frightening to have the government spying on all its citizens. What I take issue with is the idea that you can have different kinds of mass surveillance.</p>
  
  <p>If these vast databases are valuable enough, it doesn't matter who they belong to. The government will always find a way to query them. Who pays for the servers is just an implementation detail.</p>
</blockquote>

<p>Ceglowski makes the case that we need more regulation around the collection of behavioural data, specifically what is collected and how long it is kept. This should be starting to <a href="http://www2.archivists.org/glossary/terms/r/retention-period">sound familiar</a>.</p>

<p>Boyd takes a slightly more pragmatic tack by pointing out that social media companies need to establish ethics boards that allow users and scholars to enter into <em>conversation</em> with employees that have insight into how things work (policy, algorithms, etc). We need a bit more nuance than <a href="https://en.wikipedia.org/wiki/Don't_be_evil">"Don't be evil"</a>. We need to know how the companies that run the websites where we put our content are going to behave, or at least how they would like to behave, and what their moral compass is. I think we've seen this to some degree in the response of Google to the <a href="https://en.wikipedia.org/wiki/Right_to_be_forgotten">Right to be Forgotten</a> law in the EU (a topic for another panel). But I think Boyd is right, that much more coordinated and collaborative work could be done in this area.</p>

<h2>The Archive</h2>

<p>So, what is the relationship between big data and the archive? Or put a bit more concretely: given the right context, would you consider Facebook content to be archival records?</p>

<p>If you are wearing the right colored glasses, or perhaps <a href="http://www.archivists.org/periodicals/aa_v68/review-greene-aa68_2.asp">Hugh Taylor</a>'s spectacles, I think you will likely say yes, or at least maybe.</p>

<p>Quoting from SAA's <a href="http://www.archivists.org/archivesmonth/whatisanarchives.pdf">What is an Archive</a>:</p>

<blockquote>
  <p>An archive is a place where people go to find information. But rather than gathering information from books as you would in a library, people who do research in archives often gather firsthand facts, data, and evidence from letters, reports, notes, memos, photographs, audio and video recording.</p>
</blockquote>

<p>Perhaps content associated with Facebook's user, business and community group accounts are a form of <a href="http://www2.archivists.org/glossary/terms/p/provenance">provenance</a> or <a href="http://www2.archivists.org/glossary/terms/f/fonds">fonds</a>?</p>

<p>Does anyone here use Facebook? Have you downloaded your Facebook data?</p>

<p>If an individual downloads their Facebook content and donates it to an archive along with other material, does this Facebook data become part of the archival record? How can researchers then access these records? What constraints are in place that govern who can see them, and when. These are familiar questions for the archivist, aren't they?</p>

<p>Some may argue that the traditional archive doesn't <em>scale</em> in the same way that big data behemoths like Facebook or Google do. I think in one sense they are right, but this is a feature, not a bug.</p>

<p>If an archive accessions an individual's Facebook data there is an opportunity to talk with the donor about how they would like their records to be used, by whom and when. Think of an archival fonds as <a href="https://www.ideals.illinois.edu/handle/2142/39750">small data</a>. When you add up enough of these fonds, and put them on the Web, you get big data. But the scope and contents of these fonds fit in the brain of a person that can reason ethically about them, and this is a good thing, that is at the heart of our profession, and which we must not lose.</p>

<p>When you consider theories of the <a href="http://www2.archivists.org/glossary/terms/p/postcustodial-theory-of-archives">postcustodial archive</a> the scope of the archive enlarges greatly. Is there a role for the archivist and archival thinking and practices at Facebook itself? I think there is. Could archivists help balance the needs of researchers and records creators, and foster communication around the ethical use of these records? I think so. Could we help donors think about how they want their content to fit into the Web (when the time comes) by encouraging the use of creative commons licenses? Could we help organizations think about how they allow for people to download and/or delete their data, and how to package it up so it stands a chance of being readable?</p>

<p>Is it useful to look at the Web as <a href="http://onebiglibrary.net">One Big Archive</a>, where assets are moved from one archive to another, where researcher rights are balanced with the rights of record creators? Where long term access is taken more seriously in some pockets than in others? I hope so. I think it's what we do.</p>

<p>I haven't mentioned <a href="https://en.wikipedia.org/wiki/Aaron_Swartz">Aaron</a>, who worked so hard for these things. I could've spent the entire time talking about his work helping to build the Web, and interrogating power. Maybe I should have...but I guess I kind of did. There isn't any time left to help Aaron. But we still have time to help make the Web <a href="http://redecentralize.org/">a</a> <a href="https://pack.resetthenet.org/">better</a> <a href="http://www.theopeninter.net/">place</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7608</wp:post_id>
		<wp:post_date>2014-08-14 10:05:14</wp:post_date>
		<wp:post_date_gmt>2014-08-14 17:05:14</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>one-big-archive</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="creative-commons"><![CDATA[creative commons]]></category>
		<category domain="post_tag" nicename="facebook"><![CDATA[facebook]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="nsa"><![CDATA[nsa]]></category>
		<category domain="post_tag" nicename="provenance"><![CDATA[provenance]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87128</wp:comment_id>
			<wp:comment_author><![CDATA[Blogging #SAA14 &#8211; Save this space | Shifting Gears &#8211; Living in Multiple Worlds]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://raymmaxx.wordpress.com/2014/08/15/blogging-saa14-save-this-space/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-08-16 04:58:05</wp:comment_date>
			<wp:comment_date_gmt>2014-08-16 11:58:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Attended a lunch forum, Diversifying the Archival Record, where I learned about the upcoming Friday session featuring the Alabama archivists. More on that later. After lunch, I attended Ed Summer&#8217;s panel, A Trickle Becomes a Flood: Agency, Ethics and Information. He was joined by Hillel Arnold (HIghways, Wires and Tubes, an interesting talk about regulation, structure and standards of communications networks) and Elena Danielson (she talked about the Information Security Oversight Office and mentioned the US spends $11 billion per year to keep secrets. She also mentioned 10 points in secret-keeping history, starting with Bismark and ending with Snowden (1. Bismarck, 2. Keith Murdock, 3. Zimmerman telegram, 4. Winona Project, 5. Zerox machine, Daniel Ellsberg, 6. Pentagon papers, 7. Pollard case, 8. Assange and manning, 9. Schwartz, and 10. Snowden. For each she asked, was there money involved, did they do it for a higher principle, and did they screen the info for potential hazards). Ed&#8217;s talk can be found here: http://inkdroid.org/2014/08/14/one-big-archive/. [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1408499622.34721088409423828125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1408190285.5256369113922119140625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87204</wp:comment_id>
			<wp:comment_author><![CDATA[We are Big Data | new query]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://newquery.wordpress.com/2014/11/10/we-are-big-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-11-13 13:45:57</wp:comment_date>
			<wp:comment_date_gmt>2014-11-13 20:45:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] -Ed Summers [&#8230;]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1415911557.3950450420379638671875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1415926730.4945659637451171875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87199</wp:comment_id>
			<wp:comment_author><![CDATA[We are Big Data |]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://karencooperfiles.wordpress.com/2014/11/10/we-are-big-data/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-11-10 08:48:54</wp:comment_date>
			<wp:comment_date_gmt>2014-11-10 15:48:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Summers provides a similar definition in One Big Archive while addressing whether content generated on Facebook should be considered archival records.  He [&#8230;]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1415642839.11884593963623046875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1415634534.97638797760009765625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>No Place to Hide</title>
		<link>http://inkdroid.org/2014/06/05/no-place-to-hide/</link>
		<pubDate>Thu, 05 Jun 2014 09:53:41 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7615</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://www.goodreads.com/book/show/18213403-no-place-to-hide" style="float: left; padding-right: 20px"><img alt="No Place to Hide: Edward Snowden, the NSA, and the U.S. Surveillance State" border="0" src="https://d.gr-assets.com/books/1383352779m/18213403.jpg" /></a><a href="https://www.goodreads.com/book/show/18213403-no-place-to-hide">No Place to Hide: Edward Snowden, the NSA, and the U.S. Surveillance State</a> by <a href="https://www.goodreads.com/author/show/205996.Glenn_Greenwald">Glenn Greenwald</a><br />
My rating: <a href="https://www.goodreads.com/review/show/938660965">4 of 5 stars</a></p>

<p>I think Greenwald's book is a must read if you have any interest in the Snowden story, and the role of investigative journalism and its relationship to political power and the media. Greenwald is clearly a professional writer: his narrative is both lucid and compelling, and focuses on three areas that roughly correlate to sections of the book.</p>

<p>The first (and most exciting) section of the book goes behind the scenes to look at how Greenwald first came into contact with Snowden, and worked to publish his Guardian articles about the NSA wiretapping program. It is a riveting story, that provides a lot of insights into what motivated Snowden to do what he did. Snowden comes off as a very ethical, courageous and intelligent individual. Particularly striking was Snowden's efforts to make sure that the documents were not simply dumped on the Internet, but that journalists had an opportunity to interpret and contextualize the documents to encourage constructive discussion and debate.</p>

<blockquote>
  <p>In sixteen hours of barely interrupted reading, I managed to get through only a small fraction of the archive. But as the plane landed in Hong Kong, I knew two things for certain. First, the source was highly sophisticated and politically astute, evident in his recognition of the significance of most of the documents. He was also highly rational. The way he chose, analyzed, and described the thousands of documents I now had in my possession proved that. Second, it would be very difficult to deny his status as a classic whistle-blower. If disclosing proof that top-level national security officials lied outright to Congress about domestic spying programs doesn’t make one indisputably a whistle-blower, then what does?</p>
</blockquote>

<p>This section is followed by a quite detailed overview of what the documents revealed about the NSA wiretapping program, and their significance. If you are like me, and haven't read all the articles that have been published in the last year you'll enjoy this section.</p>

<p>And lastly the book analyzes the relationship between journalism and power in our media organizations, and the role of the independent journalist. The Guardian comes off as quite a progressive and courageous organization. Other media outlets like the New York Times and the Washington Post don't fare so well. I recently unsubscribed from the Washington Post, after vague feelings of uneasiness about their coverage -- so it was good to read Greenwald's pointed critique. After just having spent some time reading <a href="https://www.goodreads.com/book/show/6724695-archives-power">Archives Power</a> I was also struck by the parallels between positivist theories of the archive and journalism, and how important it is to be aware and recognize how power shapes and influences what we write, or archive.</p>

<blockquote>
  <p>Every news article is the product of all sorts of highly subjective cultural, nationalistic, and political assumptions. And all journalism serves one faction’s interest or another’s. The relevant distinction is not between journalists who have opinions and those who have none, a category that does not exist. It is between journalists who candidly reveal their opinions and those who conceal them, pretending they have none.</p>
</blockquote>

<p>The only reason I withheld the 5th star from my rating is it would've been interesting to know more about Snowden's departure from Hong Kong, his negotiations to seek asylum, and his relationship with Wikileaks and Sarah Harrison. Maybe that information wasn't known to Greenwald, but it would've been interesting to have a summary of what was publicly known.</p>

<p>One thing that No Place to Hide really did for me was underscore the importance of privacy and cryptography on the Web and the Internet. This is particularly relevant today, exactly one year after Greenwald's <a href="http://www.theguardian.com/world/2013/jun/06/nsa-phone-records-verizon-court-order" rel="nofollow">first Guardian article</a> was published, and as many people celebrate the anniversary by joining with the <a href="https://www.resetthenet.org/" rel="nofollow">Reset the Net</a> campaign. I haven't invested in a signed SSL certificate yet for inkdroid.org but I'm committing to doing that now. I've also recently started using <a href="https://gpgtools.org/" rel="nofollow">GPGTools</a> w/ Mail on my Mac. If you are curious about steps you can take check out the <a href="https://pack.resetthenet.org/" rel="nofollow">Reset the Net Privacy Pack</a>. In no place to hide Greenwald talks quite frankly about how he found cryptography tools difficult to use and understand, and how he got help in using them -- and how essential these tools are to his work.</p>

<p><a href="https://www.goodreads.com/review/list/5899086-ed-summers">View all my reviews</a></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7615</wp:post_id>
		<wp:post_date>2014-06-05 02:53:41</wp:post_date>
		<wp:post_date_gmt>2014-06-05 09:53:41</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>no-place-to-hide</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="books"><![CDATA[books]]></category>
		<category domain="post_tag" nicename="edward-snowden"><![CDATA[edward snowden]]></category>
		<category domain="post_tag" nicename="glenn-greenwald"><![CDATA[glenn greenwald]]></category>
		<category domain="post_tag" nicename="nsa"><![CDATA[nsa]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>MayDay - We Can Fix This Thing</title>
		<link>http://inkdroid.org/2014/07/03/mayday-we-can-fix-this-thing/</link>
		<pubDate>Thu, 03 Jul 2014 19:51:07 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7646</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>We won't get our democracy back until we <a href="https://mayday.us/">change</a> the way campaigns are funded.</p>
</blockquote>

<iframe width="560" height="315" src="//www.youtube.com/embed/i3X2eDCmPRY" frameborder="0" allowfullscreen></iframe>

<p><em>TL;DR if you were thinking of supporting <a href="http://mayday.us">MayDay</a> and have been putting it off please <a href="https://mayday.us/">act</a> by July 4th. Every contribution helps, and you will only be charged if they hit their 5 million dollar target (2 million to go right now as I write this). Plus, and this is a big plus, you will be able to tell yourself and maybe your grandkids that you helped make real political reform happen in the United States. Oh and it's July 4th weekend, what better way to celebrate the independence we have left!</em></p>

<p>If you are reading my blog you are most likely a fan of <a href="https://en.wikipedia.org/wiki/Lawrence_Lessig">Lawrence Lessig</a> and <a href="https://en.wikipedia.org/wiki/Aaron_Swartz">Aaron Swartz</a>'s work on <a href="http://creativecommons.org">Creative Commons</a> to help create a content ecosystem for the Web that works for its users ... you and me.</p>

<p>Before he left us Aaron convinced Lessig that we need to get to the root of the problem, how political decisions are made in Congress, in order to address macro problems like copyright reform. For a really personal interview with Lessig that covers this evolution in his thinking, and how it led to the <a href="https://en.wikipedia.org/wiki/Doris_Haddock">Granny D</a> inspired <a href="http://www.nhrebellion.org/">midwinter march</a> across New Hampshire, and the recent <a href="https://mayday.us/the-plan/">MayDay</a> effort check out last week's podcast of the <a href="http://thegoodfight.fm/episodes/25-lawrence-lessig-aaron-swartz-and-the-super-pac-to-end-super-pacs">The Good Fight</a>.</p>

<div style='position: relative; padding-bottom: 59.25%; height: 0; overflow: hidden; width: 100%;'>
  <iframe allowfullscreen frameborder='0' src='//listeners.fsj.fm/player?caption=&episodeId=25-lawrence-lessig-aaron-swartz-and-the-super-pac-to-end-super-pacs&image=https%3A%2F%2Fthegoodfight.s3.amazonaws.com%2Fuploads%2F7a3f47d4451d2a321fe2ca3cddcda0b0.jpg&mp3=http%3A%2F%2Fthegoodfight.fm%2Fepisodes%2F25-lawrence-lessig-aaron-swartz-and-the-super-pac-to-end-super-pacs%2Faudio&userId=e8b1e6a065270c6b6b1ce2016635fe3e' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;'></iframe>
</div>

<p>Or if you haven't seen it, definitely watch Lessig's 13 minute TED Talk:</p>

<iframe src="http://embed.ted.com/talks/lawrence_lessig_the_unstoppable_walk_to_political_reform.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>

<p>If you are part of the 90% of American's who think that our government is broken because of the money in politics please check out <a href="https://mayday.us/">MayDay</a>'s efforts to crowdsource enough money to fund political campaigns that are committed to legislation that will change it.</p>

<p>It doesn't matter if you are on the left or the right, or if you live in a red or blue state, or honestly whether you live in the United States or not. This is an issue that impacts all of us, and generations to come. We can fix this thing. But we have to try to fix it, we can't just sit back and expect someone else to fix it for us. Lessig has a plan, and he's raised 4 million dollars so far (if you include the previous 1 million campaign) from people like you who think he's on to something. Let's push MayDay over the five million dollar edge and see what happens next!</p>

<h2>Afterword</h2>

<p>And as my friend <a href="http://twitter.com/mjgiarlo">Mike</a> reminded me, even if you aren't sure about the politics, donate in memory of Aaron. He was such an advocate and innovator for the Web, libraries and archives ... and continues to be sorely missed. Catch the recently released <a href="http://www.takepart.com/internets-own-boy">documentary</a> about Aaron in movie theaters now or for free <a href="https://archive.org/details/TheInternetsOwnBoyTheStoryOfAaronSwartz">on the Internet Archive</a> since it is Creative Commons licensed:</p>

<iframe src="https://archive.org/embed/TheInternetsOwnBoyTheStoryOfAaronSwartz" width="640" height="480" frameborder="0" webkitallowfullscreen="true" mozallowfullscreen="true" allowfullscreen></iframe>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7646</wp:post_id>
		<wp:post_date>2014-07-03 12:51:07</wp:post_date>
		<wp:post_date_gmt>2014-07-03 19:51:07</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>mayday-we-can-fix-this-thing</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>why @congressedits?</title>
		<link>http://inkdroid.org/2014/07/10/why-congressedits/</link>
		<pubDate>Fri, 11 Jul 2014 04:44:18 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7687</guid>
		<description></description>
		<content:encoded><![CDATA[<p><em>Note: as with all the content on this blog, this post reflects my own thoughts about a personal project, and not the opinions or activities of my employer.</em></p>

<p>Two days ago a retweet from my friend <a href="http://twitter.com/iand">Ian Davis</a> scrolled past in my Twitter stream:</p>

<blockquote class="twitter-tweet" lang="en">
  <p>
    This Twitter bot will show whenever someone edits Wikipedia from within the British Parliament. It was set up by <a href="https://twitter.com/tomscott">@tomscott</a> using <a href="https://twitter.com/IFTTT">@ifttt</a>.
  </p>— Parliament WikiEdits (@parliamentedits) 
  
  <a href="https://twitter.com/parliamentedits/statuses/486539012413607936">July 8, 2014</a>
</blockquote>

<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> The simplicity of combining Wikipedia and Twitter in this way immediately struck me as a potentially useful transparency tool. So using my experience on a <a href="http://wikistream.wmflabs.org">previous side project</a> I quickly put together a short <a href="http://github.com/edsu/anon">program</a> that listens to all major language Wikipedias for <em>anonymous</em> edits from Congressional IP address ranges (<a href="https://twitter.com/JoshData/status/486825061504917504">thanks Josh</a>) ... and tweets them.</p>

<p><a href="http://twitter.com/congressedits"><img src="http://inkdroid.org/images/congressedits.png" alt="" /></a></p>

<p>In less than 48 hours the <a href="http://twitter.com/congressedits">@congressedits</a> Twitter account had more than 3,000 followers. My friend <a href="http://twitter.com/ruebot">Nick</a> set up <a href="https://twitter.com/gccaedits">gccaedits</a> for Canada using the <a href="http://github.com/edsu/anon">same software</a> ... and <a href="http://twitter.com/wikiAssemblee">@wikiAssemblee</a> (France) and <a href="http://twitter.com/RiksdagWikiEdit">@RiksdagWikiEdit</a> (Sweden) were quick to follow.</p>

<hr />

<p>Watching the followers rise, and the flood of <a href="https://twitter.com/search?f=realtime&amp;q=congress%20edits">tweets</a> from them brought home something that I believed intellectually, but hadn't felt quite so viscerally before. There is an incredible yearning in this country and around the world for using technology to provide more transparency about our democracies.</p>

<p>Sure, there were tweets and media stories that belittled the few edits that have been found so far. But by and large people on Twitter have been encouraging, supportive and above all interested in what their elected representatives are doing. Despite historically low approval ratings for Congress, people still care deeply about our democracies, our principles and dreams of a government of the people, by the people and for the people.</p>

<p>We desperately want to be part of a more informed citizenry, that engages with our local communities, sees the world as our stage, and the World Wide Web as our medium.</p>

<hr />

<p>Consider this thought experiment. Imagine if our elected representatives and their staffers <em>logged in</em> to Wikipedia, identified much like <a href="https://en.wikipedia.org/wiki/User:Dominic">Dominic McDevitt-Parks</a> (a federal employee at the National Archives) and used their knowledge of the issues and local history to help make Wikipedia better? Perhaps in the process they enter into conversation in an article's talk page, with a constituent, or political opponent and learn something from them, or perhaps compromise? The version history becomes a history of the debate and discussion around a topic. Certainly there are issues of <a href="https://en.wikipedia.org/wiki/Wikipedia:Conflict_of_interest">conflict of interest</a> to consider, but we always edit topics we are interested and knowledgeable about, don't we?</p>

<p>I think there is often fear that increased transparency can lead to increased criticism of our elected officials. It's not surprising given the way our political party system and media operate: always looking for scandal, and the salacious story that will push public opinion a point in one direction, to someone's advantage. This fear encourages us to clamp down, to decrease or obfuscate the transparency we have. We all kinda lose, irrespective of our political leanings, because we are ultimately less informed.</p>

<hr />

<p>I wrote this post to make it clear that my hope for @congressedits wasn't to expose inanity, or belittle our elected officials. The truth is, @congressedits has only announced a handful of edits, and some of them are pretty banal. But can't a staffer or politician make a grammatical change, or update an article about a movie? Is it really news that they are human, just like the rest of us?</p>

<p>I created @congressedits because I hoped it could engender more, better ideas and tools like it. More thought experiments. More care for our communities and peoples. More understanding, and willingness to talk to each other. More humor. More human.</p>

<blockquote class="twitter-tweet" lang="en">
  <p>
    <a href="https://twitter.com/congressedits">@Congressedits</a> is why we invented the Internet
  </p>— zarkinfrood (@zarkinfrood) 
  
  <a href="https://twitter.com/zarkinfrood/statuses/487438418457010177">July 11, 2014</a>
</blockquote>

<p>I'm pretty sure zarkinfrood meant @congressedits figuratively, not literally. As if perhaps @congressedits was emblematic, in its very small way, of something a lot bigger and more important. Let's not forget that when we see the inevitable mockery and bickering in the media. Don't forget the big picture. We need transparency in our government more than ever, so we can have healthy debates about the issues that matter. We need to protect and enrich our Internet, and our Web ... and to do that we need to positively engage in debate, not tear each other down.</p>

<blockquote>
  <p>Educate and inform the whole mass of the people. Enable them to see that it is their interest to preserve peace and order, and they will preserve them. And it requires no very high degree of education to convince them of this. They are the only sure reliance for the preservation of our liberty. -- Thomas Jefferson</p>
</blockquote>

<p>Who knew TJ was a Wikipedian...</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7687</wp:post_id>
		<wp:post_date>2014-07-10 21:44:18</wp:post_date>
		<wp:post_date_gmt>2014-07-11 04:44:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>why-congressedits</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87044</wp:comment_id>
			<wp:comment_author><![CDATA[Quando Twitter e Wikipedia, insieme, aiutano la trasparenza - My Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://enerd.it/quando-twitter-e-wikipedia-insieme-aiutano-la-trasparenza/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-14 01:34:20</wp:comment_date>
			<wp:comment_date_gmt>2014-07-14 08:34:20</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] si tratta di mettere alla berlina gli autori degli edit, dice Summers. E del resto, i tweet finora segnalati sono piuttosto banali. Si tratta piuttosto di [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405326860.684031963348388671875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405329988.6901400089263916015625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87045</wp:comment_id>
			<wp:comment_author><![CDATA[Sofs]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.sofs.tk/2014/07/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-14 03:13:54</wp:comment_date>
			<wp:comment_date_gmt>2014-07-14 10:13:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405332834.511478900909423828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405414004.9527130126953125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87037</wp:comment_id>
			<wp:comment_author><![CDATA[Faller @CongressEdits ta see Wikipedia edits frum t'Capitol | T&#039;Redneck Revue]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.redneckrevue.com/2014/07/12/follow-congressedits-to-see-wikipedia-edits-from-the-capitol/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 23:55:44</wp:comment_date>
			<wp:comment_date_gmt>2014-07-13 06:55:44</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] fer @congressedits wasn’t ta expose inanitee, er belittle air electid officials,” he writes n&#039; a blog post. “T&#039; truth is, @congressedits has onlee announcet a han&#039;fil o&#039;edits, an&#039; sum o&#039;um air purty [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244875.7763841152191162109375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87035</wp:comment_id>
			<wp:comment_author><![CDATA[why @congressedits? - HackerExchange.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.hackerexchange.com/why-congressedits/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 11:45:26</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 18:45:26</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Continue Reading : why @congressedits? [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405190726.22175693511962890625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244888.2460520267486572265625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87036</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly | News Alternative]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.newsalternative.net/?p=10531</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 20:30:09</wp:comment_date>
			<wp:comment_date_gmt>2014-07-13 03:30:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405222209.9247419834136962890625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244887.2097890377044677734375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87030</wp:comment_id>
			<wp:comment_author><![CDATA[Follow @CongressEdits to see Wikipedia edits from the Capitol | En.Moxo.IR &#8211; Technoloy and Gadget News And Reviews]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://en.moxo.ir/3855/follow-congressedits-to-see-wikipedia-edits-from-the-capitol/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 02:45:50</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 09:45:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] wasn&rsquo;t to expose inanity, or belittle our elected officials,&rdquo; he writes in a blog post. &ldquo;The truth is, @congressedits has only announced a handful of edits, and some of them are [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405175045.98682498931884765625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405158350.90820789337158203125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87031</wp:comment_id>
			<wp:comment_author><![CDATA[Follow @CongressEdits to see Wikipedia edits from the Capitol | Internet Tech]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://fibermodemus2013.com/follow-congressedits-to-see-wikipedia-edits-from-the-capitol/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 02:55:19</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 09:55:19</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] for @congressedits wasn’t to expose inanity, or belittle our elected officials,” he writes in a blog post. “The truth is, @congressedits has only announced a handful of edits, and some of them are pretty [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405175042.6924800872802734375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244895.664905071258544921875;s:7:"message";s:43:"ed changed the comment status to unapproved";s:5:"event";s:17:"status-unapproved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244896.5891659259796142578125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405158919.23223590850830078125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87113</wp:comment_id>
			<wp:comment_author><![CDATA[PCs do Planalto foram usados para editar Wikipédia e incluir críticas a opositores do governo | Notícia da CidadeNotícia da Cidade]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://noticiadacidade.com.br/canais/info/pcs-do-planalto-foram-usados-para-editar-wikipedia-e-incluir-criticas-a-opositores-do-governo/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-28 13:16:25</wp:comment_date>
			<wp:comment_date_gmt>2014-07-28 20:16:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] a surgir bots que avisam quando IPs do governo editam artigos da Wikip&eacute;dia &ndash; seja no Reino Unido, nos EUA ou na Fran&ccedil;a. E no Brasil? Ainda n&atilde;o temos algo parecido, o que &eacute; uma pena: parece que nosso [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1406578585.5146720409393310546875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1406578670.4333150386810302734375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87033</wp:comment_id>
			<wp:comment_author><![CDATA[Keeping An Eye on the Bastards | Daily Pundit]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.dailypundit.com/?p=88739</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 11:32:08</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 18:32:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] why @congressedits? | inkdroid [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405189928.7196750640869140625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244894.5006749629974365234375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87034</wp:comment_id>
			<wp:comment_author><![CDATA[Bot @congressedits tweets all anonymous Wikipedia edits made from Congressional IP addresses (Ed Summers/inkdroid) | Killer Apps TV]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://killerapps.tv/bot-congressedits-tweets-all-anonymous-wikipedia-edits-made-from-congressional-ip-addresses-ed-summersinkdroid/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 11:43:14</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 18:43:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Continue reading here: Bot @congressedits tweets all anonymous Wikipedia edits made from Congressional IP addresses (Ed Sum&#8230; [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405190594.8771779537200927734375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244891.166428089141845703125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86998</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly - teqarazzi]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.teqarazzi.com/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 10:17:08</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 17:17:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405099028.3785459995269775390625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245198.9949591159820556640625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86996</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly | Autoblog Market Premium WordPress Theme]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://autoblogmarket.info/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 10:12:58</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 17:12:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405098778.4810369014739990234375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245199.0063569545745849609375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87001</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly | En.Moxo.IR &#8211; Technoloy and Gadget News And Reviews]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://en.moxo.ir/3665/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 10:35:25</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 17:35:25</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245198.9644119739532470703125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405100125.7440440654754638671875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87000</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly | UxTouch]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://uxtouch.com/2014/07/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 10:24:14</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 17:24:14</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405099454.5156099796295166015625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245198.9827721118927001953125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86999</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://underlux.pp.ua/index.php/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 10:22:18</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 17:22:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245198.9891109466552734375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405099338.2854011058807373046875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87021</wp:comment_id>
			<wp:comment_author><![CDATA[New bot tweets anonymous Wikipedia edits from Congress. Now if we only had the same thing for PR firms | PandoDaily]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://pando.com/2014/07/11/new-bot-tweets-anonymous-wikipedia-edits-from-congress-now-if-we-only-had-the-same-thing-for-pr-firms/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 14:00:50</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 21:00:50</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] exposed so far have been pretty banal, like correcting grammar and punctuation errors, Summers points out that his reasoning behind the account wasn&#8217;t necessarily to shame [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405112450.732822895050048828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405113512.290832042694091796875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87105</wp:comment_id>
			<wp:comment_author><![CDATA[Help from the Hill: Wikipedia-lovers from US House of Reps edited Russia articles | Liberate Greece]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://liberategreece.com/help-from-the-hill-wikipedia-lovers-from-us-house-of-reps-edited-russia-articles/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-17 23:08:57</wp:comment_date>
			<wp:comment_date_gmt>2014-07-18 06:08:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] new ‘propagandist changes’ have been recorded by a Twitter bot that watches for anonymous Wikipedia edits from various IP addresses from the US Congress, reported [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405962166.09957790374755859375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405663737.8528940677642822265625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87086</wp:comment_id>
			<wp:comment_author><![CDATA[Staten redigerer Wikipedia]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://nrkbeta.no/2014/07/15/staten-redigerer-wikipedia/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-15 01:46:54</wp:comment_date>
			<wp:comment_date_gmt>2014-07-15 08:46:54</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] av @congressedits forklarer bakgrunnen for hvorfor han har satt opp kontoen på sin egen blogg&nbsp;og skriver at det potensielt er et nyttig verktøy for [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405414101.799128055572509765625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405414014.2228519916534423828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86997</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly | Distinct Today]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://distincttoday.com/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 10:16:49</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 17:16:49</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405099009.875009059906005859375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245199.00026798248291015625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87005</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly | Hihid News]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://news.hihid.co/read/2014/07/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 11:02:04</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 18:02:04</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405101724.7292950153350830078125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245085.1343510150909423828125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87006</wp:comment_id>
			<wp:comment_author><![CDATA[The United States Congress edits Wikipedia constantly | Technology news]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://techntech.net/?p=1343</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 11:07:02</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 18:07:02</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] world for using technology to provide more transparency about our democracies,&#8221; Summers wrote on his blog this week. While the tracking hasn&#8217;t revealed any bombshells thus far, we&#8217;re all for free, easy [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405102022.515839099884033203125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245053.892426967620849609375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87008</wp:comment_id>
			<wp:comment_author><![CDATA[The USA Congress edits Wikipedia always | Global Quest Services]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://globalquestservices.us/the-united-states-congress-edits-wikipedia-constantly/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 11:12:10</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 18:12:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] for utilizing know-how to offer extra transparency about our democracies,&#8221; Summers wrote on his blog this week. Whereas the monitoring hasn&#8217;t revealed any bombshells up to now, we&#8217;re all free of [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405102330.3868920803070068359375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405245024.36693096160888671875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87023</wp:comment_id>
			<wp:comment_author><![CDATA[Hill Staffers Are Anonymously Editing Wikipedia Pages On The Horse Head Mask, &#039;Step Up 3D&#039; | Tsangg]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://tsangg.com/2014/07/11/hill-staffers-are-anonymously-editing-wikipedia-pages-on-the-horse-head-mask-step-up-3d/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 15:58:33</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 22:58:33</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] a post explaining the new Twitter account, he explained that he was inspired by a similar account that [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405119513.8556759357452392578125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244907.908341884613037109375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87022</wp:comment_id>
			<wp:comment_author><![CDATA[@Congressedits tweets anonymous Wikipedia edits from Capitol Hill | Revere Radio Network]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.revereradionetwork.com/congressedits-tweets-anonymous-wikipedia-edits-from-capitol-hill/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 14:29:10</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 21:29:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Summers, an open source Web developer, recently saw a friend tweet about Parliament WikiEdits, a UK Twitter “bot” that watched for anonymous Wikipedia edits [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405114150.3740699291229248046875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244909.5956380367279052734375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87019</wp:comment_id>
			<wp:comment_author><![CDATA[Today in Technology July 11, 2014 | Tech Fann.com]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://techfann.com/2014/07/11/today-in-technology-july-11-2014/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 13:14:36</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 20:14:36</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] a post explaining the new Twitter account, he explained that he was inspired by a similar account that [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405109676.9178369045257568359375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405244913.50224590301513671875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86985</wp:comment_id>
			<wp:comment_author><![CDATA[Tracking Anonymous Wikipedia Edits From Specific IP Ranges | OUseful.Info, the blog...]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://blog.ouseful.info/2014/07/11/tracking-anonymous-wikipedia-edits-from-specific-ip-ranges/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 01:51:07</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 08:51:07</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Wikipedia edits that are made from IP addresses in the US Congress&#8221;. (For more info, see why @congressedits?, /via @ostephens.) I didn&#8217;t follow the link to the home page for that account (doh!), but in [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405068667.6234309673309326171875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405079657.51747989654541015625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>86986</wp:comment_id>
			<wp:comment_author><![CDATA[@Congressedits tweets anonymous Wikipedia edits from Capitol Hill - The Populist Wire]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.populistwire.com/24947/congressedits-tweets-anonymous-wikipedia-edits-from-capitol-hill/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 07:50:05</wp:comment_date>
			<wp:comment_date_gmt>2014-07-11 14:50:05</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Summers, an IT specialist at the Library of Congress and an open source Web developer, recently saw a friend tweet about Parliament WikiEdits, a UK Twitter “bot” that watched for anonymous Wikipedia edits [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405096644.749867916107177734375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405090205.57487201690673828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87027</wp:comment_id>
			<wp:comment_author><![CDATA[Hill Staffers Are Anonymously Editing Wikipedia Pages On The Horse Head Mask, &#039;Step Up 3D&#039; - AltoSky - AltoSky]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://altosky.com/hill-staffers-are-anonymously-editing-wikipedia-pages-on-the-horse-head-mask-step-up-3d/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 00:18:23</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 07:18:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] a post explaining the new Twitter account, he explained that he was inspired by a similar account that [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405149504.055860996246337890625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405175058.0526320934295654296875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87028</wp:comment_id>
			<wp:comment_author><![CDATA[Follow @CongressEdits to see Wikipedia edits from the Capitol | Digital Trends]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://www.digitaltrends.com/web/follow-congressedits-see-wikipedia-edits-capitol/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 01:55:08</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 08:55:08</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] @congressedits wasn’t to expose inanity, or belittle our elected officials,&#8221; he writes in a blog post. &#8220;The truth is, @congressedits has only announced a handful of edits, and some of them are [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405175053.504765987396240234375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405155308.886229038238525390625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87029</wp:comment_id>
			<wp:comment_author><![CDATA[Follow @CongressEdits to see Wikipedia edits from the Capitol | Social Dashboard]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://socialdashboard.com/follow-congressedits-to-see-wikipedia-edits-from-the-capitol/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-12 02:41:51</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 09:41:51</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] for @congressedits wasn’t to expose inanity, or belittle our elected officials,” he writes in a blog post. “The truth is, @congressedits has only announced a handful of edits, and some of them are pretty [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405175047.855659961700439453125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405158111.5086181163787841796875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87025</wp:comment_id>
			<wp:comment_author><![CDATA[Twitter bot alerts for anonymous Wikipedia edits from Capitol Hill | The Freedom Watch]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://freedomwat.ch/2014/07/12/twitter-bot-alerts-for-anonymous-wikipedia-edits-from-capitol-hill/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-07-11 20:24:33</wp:comment_date>
			<wp:comment_date_gmt>2014-07-12 03:24:33</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Summers, an open source Web developer, recently saw a friend tweet about Parliament WikiEdits, a UK Twitter &#8216;bot&#8217; that watched for anonymous Wikipedia [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405175074.1209480762481689453125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1405135473.37090396881103515625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87254</wp:comment_id>
			<wp:comment_author><![CDATA[#c4l15 talk extras]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://andromedayelton.com/blog/2015/02/12/c4l15-talk-extras/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-02-12 09:03:21</wp:comment_date>
			<wp:comment_date_gmt>2015-02-12 16:03:21</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] http://inkdroid.org/2014/07/10/why-congressedits/ [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1423758623.6018970012664794921875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1423757001.4675180912017822265625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Paper Work</title>
		<link>http://inkdroid.org/2014/08/07/paper-work/</link>
		<pubDate>Fri, 08 Aug 2014 02:05:32 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7809</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>The connective quality of written traces is still more visible in the most despised of all ethnographic objects: the file or the record. The "rationalization" granted to bureaucracy since Hegel and Weber has been attributed by mistake to the "mind" of (Prussian) bureaucrats. It is all in the files themselves.</p>
  
  <p>A bureau is, in many ways, and more and more every year, a small laboratory in which many elements can be connected together just because their scale and nature has been averaged out: legal texts, specifications, standards, payrolls, maps, surveys (ever since the Norman conquest, as shown by Clanchy, 1979). Economics, politics, sociology, hard sciences, do not come into contact through the grandiose entrance of "interdisciplinarity" but through the back door of the file.</p>
  
  <p>The "cracy" of bureaucracy is mysterious and hard to study, but the "bureau" is something that can be empirically studied, and which explains, because of its structure, why some power is given to an average mind just by looking at files: domains which are far apart become literally inches apart; domains which are convoluted and hidden, become flat; thousands of occurrences can be looked at synoptically.</p>
  
  <p>More importantly, once files start being gathered everywhere to insure some two-way circulation of immutable mobiles, they can be arranged in cascade: files of files can be generated and this process can be continued until a few men consider millions as if they were in the palms of their hands. Common sense ironically makes fun of these "gratte papiers" and "paper shufflers", and often wonders what all this "red tape" is for; but the same question should be asked of the rest of science and technology. In our cultures "paper shuffling" is the source of an essential power, that constantly escapes attention since its materiality is ignored.</p>
</blockquote>

<p>from <a href="http://worrydream.com/refs/Latour%20-%20Visualisation%20and%20Cognition.pdf">Visualization and Cognition: Drawing Things Together</a> by <a href="https://en.wikipedia.org/wiki/Bruno_Latour">Bruno Latour</a></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7809</wp:post_id>
		<wp:post_date>2014-08-07 19:05:32</wp:post_date>
		<wp:post_date_gmt>2014-08-08 02:05:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>paper-work</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="bruno-latour"><![CDATA[bruno latour]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Moving On to MITH</title>
		<link>http://inkdroid.org/2014/08/21/moving-on-to-mith/</link>
		<pubDate>Thu, 21 Aug 2014 20:08:47 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7951</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I'm very excited to say that I am going to be joining the <a href="http://mith.umd.edu">Maryland Institute for Technology in the Humanities</a> to work as their Lead Developer. Apart from the super impressive <a href="http://mith.umd.edu/people/">folks</a> at MITH that I will be joining, I am really looking forward to helping them continue to explore how digital media can be used in interdisciplinary scholarship and learning...and to build and sustain tools that make and remake the work of digital humanities research. MITH's role in the digital humanities field, and its connections on campus and to institutions and projects around the world were just too good of an opportunity to pass up.</p>

<p>I'm extremely grateful to my colleagues at the Library of Congress who have made working over the last 8 years such a transformative experience. It's a bit of a cliche that there are challenges to working in the .gov space. But at the end of the day, I am so proud to have been part of the mission to "further the progress of knowledge and creativity for the benefit of the American people". It's a tall order, that took me in a few directions that ended up bearing fruit, and some others that didn't. But that's always the way of things right? Thanks to all of you (you know who you are) that made it so rewarding...and so much fun.</p>

<p>When I wrote recently about the importance of <a href="http://inkdroid.org/2014/05/20/broken-world/">broken world thinking</a> I had no idea I would be leaving LC to join MITH in a few weeks. I feel incredibly lucky to be able to continue to work in the still evolving space of digital curation and preservation, with a renewed focus on putting data to use, and helping make the world a better place.</p>

<p>Oh, and I still plan on writing here ... so, till the next episode.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7951</wp:post_id>
		<wp:post_date>2014-08-21 13:08:47</wp:post_date>
		<wp:post_date_gmt>2014-08-21 20:08:47</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>moving-on-to-mith</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="dreams"><![CDATA[dreams]]></category>
		<category domain="post_tag" nicename="friends"><![CDATA[friends]]></category>
		<category domain="post_tag" nicename="jobs"><![CDATA[jobs]]></category>
		<category domain="post_tag" nicename="library-of-congress"><![CDATA[library of congress]]></category>
		<category domain="post_tag" nicename="life"><![CDATA[life]]></category>
		<category domain="post_tag" nicename="mith"><![CDATA[mith]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A Ferguson Twitter Archive</title>
		<link>http://inkdroid.org/2014/08/30/a-ferguson-twitter-archive/</link>
		<pubDate>Sun, 31 Aug 2014 01:06:55 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=7967</guid>
		<description></description>
		<content:encoded><![CDATA[<em>If you are interested in an update about where/how to get the data after reading this see <a href="http://inkdroid.org/2014/11/18/on-forgetting/">here</a>.</em>

<p>Much has been <a href="http://pinboard.in/u:user/t:twitter+ferguson">written</a> about the significance of Twitter as the <a href="https://en.wikipedia.org/wiki/Shooting_of_Michael_Brown">recent events</a> in Ferguson echoed round the Web, the country, and the world. I happened to be at the Society of American Archivists meeting 5 days after Michael Brown was killed. During our <a href="http://inkdroid.org/2014/08/14/one-big-archive/">panel discussion</a> someone asked about the role that archivists should play in documenting the event.</p>

<p>There was wide agreement that Ferguson was a painful reminder of the type of event that archivists working to "interrogate the role of power, ethics, and regulation in information systems" should be documenting. But what to do? Unfortunately we didn't have time to really discuss exactly how this agreement translated into action.</p>

<p>Fortunately the very next day the <a href="http://archiveit.org">Archive-It</a> service run by the <a href="http://archive.org">Internet Archive</a> <a href="https://twitter.com/archiveitorg/status/500315270440058880">announced</a> that they were collecting seed URLs for a Web archive related to Ferguson. It was only then, after also having finally read <a href="https://twitter.com/zeynep">Zeynep Tufekci</a>'s terrific <a href="https://medium.com/message/ferguson-is-also-a-net-neutrality-issue-6d2f3db51eb0">Medium post</a>, that I slapped myself on the forehead ... of course, we should try to archive the tweets. Ideally there would be a "we" but the reality was it was just "me". Still, it seemed worth seeing how much I could get done.</p>

<h3>twarc</h3>

<p>I had some <a href="http://inkdroid.org/2013/01/19/aaronsw/">previous experience</a> archiving tweets related to <a href="https://en.wikipedia.org/wiki/Aaron_Swartz">Aaron Swartz</a> using Twitter's search API. <em>(Full disclosure: I also worked on the Twitter archiving project at the Library of Congress, but did not use any of that code or data then, or now.)</em> I wrote a small Python command line program named <a href="http://github.com/edsu/twarc">twarc</a> (a portmanteau for Twitter Archive), to help manage the archiving.</p>

<p>You give twarc a search query term, and it will plod through the search results, in reverse chronological order (the order that they are returned in), while handling quota limits, and writing out line-oriented-json, where each line is a complete tweet. It worked quite well to collect 630,000 tweets mentioning "aaronsw", but I was starting late out of the gate, 6 days after the events in Ferguson began. One downside to twarc is it is completely dependent on Twitter's <a href="https://dev.twitter.com/docs/api/1.1/get/search/tweets">search API</a>, which only returns results for the past week or so. You can search back further in Twitter's Web app, but that seems to be a privileged client. I can't seem to convince the API to keep going back in time past a week or so.</p>

<p>So time was of the essence. I started up twarc searching for all tweets that mention <em>ferguson</em>, but quickly realized that the volume of tweets, and the order of the search results meant that I wouldn't be able to retrieve the earliest tweets. So I tried to guesstimate a Twitter ID far enough back in time to use with twarc's <code>--max_id</code> parameter to limit the initial query to tweets before that point in time. Doing this I was able to get back to 2014-08-10 22:44:43 -- most of August 9th and 10th had slipped out of the window. I used a similar technique of guessing a ID further in the future in combination with the <code>--since_id</code> parameter to start collecting from where that snapshot left off. This resulted in a bit of a fragmented record, which you can see visualized (sort of below):</p>

<p><img src="http://inkdroid.org/images/twarc-ferguson-timeline.png" style="border: thin solid #cccccc;" /></p>

<p>In the end I collected 13,480,000 tweets (63G of JSON) between August 10th and August 27th. There were some gaps because of mismanagement of twarc, and the data just moving too fast for me to recover from them: most of August 13th is missing, as well as part of August 22nd. I'll know better next time how to manage this higher volume collection.</p>

<p>Apart from the data, a nice side effect of this work is that I fixed a socket timeout error in twarc that I hadn't noticed before. I also refactored it a bit so I could use it programmatically like a library instead of only as a command line tool. This allowed me to write a program to archive the tweets, incrementing the max_id and since_id values automatically. The longer continuous crawls near the end are the result of using twarc more as a library from another program.</p>

<iframe height=371 width=600 src="//docs.google.com/spreadsheets/d/1K7rlWVcYRXz2XreIGDEqiLdrrl1oL1c_hefV6yWv68w/gviz/chartiframe?oid=1827253224" seamless frameborder=0 scrolling=no></iframe>

<h3>Bag of Tweets</h3>

<p>To try to arrange/package the data a bit I decided to order all the tweets by tweet id, and split them up into gzipped files of 1 million tweets each. Sorting 13 million tweets was pretty easy using <a href="https://pypi.python.org/pypi/leveldb">leveldb</a>. I first loaded all 16 million tweets into the db, using the tweet id as the key, and the JSON string as the value.</p>

<pre lang="python">import json
import leveldb
import fileinput

db = leveldb.LevelDB('./tweets.db')

for line in fileinput.input():
    tweet = json.loads(line)
    db.Put(tweet['id_str'], line)
</pre>

<p>This took almost 2 hours on a medium ec2 instance. Then I walked the leveldb index, writing out the JSON as I went, which took 35 minutes:</p>

<pre lang="python">import leveldb

db = leveldb.LevelDB('./tweets.db')
for k, v in db.RangeIter(None, include_value=True):
    print v,
</pre>

<p>After splitting them up into 1 million line files with <a href="https://en.wikipedia.org/wiki/Cut_(Unix)">cut</a> and gzipping them I put them in a <a href="http://en.wikipedia.org/wiki/BagIt">Bag</a> and uploaded it to s3 (8.5G).</p>

<h3> </h3>

<p>I am planning on trying to extract URLs from the tweets to try to come up with a list of seed URLs for the Archive-It crawl. If you have ideas of how to use it definitely get in touch. I haven't decided yet if/where to host the data publicly. If you have ideas please get in touch about that too!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7967</wp:post_id>
		<wp:post_date>2014-08-30 18:06:55</wp:post_date>
		<wp:post_date_gmt>2014-08-31 01:06:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-ferguson-twitter-archive</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="ferguson"><![CDATA[ferguson]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87162</wp:comment_id>
			<wp:comment_author><![CDATA[Archiving the web during the conflict and protest in Ferguson, MO | Learn More]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://aitlearnmore.archive.org/2014/09/03/archiving-the-web-during-the-conflict-and-protest-in-ferguson-mo/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-03 11:07:58</wp:comment_date>
			<wp:comment_date_gmt>2014-09-03 18:07:58</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] (MITH), used a tool he developed called twarc to collect over 13 million tweets in JSON format. As part of his project, Ed has extracted the top 50 links mentioned in these archived tweets (see also his follow-up post, [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409769534.0855519771575927734375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409767678.22016811370849609375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87159</wp:comment_id>
			<wp:comment_author><![CDATA[Editors&#8217; Choice: On Archiving Tweets | Digital Humanities Now]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://digitalhumanitiesnow.org/2014/09/editors-choice-on-archiving-tweets/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-02 08:31:11</wp:comment_date>
			<wp:comment_date_gmt>2014-09-02 15:31:11</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] A Ferguson Twitter Archive [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409671871.9866390228271484375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409675071.27422809600830078125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87166</wp:comment_id>
			<wp:comment_author><![CDATA[Recent news related to our class conversations | ACM Newberry Seminar 2014]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://acmnewberryseminar2014.wordpress.com/2014/09/04/recent-news-related-to-our-class-conversations/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-04 15:54:10</wp:comment_date>
			<wp:comment_date_gmt>2014-09-04 22:54:10</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] For those interested in Ferguson and archiving, you may be interested in this campaign for a Ferguson Twitter Archive. [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409871250.5289280414581298828125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409909311.545631885528564453125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87232</wp:comment_id>
			<wp:comment_author><![CDATA[#Ferguson Supplement: Archival Resources]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://americanethnologist.org/2015/ferguson-supplement-archival-resources/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-01-15 05:56:12</wp:comment_date>
			<wp:comment_date_gmt>2015-01-15 12:56:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Ferguson twitter archive initiative [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1421326572.1590878963470458984375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1421336504.4263041019439697265625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87351</wp:comment_id>
			<wp:comment_author><![CDATA[Archiving the web during events in Ferguson, MO &#8211;]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://qa-archive-it.org/blog/post/archiving-the-web-during-the-conflict-and-protest-in-ferguson-mo/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-05-20 10:32:18</wp:comment_date>
			<wp:comment_date_gmt>2015-05-20 17:32:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] (MITH), used a tool he developed called twarc to collect over 13 million tweets in JSON format. As part of his project, Ed  extracted the top 50 links mentioned in the first 50,000 tweets from the evening of August [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1432143138.6713678836822509765625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1432143177.5460150241851806640625;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87359</wp:comment_id>
			<wp:comment_author><![CDATA[Archiving tweets about Kingsborough]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://kingsboroughlibtech.commons.gc.cuny.edu/2015/05/25/archiving-tweets-about-kingsborough/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-05-25 20:31:17</wp:comment_date>
			<wp:comment_date_gmt>2015-05-26 03:31:17</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] events in Ferguson, MO as they unfolded over the course of 17 days. He blogged about the process here. Twarc brought an archivist’s collecting impulse to events that were happening very quickly, [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1432611077.536840915679931640625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1436443884.53849697113037109375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>On Archiving Tweets</title>
		<link>http://inkdroid.org/2014/08/31/on-archiving-tweets/</link>
		<pubDate>Sun, 31 Aug 2014 14:55:48 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8020</guid>
		<description></description>
		<content:encoded><![CDATA[<p><img src="http://inkdroid.org/images/webpresmed/package.png" style="width: 200px; float: left; margin-right: 10px;" /></p>

<p>After my last <a href="http://inkdroid.org/2014/08/30/a-ferguson-twitter-archive/">post</a> about collecting 13 million Ferguson tweets <a href="http://twitter.com/liblaura">Laura Wrubel</a> from George Washington University's <a href="http://social-feed-manager.readthedocs.org/">Social Feed Manager</a> project <a href="https://twitter.com/liblaura/status/505925215852191745">recommended</a> looking at how <a href="http://twitter.com/vphill">Mark Phillips</a> made his <a href="http://digital.library.unt.edu/ark:/67531/metadc304853/m1/">Yes All Women</a> collection of tweets available in the <a href="http://digital.library.unt.edu/">University of North Texas Digital Library</a>. <em>By the way, both are awesome projects to check out if you are interested in how access informs digital preservation.</em></p>

<p>If you take a look you'll see that only the Twitter ids are listed in the data that you can download. The full metadata that Mark collected (with <a href="http://github.com/edsu/twarc">twarc</a> incidentally) doesn't appear to be there. Laura knows from her work on the <a href="http://social-feed-manager.readthedocs.org/">Social Feed Manager</a> that it is fairly common practice in the research community to only openly distribute lists of Tweet ids instead of the raw data. I believe this is done out of concern for Twitter's <a href="https://dev.twitter.com/terms/api-terms">terms of service</a> (1.4.A):</p>

<blockquote>
  <p>If you provide downloadable datasets of Twitter Content or an API that returns Twitter Content, you may only return IDs (including tweet IDs and user IDs).</p>
  
  <p>You may provide spreadsheet or PDF files or other export functionality via non­-programmatic means, such as using a "save as" button, for up to 100,000 public Tweets and/or User Objects per user per day. Exporting Twitter Content to a datastore as a service or other cloud based service, however, is not permitted.</p>
</blockquote>

<p>There are privacy concerns here (redistributing data that users have chosen to remove). But I suspect Twitter has business reasons to discourage widespread redistribution of bulk Twitter data, especially now that they have <a href="https://blog.twitter.com/2014/twitter-welcomes-gnip-to-the-flock">bought</a> the social media data provider <a href="http://gnip.com">Gnip</a>.</p>

<p>I haven't really seen a discussion of this practice of distributing Tweet ids, and its implications for research and digital preservation. I see that the <a href="http://www.icwsm.org/">International Conference on Weblogs and Social Media</a> now have a <a href="http://www.icwsm.org/2014/datasets/datasets/">dataset service</a> where you need to agree to their "Sharing Agreement", which basically prevents re-sharing of the data.</p>

<blockquote>
  <p>Please note that this agreement gives you access to all ICWSM-published datasets. In it, you agree not to redistribute the datasets. Furthermore, ensure that, when using a dataset in your own work, you abide by the citation requests of the authors of the dataset used.</p>
</blockquote>

<p>I can certainly understand wanting to control how some of this data is made available, especially after the debate after <a href="http://inkdroid.org/2014/08/14/one-big-archive/">Facebook's Emotional Contagion Study</a> went public. But this does not bode well for digital preservation where lots of copies keeps stuff safe. What if there were a standard license that we could use that encouraged data sharing among research data repositories? A viral license like the <a href="https://en.wikipedia.org/wiki/GNU_General_Public_License">GPL</a> that allowed data to be shared and reshared within particular contexts? Maybe the <a href="https://creativecommons.org/licenses/by-nc/2.0/">CC-BY-NC</a>, or is it too weak? If each tweet is copyrighted by the person who sent it, can we even license them in bulk? What if Twitter's terms of service included a research clause that applied to more than just Twitter employees, but to downstream archives?</p>

<h2>Back of the Envelope</h2>

<p>So if I were to make the ferguson tweet ids available, to work with the dataset you would need to refetch the data using the Twitter API, one tweet at a time. I did a little bit of <a href="https://dev.twitter.com/docs/rate-limiting/1.1">reading</a> and poking at the Twitter API and it appears an access token is limited to 180 requests every 15 minutes. So how long would it take to reconstitute 13 million Twitter ids?</p>

<pre><code>13,000,000 tweets / 180 tweets per interval = 72,222 intervals
72,222 intervals * 15 minutes per interval =  1,083,330 minutes
</code></pre>

<p><a href="http://www.wolframalpha.com/input/?i=1%2C083%2C330+minutes">1,083,330 minutes</a> is <strong>two years</strong> of constant accesses to the Twitter API. Please let me know if I've done something conceptually/mathematically wrong.</p>

<p><em>Update: it turns out the <a href="https://dev.twitter.com/docs/api/1.1/get/statuses/lookup">statuses/lookup</a> API call can return full tweet data for up to 100 tweets per request. So a single access token could fetch about 72,000 tweets per hour (100 per request, 180 requests per 15 minutes) ... which only amounts to 180 hours, which is just over a week. James Jacobs rightly <a href="https://twitter.com/freegovinfo/status/506600456659812352">points out</a> that a single application could use multiple access tokens, assuming users allowed the application to use them. So if 7 Twitter users donated their Twitter account API quota, the 13 million tweets could be reconstituted from their ids in roughly a day. So the situation is definitely not as bad as I initially thought. Perhaps there needs to be an app that allows people to donate some of the API quota for this sort of task? I wonder if that's allowed by Twitter's ToS.</em></p>

<p>The big assumption here is that the Twitter API continues to operate as it currently does. If Twitter changes its API, or ceases to exist as a company, there would be no way to reconstitute the data. But what if there were a functioning Twitter archive that could reconstitute the original data using the list of Twitter ids...</p>

<h2>Digital Preservation as a Service</h2>

<p>I've hesitated to write about LC's Twitter archive while I was an employee. But now that I'm no longer working there I'll just say I think this would be a perfect experimental service for them to consider providing. If a researcher could upload a list of Twitter ids to a service at the Library of Congress and get them back a few hours, days or even weeks later, this would be much preferable to managing a two year crawl of Twitter's API. It also would allow an ecosystem of Twitter ID sharing to evolve.</p>

<p>The downside here is that all the tweets are in one basket, as it were. What if LC's Twitter archiving program is discontinued? Does anyone else have a copy? I wonder if Mark kept the original tweet data that he collected, and it is private, available only inside the UNT archive? If someone could come and demonstrate to UNT that they have a research need to see the data, perhaps they could sign some sort of agreement, and get access to the original data?</p>

<p>I have to be honest, I kind of loathe idea of libraries and archives being gatekeepers to this data. Having to decide what is valid research and what is not seems fraught with peril. But on the flip side Maciej has <a href="http://idlewords.com/bt14.htm">a point</a>:</p>

<blockquote>
  <p>These big collections of personal data are like radioactive waste. It's easy to generate, easy to store in the short term, incredibly toxic, and almost impossible to dispose of. Just when you think you've buried it forever, it comes leaching out somewhere unexpected.</p>
  
  <p>Managing this waste requires planning on timescales much longer than we're typically used to. A typical Internet company goes belly-up after a couple of years. The personal data it has collected will remain sensitive for decades.</p>
</blockquote>

<p>It feels like we (the research community) need to manage access to this data so that it's not just out there for anyone to use. Maciej's essential point is that businesses (and downstream archives) shouldn't be collecting this behavioral data in the first place. But what about a tweet (its metadata) is behavioural? Could we strip it out? If I squint right, or put on my NSA colored glasses, even the simplest metadata such as who is tweeting to who seems behavioral.</p>

<p>It's a bit of a platitude to say that social media is still new enough that we are still figuring out how to use it. Does a legitimate <a href="https://en.wikipedia.org/wiki/Right_to_be_forgotten">right to be forgotten</a> mean that we forget everything? Can businesses blink out of existence leaving giant steaming pools of informational toxic waste, while research institutions aren't able to collect and preserve small portions as datasets? I hope not.</p>

<p>To bring things back down to earth, how should I make this Ferguson Twitter data available? Are a list of tweet ids the best the archiving community can do, given the constraints of Twitter's Terms of Service? Is there another way forward that addresses very real preservation and privacy concerns around the data? Some archivists may cringe at the cavalier use of the word "archiving" in the title of this post. However, I think the issues of access and preservation bound up in this simple use case warrant the attention of the archival community. What archival practices can we draw and adapt to help us do this work?</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8020</wp:post_id>
		<wp:post_date>2014-08-31 07:55:48</wp:post_date>
		<wp:post_date_gmt>2014-08-31 14:55:48</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-archiving-tweets</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="business"><![CDATA[business]]></category>
		<category domain="post_tag" nicename="preservation"><![CDATA[preservation]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:87160;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[archiving-tweets-2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87163</wp:comment_id>
			<wp:comment_author><![CDATA[Archiving the web during the conflict and protest in Ferguson, MO | Learn More]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://aitlearnmore.archive.org/2014/09/03/archiving-the-web-during-the-conflict-and-protest-in-ferguson-mo/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-03 11:08:56</wp:comment_date>
			<wp:comment_date_gmt>2014-09-03 18:08:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] has extracted the top 50 links mentioned in these archived tweets (see also his follow-up post, “On Archiving Tweets”). We have added these top 50 links to the Archive-It collection and will continue to collaborate [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409769522.9572079181671142578125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409767736.85354709625244140625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87165</wp:comment_id>
			<wp:comment_author><![CDATA[Archiving the web during events in Ferguson, MO | Learn More]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://aitlearnmore.archive.org/2014/09/03/archiving-the-web-during-the-conflict-and-protest-in-ferguson-mo/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-04 11:40:07</wp:comment_date>
			<wp:comment_date_gmt>2014-09-04 18:40:07</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] in the first 50,000 tweets from the evening on August 10th (see also his follow-up post, “On Archiving Tweets”). We  added these 50 links to the Archive-It collection and will continue to collaborate with [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409909375.1541879177093505859375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409856007.5251080989837646484375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87160</wp:comment_id>
			<wp:comment_author><![CDATA[laurelrusswurm.wordpress.com/]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://laurelrusswurm.wordpress.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-03 09:53:53</wp:comment_date>
			<wp:comment_date_gmt>2014-09-03 16:53:53</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[TOSes already carry far much more power than they ought to, adding more is not a good idea.

You must remember that copyright is really just a state imposed monopoly, and as such it works differently around the world.   Just because the Twitter TOS requires us to license or even assign our copyright, even after we have assigned our copyright to Twitter, we Canadians retain our moral rights under law, <em>which gives <b>us</b> a say in what <b>you</b> do with it.</em>

I very strongly disagree with your conclusion that:

<blockquote>...(the research community) need to manage access to this data so that it’s not just out there for anyone to use..."</blockquote>

The data in archives needs to be OPEN data.  Why on earth should institutional archives be able to store my data, and what entitles researchers use my data, while preventing others &mdash; including <em>me</em> &mdash; from having the same access?

Certainly there are privacy issues, but rather than taking the paternalistic view that researchers (or worse, <em>institutions</em>) are somehow better equipped (<em>morally? ethically? intellectually?</em>) than the public to manage <em>our</em> personal data &mdash; presumably because most of the public continues to be ignorant of the serious consequences of private information made public &mdash; why not help educate people instead?

If more people understood privacy issues, it would be easier to deal with breaches or concerns.

Despite legal idiocy, no one has the "right to be forgotten."  Setting aside the problems of doublespeak and thoughtcrime inherent in such a dangerous legal precedent, the fact remains: <b>the only way privacy can be protected is by keeping private information private</b>.

People will continue to be deluded into believing Twitter or Facebook privacy settings will make it "private," so long as credence is given to the fairy tale that privacy can be magically restored after publication.  <b>Instead help people understand anything they post on the Internet can never again be private because the Internet is a public space.</b>.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>15033</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409763529.91654491424560546875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409763233.82087802886962890625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:28:"laurelrusswurm.wordpress.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87171</wp:comment_id>
			<wp:comment_author><![CDATA[Editors&#8217; Choice: On Archiving Tweets | Digital Humanities Now]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://digitalhumanitiesnow.org/2014/09/editors-choice-on-archiving-tweets/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-15 10:51:57</wp:comment_date>
			<wp:comment_date_gmt>2014-09-15 17:51:57</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] On Archiving Tweets [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1410803573.7725989818572998046875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1410803517.2783648967742919921875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87161</wp:comment_id>
			<wp:comment_author><![CDATA[ed]]></wp:comment_author>
			<wp:comment_author_email>ehs@pobox.com</wp:comment_author_email>
			<wp:comment_author_url>http://www.inkdroid.org</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-03 10:33:18</wp:comment_date>
			<wp:comment_date_gmt>2014-09-03 17:33:18</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[<p>Thanks for your comment. I really sympathize with your position. Actually, you've channeled my inner voice on this matter pretty well :-)</p>

<p>It's a bit of a gray area, but I believe <a href="https://twitter.com/tos" rel="nofollow">Twitter's ToS</a> do not claim copyright over your tweets:</p>

<blockquote>
  <p>Twitter has an evolving set of rules for how ecosystem partners can interact with your Content. These rules exist to enable an open ecosystem with your rights in mind. But what’s yours is yours – you own your Content (and your photos are part of that Content).</p>
</blockquote>

<p>I think it's a gray area because even though Twitter does not claim to own your content, they also don't say whether it is ownable. I guess whether a tweet can be copyrighted is open for debate, at least in some circles. Do a Google search on "is a tweet copyrightable" and get lost for a while :-)</p>

<p>Be that as it may, I think you are spot on in saying that users of social media need to understand that they should be very careful about the personal data that they put "into the cloud". It is something I talk to my kids about it, and I'm sure it's a familiar conversation everywhere. So a new generation of more savvy social media users aka "the public" will soon take center stage.</p>

<p>One other thing to keep in mind is that traditionally, when archives are given collections, there is normally a <a href="http://www2.archivists.org/publications/brochures/deeds-of-gift" rel="nofollow">deed of gift</a> that stipulates certain conditions under which the content can be used. Sometimes these deeds restrict access to particular content, or require approval from the record creators, or keep the content closed for a period of time. These deeds of gift can vary from collection to collection. For example <a href="http://blogs.loc.gov/loc/files/2010/04/LOC-Twitter.pdf" rel="nofollow">here</a> is the deed of gift of the Twitter data to the Library of Congress. Not all information in archives is public. I personally think archives that make sharing information publicly on the Web a priority will be the ones that thrive. One needs to look no further than the Wikipedia success story to see that.</p>

<p>Now in the case of my particular collection of Ferguson tweets, I used Twitter's API to generate the dataset. Yes, it was all information put on the public Web by users. But if I am going to be a responsible actor I should try to at least abide by the ToS for the API should I. However I also have responsibilities as an archivist, in particular the <a href="http://www2.archivists.org/statements/saa-core-values-statement-and-code-of-ethics" rel="nofollow">SAA Code of Ethics</a> which says:</p>

<blockquote>
  <p>Archivists promote and provide the widest possible accessibility of materials, consistent with any mandatory access restrictions, such as public statute, donor contract, business/institutional privacy, or personal privacy. Although access may be limited in some instances, archivists seek to promote open access and use when possible. Access to records is essential in personal, academic, business, and government settings, and use of records should be both welcomed and actively promoted. Even individuals who do not directly use archival materials benefit indirectly from research, public programs, and other forms of archival use, including the symbolic value of knowing that such records exist and can be accessed when needed.</p>
</blockquote>

<p>I am hopeful that we will see emerging consensus in the archival community about how to balance the needs of businesses and their ToS, the rights of individuals who put content into the public space of the Web, and the needs of researchers who would like to use these collections.</p>

<p>I strongly sympathize with your main point that archives need to focus and give preference to open content. If the archival profession is to continue in its current form I think it needs to get its donors thinking in terms of Creative Commons license, and how their content can be made part of the Web, when the time comes.</p>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>87160</wp:comment_parent>
			<wp:comment_user_id>2</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1409765598.6882579326629638671875;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Agile in Academia</title>
		<link>http://inkdroid.org/2014/09/05/agile-in-academia/</link>
		<pubDate>Fri, 05 Sep 2014 23:10:22 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8089</guid>
		<description></description>
		<content:encoded><![CDATA[<p>I'm just finishing up my first week at <a href="http://mith.umd.edu">MITH</a>. What a smart, friendly group to be a part of, and with such exciting prospects for future work. Riding along the Sligo Creek and Northwest Branch trails to and from campus certainly helps. Let's just say I couldn't be happier with my decision to join MITH, and will be writing more about the work as I learn more, and get to work.</p>

<p>But I already have a question, that I'm hoping you can help me with.</p>

<p>I've been out of academia for over ten years. In my time away I've focused on my role as an <a href="http://agilemanifesto.org/">agile</a> software developer -- increasingly with a lower case "a". Working directly with the users of software (stakeholders, customers, etc), and getting the software into their hands as early as possible to inform the next iteration of work has been very rewarding. I've seen it work again, and again, and I suspect you have too on your own projects.</p>

<p>What I'm wondering is if you know of any tips, books, articles, etc on how to apply these agile practices in the context of grant funded projects. I'm still re-aquainting myself with how grants are tracked, and reported, but it seems to me that they seem to often encourage fairly detailed schedules of work, and cost estimates based on time spent on particular tasks, which (from 10,000 ft) reminds me a bit of the <a href="https://en.wikipedia.org/wiki/Waterfall_model">waterfall</a>.</p>

<p>Who usually acts as the <a href="https://en.wikipedia.org/wiki/Scrum_(software_development)#Product_Owner">product owner</a> in grant drive software development projects? How easy is it to adapt schedules and plans based on what you have learned in a past iteration? How do you get working software into the hands of its potential users as soon as possible? How often do you meet, and what is the focus of discussion? Are there particular funding bodies that appreciate agile software development? Are grants normally focused on publishing research and data instead of software products?</p>

<p>Any links, references, citations, tips or advice you could send my way here, <a href="http://twitter.com/edsu">@edsu</a>, or <a href="mailto:ehs@pobox.com">email</a> would be greatly appreciated. I've already got <a href="">Bethany Nowviskie</a>'s <a href="http://nowviskie.org/2012/lazy-consensus/">Lazy Consensus</a> bookmarked for re-reading :-)</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8089</wp:post_id>
		<wp:post_date>2014-09-05 16:10:22</wp:post_date>
		<wp:post_date_gmt>2014-09-05 23:10:22</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>agile-in-academia</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="academia"><![CDATA[academia]]></category>
		<category domain="post_tag" nicename="agile"><![CDATA[agile]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="mith"><![CDATA[mith]]></category>
		<category domain="category" nicename="programming"><![CDATA[programming]]></category>
		<category domain="post_tag" nicename="software-development"><![CDATA[software development]]></category>
		<wp:postmeta>
			<wp:meta_key>openid_comments</wp:meta_key>
			<wp:meta_value><![CDATA[a:1:{i:0;i:87167;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87167</wp:comment_id>
			<wp:comment_author><![CDATA[Trevor Owens]]></wp:comment_author>
			<wp:comment_author_email>trevor.johnowens@gmail.com</wp:comment_author_email>
			<wp:comment_author_url>http://tjowens.myopenid.com/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-09-06 09:24:52</wp:comment_date>
			<wp:comment_date_gmt>2014-09-06 16:24:52</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[As someone who has recently read a lot of grant proposals for software projects I'd stress that I think there is an increasing understanding and desire even for people to move away from very detailed requirements docs and do more agile work.

In that vein, I think it ends up meaning that in proposals we spend a lot more of our time describing 1) the big picture idea of what problem the software is going to solve 2) the initial decisions for what environments and platforms we are going to use and 3) the users we are going to work with.

Along with that, I've increasingly seen grant proposals include workplans that have things like "Make paper prototype" followed by "do user testing" followed by a whole series of iterations with rounds of user feedback. So there are some good ways to go about describing that in a proposal.

With that said, I'd imagine a lot of what you have is a bunch of existing projects. Things that have workplans and timelines etc. In that vein, I think it's worth stressing that, in general, when someone get's the money for a grant the workplan in there is really just a good faith effort at convincing the funder that you know what you are doing. It is not a contract to be followed to the letter. Ultimately, the grant winner really needs to have something to show for themselves at the end of the grant. So the only real timeline is the day the money needs to be spent by. Thus, when I have seen folks do this work well, it often involves a lot of pivoting to different tactics and approaches as they go while just keeping in mind that in the end there is a date that you need to have something to show for yourself.
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>2211</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1410020693.0580079555511474609375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:20:"tjowens.myopenid.com";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Google&#039;s Subconscious</title>
		<link>http://inkdroid.org/2014/09/17/googles-subconscious/</link>
		<pubDate>Wed, 17 Sep 2014 11:19:17 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8134</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Can a poem provide insight into the inner workings of a complex algorithm? If Google Search had a subconscious, what would it look like? If Google mumbled in its sleep, what would it say?</p>

<p>A few days ago, I ran across these two quotes within hours of each other:</p>

<blockquote>
  <p>So if algorithms like autocomplete can defame people or businesses, our next logical question might be to ask how to hold those algorithms accountable for their actions.</p>
  
  <p><cite><a href="http://www.nickdiakopoulos.com/2013/08/06/algorithmic-defamation-the-case-of-the-shameless-autocomplete/">Algorithmic Defamation: The Case of the Shameless Autocomplete</a> by <a href="https://twitter.com/ndiakopoulos">Nick Diakopoulos</a></cite></p>
</blockquote>

<p>and</p>

<blockquote>
  <p>A beautiful poem should re-write itself one-half word at a time, in pre-determined intervals.</p>
  
  <p><cite><a href="http://www.upne.com/0819569288.html">Seven Controlled Vocabluaries</a> by <a href="https://twitter.com/chalkknit">Tan Lin</a>.</cite></p>
</blockquote>

<p>Then I got to thinking about what a poem auto-generated from Google's autosuggest might look like. Ok, the idea is of dubious value, but it turned out to be pretty easy to do in just <a href="http://inkdroid.org/vogon">HTML and JavaScript</a> (low computational overhead), and I quickly pushed it up to <a href="http://github.com/edsu/vogon">GitHub</a>.</p>

<p>Here's the <a href="https://en.wikipedia.org/wiki/Heuristic">heuristic</a>:</p>

<ol>
<li>Pick a title for your poem, which also serves as a <code>seed</code>.</li>
<li>Look up the <code>seed</code> in Google's <a href="https://stackoverflow.com/questions/5102878/google-suggest-api">lightly documented</a> suggestion API.</li>
<li>Get the longest <code>suggestion</code> (text length).</li>
<li>Output the <code>suggestion</code> as a line in the poem.</li>
<li>Stop if more than <code>n</code> lines have been written.</li>
<li>Pick a random substring in the <code>suggestion</code> as the <code>seed</code> for the next line.</li>
<li>GOTO 2</li>
</ol>

<p>The initial results were kind of light on verbs, so I found a list of verbs and randomly added them to the suggested text, occasionally. The poem is generated in your browser using JavaScript so hack on it and send me a pull request.</p>

<p>Assuming that Google's suggestions are personalized for you (if you are logged into Google) and your location (your IP address), the poem is dependent on you. So I suppose it's more of a collective subconscious in a way.</p>

<p>If you find an amusing phrase, please hover over the stanza and tweet it -- I'd love to see it!</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8134</wp:post_id>
		<wp:post_date>2014-09-17 04:19:17</wp:post_date>
		<wp:post_date_gmt>2014-09-17 11:19:17</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>googles-subconscious</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="algorithms"><![CDATA[algorithms]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="category" nicename="libraries"><![CDATA[libraries]]></category>
		<category domain="post_tag" nicename="poetry"><![CDATA[poetry]]></category>
		<category domain="post_tag" nicename="writing"><![CDATA[writing]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Satellite of Art</title>
		<link>http://inkdroid.org/2014/09/18/satellite-of-art/</link>
		<pubDate>Thu, 18 Sep 2014 13:26:33 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8153</guid>
		<description></description>
		<content:encoded><![CDATA[<iframe src="https://www.google.com/maps/embed?pb=!1m10!1m8!1m3!1d1589.448647362392!2d-112.6694107!3d41.4374347!3m2!1i1024!2i768!4f13.1!5e1!3m2!1sen!2sus!4v1411042952882" width="800" height="600" frameborder="0" style="border:0"></iframe>

<p>... still there</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8153</wp:post_id>
		<wp:post_date>2014-09-18 06:26:33</wp:post_date>
		<wp:post_date_gmt>2014-09-18 13:26:33</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>satellite-of-art</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="art"><![CDATA[art]]></category>
		<category domain="category" nicename="art"><![CDATA[art]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="post_tag" nicename="satellites"><![CDATA[satellites]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Hong Kong Tags</title>
		<link>http://inkdroid.org/2014/09/29/hong-kong-tags/</link>
		<pubDate>Mon, 29 Sep 2014 13:58:16 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8157</guid>
		<description></description>
		<content:encoded><![CDATA[<p>The top 25 tags in 166,246 tweets between 2014-09-29 09:54:18 - 2014-09-21 10:31:00 (EDT) mentioning #occupycentral.</p>

<table>
  <tr>
    <td>
      hongkong
    </td>
    
    <td style="text-align: right;">
      37,836
    </td>
  </tr>
  
  <tr>
    <td>
      hkstudentstrike
    </td>
    
    <td style="text-align: right;">
      13,667
    </td>
  </tr>
  
  <tr>
    <td>
      hk
    </td>
    
    <td style="text-align: right;">
      12,819
    </td>
  </tr>
  
  <tr>
    <td>
      hk926
    </td>
    
    <td style="text-align: right;">
      9,928
    </td>
  </tr>
  
  <tr>
    <td>
      hkclassboycott
    </td>
    
    <td style="text-align: right;">
      7,439
    </td>
  </tr>
  
  <tr>
    <td>
      china
    </td>
    
    <td style="text-align: right;">
      5,297
    </td>
  </tr>
  
  <tr>
    <td>
      occupyhongkong
    </td>
    
    <td style="text-align: right;">
      5,273
    </td>
  </tr>
  
  <tr>
    <td>
      occupyadmiralty
    </td>
    
    <td style="text-align: right;">
      5,075
    </td>
  </tr>
  
  <tr>
    <td>
      umbrellarevolution
    </td>
    
    <td style="text-align: right;">
      4,271
    </td>
  </tr>
  
  <tr>
    <td>
      hkdemocracy
    </td>
    
    <td style="text-align: right;">
      3,863
    </td>
  </tr>
  
  <tr>
    <td>
      occupyhk
    </td>
    
    <td style="text-align: right;">
      3,626
    </td>
  </tr>
  
  <tr>
    <td>
      hk929
    </td>
    
    <td style="text-align: right;">
      3,195
    </td>
  </tr>
  
  <tr>
    <td>
      hk928
    </td>
    
    <td style="text-align: right;">
      3,063
    </td>
  </tr>
  
  <tr>
    <td>
      hongkongdemocracy
    </td>
    
    <td style="text-align: right;">
      2,500
    </td>
  </tr>
  
  <tr>
    <td>
      hongkongprotests
    </td>
    
    <td style="text-align: right;">
      2,144
    </td>
  </tr>
  
  <tr>
    <td>
      solidarityhk
    </td>
    
    <td style="text-align: right;">
      1,983
    </td>
  </tr>
  
  <tr>
    <td>
      hkstudentboycott
    </td>
    
    <td style="text-align: right;">
      1,702
    </td>
  </tr>
  
  <tr>
    <td>
      democracy
    </td>
    
    <td style="text-align: right;">
      1,466
    </td>
  </tr>
  
  <tr>
    <td>
      ferguson
    </td>
    
    <td style="text-align: right;">
      1,449
    </td>
  </tr>
  
  <tr>
    <td>
      umbrellamovement
    </td>
    
    <td style="text-align: right;">
      1,168
    </td>
  </tr>
  
  <tr>
    <td>
      globalforhk
    </td>
    
    <td style="text-align: right;">
      1,157
    </td>
  </tr>
  
  <tr>
    <td>
      ??
    </td>
    
    <td style="text-align: right;">
      1,080
    </td>
  </tr>
  
  <tr>
    <td>
      imperialism
    </td>
    
    <td style="text-align: right;">
      1,003
    </td>
  </tr>
  
  <tr>
    <td>
      gonawazgo
    </td>
    
    <td style="text-align: right;">
      800
    </td>
  </tr>
  
  <tr>
    <td>
      handsupdontshoot
    </td>
    
    <td style="text-align: right;">
      777
    </td>
  </tr>
</table>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8157</wp:post_id>
		<wp:post_date>2014-09-29 06:58:16</wp:post_date>
		<wp:post_date_gmt>2014-09-29 13:58:16</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>hong-kong-tags</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="hong-kong"><![CDATA[hong kong]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Social Machines and the Archive</title>
		<link>http://inkdroid.org/2014/10/02/social-machines-and-the-archive/</link>
		<pubDate>Thu, 02 Oct 2014 15:56:06 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8167</guid>
		<description></description>
		<content:encoded><![CDATA[Yesterday MIT announced that Twitter made a 5 million dollar investment to help them create a <a href="http://socialmachines.media.mit.edu/">Laboratory for Social Machines (LSM)</a> as part of the <a href="http://www.media.mit.edu/">MIT Media Lab</a> proper:

<blockquote class="twitter-tweet" lang="en"><p>MIT launches Laboratory for Social Machines with major Twitter investment <a href="http://t.co/HPupRkTWAm">http://t.co/HPupRkTWAm</a> <a href="https://twitter.com/mitlsm">@MITLSM</a> <a href="https://twitter.com/dkroy">@dkroy</a></p>&mdash; MIT Media Lab (@medialab) <a href="https://twitter.com/medialab/status/517361268580429824">October 1, 2014</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

It seems like an important move for MIT to formally recognize that social media is a new medium that deserves its own research focus, and investment in infrastructure. The language on the homepage gives a nice flavor for the type of work they plan to be doing. I was particularly struck by their frank assessment of how our governance systems are failing us, and social media's potential role in understanding and helping solve the problems we face:

<blockquote>In a time of growing political polarization and institutional distrust, social networks have the potential to remake the public sphere as a realm where institutions and individuals can come together to understand, debate and act on societal problems. To date, large-scale, decentralized digital networks have been better at disrupting old hierarchies than constructing new, sustainable systems to replace them. Existing tools and practices for understanding and harnessing this emerging media ecosystem are being outstripped by its rapid evolution and complexity.</blockquote>

Their notion of "social machines" as "networked human-machine collaboratives" reminds me a lot of my somewhat stumbling work on <a href="http://inkdroid.org/2014/07/10/why-congressedits/">@congressedits</a> and <a href="http://inkdroid.org/journal/2014/08/30/a-ferguson-twitter-archive/">archiving Ferguson Twitter data</a>. As <a href="http://www.nickdiakopoulos.com/2014/07/16/diversity-in-the-robot-reporter-newsroom/">Nick Diakopoulos</a> has pointed out we really need a theoretical framework for thinking about what sorts of interactions these automated social media agents can participate in, formulating their objectives, and for measuring their effects. <em>Full disclosure: I work with Nick at the University of Maryland, but he wrote that post mentioning me before we met here, which was kind of awesome to discover after the fact.</em>

Some of the <a href="http://blogs.wsj.com/digits/2014/10/01/twitter-mit-create-new-research-lab-to-analyze-social-media/">news stories</a> about the Twitter/MIT announcement have included this quote from <a href="https://twitter.com/dkroy">Deb Roy</a> from MIT who will lead the LSM:

<blockquote>The Laboratory for Social Machines will experiment in areas of public communication and social organization where humans and machines collaborate on problems that can’t be solved manually or through automation alone.</blockquote>

What a lovely encapsulation of the situation we find ourselves in today, where the problems we face are localized and yet global. Where algorithms and automation are indispensable for analysis and data gathering, but people and collaborative processes are all the more important. The ethical dimensions to algorithms and our understanding of them is also of growing importance, as the stories we read are mediated more and more by automated agents. It is super that Twitter has decided to help build this space at MIT where people can answer these questions, and have the infrastructure to support asking them.

When I read the quote I was immediately reminded of the problem that some of us were discussing at the last Society of American Archivists meeting in DC: how do we document the protests going on in Ferguson? 

Much of the primary source material was being distributed through Twitter. Internet Archive were <a href="https://docs.google.com/forms/d/1uTyINjaFgLMzizrxqGzGbS8avb5_1xJJ2ostRnhvxXo/viewform">looking for</a> nominations of URLs to use in their web crawl. But weren't all the people tweeting about Ferguson including URLs for stories, audio and video that were of value? If people are talking about something can we infer its value in an archive? Or rather, is it a valuable place to start inferring from?

I ended up <a href="http://inkdroid.org/2014/08/30/a-ferguson-twitter-archive/">archiving</a> 13 million of the tweets that mention "ferguson" for the 2 week period after the killing of Michael Brown. I then went through the URLs in these tweets, and <a href="http://github.com/edsu/unshrtn">unshortened</a> them and came up with a list of 417,972 unshortened URLs. You can see the top 50 of them <a href="https://edsu.github.io/ferguson-urls/">here</a>, and the top 50 for August 10th (the day after Michael Brown was killed) <a href="https://edsu.github.io/ferguson-urls/day1.html">here</a>.

I did a lot of this work in prototyping mode, writing quick one off scripts to do this and that. One nice unintended side effect was <a href="http://github.com/edsu/unshrtn">unshrtn</a> which is a microservice for unshortening URLs, which <a href="http://www.cdlib.org/contact/staff_directory/jkunze.html">John Kunze</a> gave me the idea for years ago. It gets a bit harder when you are unshortening millions of URLs. 

But what would a tool look like that let us analyze events in social media, and helped us (archivists) collect information that needs to be preserved for future use? These tools are no doubt being created by those in positions of power, but we <a href="http://www2.archivists.org/statements/saa-core-values-statement-and-code-of-ethics">need</a> them for the archive as well. We also desperately need to explore what it means to explore these archives: how do we provide access to them, and share them? It feels like there could be a project here along the lines of what George Washington University University are doing with their <a href="http://social-feed-manager.readthedocs.org/">Social Feed Manager</a>. <em>Full disclosure again: I've done some contracting work with the fine folks at GW on a <a href="http://findit.library.gwu.edu/catalog/">new interface</a> to their library catalog.</em>

The 5 million dollars aside, an important contribution that Twitter is making here (that's probably worth a whole lot more) is firehose access to the Tweets that are happening now, as well as the historic data. I suspect Deb Roy's <a href="http://dkroy.media.mit.edu/">role</a> at MIT as a professor and as Chief Media Scientist at Twitter helped make that happen. Since MIT has such strong history of supporting open research, it will be interesting to see how the LSM chooses to share data that supports its research.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8167</wp:post_id>
		<wp:post_date>2014-10-02 08:56:06</wp:post_date>
		<wp:post_date_gmt>2014-10-02 15:56:06</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>social-machines-and-the-archive</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="data-curation"><![CDATA[data curation]]></category>
		<category domain="post_tag" nicename="mit"><![CDATA[mit]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_0caf651a55f6fbd362b02f6240ebbdc2</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87200</wp:comment_id>
			<wp:comment_author><![CDATA[Running on Social Machines |]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://karencooperfiles.wordpress.com/2014/11/10/running-on-social-machines/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2014-11-10 08:49:56</wp:comment_date>
			<wp:comment_date_gmt>2014-11-10 15:49:56</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Ed Summers, who frequently discusses archiving social media, supports LSM as a formal recognition of these platforms as a &#8220;new medium that deserves its own research focus, and investment in infrastructure.&#8221;  He relates his own project to collect and store tweets documenting the Ferguson protests. LSM will ideally develop a critical lens to understand &#8220;what sorts of interactions these automated social media agents can participate in, formulating their objectives, and for measuring their effects.&#8221; [&#8230;]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1415634596.4382669925689697265625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1415642804.190061092376708984375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Sign o&#039; the Times </title>
		<link>http://inkdroid.org/2014/10/05/sign-o-the-times/</link>
		<pubDate>Sun, 05 Oct 2014 17:42:25 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8193</guid>
		<description></description>
		<content:encoded><![CDATA[<br>
<br>
<br><figure style="text-align: center;">
  <a href="https://secure.flickr.com/photos/maisonbisson/15261784538"><img src="http://inkdroid.org/images/sign.jpg" alt="a sign, a metaphor"></a>
  <figcaption>
    <a href="https://secure.flickr.com/photos/maisonbisson/15261784538">a sign, a metaphor</a> by <a href="https://twitter.com/misterbisson">Casey Bisson</a>.
  </figcaption>
</figure>

An old acquaintance took this photo in <a href="https://en.wikipedia.org/wiki/Coaldale,_Nevada">Coaldale, Nevada</a>. I had to have a copy for myself.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8193</wp:post_id>
		<wp:post_date>2014-10-05 10:42:25</wp:post_date>
		<wp:post_date_gmt>2014-10-05 17:42:25</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>sign-o-the-times</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="art"><![CDATA[art]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>On Forgetting</title>
		<link>http://inkdroid.org/2014/11/18/on-forgetting/</link>
		<pubDate>Tue, 18 Nov 2014 21:17:45 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8202</guid>
		<description></description>
		<content:encoded><![CDATA[After writing about the <a href="http://inkdroid.org/2014/08/30/a-ferguson-twitter-archive/">Ferguson Twitter</a> archive a few months ago three people have emailed me out of the blue asking for access to the data. One was a principal at a small, scaryish defense contracting company, and the other two were from a prestigious university. I've also had a handful of people interested where I work at the University of Maryland.

I ignored the defense contractor. Maybe that was mean, but I don't want to be part of that. I'm sure they can go buy the data if they really need it. My response to the external academic researchers wasn't much more helpful since I mostly pointed them to  <a href="https://dev.twitter.com/overview/terms/policy#6._Be_a_Good_Partner_to_Twitter">Twitter's Terms of Service</a> which says:

<blockquote>
If you provide Content to third parties, including downloadable datasets of Content or an API that returns Content, you will only distribute or allow download of Tweet IDs and/or User IDs.

You may, however, provide export via non-automated means (e.g., download of spreadsheets or PDF files, or use of a “save as” button) of up to 50,000 public Tweets and/or User Objects per user of your Service, per day. 

Any Content provided to third parties via non-automated file download remains subject to this Policy.
</blockquote>

It's my understanding that I can share the data with others at the University of Maryland, but I am not able to give it to the external parties. What I can do is give them the Tweet IDs. But there are 13,480,000 of them.

So that's what I'm doing today: publishing the tweet ids. You can download them from the Internet Archive:

<blockquote>
<a href="https://archive.org/details/ferguson-tweet-ids">https://archive.org/details/ferguson-tweet-ids</a>
</blockquote>

I'm making it available using the CC-BY license. 

<h3>Hydration</h3>

On the one hand, it seems unfair that this portion of the public record is unshareable in its most information rich form. The barrier to entry to using the data seems set artificially high in order to protect Twitter's business interests. These messages were posted to the public Web, where I was able to collect them. Why are we prevented from re-publishing them since they are already on the Web? Why can't we have lots of copies to keep stuff safe? More on this in a moment.

Twitter limits users to 180 requests every 15 minutes. A user is effectively a unique access token. Each request can hydrate up to 100 Tweet IDs using the <a href="https://dev.twitter.com/rest/reference/get/statuses/lookup">statuses/lookup</a> REST API call. 

<pre>
180 requests * 100 tweets = 18,000 tweets/15 min 
                          = 72,000 tweets/hour
</pre>

So to hydrate all of the 13,480,000 tweets will take about 7.8 days. This is a bit of a pain, but realistically it's not so bad. I'm sure people doing research have plenty of work to do before running any kind of analysis on the full data set. And they can use a portion of it for testing as it is downloading. But how do you download it?

<a href="http://gnip.com">Gnip</a>, who were recently acquired by Twitter, offer a <a href="http://support.gnip.com/apis/rehydration_api/">rehydration API</a>. Their API is limited to tweets from the last 30 days, and similar to Twitter's API you can fetch up to 100 tweets at a time. Unlike the Twitter API you can issue a request every second. So this means you could download the results in about 1.5 days. But these Ferguson tweets are more than 30 days old. And a Gnip account costs some indeterminate amount of money, starting at $500...

I suspect there are other hydration services out there. But I adapted <a href="http://github.com/edsu/twarc'>twarc</a> the tool I used to collect the data, which already handled rate-limiting, to also do hydration. Once you have the tweet IDs in a file you just need to install twarc, and run it. Here's how you would do that on an Ubuntu instance: 

<pre>
    <code>
    sudo apt-get install python-pip
    sudo pip install twarc
    twarc.py --hydrate ids.txt > tweets.json
    </code>
</pre>

After a week or so, you'll have the full JSON for each of the tweets.

<h3>Archive Fever</h3>

Well, not really. You will have <em>most</em> of them. But you won't have the ones that have been deleted. If a user decided to remove a Tweet they made, or decided to remove their account entirely you won't be able to get their Tweets back from Twitter using their API. I think it's interesting to consider Twitter's Terms of Service as what <a href="http://ischool.umd.edu/faculty-staff/katie-shilton">Katie Shilton</a> would call a <a href="http://mith.umd.edu/podcasts/katie-shilton-finding-values-levers-building-ethics-into-emerging-technologies/">value lever</a>.

The metadata rich JSON data (which often includes geolocation and other behavioral data) wasn't exactly posted to the Web in the typical way. It was made available through a Web API designed to be used directly by automated agents, not people. Sure, a tweet appears on the Web but it's in with the other half a trillion Tweets out on the Web, all the way back to the <a href="https://twitter.com/biz/status/21">first one</a>. Requiring researchers to go back to the Twitter API to get this data and not allowing it circulate freely in bulk means that users have an opportunity to remove their content. Sure it has already been collected by other people, and it's pretty unlikely that the NSA are deleting their tweets. But in a way Twitter is taking an ethical position for their publishers to be able to remove their data. To exercise their right to be forgotten. Removing a teensy bit of <a href="http://idlewords.com/bt14.htm">informational toxic waste</a>.

As any archivist will tell you, forgetting is an essential and unavoidable part of the archive. Forgetting is the <em>why</em> of an archive. Negotiating what is to be remembered and by whom is the principal concern of the archive. Ironically it seems it's the people who deserve it the least, those in positions of power, who are often most able to exercise their right to be forgotten. Maybe putting a value lever back in the hands of the people isn't such a bad thing. If I were Twitter I'd highlight this in the API documentation. I think we are still learning how the contours of the Web fit into the archive. I know I am.

<em>If you are interested in learning more about value levers you can download a pre-print of Shilton's <a href="http://terpconnect.umd.edu/~kshilton/ShiltonSTHVpreprint.pdf">Value Levers: Building Ethics into Design</a>.</em>





]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8202</wp:post_id>
		<wp:post_date>2014-11-18 14:17:45</wp:post_date>
		<wp:post_date_gmt>2014-11-18 21:17:45</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-forgetting</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="ferguson"><![CDATA[ferguson]]></category>
		<category domain="post_tag" nicename="right-to-be-forgotten"><![CDATA[right to be forgotten]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_c9815e856d096043ec231d5ada6d7487</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87318</wp:comment_id>
			<wp:comment_author><![CDATA[Tweets and Deletes | inkdroid]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://inkdroid.org/2015/04/14/tweets-and-deletes/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-04-14 09:08:09</wp:comment_date>
			<wp:comment_date_gmt>2015-04-14 16:08:09</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This Article was mentioned on <a href="http://inkdroid.org/2015/04/14/tweets-and-deletes/" rel="nofollow">inkdroid.org</a>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>webmention</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1429884347.2404739856719970703125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1429027690.0372219085693359375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Inter-face</title>
		<link>http://inkdroid.org/2014/12/01/inter-face/</link>
		<pubDate>Tue, 02 Dec 2014 01:49:30 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8253</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="https://www.flickr.com/photos/internetarchivebookimages/14779113111/"><img src="http://inkdroid.org/images/radiant.jpg"></a>
<cite><a href="https://www.flickr.com/photos/internetarchivebookimages/14779113111/">Image from page 315</a> of "The elements of astronomy; a textbook" (1919)</cite>

<blockquote>
Every <a href="https://en.wikipedia.org/wiki/Document">document</a>, every moment in every document, conceals (or reveals) an indeterminate set of <a href="https://en.wikipedia.org/wiki/Interface_(computing)">interfaces</a> that open into alternate spaces and temporal relations.

Traditional criticism will engage this kind of radiant textuality more as a problem of context than a problem of text, and we have no reason to fault that way of seeing the matter. But as the word itself suggests, "context" is a cognate of text, and not in any abstract <a href="https://en.wikipedia.org/wiki/Roland_Barthes">Barthesian</a> sense. We construct the poem's context, for example, by searching out the meanings marked in the <a href="https://en.wikipedia.org/wiki/Textual_criticism">physical witnesses</a> that bring the poem to us. We read those witnesses with <a href="https://en.wikipedia.org/wiki/Scrupulosity">scrupulous</a> attention, that is to say, we make our detailed way <a href="https://en.wikipedia.org/wiki/Through_the_Looking-Glass">through the looking glass</a> of the book and thence to the endless reaches of the <a href="https://en.wikipedia.org/wiki/The_Library_of_Babel">Library of Babel</a>, where every text is catalogued and multiple cross-referenced. In making the journey we are driven far out into the <a href="https://en.wikipedia.org/wiki/Extragalactic_astronomy">deep space</a>, as we say these days, occupied by our <a href="https://en.wikipedia.org/wiki/Orbit">orbiting</a> texts. There objects pivot about many different points and poles, the objects themselves <a href="https://en.wikipedia.org/wiki/Shapeshifting">shapeshift</a> continually and the pivots move, drift, <a href="https://en.wikipedia.org/wiki/Shivering">shiver</a>, and even <a href="https://en.wikipedia.org/wiki/Dissolution_(chemistry)">dissolve</a> away. Those transformations occur because "the text" is always a negotiated text, half perceived and half created by those who engage with it.

<cite>Radiant Textuality by <a href="https://en.wikipedia.org/wiki/Jerome_McGann">Jerome McGann</a></cite>
</blockqoute>

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8253</wp:post_id>
		<wp:post_date>2014-12-01 18:49:30</wp:post_date>
		<wp:post_date_gmt>2014-12-02 01:49:30</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>inter-face</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>An Invitation to Study</title>
		<link>http://inkdroid.org/2014/12/03/an-invitation-to-study/</link>
		<pubDate>Wed, 03 Dec 2014 16:36:23 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8267</guid>
		<description></description>
		<content:encoded><![CDATA[<em>These are some brief remarks I prepared for a 5 minute lightning talk at the <a href="http://www.bsos.umd.edu/featuredstory/1219">Ferguson Town Hall meeting</a> at UMD on December 3, 2014.</em>

Thank you for the opportunity to speak here this evening. It is a real privilege. I'd like to tell you a little bit about an <a href="http://mith.umd.edu/miths-ed-summers-discusses-ferguson-twitter-archive/">archive</a> of 13 million Ferguson related tweets we've assembled here at the University of Maryland. You can see a random sampling of some of them up on the screen <a href="https://edsu.github.io/ferguson-tweet-viewer/">here</a>. The 140 characters of text in a tweet only makes up about 2% of the data for each tweet. The other 98% includes metadata such as who sent it, their profile, how many followers they have, what tweet they are replying to, who they are retweeting, when the tweet was sent, (sometimes) where the tweet was sent from, embedded images and video. I'm hoping that I can interest some of you in studying this data.

I intentionally used the word privilege in my opening sentence to recognize that my ethnicity and my gender enabled me to be here speaking to you this evening. I'd actually like to talk very quickly about about a different set of privileges I have: those of my profession, and as a member of the UMD academic community that we are all a part of:

In a <a href="http://www.democracynow.org/blog/2014/8/22/black_life_is_treated_with_short">Democracy Now interview</a> back in August, hip-hop artist and activist <a href="https://en.wikipedia.org/wiki/Talib_Kweli">Talib Kweli</a> characterized the Ferguson related activity on Twitter in a deeply insightful way:

<blockquote>
I remember a world before the Internet. And I remember what it really takes to have movement on the ground. Someone tweeted me back and said, "Well, you know, back in the day they didn’t have Twitter, but they had letters, and they wrote letters to each other, so..." I said, "Yeah, but ain’t nobody saying that the letters started the revolution."

When I look at the Green Revolution, when I look what happened to Egypt, when I look at what happened to Occupy Wall Street, yeah, the tweets helped—they helped a lot—but without those bodies in the street, without the people actually being there, ain’t nothing to tweet about. If Twitter worked like that, Joseph Kony would be locked down in a jail right now. 
</blockquote>

<iframe width="560" height="315" src="//www.youtube.com/embed/S0JMd6uz4JQ#t=7m30s" frameborder="0" allowfullscreen></iframe>

Of course, Talib is right. It's why we are all here this evening. In some sense it doesn't matter what happens on Twitter. What matters is what happened in Ferguson, what is happening in Ferguson, and in meetings and demonstrations like this one all around the country.

Talib's comparison of a tweet to a letter struck me as particularly insightful. I work as an archivist and software developer in the <a href="http://mith.umd.edu">Maryland Institute for Technology in the Humanities</a> here at UMD. Humanities scholars have traditionally studied a particular set of historical materials, of which letters are one. These materials form the heart of what we call the archive. What gets collected in archives and studied is inevitably what forms our cultural canon. It is a site of controversy, for as George Orwell wrote in <a href="https://en.wikiquote.org/wiki/Nineteen_Eighty-Four">1984</a>:

<blockquote>
Who controls the past controls the future. Who controls the present controls the past.
</blockquote>

Would we be here tonight if it wasn't for Twitter? Would the President be talking about Ferguson if it wasn't for the groundswell of activity on Twitter? Without Twitter what would the main stream media have reported about Mike Brown, John Crawford, Eric Garner, Renisha McBride, Trayvon Martin, and Tamir Rice? As you know this list of injustices is long...it is vast and overwhelming. It extends back to the beginnings of this state and this country. But what trace do we have of these injustices and these struggles in our archives?

The famed historian and social activist <a href="https://en.wikipedia.org/wiki/Howard_Zinn">Howard Zinn</a> said <a href="http://minds.wisconsin.edu/handle/1793/44118">this</a> when addressing a group of archivists in 1970:

<blockquote>
the existence, preservation and availability of archives, documents, records in our society are very much determined by the distribution of wealth and power. That is, the most powerful, the richest elements in society have the greatest capacity to find documents, preserve them, and decide what is or is not available to the public. This means government, business and the military are dominant.
</blockquote>

This is where social media and the Web present such a profoundly new opportunity for us, as we struggle to understand what happened in Ferguson...as we struggle to understand how best to act in the present. We need to work to make sure the voices of Ferguson are available for study--and not just in the future, but study now. Let's put our privilege as members of this academic community to work. Is there something to learn in these 13 million tweets, these letters from ordinary people. The thousands of videos and photographs, and links to stories? I think there is. I'm hopeful that these digital traces provide us with a new insight into an old problem...insights that can guide our actions here in the present.

If you have questions you'd like to ask of the data please get in touch with either me or <a href="http://mith.umd.edu/people/person/neil-fraistat/">Neil Fraistat</a> (Director of MITH) here tonight, or via <a href="mail:ehs@pobox.com">email</a> or <a href="http://twitter.com/edsu">Twitter</a>.






]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8267</wp:post_id>
		<wp:post_date>2014-12-03 09:36:23</wp:post_date>
		<wp:post_date_gmt>2014-12-03 16:36:23</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>an-invitation-to-study</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="ferguson"><![CDATA[ferguson]]></category>
		<category domain="category" nicename="politics"><![CDATA[politics]]></category>
		<category domain="post_tag" nicename="umd"><![CDATA[umd]]></category>
		<wp:postmeta>
			<wp:meta_key>_oembed_f8c6837ae6fb99a234f127eed6086055</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_32426f97f4d1a1a6436359a0efce398b</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87256</wp:comment_id>
			<wp:comment_author><![CDATA[#c4l15 talk extras]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://andromedayelton.com/blog/2015/02/12/c4l15-talk-extras/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-02-15 07:23:00</wp:comment_date>
			<wp:comment_date_gmt>2015-02-15 14:23:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] http://inkdroid.org/2014/12/03/an-invitation-to-study/ [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1424011359.1292130947113037109375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1424010180.4495689868927001953125;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Removing Bias</title>
		<link>http://inkdroid.org/2014/12/09/removing-bias/</link>
		<pubDate>Wed, 10 Dec 2014 01:00:42 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8315</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote class="twitter-tweet" lang="en"><p>Senate Intelligence Committee report on CIA torture Wikipedia article edited anonymously from US Senate <a href="http://t.co/Bj4q8Naed1">http://t.co/Bj4q8Naed1</a></p>&mdash; congress-edits (@congressedits) <a href="https://twitter.com/congressedits/status/542441545002004481">December 9, 2014</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8315</wp:post_id>
		<wp:post_date>2014-12-09 18:00:42</wp:post_date>
		<wp:post_date_gmt>2014-12-10 01:00:42</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>removing-bias</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="congress"><![CDATA[congress]]></category>
		<category domain="post_tag" nicename="congressedits"><![CDATA[congressedits]]></category>
		<category domain="post_tag" nicename="torture"><![CDATA[torture]]></category>
		<category domain="category" nicename="wikipedia"><![CDATA[wikipedia]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Languages on Twitter.</title>
		<link>http://inkdroid.org/2014/12/15/languages-on-twitter/</link>
		<pubDate>Mon, 15 Dec 2014 16:12:58 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8318</guid>
		<description></description>
		<content:encoded><![CDATA[There have been some interesting visualizations of languages in use on Twitter, like <a href="http://bits.blogs.nytimes.com/2014/03/09/the-languages-of-twitter-users/">this one</a> done by Gnip and published in the New York Times. Recently I've been involved in some research on particular a topical collection of tweets. One angle that's been particularly relevant for this dataset is language. When perusing some of the tweet data we retrieved from Twitter's API we noticed that there were two <code>lang</code> properties in the JSON. One was attached to the embedded user profile stanza, and the other was a top level property of the tweet itself.

We presumed that the user profile language was the language the user (who submitted the tweet) had selected, and that the second language on the tweet was the language of the tweet itself. The first is what Gnip used in its visualization. Interestingly, Twitter's own <a href="https://dev.twitter.com/rest/reference/get/statuses/show/%3Aid">documentation</a> for the <code>/get/statuses/:id</code> API call only shows the user profile language. 

When you send a tweet you don't indicate what language it is in. For example you might indicate in your profile that you speak primarily English, but send some tweets in French. I can only imagine that detecting language for each tweet isn't a cheap operation for the scale that Twitter operates at. Milliseconds count when you are sending 500 million tweets a day, in real time. So at the time I was skeptical that we were right...but I added a mental note to do a little experiment.

This morning I noticed my friend <a href="http://twitter.com/dchud">Dan</a> had posted a tweet in Hebrew, and figured now was as a good a time as any.

<blockquote class="twitter-tweet" lang="en"><p>????? ?????</p>&mdash; Dan Chudnov (@dchud) <a href="https://twitter.com/dchud/status/540623422469185537">December 4, 2014</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

I downloaded the JSON for the Tweet from the Twitter API and sure enough, the user profile had language <code>en</code> and the tweet itself had language <code>iw</code> which is the deprecated ISO 639-1 code for Hebrew (current is <code>he</code>. Here's the raw JSON for the tweet, search for <code>lang</code>:

<pre lang="javascript">
{
  "contributors": null,
  "truncated": false,
  "text": "\u05d0\u05e0\u05d7\u05e0\u05d5 \u05e0\u05ea\u05d2\u05d1\u05e8",
  "in_reply_to_status_id": null,
  "id": 540623422469185537,
  "favorite_count": 2,
  "source": "<a href=\"http://tapbots.com/software/tweetbot/mac\" rel=\"nofollow\">Tweetbot for Mac</a>",
  "retweeted": false,
  "coordinates": null,
  "entities": {
    "symbols": [],
    "user_mentions": [],
    "hashtags": [],
    "urls": []
  },
  "in_reply_to_screen_name": null,
  "id_str": "540623422469185537",
  "retweet_count": 0,
  "in_reply_to_user_id": null,
  "favorited": true,
  "user": {
    "follow_request_sent": false,
    "profile_use_background_image": true,
    "profile_text_color": "333333",
    "default_profile_image": false,
    "id": 17981917,
    "profile_background_image_url_https": "https://pbs.twimg.com/profile_background_images/3725850/woods.jpg",
    "verified": false,
    "profile_location": null,
    "profile_image_url_https": "https://pbs.twimg.com/profile_images/524709964905218048/-CuYZQQY_normal.jpeg",
    "profile_sidebar_fill_color": "DDFFCC",
    "entities": {
      "description": {
        "urls": []
      }
    },
    "followers_count": 1841,
    "profile_sidebar_border_color": "BDDCAD",
    "id_str": "17981917",
    "profile_background_color": "9AE4E8",
    "listed_count": 179,
    "is_translation_enabled": false,
    "utc_offset": -18000,
    "statuses_count": 14852,
    "description": "",
    "friends_count": 670,
    "location": "Washington DC",
    "profile_link_color": "0084B4",
    "profile_image_url": "http://pbs.twimg.com/profile_images/524709964905218048/-CuYZQQY_normal.jpeg",
    "following": true,
    "geo_enabled": false,
    "profile_banner_url": "https://pbs.twimg.com/profile_banners/17981917/1354047961",
    "profile_background_image_url": "http://pbs.twimg.com/profile_background_images/3725850/woods.jpg",
    "name": "Dan Chudnov",
    "lang": "en",
    "profile_background_tile": true,
    "favourites_count": 1212,
    "screen_name": "dchud",
    "notifications": false,
    "url": null,
    "created_at": "Tue Dec 09 02:56:15 +0000 2008",
    "contributors_enabled": false,
    "time_zone": "Eastern Time (US & Canada)",
    "protected": false,
    "default_profile": false,
    "is_translator": false
  },
  "geo": null,
  "in_reply_to_user_id_str": null,
  "lang": "iw",
  "created_at": "Thu Dec 04 21:47:22 +0000 2014",
  "in_reply_to_status_id_str": null,
  "place": null
}
</pre>

Although tweets are short they certainly can contain multiple languages. I was curious what would happen if I tweeted two words, one in English and one in French.

<blockquote class="twitter-tweet" lang="en"><p>testing, essai</p>&mdash; Ed Summers (@edsu) <a href="https://twitter.com/edsu/status/544516240665227264">December 15, 2014</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

When I fetched the JSON data for this tweet the language of the tweet was indicated to be <code>pt</code> or Portuguese! As far as I know neither <em>testing</em> nor <em>essai</em> are Portuguese.  

This made me think perhaps the tweet was a bit short so I tried something a bit longer, with the number of words in each language being equal.

<blockquote class="twitter-tweet" lang="en"><p>Désolé for le noise, je suis just seeing how détection de la language works.</p>&mdash; Ed Summers (@edsu) <a href="https://twitter.com/edsu/status/544518525109698561">December 15, 2014</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

This one came across with lang <code>fr</code>. So having the text be a bit longer helped in this case. Admittedly this isn't a very sound experiment, but it seems interesting and useful to see that Twitter is detecting language in tweets. It isn't perfect, but that shouldn't be surprising at all given the nature of human language. It might be useful to try a more exhaustive test using a more complete list of languages to see how it fairs. I'm adding another mental note...

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8318</wp:post_id>
		<wp:post_date>2014-12-15 09:12:58</wp:post_date>
		<wp:post_date_gmt>2014-12-15 16:12:58</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>languages-on-twitter</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="languages"><![CDATA[languages]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Bowie</title>
		<link>http://inkdroid.org/2015/01/12/bowie/</link>
		<pubDate>Mon, 12 Jan 2015 17:19:31 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8343</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://www.goodreads.com/book/show/21839107-bowie" style="float: left; padding-right: 20px"><img alt="Bowie" border="0" src="https://d.gr-assets.com/books/1404315685m/21839107.jpg" /></a><a href="https://www.goodreads.com/book/show/21839107-bowie">Bowie</a> by <a href="https://www.goodreads.com/author/show/57674.Simon_Critchley">Simon Critchley</a><br />
My rating: <a href="https://www.goodreads.com/review/show/1165872948">5 of 5 stars</a></p>

<p>If you are a Bowie fan, you will definitely enjoy this. If you are curious why other people are so into Bowie you will enjoy this. If you've never read any <a href="https://en.wikipedia.org/wiki/Simon_Critchley">Critchley</a> and are interested in something quick and accessible by him you will enjoy this. I fell into the first and third categories so I guess I'm guessing about the second. But I suspect it's true.</p>

<p>I finished the book feeling like I understand the why and how of my own fascination with Bowie's work much better. I also want to revisit some of his albums like Diamond Dogs, Heathen and Outside which I didn't quite connect with at first. I would've enjoyed a continued discussion of Bowie's use of the cutup technique, but I guess that fell out of the scope of the book.</p>

<p>I also want to read some more Critchley too -- so if you have any recommendations please let me know. The sketches at the beginning of each chapter are wonderful. <a href="http://www.orbooks.com/" rel="nofollow">OR Books</a> continues to impress.</p>

<p><a href="http://dollychops.tumblr.com/post/107517113745/happy-birthday-david-bowie"><img src="http://inkdroid.org/images/bowie.gif"></a></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8343</wp:post_id>
		<wp:post_date>2015-01-12 10:19:31</wp:post_date>
		<wp:post_date_gmt>2015-01-12 17:19:31</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>bowie</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="book-review"><![CDATA[book review]]></category>
		<category domain="category" nicename="music"><![CDATA[music]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>When Google Met WikiLeaks</title>
		<link>http://inkdroid.org/2015/01/13/when-google-met-wikileaks/</link>
		<pubDate>Tue, 13 Jan 2015 19:59:55 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8349</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://www.goodreads.com/book/show/21839908-when-google-met-wikileaks" style="float: left; padding-right: 20px;"><img style="border: thin solid gray;" alt="When Google Met Wikileaks" border="0" src="https://d.gr-assets.com/books/1399646585m/21839908.jpg" /></a><a href="https://www.goodreads.com/book/show/21839908-when-google-met-wikileaks">When Google Met Wikileaks</a> by <a href="https://www.goodreads.com/author/show/4553283.Julian_Assange">Julian Assange</a><br />
My rating: <a href="https://www.goodreads.com/review/show/1167040098">4 of 5 stars</a></p>

<p>This book is primarily the transcript of a conversation between <a href="https://en.wikipedia.org/wiki/Julian_Assange">Julian Assange</a> and <a href="https://en.wikipedia.org/wiki/Eric_Schmidt">Eric Schmidt</a> (then CEO of Google) and <a href="https://en.wikipedia.org/wiki/Jared_Cohen" rel="nofollow">Jared Cohen</a> for their book <a href="http://www.amazon.com/The-New-Digital-Age-Transforming-ebook/dp/B00ALBR2N6" rel="nofollow">The New Digital Age</a>. The transcript is also <a href="https://wikileaks.org/Transcript-Meeting-Assange-Schmidt.html" rel="nofollow">available</a> in its entirety (fittingly) on the WikiLeaks website along with the actual audio of the conversation. The transcript is book-ended by several essays: Beyond Good and "Don't Be Evil", the Banality of "Don't Be Evil" (also published in <a href="http://www.nytimes.com/2013/06/02/opinion/sunday/the-banality-of-googles-dont-be-evil.html?pagewanted=all&_r=0" rel="nofollow">New York Times</a>) and Deliver us from "Don't Be Evil".</p>

<p>Assange read The New Digital Age and wasn't happy with the framing of the conversation, or the degree to which his interview wasn't included. When Google Met WikiLeaks is Assange's attempt to reframe the discussion in terms of the future of publishing, information and the Internet. In particular Assange takes issue with Schmidt and Cohen's assertion that:</p>

<blockquote>
  <p>The information released on WikiLeaks put lives at risk and inflicted serious diplomatic damage.</p>
</blockquote>

<p>Schmidt and Cohen offer no source for this bold assertion, and in a note they equate WikiLeaks with minimally enabling espionage, again with no citation. Assange makes the case that WikiLeaks is actually in the business of publishing and journalism, not secretly selling information for private gain. I think Assange does this, but more importantly, he presents a view of the near future of the Internet, that is presaged by WikiLeaks, which is actually interesting and compelling. The transcript itself is heavily annotated with footnotes, many of which have URLs, that are archived at <a href="http://archive.today/" rel="nofollow">archive.today</a>.</p>

<p>For me the most interesting parts of the book center on what Assange calls the Naming of Things:</p>

<blockquote>
  <p>The naming of human intellectual work and our entire intellectual record is possibly the most important thing. So we all have words for different objects, like "tomato." But we use a simple word, "tomato," instead of actually describing every little aspect of this god damn tomato...because it takes too long. And because it takes too long to describe this tomato precisely we use an abstraction so we can think about it so we can talk about it. And we do that also when we use URLs. Those are frequently used as a short name for some human intellectual content. And we build all of our civilization, other than on bricks, on human intellectual content. And so we currently have system with URLs where the structure we are building our civilization out of is the worst kind of melting plasticine imaginable. And that is a big problem.<br />
  <br />
  <cite><br /><a href="https://wikileaks.org/Transcript-Meeting-Assange-Schmidt.html#440" rel="nofollow">Transcript of secret meeting between Julian Assange and Google CEO Eric Schmidt</a><br /></cite></p>
</blockquote>

<p>This particular section goes on to talk about some really interesting topics: such as the effects of <a href="https://en.wikipedia.org/wiki/Right_to_be_forgotten" rel="nofollow">right to be forgotten</a> laws, DNS, Bittorrent <a href="https://en.wikipedia.org/wiki/Magnet_URI_scheme" rel="nofollow">magnet URIs</a>, how not to pick ISPs, hashing algorithms, digital signatures, public key cryptography, <a href="https://en.wikipedia.org/wiki/Bitcoin" rel="nofollow">Bitcoin</a>, <a href="https://en.wikipedia.org/wiki/Namecoin" rel="nofollow">NameCoin</a>, <a href="https://en.wikipedia.org/wiki/Flooding_(computer_networking)" rel="nofollow">flood networks</a>, and <a href="https://en.wikipedia.org/wiki/Distributed_hash_table" rel="nofollow">distributed hash tables</a>. The fascinating thing is that Schmidt is asking Assange for these details to understand how WikiLeaks operates; but Assange's response is to discuss some general technologies that may influence a new kind of Web of documents. A Web where identity matters, where documents are signed and mirrored, republished and resilient.</p>

<p>Assange has been largely demonized by the mainstream press, and this book humanizes him quite a bit. It's hard not to think of him in the Ecuadorian Embassy in London (where he will have been for 1500 days tomorrow) quietly adding footnotes to the transcript, and archiving web content.</p>

<p>OR Books role in printing this content on paper, for bookshelves everywhere is another aspect to this process of replication. Hats off to them for putting this project together.</p>

<p>Here's some musical accompaniment to go along with this post:</p>

<iframe width="420" height="315" src="//www.youtube.com/embed/E48B0NwTUjQ" frameborder="0" allowfullscreen></iframe>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8349</wp:post_id>
		<wp:post_date>2015-01-13 12:59:55</wp:post_date>
		<wp:post_date_gmt>2015-01-13 19:59:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>when-google-met-wikileaks</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="book-review"><![CDATA[book review]]></category>
		<category domain="post_tag" nicename="google"><![CDATA[google]]></category>
		<category domain="category" nicename="publishing"><![CDATA[publishing]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<category domain="post_tag" nicename="wikileaks"><![CDATA[wikileaks]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Library of Alexandria v2.0</title>
		<link>http://inkdroid.org/2015/01/23/library-of-alexandria-v2-0/</link>
		<pubDate>Fri, 23 Jan 2015 19:16:49 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8367</guid>
		<description></description>
		<content:encoded><![CDATA[<p>In case you missed <a href="https://en.wikipedia.org/wiki/Jill_Lepore">Jill Lepore</a> has written a superb <a href="http://www.newyorker.com/magazine/2015/01/26/cobweb">article</a> for the New Yorker about the <a href="https://en.wikipedia.org/wiki/Internet_Archive">Internet Archive </a>and archiving the Web in general. The story of the Internet Archive is largely the story of its creator <a href="https://en.wikipedia.org/wiki/Brewster_Kahle">Brewster Kahle</a>. If you've heard Kahle speak you've probably heard the <a href="https://en.wikipedia.org/wiki/Destruction_of_the_Library_of_Alexandria">Library of Alexandria</a> v2.0 metaphor before. As a historian Lepore is particularly tuned to this dimension to the story of the Internet Archive:</p>

<blockquote>
  <p>When Kahle started the Internet Archive, in 1996, in his attic, he gave everyone working with him a book called “The Vanished Library,” about the burning of the Library of Alexandria. "The idea is to build the Library of Alexandria Two,” he told me. (The Hellenism goes further: there’s a partial backup of the Internet Archive in Alexandria, Egypt.)</p>
</blockquote>

<p>I'm kind of embarrassed to admit that until reading Lepore's article I never quite understood the metaphor...but now I think I do. The Web is on fire and the Internet Archive is helping save it, one HTTP request and response at a time. Previously I couldn't get the image of this vast collection of Web content that the Internet Archive is building as yet another centralized collection of valuable material that, as with v1.0, is vulnerable to disaster but more likely, as Heather Phillips <a href="http://www.cdlib.org/cdlinfo/2015/01/14/announcing-a-new-partnership-california-digital-library-uc-libraries-and-internet-archives-archive-it-service/">writes</a>, creeping neglect:</p>

<blockquote>
  <p>Though it seems fitting that the destruction of so mythic an institution as the Great Library of Alexandria must have required some cataclysmic event like those described above – and while some of them certainly took their toll on the Library - in reality, the fortunes of the Great Library waxed and waned with those of Alexandria itself. Much of its downfall was gradual, often bureaucratic, and by comparison to our cultural imaginings, somewhat petty.</p>
</blockquote>

<p>I don't think it can be overstated: like the Library of Alexandria before it, the Internet Archive is an amazingly bold and priceless resource for human civilization. I've visited the Internet Archive on multiple occasions, and each time I've been struck by how unlikely it is that such a small and talented team have been able to build and sustain a service with such impact. It's almost as if it's too good to be true. I'm nagged by the thought that perhaps <a href="https://en.wikipedia.org/wiki/Single_point_of_failure">it is</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Herbert_Van_de_Sompel">Herbert van de Sompel</a> is quoted by Lepore:</p>

<blockquote>
  <p>A world with one archive is a really bad idea.</p>
</blockquote>

<p>Van de Sompel and his collaborator <a href="https://twitter.com/phonedude_mln">Michael Nelson</a> have <a href="http://ws-dl.blogspot.com/2013/11/2013-11-21-conservative-party-speeches.html">repeatedly pointed out</a> just how important it is for there to be multiple archives of Web content, and for there to be a <a href="http://www.mementoweb.org/">way</a> for them to be discoverable, and work together. Another thing I learned from Lepore's article is that Brewster's initial vision for the Internet Archive was much more collaborative, which gave birth to the <a href="http://netpreserve.org/">International Internet Preservation Consortium</a>, which is made up of 32 member organizations who do Web archiving.</p>

<p>A couple weeks ago one prominent IIPC member, the <a href="http://www.cdlib.org/">California Digital Library</a> <a href="http://www.cdlib.org/cdlinfo/2015/01/14/announcing-a-new-partnership-california-digital-library-uc-libraries-and-internet-archives-archive-it-service/">announced</a> that it was retiring its in house archiving infrastructure and out sourcing its operation to ArchiveIt, which is the subscription web archiving service from the Internet Archive.</p>

<blockquote>
  <p>The CDL and the UC Libraries are partnering with Internet Archive’s Archive-It Service. In the coming year, CDL’s Web Archiving Service (WAS) collections and all core infrastructure activities, i.e., crawling, indexing, search, display, and storage, will be transferred to Archive-It. The CDL remains committed to web archiving as a fundamental component of its mission to support the acquisition, preservation and dissemination of content. This new partnership will allow the CDL to meet its mission and goals more efficiently and effectively and provide a robust solution for our stakeholders.</p>
</blockquote>

<p>I happened to tweet this at the time:</p>

<blockquote class="twitter-tweet" lang="en">
  <p>
    good news for ArchiveIt and CDL, but probably bad news for web archiving in general <a href="http://t.co/mV3xvqyzi8">http://t.co/mV3xvqyzi8</a>
  </p>— Ed Summers (@edsu) 
  
  <a href="https://twitter.com/edsu/status/555486409952608256">January 14, 2015</a>
</blockquote>

<p>Which at least inspired some <a href="https://twitter.com/textfiles/status/555491909243011072">mirth</a> from Jason Scott, who is an Internet Archive employee, and also a noted Internet historian and documentarian.</p>

<blockquote class="twitter-tweet" data-conversation="none" lang="en">
  <p>
    <a href="https://twitter.com/edsu">@edsu</a> bwa ha ha
  </p>— Jason Scott (@textfiles) 
  
  <a href="https://twitter.com/textfiles/status/555491909243011072">January 14, 2015</a>
</blockquote>

<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> Jason is also well known for his work with <a href="http://archiveteam.org/index.php?title=Main_Page">ArchiveTeam</a>, which quickly mobilizes volunteers to save content on websites that are being shutdown. This content is often then transferred to the Internet Archive. He gets his hands dirty doing the work, and inspires others to do the same. So I deserved a bit of derisive laughter for my hand-wringing.</p>

<p>But here's the thing. What does it mean if one of the pre-eminent digital library organizations needs to outsource their Web archiving operation? And what if, as the announcement indicates, Harvard, MIT, Stanford, UCLA, and others might not be far behind. Should we be concerned that the technical expertise and infrastructure for doing this work is becoming consolidated in a single organization? What does it say about our Web archiving tools that it is more cost-effective for CDL to outsource this work?</p>

<p>The situation isn't as dire as it might sound since ArchiveIt subscribers retain the right to download their content and store it themselves. How many institutions do that with regularity isn't well known (at least to me). But Web content isn't like paper that you can put in a box, in a climate controlled room, and return to years hence. As <a href="https://twitter.com/mkirschenbaum">Matt Kirschenbaum</a> has <a href="http://www.digitalhumanities.org/dhq/vol/7/1/000151/000151.html">pointed out</a>:</p>

<blockquote>
  <p>the preservation of digital objects is logically inseparable from the act of their creation — the lag between creation and preservation collapses completely, since a digital object may only ever be said to be preserved if it is accessible, and each individual access creates the object anew</p>
</blockquote>

<p>Can an organization download their <a href="https://en.wikipedia.org/wiki/Web_ARChive">WARC content</a>, not provide any meaningful access to it, and say that it is being preserved? I don't think so. You can't do digital preservation without thinking about some kind of access to make sure things are working and people can use the stuff. If the content you are accessing is on a platform somewhere else that you have no control over you should probably be concerned.</p>

<p>I'm hopeful that this collaboration between CDL and ArchiveIt, and other organizations, will lead to a fruitful collaboration and improved tools. But I'm worried that it will mean organizations can simply outsource the expertise and infrastructure of web archiving, while helping reinforce what is already a huge single point of failure. David Rosenthal of Stanford University <a href="http://www.dlib.org/dlib/november05/rosenthal/11rosenthal.html">notes</a> that diversity is a vital component to digital preservation:</p>

<blockquote>
  <p>Media, software and hardware must flow through the system over time as they fail or become obsolete, and are replaced. The system must support diversity among its components to avoid monoculture vulnerabilities, to allow for incremental replacement, and to avoid vendor lock-in.</p>
</blockquote>

<p>I'd like to see more Web archiving classes in iSchools and computer science departments. I'd like to see improved and simplified tools for doing the work of Web archiving. Ideally I'd like to see more in house crawling and access of web archives, not less. I'd like to see more organizations like the Internet Archive that are not just technically able to do this work, but are also bold enough to collect what they think is important to save on the Web and make it available. If we can't do this together I think the Library of Alexandria metaphor will be all too literal.</p>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8367</wp:post_id>
		<wp:post_date>2015-01-23 12:16:49</wp:post_date>
		<wp:post_date_gmt>2015-01-23 19:16:49</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>library-of-alexandria-v2-0</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="internet-archive"><![CDATA[internet archive]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87248</wp:comment_id>
			<wp:comment_author><![CDATA[Editors&#8217; Choice: Library of Alexandria v2.0 | Digital Humanities Now]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://digitalhumanitiesnow.org/2015/01/editors-choice-library-of-alexandria-v2-0/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-01-27 10:01:12</wp:comment_date>
			<wp:comment_date_gmt>2015-01-27 17:01:12</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Read More: Ed Summers: Library of Alexandria v2.0 [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1422378073.397129058837890625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1422379103.70407009124755859375;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87334</wp:comment_id>
			<wp:comment_author><![CDATA[Web Archiving in Libraries and Archives (and at Penn State) | 100 Digital Discoveries]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://sites.psu.edu/100digidisc/2015/05/01/web-archiving-in-libraries-and-archives-and-at-penn-state/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-05-01 05:51:06</wp:comment_date>
			<wp:comment_date_gmt>2015-05-01 12:51:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Digital Library&#8217;s recent decision to discontinue its Web Archiving Services (WAS), some have questioned the wisdom of consolidating all that expertise and infrastructure under one organization. The Library of Alexandria analogy is perhaps imprecise, but not by much [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1430488919.2627849578857421875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1430484667.0010340213775634765625;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>A Life Worth Noting</title>
		<link>http://inkdroid.org/2015/01/27/a-life-worth-noting/</link>
		<pubDate>Tue, 27 Jan 2015 14:16:30 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8409</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>There are no obituaries for the war casualties that the United States inflicts, and there cannot be. If there were to be an obituary there would had to have been a life, a life worth noting, a life worth valuing and preserving, a life that qualifies for recognition. Although we might argue that it would be impractical to write obituaries for all those people, or for all people, I think we have to ask, again and again, how the obituary functions as the instrument by which grievability is publicly distributed. It is the means by which a life becomes, or fails to become, a publicly grievable life, an icon for national self-recognition, the means by which a life becomes noteworthy. As a result, we have to consider the obituary as an act of nation-building. The matter is not a simple one, for, if a life is not grievable, it is not quite a life; it does not qualify as a life and is not worth a note. It is already the unburied, if not the unburiable.</p>
  
  <p><a href="http://www.amazon.com/Precarious-Life-Powers-Mourning-Violence/dp/1844675440">Precarious Life</a> by <a href="https://en.wikipedia.org/wiki/Judith_Butler">Judith Butler</a>, (p. 34)</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8409</wp:post_id>
		<wp:post_date>2015-01-27 07:16:30</wp:post_date>
		<wp:post_date_gmt>2015-01-27 14:16:30</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-life-worth-noting</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="death"><![CDATA[death]]></category>
		<category domain="post_tag" nicename="judith-butler"><![CDATA[Judith Butler]]></category>
		<category domain="category" nicename="life"><![CDATA[life]]></category>
		<category domain="post_tag" nicename="obituaries"><![CDATA[obituaries]]></category>
		<category domain="post_tag" nicename="war"><![CDATA[war]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Documenting Ferguson Emails</title>
		<link>http://inkdroid.org/2015/02/01/documenting-ferguson-emails/</link>
		<pubDate>Sun, 01 Feb 2015 12:50:43 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8429</guid>
		<description></description>
		<content:encoded><![CDATA[<p>If you are an <a href="https://ifttt.com/myrecipes/personal">IfThisThenThat</a> user and are interested in archives maybe you'll be interested in <a href="https://ifttt.com/view_embed_recipe/252961-documenting-ferguson-emails">this recipe</a> that will email you when a new item is added to the <a href="http://digital.wustl.edu/ferguson/">Documenting Ferguson</a> repository. Let <a href="http://twitter.com/edsu">me</a> know if you give it a try! I just created the recipe and it hasn't emailed me yet. But the <a href="http://omeka.wustl.edu/omeka/items/browse?collection=62&amp;sort_field=added&amp;sort_dir=d&amp;output=rss2">RSS Feed</a> from Washington University's Omeka instance reports that the last item was added on January 30th, 2015. So the collection is still being added to.</p>

<p><a href="https://ifttt.com/view_embed_recipe/252961-documenting-ferguson-emails" target = "\_blank" class="embed\_recipe embed_recipe-l_27" id= "embed_recipe-252961"><img src= 'https://ifttt.com/recipe_embed_img/252961' alt="IFTTT Recipe: Documenting Ferguson Emails connects feed to email" width="370px" style="max-width:100%"/></a><script async type="text/javascript" src= "//ifttt.com/assets/embed_recipe.js"></script></p>

<p>I thought about having it tweet, but that would involve creating a Twitter account for the project and that isn't my place. Plus, RSS and Email are still fun Web 1.0 technologies that don't get enough love. Well I guess Email predates the Web entirely heh, but you get my drift.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8429</wp:post_id>
		<wp:post_date>2015-02-01 05:50:43</wp:post_date>
		<wp:post_date_gmt>2015-02-01 12:50:43</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>documenting-ferguson-emails</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[documenting-ferguson-email-updates]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wp_old_slug</wp:meta_key>
			<wp:meta_value><![CDATA[email-updates-from-documenting-ferguson]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>#c4l15</title>
		<link>http://inkdroid.org/2015/02/10/c4l15/</link>
		<pubDate>Tue, 10 Feb 2015 16:40:39 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8446</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="http://code4lib.org/conference/2015/">code4lib 2015</a> is about to kick off in Portland this morning. Unfortunately I couldn't make it this year, but I'm looking forward to watching the <a href="https://www.youtube.com/user/code4lib">livestream</a> over the next few days. Thanks so much to the conference organizers for setting up the livestream. The <a href="http://code4lib.org/conference/2015/schedule">schedule</a> has the details about who is speaking when.</p>

<p>As a little gift to real and virtual conference goers (mostly myself) I quickly created <a href="http://inkdroid.org/c4l15-urls/">a little web app</a> that will watch the Twitter stream for #c4l15 tweets, and keep track of which URLs people are talking about. You can see it running, at least while the conference is going <a href="http://inkdroid.org/c4l15-urls/">here</a>.</p>

<p>I've done this sort of thing in an ad hoc way with <a href="http://github.com/edsu/twarc">twarc</a> and some scripts--mostly after (rather than during) an event. For example here's <a href="https://gist.github.com/edsu/c8027057f20dc8c29eb6">a report</a> of URLs mentioned during <a href="https://twitter.com/search?q=%23dlfforum">#dlfforum</a>. But I wanted something a bit more dynamic. As usual the somewhat unkempt code is up on Github as a project named <a href="http://github.com/edsu/earls">earls</a>, in case you have ideas you'd like to try out.</p>

<p><a href="http://inkdroid.org/c4l15-urls/"><img src="http://inkdroid.org/images/earls.png" alt="#c4l15 urls, or earls" /></a></p>

<p>earls is a <a href="http://nodejs.org">node</a> app that listens to Twitter's <a href="https://dev.twitter.com/streaming/overview/request-parameters">filter stream API</a> for tweets mentioning <code>#c4l15</code>. When it finds one it then looks for 1 or more links in the tweet. Each link is fetched (which also unshortens it), it tries to parse any HTML (thanks <a href="https://github.com/cheeriojs/cheerio">cheerio</a>) to find a page title, and then stashes these details as well as the tweet in <a href="http://redis.io">redis</a>.</p>

<p>When you load the page it will show you the latest counts for all URLs it has found so far. Unfortunately at the moment you need to reload the page to get an update. If I have time I will work on making it update live in the page with <a href="http://socket.io">socket.io</a>. earls could be used for other conferences, and ought to run pretty easily on heroku for free.</p>

<p>Oh, and you can see the JSON data <a href="http://inkdroid.org/c4l15-urls/stats.json">here</a> in case you have other ideas of things you'd like to do with the data.</p>

<p><strong>Have a superb conference you crazy dreamers and doers!</strong></p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8446</wp:post_id>
		<wp:post_date>2015-02-10 09:40:39</wp:post_date>
		<wp:post_date_gmt>2015-02-10 16:40:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>c4l15</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="conferences"><![CDATA[conferences]]></category>
		<category domain="post_tag" nicename="node"><![CDATA[node]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<category domain="post_tag" nicename="url"><![CDATA[url]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Human Twist</title>
		<link>http://inkdroid.org/2015/02/16/human-twist/</link>
		<pubDate>Mon, 16 Feb 2015 14:00:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8461</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>Human motives sharpen all our questions, human satisfactions lurk in all our answers, all our formulas have a human twist.</p>
</blockquote>

<p><a href="https://en.wikipedia.org/wiki/William_James">William James</a> in Pragmatism and Humanism.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8461</wp:post_id>
		<wp:post_date>2015-02-16 07:00:57</wp:post_date>
		<wp:post_date_gmt>2015-02-16 14:00:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>human-twist</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="pragmatism"><![CDATA[pragmatism]]></category>
		<category domain="post_tag" nicename="william-james"><![CDATA[William James]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Repetition</title>
		<link>http://inkdroid.org/2015/03/02/repetition/</link>
		<pubDate>Mon, 02 Mar 2015 14:00:18 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8468</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>To be satisfied with repeating, with traversing the ruts which in other conditions led to good, is the surest way of creating carelessness about present and actual good.</p>
</blockquote>

<p><a href="https://en.wikipedia.org/wiki/John_Dewey">John Dewey</a> in <a href="https://archive.org/details/humannatureandco011182mbp">Human Nature and Conduct</a> (p. 67).</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8468</wp:post_id>
		<wp:post_date>2015-03-02 07:00:18</wp:post_date>
		<wp:post_date_gmt>2015-03-02 14:00:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>repetition</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="john-dewey"><![CDATA[John Dewey]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="pragmantism"><![CDATA[pragmantism]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Facts are Mobile</title>
		<link>http://inkdroid.org/2015/03/10/facts-are-mobile/</link>
		<pubDate>Tue, 10 Mar 2015 19:42:28 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8482</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>To classify is, indeed, as useful as it is natural. The indefinite multitude of particular and changing events is met by the mind with acts of defining, inventorying and listing, reducing the common heads and tying up in bunches. But these acts like other intelligent acts are performed for a purpose, and the accomplishment of purpose is their only justification. Speaking generally, the purpose is to facilitate our dealing with unique individuals and changing events. When we assume that our clefts and bunches represent fixed separations and collections in rerum natura, we obstruct rather than aid our transactions with things. We are guilty of a presumption which nature promptly punishes. We are rendered incompetent to deal effectively with the delicacies and novelties of nature and life. Our thought is hard where facts are mobile ; bunched and chunky where events are fluid, dissolving.</p>
  
  <p><a href="https://en.wikipedia.org/wiki/John_Dewey">John Dewey</a> in <a href="https://archive.org/details/humannatureandco011182mbp">Human Nature and Conduct</a> (p. 131)</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8482</wp:post_id>
		<wp:post_date>2015-03-10 12:42:28</wp:post_date>
		<wp:post_date_gmt>2015-03-10 19:42:28</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>facts-are-mobile</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="categories"><![CDATA[categories]]></category>
		<category domain="post_tag" nicename="john-dewey"><![CDATA[John Dewey]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>JavaScript and Archives</title>
		<link>http://inkdroid.org/2015/03/12/javascript-and-archives/</link>
		<pubDate>Thu, 12 Mar 2015 09:09:39 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8484</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Tantek_%C3%87elik">Tantek Çelik</a> has some <a href="http://tantek.com/2015/069/t1/js-dr-javascript-required-dead">strong words</a> about the use of JavaScript in Web publishing, specifically regarding it's accessibility and longevity:</p>

<blockquote>
  <p>... in 10 years nothing you built today that depends on JS for the content will be available, visible, or archived anywhere on the web</p>
</blockquote>

<p>It is a dire warning. It sounds and feels true. I am in the middle of writing a webapp that happens to use <a href="https://facebook.github.io/react/">React</a>, so Tantek's words are particularly sobering.</p>

<p>And yet, consider for a moment how Twitter make personal downloadable archives available. When you request your archive you eventually get a zip file. When you unzip it, you open an index.html file in your browser, and are provided you with a view of all the tweets you've ever sent.</p>

<p>If you take a look under the covers you'll see it is actually a JavaScript application called <a href="https://www.quora.com/What-is-Grailbird">Grailbird</a>. If you have JavaScript turned on it looks something like this:</p>

<p><a href="http://inkdroid.org/images/tweet-archive-1.png"><img src="http://inkdroid.org/images/tweet-archive-1.png" alt="JavaScript On" /></a></p>

<p>If you have JavaScript turned off it looks something like this:</p>

<p><a href="http://inkdroid.org/images/tweet-archive-2.png"><img src="http://inkdroid.org/images/tweet-archive-2.png" alt="JavaScript Off" /></a></p>

<p>But remember this is a static site. There is no server side piece. Everything is happening in your browser. You can disconnect from the Internet and as long as your browser has JavaScript turned on it is fully functional. (Well the avatar URLs break, but that could be fixed). You can search across your tweets. You can drill into particular time periods. You can view your account summary. It feels pretty durable. I could stash it away on a hard drive somewhere, and come back in 10 years and (assuming there are still web browsers with a working JavaScript runtime) I could still look at it right?</p>

<p>So is Tantek right about JavaScript being at odds with preservation of Web content? I think he is, but I also think JavaScript can be used in the service of archiving, and that there are starting to be some options out there that make archiving JavaScript heavy websites possible.</p>

<p>The real problem that Tantek is talking about is when human readable content isn't available in the HTML and is getting loaded dynamically from Web APIs using JavaScript. This started to get popular back in 2005 when Jesse James Garrett coined the term <a href="https://en.wikipedia.org/wiki/Ajax_%28programming%29">AJAX</a> for building app-like web pages using asynchronous requests for XML, which is now mostly JSON. The scene has since exploded with all sorts of client side JavaScript frameworks for building web <em>applications</em> as opposed to web <em>pages</em>.</p>

<p>So if someone (e.g. <a href="http://archive.org">Internet Archive</a>) comes along and tries to archive a URL it will get the HTML and associated images, stylesheets and JavaScript files that are referenced in that HTML. These will get saved just fine. But when the content is <em>played back</em> later in (e.g. <a href="https://archive.org/web/">Wayback Machine</a>) the JavaScript will run and try to talk to these external Web APIs to load content. If those APIs no longer exist, the content won't load.</p>

<p>One solution to this problem is for the web archiving process to execute the JavaScript and to archive any of the dynamic content that was retrieved. This can be done using headless browsers like <a href="http://phantomjs.org/">PhantomJS</a>, and <a href="http://googlewebmastercentral.blogspot.com/2014/10/updating-our-technical-webmaster.html">supposedly</a> Google has started executing JavaScript. Like Tantek I'm dubious about how widely they execute JavaScript. I've had trouble getting Google to index a JavaScript heavy site that I've inherited at work. But even if the crawler does execute the JavaScript, user interactions can cause different content to load. So does the bot start clicking around in the application to get content to load? This is yet more work for a archiving bot to do, and could potentially result in write operations which might not be great.</p>

<p>Another option is to change or at least augment the current web archiving paradigm by adding curator driven web archiving to the mix. The best examples I've seen of this are Ilya Kreymer's work on <a href="https://github.com/ikreymer/pywb-webrecorder">pywb</a> and <a href="https://github.com/ikreymer/pywb-webrecorder">pywb-recorder</a>. Ilya is a former Internet Archive engineer, and is well aware of the limitations in the most common forms of web archiving today. pywb is a new <em>player</em> for web archives and pywb-recorder is a new recording environment. Both work in concert to let archivists interactively select web content that needs to be archived, and then for that content to be played back. The best example of this is his demo service <a href="https://webrecorder.io/">webrecorder.io</a> which composes pywb and pywb-recorder so that anyone can create a web archive of a highly dynamic website, download the WARC archive file, and then reupload it for playback.</p>

<p>The nice thing about Ilya's work is that it is geared at archiving this JavaScript heavy content. Rhizome and the New Museum in New York City have <a href="http://bits.blogs.nytimes.com/2014/10/19/a-new-tool-to-preserve-moments-on-the-internet/?_r=0">started working</a> with Ilya to use pywb to archive highly dynamic Web content. I think this represents a possible bright future for archives, where curators or archivists are more part of the equation, and where Web archives are more distributed, not just at Internet Archive and some major national libraries. I think the work <a href="http://genius.com">Genius</a> are doing to annotate the Web, archived versions of the Web is in a similar space. It's exciting times for Web archiving. You know, exciting if you happen to be an archivist and/or archiving things.</p>

<p>At any rate, getting back to Tantek's point about JavaScript. If you are in the business of building archives on the Web definitely think twice about using client side JavaScript frameworks. If you do, make sure your site degrades so that the majority of the content is still available. You want to make it <em>easy</em> for Internet Archive to archive your content (lots of copies keeps stuff safe) and you want to make it easy for Google et al to index it, so people looking for your content can actually find it. Stanford University's Web Archiving team have a super set of pages describing <a href="https://library.stanford.edu/projects/web-archiving/archivability">archivability</a> of websites. We can't control how other people publish on the Web, but I think as archivists we have a responsibility to think about these issues as we create archives on the Web.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8484</wp:post_id>
		<wp:post_date>2015-03-12 02:09:39</wp:post_date>
		<wp:post_date_gmt>2015-03-12 09:09:39</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>javascript-and-archives</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="javascript"><![CDATA[javascript]]></category>
		<category domain="post_tag" nicename="web-archiving"><![CDATA[web archiving]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87287</wp:comment_id>
			<wp:comment_author><![CDATA[RT @t: Superb follow-up by @edsu to my "js;dr" JavaScript required did not read post: inkdroid.org/2015/0… via @kylewm2 (ttk.me t4aA4)]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://brid-gy.appspot.com/repost/twitter/t/577651884434944001/577731658281017344</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-03-17 00:25:06</wp:comment_date>
			<wp:comment_date_gmt>2015-03-17 07:25:06</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This Article was mentioned on <a href="https://brid-gy.appspot.com/repost/twitter/t/577651884434944001/577731658281017344" rel="nofollow">brid-gy.appspot.com</a>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>webmention</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1426578695.927895069122314453125;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
		<wp:comment>
			<wp:comment_id>87300</wp:comment_id>
			<wp:comment_author><![CDATA[RT @t: Superb follow-up by @edsu to my "js;dr" JavaScript required did not read post: inkdroid.org/2015/0… via @kylewm2 (ttk.me t4aA4)]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>https://brid-gy.appspot.com/repost/twitter/t/577651884434944001/577765199974952960</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-03-17 02:49:30</wp:comment_date>
			<wp:comment_date_gmt>2015-03-17 09:49:30</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This Article was mentioned on <a href="https://brid-gy.appspot.com/repost/twitter/t/577651884434944001/577765199974952960" rel="nofollow">brid-gy.appspot.com</a>
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>webmention</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1426591127.810904979705810546875;s:7:"message";s:41:"ed changed the comment status to approved";s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>twarc &amp; Ferguson demo</title>
		<link>http://inkdroid.org/2015/03/12/twarc-ferguson-demo/</link>
		<pubDate>Thu, 12 Mar 2015 09:17:45 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8503</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Here's a brief demo of what it looks like to use <a href="http://github.com/edsu/twarc">twarc</a> on the command line to archive tweets that are mentioning Ferguson. I've been doing archiving around this topic <a href="http://inkdroid.org/2014/08/30/a-ferguson-twitter-archive/">off and on</a> since August of last year, and happened to start it up again recently to collect the response to the <a href="http://www.nytimes.com/2015/03/05/us/us-calls-on-ferguson-to-overhaul-criminal-justice-system.html">Justice Department report</a>.</p>

<iframe src="https://player.vimeo.com/video/121974231" width="500" height="313" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

<p>I kind of glossed over getting your Twitter keys set up, which is a bit tedious. I have them set in environment variables for that demo, but you can pass them in on the command line now. I guess that could be another demo sometime. If you are interested send me a <a href="http://twitter.com/edsu">tweet</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8503</wp:post_id>
		<wp:post_date>2015-03-12 02:17:45</wp:post_date>
		<wp:post_date_gmt>2015-03-12 09:17:45</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>twarc-ferguson-demo</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="ferguson"><![CDATA[ferguson]]></category>
		<category domain="post_tag" nicename="web-archiving"><![CDATA[web archiving]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>The Adventure of Experiment</title>
		<link>http://inkdroid.org/2015/03/28/the-adventure-of-experiment/</link>
		<pubDate>Sat, 28 Mar 2015 11:49:57 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8516</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>Love of certainty is a demand for guarantees in advance of action. Ignoring the fact that truth can be bought only by the adventure of experiment, dogmatism turns truth into an insurance company. Fixed ends upon one side and fixed "principles" -- that is authoritative rules -- on the other, are props for a feeling of safety, the refuge of the timid, and the means by which the bold prey upon the timid.</p>
  
  <p><a href="https://en.wikipedia.org/wiki/John_Dewey">John Dewey</a> in <a href="https://archive.org/details/humannatureandco011182mbp">Human Nature and Conduct</a> (p. 237)</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8516</wp:post_id>
		<wp:post_date>2015-03-28 04:49:57</wp:post_date>
		<wp:post_date_gmt>2015-03-28 11:49:57</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>the-adventure-of-experiment</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="john-dewey"><![CDATA[John Dewey]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="pragmatism"><![CDATA[pragmatism]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Tweets and Deletes</title>
		<link>http://inkdroid.org/2015/04/14/tweets-and-deletes/</link>
		<pubDate>Tue, 14 Apr 2015 16:07:18 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8520</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Archives are full of silences. Archivists try to surface these silences by making <a href="https://en.wikipedia.org/wiki/Archival_appraisal">appraisal</a> decisions about what to collect and what not to collect. Even after they are accessioned, records can be silenced by culling, weeding and purging. We do our best to document these activities, to leave a trail of these decisions, but they are inevitably deeply contingent. The context for the records and our decisions about them unravels endlessly.</p>

<p>At some point we must accept that the archival record is not perfect, and that it's a bit of a <a href="http://www.nyu.edu/pages/classes/bkg/methods/harris.pdf">miracle</a> that it exists at all. But in all these cases it is the archivist who has agency: the deliberate or subliminal decisions that determine what comprises the archival record are enacted by an archivist. In addition the record creator has agency, in their decision to give their records to an archive.</p>

<p>Perhaps I'm over-simplifying a bit, but I think there is a curious new dynamic at play in social media archives, specifically archives of Twitter data. I wrote in a <a href="http://inkdroid.org/2014/11/18/on-forgetting/">previous post</a> about how Twitter’s Terms of Service prevent distribution of Twitter data retrieved from their API, but do allow for the distribution of Tweet IDs and relatively small amounts of derivative data (spreadsheets, etc).</p>

<p>Tweet IDs can then be <em>hydrated</em>, or turned back into raw original data, by going back to the Twitter API. If a tweet has been deleted you cannot get it back from the API. The net effect this has is of cleaning, or purging, the archival record as it is made available on the Web. But the decision of what to purge is made by the record creator (the creator of the tweet) or by Twitter themselves in <a href="http://www.theguardian.com/world/2014/sep/24/isis-twitter-youtube-message-social-media-jihadi">cases</a> where tweets or users are deleted.</p>

<p>For example lets look at the <a href="http://hdl.handle.net/10864/10830">collection</a> of Twitter data that <a href="https://twitter.com/ruebot">Nick Ruest</a> has assembled in the wake of the <a href="https://en.wikipedia.org/wiki/Charlie_Hebdo_shooting">attack</a> on the offices of Charlie Hebdo earlier this year. Nick collected 13 million tweets mentioning four hashtags related to the attacks, for the period of January 9th to January 28th, 2015. He has made the tweet IDs available as a <a href="http://hdl.handle.net/10864/10830">dataset</a> for researchers to use (a separate file for each hashtag). I was interested in replicating the dataset for potential researchers at the University of Maryland, but also in seeing how many of the tweets had been deleted.</p>

<p>So on February 20th (42 days after Nick started his collecting) I began hydrating the IDs. It took 4 days for <a href="http://github.com/edsu/twarc">twarc</a> to finish. When it did I counted up the number of tweets that I was able to retrieve. The results are somewhat interesting:</p>

<p><style>
  table tr td {
  font-size: smaller;
}
</style></p>

<table>
<thead>
<tr>
  <th>hashtag</th>
  <th>archived tweets</th>
  <th>hydrated</th>
  <th>deletes</th>
  <th>percent deleted</th>
</tr>
</thead>
<tbody>
<tr>
  <td>#JeSuisJuif</td>
  <td>96,518</td>
  <td>89,584</td>
  <td>6,934</td>
  <td>7&#46;18%</td>
</tr>
<tr>
  <td>#JeSuisAhmed</td>
  <td>264,097</td>
  <td>237,674</td>
  <td>26,423</td>
  <td>10&#46;01%</td>
</tr>
<tr>
  <td>#JeSuisCharlie</td>
  <td>6,503,425</td>
  <td>5,955,278</td>
  <td>548,147</td>
  <td>8&#46;43%</td>
</tr>
<tr>
  <td>#CharlieHebdo</td>
  <td>7,104,253</td>
  <td>6,554,231</td>
  <td>550,022</td>
  <td>7&#46;74%</td>
</tr>
<tr>
  <td>Total</td>
  <td>13,968,293</td>
  <td>12,836,767</td>
  <td>1,131,526</td>
  <td>8&#46;10%</td>
</tr>
</tbody>
</table>

<p>It looks like 1.1 million tweets out of the 13.9 million tweet dataset have been deleted. That's about 8.1%. I suspect now even more have been deleted. While the datasets themselves are significantly smaller the number of deletes for #JeSuiAhmed and #JeSuisJuif seem quite a bit higher than #JeSuisCharlie and #CharlieHebdo. Could this be that users were concerned about how their tweets would be interpreted by parties analyzing the data?</p>

<p>Of course, it's very hard for me to say since I don't have the deleted tweets. I don't even know who sent them. A researcher interested in these questions would presumably need to travel to York University to work with the dataset. In a way this seems to be how archives usually work. But if you add the Web as a global, public access layer into the mix it complicates things a bit.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8520</wp:post_id>
		<wp:post_date>2015-04-14 09:07:18</wp:post_date>
		<wp:post_date_gmt>2015-04-14 16:07:18</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>tweets-and-deletes</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="post_tag" nicename="twitter"><![CDATA[twitter]]></category>
		<wp:postmeta>
			<wp:meta_key>_post_restored_from</wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:20:"restored_revision_id";i:8544;s:16:"restored_by_user";i:2;s:13:"restored_time";i:1429027356;}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>87483</wp:comment_id>
			<wp:comment_author><![CDATA[August Library Tech Roundup | LITA Blog]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://litablog.org/2015/08/august-library-tech-roundup/</wp:comment_author_url>
			<wp:comment_author_IP>127.0.0.1</wp:comment_author_IP>
			<wp:comment_date>2015-08-27 06:26:13</wp:comment_date>
			<wp:comment_date_gmt>2015-08-27 13:26:13</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Archiving and Twitter, how the user affects the archive by deleting tweets. [&#8230;]
]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>akismet_result</wp:meta_key>
				<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1440681973.4631049633026123046875;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>akismet_history</wp:meta_key>
				<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1440758140.2348420619964599609375;s:5:"event";s:15:"status-approved";s:4:"user";s:2:"ed";}]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Human Nature and Conduct</title>
		<link>http://inkdroid.org/2015/04/24/human-nature-and-conduct/</link>
		<pubDate>Fri, 24 Apr 2015 20:28:32 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8556</guid>
		<description></description>
		<content:encoded><![CDATA[<p><a href="https://www.goodreads.com/book/show/2080244.Human_Nature_and_Conduct" style="float: left; padding-right: 20px"><img alt="Human Nature and Conduct" border="0" src="https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png" /></a><a href="https://www.goodreads.com/book/show/2080244.Human_Nature_and_Conduct">Human Nature and Conduct</a> by <a href="https://www.goodreads.com/author/show/42738.John_Dewey">John Dewey</a><br />
My rating: <a href="https://www.goodreads.com/review/show/1216511030">5 of 5 stars</a></p>

<p>This book came recommended by <a href="http://sjackson.infosci.cornell.edu/" rel="nofollow">Steven Jackson</a> when he <a href="http://casci.umd.edu/2014/11/25/casci-talk-how-to-fix-the-world-repair-as-practice-and-worldview/" rel="nofollow">visited</a> UMD last year. I'm a fan of Jackson's work on <a href="http://sjackson.infosci.cornell.edu/RethinkingRepairPROOFS%28reduced%29Aug2013.pdf" rel="nofollow">repair</a>, and was curious about how his ideas connected back to Dewey's Human Nature and Conduct.</p>

<p>I've been slowly reading it, savoring each chapter on my bus rides to work since then. It's a lovely &amp; wise book. Some of the language puts you back into 1920s, but the ideas are fresh and still so relevant. I'm not going to try to summarize it here. You may have noticed I've <a href="http://inkdroid.org/tag/john-dewey/">posted some quotes</a> here. Let's just say it is a very hopeful book and provides a very clear and yet generous view of the human enterprise.</p>

<p>I don't know if I was imagining it, but I seemed to see a lot of parallels between it and some reading I'm doing about Buddhism. I noticed over at <a href="https://en.wikipedia.org/wiki/John_Dewey#Visits_to_China_and_Japan" rel="nofollow">Wikipedia</a> that Dewey spent some time in China and Japan just prior to delivering these lectures. So maybe it's not so far fetched a connection.</p>

<p>I checked it out of the library, but I need to buy a copy of my own so I can re-read it. You can find a copy at <a href="https://archive.org/details/humannatureandco011182mbp">Internet Archive</a> for your ebook reader too.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8556</wp:post_id>
		<wp:post_date>2015-04-24 13:28:32</wp:post_date>
		<wp:post_date_gmt>2015-04-24 20:28:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>human-nature-and-conduct</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="book-review"><![CDATA[book review]]></category>
		<category domain="post_tag" nicename="john-dewey"><![CDATA[John Dewey]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="pragmatism"><![CDATA[pragmatism]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Energies</title>
		<link>http://inkdroid.org/2015/04/24/energies/</link>
		<pubDate>Fri, 24 Apr 2015 20:14:35 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8560</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>Human nature exists and operates in an environment. And it is not "in" that environment as coins are in a box, but as a plant is in the sunlight and soil. It is of them, continuous with their energies, dependent upon their support, capable of increase only as it utilizes them, and as it gradually rebuilds from their crude indifference an environment genially civilized.</p>
  
  <p><a href="https://en.wikipedia.org/wiki/John_Dewey">John Dewey</a> in <a href="https://archive.org/details/humannatureandco011182mbp">Human Nature and Conduct</a> (p. 296)</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8560</wp:post_id>
		<wp:post_date>2015-04-24 13:14:35</wp:post_date>
		<wp:post_date_gmt>2015-04-24 20:14:35</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>energies</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="john-dewey"><![CDATA[John Dewey]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="pragmatism"><![CDATA[pragmatism]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Something Horrible</title>
		<link>http://inkdroid.org/2015/04/24/something-horrible/</link>
		<pubDate>Fri, 24 Apr 2015 20:18:58 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8563</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>There is something horrible, something that makes one fear for civilization, in denunciations of class-differences and class struggles which proceed from a class in power, one that is seizing every means, even to a monopoly of moral ideals, to carry on its struggle for class-power.</p>
  
  <p><a href="https://en.wikipedia.org/wiki/John_Dewey">John Dewey</a> in <a href="https://archive.org/details/humannatureandco011182mbp">Human Nature and Conduct</a> (p. 301)</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8563</wp:post_id>
		<wp:post_date>2015-04-24 13:18:58</wp:post_date>
		<wp:post_date_gmt>2015-04-24 20:18:58</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>something-horrible</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="john-dewey"><![CDATA[John Dewey]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="pragmatism"><![CDATA[pragmatism]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Method and Materials</title>
		<link>http://inkdroid.org/2015/04/24/method-and-materials/</link>
		<pubDate>Fri, 24 Apr 2015 20:26:37 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8565</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  <p>Now it is a wholesome thing for any one to be made aware that thoughtless, self-centered action on his part exposes him to the indignation and dislike of others. There is no one who can be safely trusted to be exempt from immediate reactions of criticism, and there are few who do not need to be braced by occasional expressions of approval. But these influences are immensely overdone in comparison with the assistance that might be given by the influence of social judgments which operate without accompaniments of praise and blame; which enable an individual to see for himself what he is doing, and which put him in command of a method of analyzing the obscure and usually unavowed forces which move him to act. We need a permeation of judgments on conduct by the method and materials of a science of human nature. Without such enlightenment even the best-intentioned attempts at the moral guidance and improvement of others often eventuate in tragedies of misunderstanding and division, as is so often seen in the relations of parents and children.</p>
  
  <p><a href="https://en.wikipedia.org/wiki/John_Dewey">John Dewey</a> in <a href="https://archive.org/details/humannatureandco011182mbp">Human Nature and Conduct</a> (p. 321)</p>
</blockquote>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8565</wp:post_id>
		<wp:post_date>2015-04-24 13:26:37</wp:post_date>
		<wp:post_date_gmt>2015-04-24 20:26:37</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>method-and-materials</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="john-dewey"><![CDATA[John Dewey]]></category>
		<category domain="category" nicename="philosophy"><![CDATA[philosophy]]></category>
		<category domain="post_tag" nicename="pragmatism"><![CDATA[pragmatism]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>VirtualEnv Builds in Sublime Text 3</title>
		<link>http://inkdroid.org/2015/05/05/virtualenv-builds-in-sublime-text-3/</link>
		<pubDate>Tue, 05 May 2015 16:54:54 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8571</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Back in 1999 I was a relatively happy Emacs user, and was beginning work at a startup where I was one of the first employees after the founders. Like many startups, in addition to owning the company, the founders were hackers, and were routinely working on the servers. When I asked if Emacs could be installed on one of the machines I was told to learn Vi ... which I proceeded to do. I needed the job.</p>

<p>Here I am 15 years later, and am finally starting to use <a href="http://www.sublimetext.com/3">Sublime Text 3</a> a bit more in my work. I'm not be a cool kid anymore, but I can still <em>pretend</em> to be one, eh? The <a href="http://guillermooo.bitbucket.org/Vintageous/">Vintageous</a> plugin lets my fingers feel like they are in Vim, while being able to take advantage of other packages for <a href="https://github.com/ttscoff/MarkdownEditing">editing Markdown</a>, <a href="https://sublimegit.net/">interacting with Git</a> and the lovely eye-pleasing themes that are available. I still feel a bit dirty because unlike Vim, Sublime is not opensource ; but at the same time it does feel good to support a small software publisher who is doing good work. Maybe I'll end up switching back to Vim and <a href="http://www.vim.org/sponsor/index.php">supporting it</a>.</p>

<p>Anyway, as a Python developer one thing I immediately wanted to be able to do was to use my project's VirtualEnv during development, and to run the test suite from inside Sublime. The <a href="https://packagecontrol.io/packages/Virtualenv">Virtualenv</a> package makes creating, activating, deactivating, deleting a virtualenv a snap. But I couldn't seem to get the build to work properly with the virtualenv, even after setting the <code>Build System</code> to <code>Python - Virtualenv</code></p>

<p><a href="sublime3-virtualenv.png"><img src="http://inkdroid.org/images/sublime3-virtualenv.png" alt="Sublime Text 3 - VirtualEnv" /></a></p>

<p>After what felt like a lot of googling around (it was probably just 20 minutes) I didn't seem to find an answer until I discovered in the <a href="https://www.sublimetext.com/docs/3/projects.html">Project documentation</a> that I could save my Project, and then go to <code>Project -&gt; Edit Project</code> and add a <code>build_systems</code> stanza like this:</p>

<pre lang="json">{
 "folders":
 [
  {
   "path": "."
  }
 ],
 "virtualenv": "/Users/ed/.virtualenvs/curio",
 "build_systems": [
  {
   "name": "Test",
   "shell_cmd": "/Users/ed/.virtualenvs/curio/bin/python setup.py test"
  }
 ] 
}
</pre>

<p>Notice how the shell_cmd is using the Python executable in my VirtualEnv? After saving that I was able to go into <code>Tools -&gt; Build System</code> and set the build system to <code>Test</code>, which matches the name of the build system you added in the JSON. Now a command-B will run my test suite with the VirtualEnv.</p>

<p><a href="http://inkdroid.org/images/sublime3-virtualenv2.png"><img src="http://inkdroid.org/images/sublime3-virtualenv2.png" alt="Sublime Text 3 - VirtualEnv w/ Build" /></a></p>

<p>I guess it would be nice if the VirtualEnv plugin for Sublime did something to make this easier. But rather than go down that <a href="https://en.wikipedia.org/wiki/Alice%27s_Adventures_in_Wonderland#Famous_lines_and_expressions">rabbit hole</a> I decided to write it down here for the benefit of my future self (and perhaps you).</p>

<p>If you know of a better way to do this please <a href="https://twitter.com/edsu">let me know</a>.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8571</wp:post_id>
		<wp:post_date>2015-05-05 09:54:54</wp:post_date>
		<wp:post_date_gmt>2015-05-05 16:54:54</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>virtualenv-builds-in-sublime-text-3</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="editor"><![CDATA[Editor]]></category>
		<category domain="category" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="sublime-text"><![CDATA[Sublime Text]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A Personal Panopticon</title>
		<link>http://inkdroid.org/2015/05/09/a-personal-panopticon/</link>
		<pubDate>Sat, 09 May 2015 12:16:34 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8597</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Here is how you can use your <a href="https://google.com/history">Google Search History</a> and <a href="http://stedolan.github.io/jq/">jq</a> to create a top 10 list of the things you've googled for the most.</p>

<p>First download your data from your <a href="https://google.com/history">Google Search History</a>. Yeah, <em>creepy</em>. Then install <a href="http://stedolan.github.io/jq/">jq</a>. Wait for the email from Google that your archive is ready and download then unzip it. Open a terminal window in the Searches directory, and run this:</p>

<pre><code>jq --raw-output '.event[].query.query_text' *.json \
  | sort | uniq -c | sort -rn | head -10
</code></pre>

<p>Here's what I see for the 75,687 queries I've typed into google since July 2005.</p>

<pre lang="text">309 google analytics
 130 hacker news
 116 this is my jam
  83 site:chroniclingamerica.loc.gov
  68 jquery
  54 bagit
  48 twitter api
  44 google translate
  37 wikistream
  37 opds
</pre>

<p>These are (mostly) things that I hadn't bothered to bookmark, but visited regularly. I suspect there is something more compelling and interesting that could be done with the data. A personal <a href="https://en.wikipedia.org/wiki/Panopticon">panopticon</a> perhaps.</p>

<p>Oh, and I'd delete the archive from your Google Drive after you've downloaded it. If you ever grant other apps the ability to read from your drive they could read your search history. Actually maybe this whole exercise is fraught with peril. You should just ignore it.</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8597</wp:post_id>
		<wp:post_date>2015-05-09 05:16:34</wp:post_date>
		<wp:post_date_gmt>2015-05-09 12:16:34</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>a-personal-panopticon</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>SKOS and Wikidata</title>
		<link>http://inkdroid.org/2015/05/20/skos-and-wikidata/</link>
		<pubDate>Wed, 20 May 2015 17:53:51 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8615</guid>
		<description></description>
		<content:encoded><![CDATA[<p>For <a href="https://twitter.com/dayofdh">#DayOfDH</a> yesterday I created a <a href="https://vimeo.com/128305641">quick video</a> about some data normalization work I have been doing using Wikidata entities. I may write more about this work later, but the short version is that I have a bunch of spreadsheets with names in them (authors) in a variety of formats and transliterations, which I need to collapse into a unique identifier so that I can provide a unified display of the data per unique author. So for example, my spreadsheets have information for Fyodor Dostoyevsky using the following variants:</p>

<ul>
<li>Dostoeieffsky, Feodor</li>
<li>Dostoevski</li>
<li>Dostoevski, F. M.</li>
<li>Dostoevski, Fedor</li>
<li>Dostoevski, Feodor Mikailovitch</li>
<li>Dostoevskii</li>
<li>Dostoevsky</li>
<li>Dostoevsky, Fiodor Mihailovich</li>
<li>Dostoevsky, Fyodor</li>
<li>Dostoevsky, Fyodor Michailovitch</li>
<li>Dostoieffsky</li>
<li>Dostoieffsky, Feodor</li>
<li>Dostoievski</li>
<li>Dostoievski, Feodor Mikhailovitch</li>
<li>Dostoievski, Feodore M.</li>
<li>Dostoievski, Thedor Mikhailovitch</li>
<li>Dostoievsky</li>
<li>Dostoievsky, Feodor Mikhailovitch</li>
<li>Dostoievsky, Fyodor</li>
<li>Dostojevski, Feodor</li>
<li>Dostoyeffsky</li>
<li>Dostoyefsky</li>
<li>Dostoyefsky, Theodor Mikhailovitch</li>
<li>Dostoyevski, Feodor</li>
<li>Dostoyevsky</li>
<li>Dostoyevsky, Fyodor</li>
<li>Dostoyevsky, F. M.</li>
<li>Dostoyevsky, Feodor Michailovitch</li>
<li>Dostoyevsky, Feodor Mikhailovich</li>
</ul>

<p>So, obviously, I wanted to normalize these. But I also want to link the name up to an identifier that could be useful for obtaining other information, such as an image of the author, a description of their work, possibly link to works by the author, etc. I'm going to try to map the authors to Wikidata, largely because there are links from Wikidata to other places like the Virtual International Authority File, and Freebase, but there are also images on Wikimedia Commons, and nice descriptive text for the people. As an example <a href="https://www.wikidata.org/wiki/Q991">here</a> is the Wikidata page for Dostoyevsky.</p>

<p>To aid in this process I created a very simple command line tool and library called <a href="http://github.com/edsu/wikidata_suggest">wikidata_suggest</a> which uses Wikidata's suggest API to interactively match up a string of text to a Wikidata entity. If Wikidata doesn't have any suggestions as a fallback the utility looks in a page of Google's search results for a Wikipedia page and then will optionally let you use that text.</p>

<h2>SKOS</h2>

<p>Soon after tweeting about the utility and the video I made about it I heard from Alberto who works on the <a href="https://en.wikipedia.org/wiki/Astrophysics_Data_System">NASA Astrophysics Data System</a> and was interested in using wikidata_suggest to try to link up the <a href="http://astrothesaurus.org/">Unified Astronomy Thesaurus</a> to Wikidata.</p>

<blockquote class="twitter-tweet" lang="en">
  <p lang="en" dir="ltr">
    <a href="https://twitter.com/libcce">@libcce</a> map UAT to <a href="https://twitter.com/wikidata">@wikidata</a>? <a href="https://t.co/sqyPRdqd9U">https://t.co/sqyPRdqd9U</a>
  </p>— Alberto Accomazzi (@aaccomazzi) 
  
  <a href="https://twitter.com/aaccomazzi/status/600842809910894593">May 20, 2015</a>
</blockquote>

<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> Fortunately the UAT is made available as a SKOS RDF file. So I wrote a little proof of concept script named <a href="https://github.com/edsu/skos_wikidata">skos_wikidata.py</a> that loads a SKOS file, walks through each skos:Concept and asks you to match the skos:prefLabel to Wikidata using <a href="http://github.com/edsu/wikidata_suggest">wikidata_suggest</a>. Here's a quick video I made of what this process looks like:</p>

<iframe src="https://player.vimeo.com/video/128396304" width="500" height="313" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

<p>I guess this is similar to what you might do in <a href="http://openrefine.org/">OpenRefine</a>, but I wanted a bit more control over how the data was read in, modified and matched up. I'd be interested in your ideas on how to improve it if you have any.</p>

<p>It's kind of funny how Day of Digital Humanities quickly morphed into Day of Astrophysics...</p>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8615</wp:post_id>
		<wp:post_date>2015-05-20 10:53:51</wp:post_date>
		<wp:post_date_gmt>2015-05-20 17:53:51</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>skos-and-wikidata</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="astronomy"><![CDATA[astronomy]]></category>
		<category domain="category" nicename="metadata"><![CDATA[metadata]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
		<category domain="post_tag" nicename="rdf"><![CDATA[rdf]]></category>
		<category domain="post_tag" nicename="skos"><![CDATA[skos]]></category>
		<category domain="post_tag" nicename="wikidata"><![CDATA[wikidata]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Links in Obergefell v. Hodges</title>
		<link>http://inkdroid.org/2015/06/30/links-in-obergefell-v-hodges/</link>
		<pubDate>Tue, 30 Jun 2015 16:52:46 +0000</pubDate>
		<dc:creator><![CDATA[ed]]></dc:creator>
		<guid isPermaLink="false">http://inkdroid.org/?p=8641</guid>
		<description></description>
		<content:encoded><![CDATA[<p>Last week's landmark ruling from the Supreme Court on same sex marriage was routinely <a href="http://www.supremecourt.gov/opinions/14pdf/14-556_3204.pdf">published</a> on the Web as a PDF. Given the <a href="http://www.nytimes.com/2013/09/24/us/politics/in-supreme-court-opinions-clicks-that-lead-nowhere.html?_r=0">past history</a> of URL use in Supreme Court opinions I thought I would take a quick look to see what URLs were present. There are two, both are in Justice Alito's dissenting opinion, and one is broken ... just four days after the PDF was published. You can see it yourself at the bottom of page 100 in the <a href="http://www.supremecourt.gov/opinions/14pdf/14-556_3204.pdf">PDF</a>.</p>

<p><a href="http://www.supremecourt.gov/opinions/14pdf/14-556_3204.pdf"><img src="http://inkdroid.org/images/scotus-1.png" alt="" /></a></p>

<p>If you point your browser at</p>

<blockquote>
  <p>http://www.cdc.gov/nchs/data/databrief/db18.pdf</p>
</blockquote>

<p>you will get a page not found error:</p>

<p><img src="http://inkdroid.org/images/scotus-2.png" alt="" /></p>

<p>Sadly even the Internet Archive doesn't have a snapshot of the page available.</p>

<p><a href="http://www.cdc.gov/nchs/data/databriefs/db18.pdf"><img src="http://inkdroid.org/images/scotus-3.png" alt="" /></a></p>

<p>But notice it thinks it can get a copy of it still. That's because the Center for Disease Control's website is responding with a 200 OK instead of a 404 Not Found:</p>

<pre>zen:~ ed$ curl -I http://www.cdc.gov/nchs/data/databrief/db18.pdf
HTTP/1.1 200 OK
Content-Type: text/html
X-Powered-By: ASP.NET
X-UA-Compatible: IE=edge,chrome=1
Date: Tue, 30 Jun 2015 16:22:18 GMT
Connection: keep-alive
</pre>

<p>At any rate, it's not Internet Archive's fault that they haven't archived the Webpage originally published in 2009, because the URL is actually a typo. Instead it should be</p>

<blockquote>
  <p>http://www.cdc.gov/nchs/data/databriefs/db18.pdf</p>
</blockquote>

<p>which leads to:</p>

<p><a href="http://www.cdc.gov/nchs/data/databriefs/db18.pdf"><img src="http://inkdroid.org/images/scotus-4.png" alt="" /></a></p>

<p>So between the broken URL and the 200 OK for something not found we've got issues of link rot and reference rot all rolled up into a one character typo. Sigh.</p>

<p>I think a couple lessons for web publishers can be distilled from this little story:</p>

<ul>
<li>when publishing on the Web include link checking as part of your editorial process</li>
<li>if you are going to publish links on the Web use a format that's easy to check ... like HTML.</li>
</ul>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8641</wp:post_id>
		<wp:post_date>2015-06-30 09:52:46</wp:post_date>
		<wp:post_date_gmt>2015-06-30 16:52:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>links-in-obergefell-v-hodges</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="archives"><![CDATA[archives]]></category>
		<category domain="category" nicename="web"><![CDATA[web]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
	</item>
</channel>
</rss>
